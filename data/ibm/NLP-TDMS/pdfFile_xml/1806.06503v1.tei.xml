<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Sahasrabudhe</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVN</orgName>
								<address>
									<addrLine>CentraleSuplec 3 INRIA 4 Therapanacea</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Guler</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVN</orgName>
								<address>
									<addrLine>CentraleSuplec 3 INRIA 4 Therapanacea</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University College London</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we introduce Deforming Autoencoders, a generative model for images that disentangles shape from appearance in an unsupervised manner. As in the deformable template paradigm, shape is represented as a deformation between a canonical coordinate system ('template') and an observed image, while appearance is modeled in 'canonical', template, coordinates, thus discarding variability due to deformations. We introduce novel techniques that allow this approach to be deployed in the setting of autoencoders and show that this method can be used for unsupervised group-wise image alignment. We show experiments with expression morphing in humans, hands, and digits, face manipulation, such as shape and appearance interpolation, as well as unsupervised landmark localization. A more powerful form of unsupervised disentangling becomes possible in template coordinates, allowing us to successfully decompose face images into shading and albedo, and further manipulate face images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_8">Fig. 1</ref><p>. Deforming Autoencoders follow the deformable template paradigm and model image generation through a cascade of appearance (or, 'texture') synthesis in a canonical coordinate system and a spatial deformation that warps the texture to the observed image coordinates. By keeping the latent vector for texture short the network is forced to model shape variability through the deformation branch, so as to minimize a reconstruction loss. This allows us to train a deep generative image model that disentangles shape and appearance in an entirely unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Disentangling factors of variation is important for the broader goal of controlling and understanding deep networks, but also for applications such as image manipulation arXiv:1806.06503v1 [cs.CV] 18 Jun 2018 through interpretable operations. Progress in the direction of disentangling the latent space of deep generative models has facilitated the separation of latent image representations into dimensions that account for independent factors of variation, such as identity, illumination, normals, and spatial support <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, low-dimensional transformations, such as rotations, translation, or scaling, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> or finer-levels of variation, including age, gender, wearing glasses, or other attributes e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> for particular classes, such as faces.</p><p>Shape variation is more challenging as it amounts to a transformation of a function's domain, rather than its values. Even simple, supervised additive models of shape result in complex nonlinear optimization problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Despite this challenge several works in the previous decade aimed at learning shape/appearance factorizations in an unsupervised manner, exploring groupwise image alignment, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. In the context of deep learning several works have aimed at incorporating deformations and alignment in a supervised setting, including Spatial Transformers <ref type="bibr" target="#b14">[15]</ref>, Deep Epitomic Networks <ref type="bibr" target="#b15">[16]</ref>, Deformable CNNs <ref type="bibr" target="#b16">[17]</ref>, Mass Displacement Networks <ref type="bibr" target="#b17">[18]</ref>, Mnemonic Descent <ref type="bibr" target="#b18">[19]</ref>, or Densereg <ref type="bibr" target="#b19">[20]</ref>. These works have shown that one can improve the accuracy of both classification and localization tasks by injecting deformations and alignment within traditional CNN architectures.</p><p>Turning to unsupervised deep learning, even though most works focus on rigid, or low-dimensional parametric deformations, e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, several works have attempted to incorporate richer non-rigid deformations within learning. A thread of works has been aimed at dynamically rerouting the processing of information within the network's graph based on the input, starting from neural computation arguments <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and eventually translating into concrete algorithms, such as the 'capsule' works of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> that bind neurons on-the-fly. Still, these works lack a transparent, parametric handling of non-rigid deformations. Working on a more geometric direction, several works have recently aimed at recovering dense correspondences between pairs <ref type="bibr" target="#b25">[26]</ref> or sets of RGB images, as e.g. in the recent works of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. These works however do not have the notion of a reference coordinate system ('template') to which images can get mappedthis makes the image generation and manipulation harder. More recently, <ref type="bibr" target="#b28">[29]</ref> use the equivariance principle in order to align sets of images to a common coordinate system, but do not develop this into a full-blown generative model of images.</p><p>Our work pushes the envelope of this line of research by following the deformable template paradigm <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, we consider that object instances are obtained by deforming a prototypical object, or 'template', through dense, diffeomorphic deformation fields. This makes it possible to factor object variability within a category into variations that are associated to spatial transformations, generally linked to the object's 2D/3D shape, and variations that are associated to appearance (or, 'texture' in graphics), e.g. due to facial hair, skin color, or illumination. In particular we consider that both sources of variation can be modelled in terms of a low-dimensional latent code that is learnable in an unsupervised manner from images. We achieve disentangling by breaking this latent code into separate parts that are fed into separate decoder networks that deliver appearance and deformation estimates. Even though one could hope that a generic convolutional architecture will learn to represent such effects, we argue that explicitly injecting this inductive bias in a network can help with the training, while also yielding control over the generative process.</p><p>Our main contributions in this work can be summarized as follows: First, we introduce the Deforming Autoencoder architecture, bringing together the deformable modeling paradigm with unsupervised deep learning. We treat the templateto-image correspondence task as that of predicting a smooth and invertible transformation. As shown in <ref type="figure" target="#fig_8">Fig. 1</ref>, our network predicts this transformation field alongside with the template-aligned appearance and subsequently deforms the synthesized appearance to generate an image similar to its input. This allows for a disentanglement of the shape and appearance parts of image generation by explicitly modelling the effects of image deformation during the decoding stage.</p><p>Second, we explore different ways in which deformations can be represented and predicted by the decoder. Instead of building a generic deformation model, we compose a global, affine deformation field, with a non-rigid field that is synthesized as a convolutional decoder network. We develop a method that allows us to constrain the synthesized field to be a diffeomorphism, namely an invertible and smooth transformation, and show that it simplifies training and improves accuracy. We also show that class-related information can be exploited, when available, to learn better deformation models: this yields sharper images and can be used to learn models that jointly account for multiple classes -e.g. all MNIST digits.</p><p>Third, we show that disentangling appearance from deformation comes with several advantages when it comes to modeling and manipulating images. By using disentangling we obtain clearly better synthesis results when manipulating images for tasks such as expression, pose or identity interpolation when compared to standard autoencoder architectures. Along the same lines, we show that accounting for deformations facilitates a further disentangling of the appearance components into an intrinsic, shading-albedo decomposition which completely fails when naively performed in the original image coordinates. This allows us to perform re-shading through simple operations on the latent shading coordinate space.</p><p>We complement these qualitative results with a quantitative analysis of the learned model in terms of landmark localization accuracy. We show that our method is not too far below supervised methods and outperforms with a margin the latest state-of-theart works on self-supervised correspondence estimation <ref type="bibr" target="#b28">[29]</ref>, even though we never explicitly trained our network for correspondence estimation, but rather only aimed at reconstructing pixel intensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deforming Autoencoders</head><p>Our architecture embodies the deformable template paradigm in an autoencoder architecture. The premise of our work is that image generation can be interpreted as the combination of two processes: a synthesis of appearance on a deformation-free coordinate system ('template'), followed by a subsequent deformation that introduces shape variability. Denoting by T (p) the value of the synthesized appearance (or, texture) at coordinate p = (x, y) and by W (p) the estimated deformation field, we consider that the observed image, I(p), can be reconstructed as follows:</p><formula xml:id="formula_0">I(p) T (W (p)),<label>(1)</label></formula><p>namely the image appearance at position p is obtained by looking up the synthesized appearance at position W (p). This is implemented in terms of a spatial transformer layer <ref type="bibr" target="#b14">[15]</ref> that allows us to pass gradients through the warping process. The appearance and deformation functions are synthesized by independent decoder networks. The inputs to the decoders are delivered by a joint encoder network that takes as input the observed image and delivers a low-dimensional latent representation, Z, of shape and appearance. This is split into two parts, Z = [Z T , Z S ] which feed into the appearance and shape networks respectively, providing us with a clear separation of shape and appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deformation field modeling</head><p>Rather than leave deformation modeling entirely to back-propagation, we use some domain knowledge to simplify and accelerate learning. The first observation is that global aspects can be expressed using low-dimensional linear models. We account for global deformations by an affine Spatial Transformer layer, that uses a six-dimensional input to synthesize a deformation field as an expansion on a fixed basis <ref type="bibr" target="#b14">[15]</ref>. This means that the shape representation, Z S described above is decomposed into two parts, Z W , Z A , where Z A accounts for the affine, and Z W for the non-rigid, learned part of the deformation field. These deformation fields are generated by separate decoders, and are composed, so that the affine transformation warps the detailed non-rigid warps to the image positions where they should apply. This is also a common decomposition in deformable models for faces <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Turning to local deformation effects, we quickly realized that not every deformation field is plausible. Without appropriate regularization we would often obtain deformation fields that could expand small areas to occupy whole regions, and/or would be nondiffeomorphic, meaning that the deformation could spread a connected texture pattern to a disconnected image area ( <ref type="figure" target="#fig_0">Figure 2</ref>-(f)).</p><p>To prevent this problem, instead of making the shape decoder CNN directly predict the local warping field W (p) = (W x (x, y), W y (x, y)), we consider a 'differential decoder' that generates the spatial gradient of the warping field: ∇ x W x and ∇ y W y , where ∇ c denotes the c − th component of the spatial gradient vector. These two quantities measure the displacement of consecutive pixels -for instance ∇ x W x = 1 amounts to translation in the horizontal axis, ∇ x W x = 2 amounts to horizontal shifting by a size of 2, while ∇ x W x = −1 amounts to left-right flipping; a similar behavior is associated with ∇ y W y in the vertical axis. We note that global rotations are handled by the affine warping field, and the ∇ x W y , ∇ y W x are associated with small local rotations of minor importance -we therefore focus on ∇ x W x , ∇ y W y .</p><p>Having access to these two values gives us a handle on the deformation field, since we can prevent folding/excessive stretching by controlling ∇ x W x , ∇ y W y .</p><p>In particular, we pass the outputs of our differential decoder through a Rectified Linear Unit (ReLU) module, which enforces positive horizontal offsets on horizontally adjacent pixels, and positive vertical offsets on vertically adjacent pixels. We Our warping module design only permits locally consistent warping, as shown in (b), while the flipping of relative pixel positions, as shown in (c), is not allowed by design. To achieve this, we let the deformation decoder predict the horizontal and vertical increments of the deformation (∇xW and ∇yW , respectively) and use a ReLU transfer function to remove local flips, caused by going back in the vertical or horizontal direction. A spatial integral module is subsequently applied to generate the grid. This simple mechanism serves as an effective constraint for the deformation generation process, while allowing us to model free-form/non-rigid local deformation.</p><p>subsequently apply a spatial integration layer, implemented in terms of a fixed network layer, on top of the output of the ReLU layer to reconstruct the warping field from its spatial gradient. By doing so, the new deformation module enforces the generation of smooth and regular warping fields that avoid self-crossings. In practice we found that also clipping the decoded offsets by a maximal value significantly eases the training, which amounts to replacing the ReLU layer, ReLU(x) = max(x, 0) with a HardTanh 0,δ (x) = min(max(x, 0, δ) layer. In our experiments, we set δ = 5/w where w denotes the number of pixels along one dimension of the image. We can require our network's latent representation to be predictive of not only shape and appearance, but also of instance class, if that is available during training. We note that this information, being discrete may be easier to acquire than the actual deformation field, which would require manual landmark annotation. For instance, for faces such discrete information could represent the expression or a person's identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Class-aware Deforming Autoencoder</head><p>In particular we consider that the latent representation can be decomposed as follows: Z = [Z T , Z C , Z S ], where Z T , Z S are as previously the appearance-and shaperelated parts of the representation, respectively, while Z C is fed as input to a subnetwork trained to predict the class associated with the input image. Apart from assisting the classification task, the latent vector Z C is fed into both the appearance and shape decoders. Intuitively this allows our decoder network to learn a mixture model that is conditioned on class information, rather than treating the joint, multi-modal distribution through a monolithic model. Even though the class label is only used during training, and not for reconstruction, our experimental results show that a network trained with class supervision can deliver more accurate synthesis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Intrinsic Deforming Autoencoder: Deformation, Albedo and Shading Decomposition</head><p>Having outlined Deforming Autoencoders, we now use a Deforming Autoencoder to model complex physical image signals, such as illumination effects, without a supervision signal. For this we design the Intrinsic Deforming-Autoencoder, named Intrinsic-DAE to model shading and albedo for in-the-wild face images. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>-(a), we introduce two separate decoders for shading S and albedo A, each of with has the same structure as the original texture decoder. The texture is computed by T = S • A where • denotes the Hadamard product. In order to model the physical properties of shading and albedo, we follow the intrinsic decomposition regularization loss used in <ref type="bibr" target="#b1">[2]</ref>: we apply the L2 smoothness loss on ∇S, meaning that shading is expected to be smooth, while leaving albedo unconstrained. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref> and more extensively in the experimental results section, when used in tandem with an Deforming Autoencoder this allows us to successfully decompose of face image into shape, albedo, and shading components, while a standard Autoencoder completely fails at decomposing unaligned images into shading and albedo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Our objective function is formed as the sum of three losses, combining the reconstruction error with the regularization terms required for the modules described above. Concretely, the loss of the deforming autoencoder can be written as</p><formula xml:id="formula_1">E DAE = E Reconstruction + E Warp ,<label>(2)</label></formula><p>where the reconstruction loss is defined as the standard 2 loss</p><formula xml:id="formula_2">E Reconstruction = I Output − I Input 2 ,<label>(3)</label></formula><p>and the warping loss is decomposed as follows: In particular the smoothness cost, E smooth , penalizes quickly-changing deformations encoded by the local warping field. It is measured in terms of the total variation norm of the horizontal and vertical differential warping fields, and is given by</p><formula xml:id="formula_3">E Warp = E Smooth + E BiasReduce .<label>(4)</label></formula><formula xml:id="formula_4">E Smooth = λ 1 ( ∇W x (x, y) 1 + ∇W y (x, y) 1 ) ,<label>(5)</label></formula><p>where λ 1 = 1e−6. Finally, E BiasReduce aims at removing any systematic bias introduced by the fitting process, e.g. the average template becoming small, or a distorted version of the data. It consists of regularization on (1) the affine parameters defined as the L2distance between S A and S 0 , with S 0 being the identity affine transform and (2) on free-form deformations defined as the L2-distance between the average deformation grid within a minibatch,W and the identity grid W 0 :</p><formula xml:id="formula_5">E BiasReduce = λ 2 S A − S 0 2 + λ 2 W − W 0 2 ,<label>(6)</label></formula><p>where λ 2 = λ 2 = 0.01.</p><p>In the class-aware variant described in Sec. 2.2 we augment the loss above with the cross-entropy loss evaluated on the classification network's outputs, while for Intrinsic-DAE, we add the following objective function in training:</p><formula xml:id="formula_6">E Shade = λ 3 ∇S 2 where λ 3 =1e-6.</formula><p>We experiment with two types of architectures; the majority of our results are obtained with a standard auto-encoder architecture, where both encoder and decoders are CNNs with standard convolution-BatchNorm-ReLU blocks. The number of filters and the texture bottleneck capacity can vary per experiment, image resolution, and dataset, as detailed in the Appendix A.</p><p>Follow the recent work on densely connected convolutional networks <ref type="bibr" target="#b32">[33]</ref>, we have also experimented with incorporating dense connections into our encoder and decoders architectures respectively (no skip connections over the bottleneck layer for latent representations). In particular, we follow the architecture of DenseNet-121, but without the 1 × 1 convolutional layers inside each dense block. These have been shown to better exploit larger datasets, as indicated in the quantitative analysis of unsupervised face alignment. We call this version of the deforming autoencoder Dense-DAE. <ref type="figure">Fig. 5</ref>. Unsupervised deformation-appearance disentangling on a single MNIST digit. Our network learns to reconstruct the input image while automatically deriving a canonical appearance for the input image class. In this experiment, the dimension of the latent representation for appearance ZT is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To demonstrate the properties of our deformation disentangling network, we conduct experiments on the following three datasets:</p><p>-Deformed MNIST. A synthetic dataset designed specifically to explore the deformation modelling power of our network. Deformed MNIST consists of handwritten MNIST images randomly distorted using a mixture of sinusoidal waveforms. -MUG facial expression dataset <ref type="bibr" target="#b33">[34]</ref>. This dataset consists of videos of individuals performing facial expressions, with simple blue background and minor translation. The dataset also offers frames from the videos, classified according to the facial expression, as well as the subject. -Faces-in-the-wild dataset: MAFL <ref type="bibr" target="#b34">[35]</ref> and CelebA <ref type="bibr" target="#b35">[36]</ref>. These datasets consist of uncontrolled "in-the-wild" faces with variability in pose, illumination, expression, age, etc.</p><p>Using these datasets we experimentally explored the ability of the unsupervised appearance-shape (or texture-deformation) disentangling network on 1) unsupervised image alignment/appearance inference; 2) learning semantically meaningful manifolds for shape and appearance; 3) decomposition into illumination intrinsics (shading, albedo); 4) unsupervised landmark detection, as detailed below. We intend to make all of the code of our system publicly available in order to facilitate the reproduction of our results. <ref type="figure">Fig. 6</ref>. Class-aware Deforming Autoencoders effectively model the appearance and deformation for multi-class data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Appearance Inference</head><p>We first use our network to model canonical appearance and deformation for single category objects. For this purpose, we demonstrate the results in the MNIST and MUG facial expression datasets ( <ref type="figure" target="#fig_3">Fig. 5, 6, 7)</ref>.</p><p>We observe that by heavily limiting the size of Z T (1 in <ref type="figure">Fig. 5</ref> and 0 in <ref type="figure" target="#fig_3">Fig. 7</ref>), we can successfully infer a canonical appearance for such a class. In <ref type="figure">Fig. 5</ref>, all different types of handwritten digits '3' are aligned to a simple canonical shape. In <ref type="figure" target="#fig_3">Fig. 7</ref>, by limiting the dimension of Z T to 0, the network learns to encode a single texture image for all expressions, and successfully distills expression-related information exclusively in the shape space. In <ref type="figure" target="#fig_3">Fig. 7-(b)</ref> we show that by interpolating the learned latent representations, we can generate meaningful shape interpolations that mimic facial expressions.</p><p>In cases where data has a multi-modal distribution exhibiting multiple different canonical appearances, e.g., multi-class MNIST digit images, learning a single appearance is less meaningful and often challenging ( <ref type="figure">Fig. 6-(b)</ref>). In such cases, utilizing class information (Sec. 2.2) significantly improves the quality of multi-modal appearance learning ( <ref type="figure">Fig. 6-(d)</ref>). As the network learns to classify the images implicitly in its latent space, it learns to generate a single canonical appearance for each class. Misclassified data will be decoded into an incorrect class: the image at position <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref> in <ref type="figure">Fig. 6-(c,d)</ref> is interpreted as a 6.</p><p>We now demonstrate the effectiveness of texture inference using our network on inthe-wild human faces. Using the MAFL face dataset, we show that our network is able to align the faces to a common texture space under various poses, illumination conditions, or facial expressions ( <ref type="figure" target="#fig_8">Fig. 10)-(d)</ref>. The aligned textures retain the information of the input image such as lighting, gender, and facial hair, without a relevant supervision training signal. We further demonstrate the alignment on the 11k Hands dataset <ref type="bibr" target="#b36">[37]</ref>, where we align palmar images of the left hand of several subjects 8. This property of our network is especially useful for applications such as computer graphics, where establishing correspondences (UV map) between a class of objects is important but usually difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Autoencoders vs. Deforming Autoencoders</head><p>We show the ability of our network to learn meaningful deformation representations without supervision. We compare our disentangling network with a plain auto-encoder  ( <ref type="figure">Fig. 9</ref>). Contrary to our network which disentangles an image into a template texture and a deformation field, the auto-encoder is trained to encode all of the image in a single latent representation, i.e., the bottleneck.</p><p>We train both networks in the MAFL faces-in-the-wild dataset. To evaluate the learned representation, we conduct manifold traversal (i.e., latent representation interpolation) between two randomly sampled face images: given a source face image I s and a target image I t , we first compute their latent representations Zs. We use Z T (I s ) and Z S (I s ) to denote the latent representations in our network for I s , and Z ae (I s ) for the latent representation learned by a plain autoencoder. We then conduct linear interpolation on Z, between Z s and Z t : Z λ = λZ s + (1 − λ)Z t . We subsequently reconstruct the image I λ from Z λ using the corresponding decoder(s), as shown in <ref type="figure">Fig. 9</ref>.</p><p>By traversing the learned deformation representation only, we can change the shape and pose of a face while maintaining its texture ( <ref type="figure" target="#fig_8">Fig. 9-(1)</ref>); interpolating the texture representation results in pose-aligned texture transfer ( <ref type="figure" target="#fig_0">Fig. 9-(2)</ref>); traversing on both representations will generate a smooth deformation from one image to another ( <ref type="figure" target="#fig_1">Fig. 9-(3,5,7)</ref>). Compared to the interpolation using the autoencoder ( <ref type="figure" target="#fig_2">Fig. 9-(4,6,8)</ref>), which <ref type="figure">Fig. 9</ref>. Latent representation interpolation: we embed a face image in the latent space provided by an encoder network trained on the MAFL dataset. Our network disentangles the texture and deformation in the respective parts of the latent representation vector, allowing a meaningful interpolation between images. Interpolating the deformation-specific part of the latent representation changes the face shape and pose (1); interpolating the latent representation for texture will generate a pose-aligned texture transfer between the images (2); traversing both latent representations will generate smooth and sharp image deformations <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref>. In contrast, when using a standard auto-encoder (4,6,8) such an interpolation often yields artifacts. For more results, please see <ref type="figure" target="#fig_4">Figure 18</ref>,19 in Appendix. often exhibits artifacts, our traversal stays on the semantic manifold of faces and generates sharp facial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intrinsic Deforming Autoencoders</head><p>Having demonstrated the disentanglement abilities of Deforming Autoencoders, we now explore the disentanglement capabilities of Intrinsic-DAE described in Sec. 2.3. Using only the E DA and regularization losses, the Intrinsic-DAE is able to generate convincing shading and albedo estimates without direct supervision ( <ref type="figure" target="#fig_8">Fig. 10-(b)</ref> to (g)).</p><p>Without the "learning-to-align" property, a baseline autoencoder structure with an intrinsic decomposition design ( <ref type="figure" target="#fig_2">Fig. 4-(b)</ref>) cannot decompose the image into plausible shading and albedo components ( <ref type="figure" target="#fig_8">Fig. 10-(h)</ref>,(i),(j)).</p><p>In addition, we show that by manipulating the learned latent representation of S, Intrinsic-DAE allows us to simulate illumination effects for face images, such as interpolating lighting directions <ref type="figure" target="#fig_8">(Fig. 11</ref>). <ref type="figure" target="#fig_8">Fig. 10</ref>. Unsupervised intrinsic decompostion with Deforming Autoencoders (Intrinsic-DAE). Thanks to the "automatic dense aligment" property of DAE, shading and albedo are faithfully separated (e,f) by the intrinsic decomposition loss. Shading (b) and albedo (c) are learned in an unsupervised manner in the densely aligned canonical space. With the deformation field also learned without supervision, we can recover the intrinsic image components for the original shape and viewpoint (e,f). Without dense alignment, the intrinsic decomposition loss fails to decompose shading and albedo (h,i,j).</p><p>Training with L2 reconstruction losses, autoencoder-like architectures are prone to generating smooth images which lack visual realism <ref type="figure" target="#fig_8">(Fig. 10</ref>). Inspired by the success of generative adversarial networks (GANs) <ref type="bibr" target="#b37">[38]</ref>, we follow previous work <ref type="bibr" target="#b1">[2]</ref> where an adversarial loss is adopted to generate visually realistic images: we train the Intrinsic-DAE with an extra adversarial loss term E Adversarial applied on the final output. The loss function becomes:</p><formula xml:id="formula_7">E Intinsic-DAE = E Reconstruction + E Warp + λ 4 E Adversarial .<label>(7)</label></formula><p>In practice, we apply a PatchGAN <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> as the discriminator and set λ 4 = 0.1. We found that the adversarial loss improves the visual sharpness of the reconstruction while the deformation, shading are still successfully disentangled <ref type="figure" target="#fig_0">(Fig. 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unsupervised alignment evaluation</head><p>Having qualitatively analyzed the disentanglement capabilities of our networks, we now turn to quantifying their performance on the task of unsupervised image alignment. We <ref type="figure" target="#fig_8">Fig. 11</ref>. Lighting interpolation with Intrinsic-DAE. With latent representations learned in an unsupervised manner for shading, albedo, and defomation, the DAE allows us to simulate smooth transitions of the lighting direction. In this example, we interpolate the latent representation of the shading from source (lit from the left) to target (mirrored source, hence lit from the right). The network generates smooth lighting transitions, without explicitly learning geometry, as shown in shading (1) and texture <ref type="bibr" target="#b1">(2)</ref>. Together with the learned deformation of the source image, DAE enables the relighting of the face in its original pose (3).</p><p>report the performance of our face DAE's alignment on landmark detection on face images, specifically, the eyes, the nose, and corners of the mouth. We report performance on the MAFL dataset, which contains manually annotated landmark locations for 19,000 training and 1,000 test images. In our experiments, we use a model trained on the CelebA dataset without any form of supervision to estimate deformation fields on the MAFL training set. Following the evaluation protocol of the work that we directly compare to <ref type="bibr" target="#b28">[29]</ref>, we train a landmark regressor post-hoc on these deformation fields using the provided annotations. We use landmark locations from the MAFL training set as training data for this regressor, but do not pass gradients to the Deforming Autoencoder, which thereby remains fixed to the model learned without supervision. The regressor is a 2-layer fully-connected neural network. Its inputs are flattened deformation fields (vectors of size 64×64×2), which are provided as input to a 100-dimensional hidden layer, followed by a ReLU and a 10-D output layer to predict the spatial coordinates ((x, y)) for five landmarks corresponding to the eyes, nose, and mouth corner landmarks. We use L1 loss as the objective function for this regression task.</p><p>In testing, we predict landmark locations using the trained regressor and the deformation fields on the MAFL test set. In <ref type="table">Table 1</ref> we report the mean error in landmark localization as a percentage of the inter-ocular distance. As the deformation field determines the alignment in the texture space, it serves as an effective mapping between landmark locations on the aligned texture and those on the original, unaligned faces. Hence, the mean error we report directly quantifies the quality of the (unsupervised) face alignment.</p><p>In <ref type="table">Table 2</ref> we compare with the results of the best current method for semi-supervised image registration <ref type="bibr" target="#b28">[29]</ref>. We observe that by better modeling of the deformation space we quickly bridge the gap in performance, even though we never explicitly trained to learn correspondences.  <ref type="table">Table 1</ref>. Improvement in landmark localization errors on the MAFL test set as we add new types of deformation and new data. In the table, A indicates a model which uses the affine transformation, I indicates one with the integral transformation, whereas MAFL and CelebA denote which dataset the deforming autoencoder was trained on. For columns 1 to 4, we manually annotate landmarks on the average texture image, while for column 5, we train a regressor on the deformation fields to predict them. In all experiments, each latent vector in the DAE is of size 32.   <ref type="table">Table 2</ref>. Mean error on unsupervised landmark detection on the MAFL test set, expressed as a percentage of the inter-ocular distance: modeling non-rigid deformations clearly reduces error more than just modeling affine ones. DAE and Dense-DAE denote two flavours of the deforming autoencoder -with and without dense convolutional connections, respectively. Under DAE and Dense-DAE we specify the size of each latent vector in the deforming autoencoder. NR signifies training without regularization on the estimated deformations, while Res signifies training by estimating the residual deformation grid instead of the integral. Our results clearly outperform the self-supervised method of <ref type="bibr" target="#b28">[29]</ref> trained specifically for establishing correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper we have developed deep autoencoders that can disentangle shape and appearance in latent representation space. We have shown that this method can be used for unsupervised groupwise image alignment. Our experiments with expression morphing in humans, image manipulation, such as shape and appearance interpolation, as well as unsupervised landmark localization, show the generality of our approach. We have shown that bringing images in a canonical coordinate system allows for a more extensive form of image disentangling, facilitating the estimation of decompositions into shape, albedo and shading without any form of supervision. We expect that this will lead in the future to a full-fledged disentanglement into normals, illumination, and 3D geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>This work was supported by a gift from Adobe, NSF grants CNS-1718014 and DMS 1737876, the Partner University Fund, and the SUNY2020 Infrastructure Transportation Security Center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Architectural Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Convolutional Encoders and Decoders</head><p>In our experiments, where input images are of size 64 × 64 × Nc (Nc is 1 for MNIST and 3 for faces), we use identical architectures for convolutional encoders and decoders.</p><p>The encoder architecture is We describe the tensor sizes for intermediate convolution operations in <ref type="table">Tables 3 and  4</ref>.   if ZT is set to be 0D dimension, the texture becomes a "bag of colored pixels" which, when deformed (at will) can reconstruct an image. Increasing the dimension of ZT (4-32D) lets the network generates aligned texture maps and more exact appearance; further increasing ZT (128-D) reduces the alignment effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Methods for deformation modeling</head><p>In this section, we demonstrate the effect of using different warping modules. We first show additional comparisons between using our proposed affine + integral warping and a non-rigid warping field directly output from a convolutional decoder for non-rigid deformation modeling ( <ref type="figure" target="#fig_8">Figure 16</ref>).</p><p>We visualize the utility of affine and integral warping modules in our network with face images <ref type="figure" target="#fig_3">(Figure 17</ref>). We can see that the affine transformation handles global pose variance ( <ref type="figure" target="#fig_3">Figure. 17-(b)</ref>) but not local non-rigid deformation. Our proposed integral warping module aligns the faces in a non-rigid manner <ref type="figure" target="#fig_3">(Figure 17-(c)</ref>). Incorporating both deformation modules improves the non-rigid alignment <ref type="figure" target="#fig_3">(Figure 17-(d)</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Latent Manifold Traversal</head><p>We provide additional results and comparisons with a plain autoencoder on traversing the learned manifolds. In addition to <ref type="figure" target="#fig_1">Figure 13</ref> in our manuscript, we provide two more sets of results in <ref type="figure" target="#fig_4">Figure 18</ref> and <ref type="figure" target="#fig_8">Figure 19</ref>. Compared to a plain autoencoder, our deforming autoencoder not only generates better reconstructions, but also learns a better face manifold -interpolating between learned latent representations generates sharper and more realistic face images. For this experiment, we use the convolutional encoder and decoder architecture as described in Sec. A.1. <ref type="figure" target="#fig_4">Fig. 18</ref>. Interpolating learned representations using networks learned on MAFL dataset. Deforming autoencoder learns better latent representations for face compared to a plain autoencoder. By interpolating the latent representations ZT and/or ZW , we observe smooth transition of pose, shape and skin texture. Interpolated results also stays on the face manifold and, generates more realistic image compared to a plain autoencoder. <ref type="figure" target="#fig_8">Fig. 19</ref>. Interpolating learned representations using networks learned on MAFL dataset. Deforming autoencoder learns better latent representations for face compared to a plain autoencoder. By interpolating the latent representations ZT and/or ZW , we observe smooth transition of pose, shape and skin texture. Interpolated results also stays on the face manifold and, generates more realistic image compared to a plain autoencoder. <ref type="figure" target="#fig_0">Fig. 20</ref>. Expression interpolation: Trained on the MUG facial expression dataset, our network is able to disentangle the facial expression deformation and encode this information in a meaningful latent representation. By interpolating the latent deformation representation from the source (in orange) to the target (in blue), our network generates sharp images and smooth deformation interpolation between expressions as shown in each row. In this experiment, ther model for each subject is independently trained, where we set dimension of ZT to 0 (assuming single texture for each subject) and dimension of ZW to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Intrinsic Decomposition with DAE</head><p>In <ref type="figure" target="#fig_0">Fig.21</ref> we provide additional results of unsupervised intrinsic disentangling for facesin-the-wild using Intrinsic-DAE. Using the architecture and objective functions described in Sec. 2.3 of the main paper the network learns to bring faces under different poses and illumination conditions, shown in <ref type="figure" target="#fig_0">Fig. 21-(a)</ref>, to a canonical view, as shown in <ref type="figure" target="#fig_0">Fig. 21-(d)</ref>, while separating the shading, shown in <ref type="figure" target="#fig_0">Fig. 21-(b)</ref> and albedo, shown in <ref type="figure" target="#fig_0">Fig. 21-(c)</ref> components in the canonical view using two independent decoders. With the learned deformation from the deformation decoder, we can warp the aligned shading and aligned albedo to its original view as in the input image, as shown in <ref type="figure" target="#fig_0">Fig. 21-(e,f)</ref>.</p><p>In <ref type="figure" target="#fig_0">Fig. 22</ref>, we provide additional results for "changing lighting direction" of a face image using Intinsic-DAE. We show that even without explicitly modeling of geometry, we can simulate smooth and reasonable lighting direction changes in the image by interpolating the learned latent representation for shading, as shown in <ref type="figure" target="#fig_0">Fig. 22</ref>-a-(4),b-(4).</p><p>For Intrinsic-DAE, we use the DenseNet architecture as the encoders and decoders (A.2). The network is trained with a subset of 200, 000 images in the CelebA dataset. The dimensions of latent representations are: 16 for albedo, 16 for shading, and 128 for deformation field.  Lighting manipulation by interpolating latent representation of shading: Intrinsic-DAE allows us to disentangle a latent representation for shading for a given face image in an unsupervised manner. Therefore, manipulating the shading component will result in lighting effects in the output images. In this experiment, we interpolate the latent representation of shading from source to target, which is the mirror of the source with reversed lighting direction. In the result, we can observe that, even without explicitly modeling geometry in our network, we can simulate smooth lighting direction change in both the shading (a-(3), b-(3)) and the final reconstruction (a-(4), b-(4)).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our warping module design only permits locally consistent warping, as shown in (b), while the flipping of relative pixel positions, as shown in (c), is not allowed by design. To achieve this, we let the deformation decoder predict the horizontal and vertical increments of the deformation (∇xW and ∇yW , respectively) and use a ReLU transfer function to remove local flips, caused by going back in the vertical or horizontal direction. A spatial integral module is subsequently applied to generate the grid. This simple mechanism serves as an effective constraint for the deformation generation process, while allowing us to model free-form/non-rigid local deformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>A class-aware model can account for multi-modal deformation distributions by utilizing class information. Introducing a classification loss into latent space helps the model learn a better representation of the input as demonstrated on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Autoencoders with intrinsic decomposition. (a) Deforming Autoencoder with intrinsic decomposition (Intrinsic-DAE): we model the texture by the Hadamard product of shading and albedo components, each of which is decoded by an individual decoder. The texture is subsequently warped by the predicted deformation field. (b) A plain autoencoder with intrinsic decomposition. Both networks are trained with reconstruction loss (EReconstruction) on the final output and regularization losses on shading (EShade) and deformation (EWarp), if it exists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Experiment on MUG dataset of face expressions: (a) With 0-length ZT , Deforming Autoencoders learn a single texture (row 3) from a subject in the MUG facial expression dataset. By doing so, the subject's facial expression is encoded only in the deformation domain. (b): Our network is able to disentangle the facial expression deformation and encode this information in a meaningful latent representation. By interpolating the latent deformation representation from the source (in orange) to the target (in blue), it generates sharp images and a smooth deformation interpolation between expressions as shown in each row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Unsupervised alignment on images of palms of left hands. (a) The input images; (b) reconstructed images; (c) texture images warped with the average of the decoded deformation; (d) the average input image; and (e) the average texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 .</head><label>12</label><figDesc>Intrinsic-DAE with an adversarial loss: (a/d) reconstruction (b/e) albedo, (c/f) shading, in image and template coordinates, respectively. Applying an adversarial loss to the final output results improves the visual quality of the image reconstruction (a) of Intrinsic-DAE, while the deformation, albedo, and shading can still be successfully disentangled. A, MAFL I, MAFL A + I, MAFL A + I, CelebA A + I, CelebA, with Regressor 14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 13 .</head><label>13</label><figDesc>1st row: Sample images from the MAFL test set; 2nd row: Estimated deformation grid; and 3rd row: Image reverse-transformed to texture space 4th row: semantic landmark locations (green: ground truth landmark locations, blue: estimated landmark locations, red: error lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Conv( 32 )</head><label>32</label><figDesc>-LeakyReLU-Conv(64)-BN-LeakyReLU-Conv(128)-&gt; -&gt;BN-LeakyReLU-Conv(256)-BN-LeakyReLU-Conv(Nz)-&gt; -&gt;Sigmoid; while the decoder architecture is ConvT(256)-BN-ReLU-ConvT(128)-BN-ReLU-ConvT(64)-&gt; -&gt;BN-ReLU-ConvT(32)-BN-ReLU-ConvT(32)-BN-ReLU-ConvT(Nc)-&gt; -&gt;Threshold(0,1), where -Conv(n): convolution layer with n output feature map; -ConvT(n): transposed convolution (deconvolution) layer with n output feature map; -BN: batch normalization layer -Nz: latent representation dimension -Nc: number of output image channel A.2 DenseNet-style Encoders and Decoders For DenseNet-style architectures, we employ dense convolutional connections. The architecture for the encoder is BN-ReLU-Conv(32)-DBE(32,6)-TBE(32,64,2)-&gt; -&gt;DBE(64,12)-TBE(64,128,2)-DBE(128,24)-TBE(128,256,2)-&gt; -&gt;DBE(256,16)-TBE(256,Nz,4)-Sigmoid; whereas the architecture for the decoder is BN-Tanh-ConvT(256)-DBD(256,16)-TBD(256,128)-&gt; -&gt;DBD(128,24)-TBD(128,64)-DBD(64,12)-TBD(64,32)-&gt; -&gt;DBD(32,6)-TBD(32,32)-BN-Tanh-ConvT(Nc)-Threshold(0,1), where -DBE(n,k): A dense encoder block with k 3 × 3 convolutions with n channels. -TBE(m,n,p): An encoder transition block of 1 × 1 convolutions with m input channels and n output channels. Also includes a max-pooling operation of size p. -DBD(n,k): A dense decoder block with k 3×3 transposed convolution operations with n channels. -TBD(m,n): A decoder transition block of 4 × 4 convolutions, stride of 2 and padding of 1. It has m input channels, and n output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1</head><label>1</label><figDesc>Dimension of Z TIn this section, we show experimental results on single deformed MNIST images of the digit 3 (Figure 14)as well as in-the-wild faces (without masking) from the MAFL dataset (Figure 15) to demonstrate the effect of varying the dimension of Z T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Effect of varying the dimensionality of the latent vector for the texture encoding, ZT : The dimension of ZT is 0 for (b), 1 for (c), 4 for (d), 8 for (e), 16 for (f). ZW is fixed to 128. When ZT is 0-Dimensional, the texture decoder is forced to generate an identical texture for every image (b). When we increase the dimension of ZT to 1, the texture decoder learns to align the pose (c) with varying stroke width. When further increasing the dimension of ZT , the network learns a more diverse texture map for each image (d, e, f)Effect of varying the dimensionality of the latent vector for the texture envoding, ZT , on the MAFL face dataset; ZW is fixed to 128. The problem is ill-posed and affords many solutions;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 .</head><label>16</label><figDesc>Comparison between using our proposed affine + integral warping modules (c) and using a warping field directly predicted from a convolutional decoder (b) for non-rigid deformation modeling. Our non-rigid deformation modeling generates better reconstructions and visually plausible texture maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17 .</head><label>17</label><figDesc>Effect of affine and integral warping modules using in our network, using faces in-thewild. The affine transformation can handle global pose variation, as shown in (b) but not local non-rigid deformation-eyes, noses, or other landmarks are not aligned in the decoded texture images. The proposed integral warping module aligns the faces in a non-rigid manner (c), but in an exaggerated manner, causing smears in the texture image, e.g. around eyebrows. Incorporating both deformation modules improves the non-rigid alignment (d). In this experiment, we set ZA = 32, ZT = 32 and ZW = 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 21 .</head><label>21</label><figDesc>Unsupervised intrinsic decomposition of faces-in-the-wild using Intrinsic-DAE: The network learns to bring faces under different poses and illumination conditions (a) to a canonical view (d), and further separate the shading (b) and albedo (c) component in the canonical view using two independent decoders. With the learned deformation from the deformation decoder we can warp the aligned shading and aligned albedo to its original view as in the input image (e,f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Lighting manipulation by interpolating latent representation of shading: Intrinsic-DAE allows us to disentangle a latent representation for shading for a given face image in an unsupervised manner. Therefore, manipulating the shading component will result in lighting effects in the output images. In this experiment, we interpolate the latent representation of shading from source to target, which is the mirror of the source with reversed lighting direction. In the result, we can observe that, even without explicitly modeling geometry in our network, we can simulate smooth lighting direction change in both the shading (a-(3), b-(3)) and the final reconstruction (a-(4), b-(4)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>32 × 32 4 × 4 Conv(32) 4 × 4 × 256 4 × 4 ConvT(256) 16 × 16 × 64 4 × 4 Conv(64) 4 × 4 × 128 4 × 4 ConvT(128) 8 × 8 × 128 4 × 4 Conv(128) 4 × 4 × 64 4 × 4 ConvT(64) 4 × 4 × 256 4 × 4 Conv(256) 4 × 4 × 32 4 × 4 ConvT(32) Tensor sizes for intermediate convolutional operations in the convolutional encoder and decoder architectures. The output shape denoted h × w × C, where h and w are height and width of the feature maps, respectively, and C is the number of channels. × 128 TBE(64,128,2) 16 × 16 × 64 TBD(128,64) 8 × 8 × 128 DBE(128,24) 16 × 16 × 64 DBD(64,12) 4 × 4 × 256 TBE(128,256,2) 32 × 32 × 32 TBD(64,32) 4 × 4 × 256 DBE(256,16) 32 × 32 × 32 DBD(32,6) Nz TBE(256,Nz,4) 64 × 64 × 32 TBD(32,32) 64 × 64×Nc 3 × 3 ConvT(Nc) Tensor sizes for intermediate convolutional operations in the dense encoder and decoder architectures. The output shape denoted h × w × C, where h and w are height and width of the feature maps, respectively, and C is the number of channels.</figDesc><table><row><cell cols="2">Conv Encoder</cell><cell cols="2">Conv Decoder</cell></row><row><cell>Output Size</cell><cell>Operation</cell><cell>Output size</cell><cell>Operation</cell></row><row><cell>32 × Nz</cell><cell cols="3">4 × 4 Conv(Nz) 4 × 4 × 32 4 × 4 ConvT(32)</cell></row><row><cell></cell><cell></cell><cell cols="2">4 × 4×Nc 4 × 4 ConvT(Nc)</cell></row><row><cell cols="2">Dense Conv Encoder</cell><cell cols="2">Dense Conv Decoder</cell></row><row><cell>Output Size</cell><cell>Operation</cell><cell>Output size</cell><cell>Operation</cell></row><row><cell cols="2">32 × 32 × 32 4 × 4 Conv(32)</cell><cell cols="2">4 × 4 × 256 4 × 4 ConvT(256)</cell></row><row><cell>32 × 32 × 32</cell><cell>DBE(32,6)</cell><cell cols="2">4 × 4 × 256 DBD(256,16)</cell></row><row><cell cols="4">16 × 16 × 64 TBE(32,64,2) 8 × 8 × 128 TBD(256,128)</cell></row><row><cell>16 × 16 × 64</cell><cell>DBE(64,12)</cell><cell cols="2">8 × 8 × 128 DBD(128,24)</cell></row><row><cell>8 × 8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Shu, Sahasrabudhe, Guler, Samaras, Paragios, Kokkinos.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01261</idno>
		<title level="m">Sfsnet: Learning shape, reflectance and illuminance of faces in the wild</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to represent spatial transformations with factored higher-order boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<title level="m">Harmonic networks: Deep translation and rotation equivariance</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="702" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1706.00409</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European conference on computer vision</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data driven image models through continuous joint alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object deformation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformation-invariant clustering using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epitomic analysis of appearance and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th IEEE International Conference on Computer Vision (ICCV 2003</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1506.02025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savalle</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mass displacement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A parallel computation that assigns canonical object-based frames of reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence, IJCAI &apos;81</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence, IJCAI &apos;81<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="683" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiscale dynamic routing circuit for forming size-and position-invariant object representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C V</forename><surname>Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The correlation theory of brain function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internal Report 81-2. Gottingen Max-Planck-Institute for Biophysical Chemistry</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2011 -21st International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1710.09829</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly supervised manifold learning for dense semantic object correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised object learning from dense equivariant image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structural image restoration through deformable templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">414</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable templates for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">11th International Workshop on Image Analysis for Multimedia Interactive Services, WIAMIS 2010</title>
		<meeting><address><addrLine>Desenzano del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV</title>
		<meeting>International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gender recognition and biometric identification using a large dataset of hand images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Afifi</surname></persName>
		</author>
		<idno>abs/1711.04322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="918" to="930" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
