<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Numerical Coordinate Regression with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Numerical Coordinate Regression with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study deep learning approaches to inferring numerical coordinates for points of interest in an input image. Existing convolutional neural network-based solutions to this problem either take a heatmap matching approach or regress to coordinates with a fully connected output layer. Neither of these approaches is ideal, since the former is not entirely differentiable, and the latter lacks inherent spatial generalization. We propose our differentiable spatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no trainable parameters, is fully differentiable, and exhibits good spatial generalization. Unlike heatmap matching, DSNT works well with low heatmap resolutions, so it can be dropped in as an output layer for a wide range of existing fully convolutional architectures. Consequently, DSNT offers a better trade-off between inference speed and prediction accuracy compared to existing techniques. When used to replace the popular heatmap matching approach used in almost all state-of-the-art methods for pose estimation, DSNT gives better prediction accuracy for all model architectures tested.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep convolutional neural networks (CNNs) have proven to be highly effective general models for a multitude of computer vision problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. One such problem is coordinate regression, where the goal is to predict a fixed number of location coordinates corresponding to points of interest in an input image. A well-known instance of this problem is human pose estimation, for which CNNs are state-of-the-art. In this paper we study CNNbased solutions to coordinate regression, using the singleperson pose estimation task as an exemplar. Such solutions may exhibit the desirable properties of spatial generalization and/or end-to-end differentiability.</p><p>Spatial generalization is the ability of a model to generalize knowledge obtained at one location during training to another at inference time. If a spatially generalizable model observes a tennis ball in the top-left of an image during training, it should be able to successfully locate a similar tennis ball at a previously unseen location in a new im-age (e.g. the bottom right). It follows that this property will make a positive contribution to the overall generalization of a coordinate regression model, since the goal is to find items anywhere in the image. In general, the success of CNNs is understood to be a result of the high generalization ability afforded by spatially shared parameters <ref type="bibr" target="#b4">[5]</ref>. To maximize this advantage, care must be taken to avoid trainable layers which can overfit on global structure. Lin et al. <ref type="bibr" target="#b5">[6]</ref> note that "fully connected layers are prone to overfitting, thus hampering the generalization ability of the overall network".</p><p>An end-to-end differentiable model can be composed with other differentiable layers to form a larger model without losing the ability to train using backpropagation <ref type="bibr" target="#b6">[7]</ref>. In the case of coordinate regression, being end-to-end differentiable means being able to propagate gradients all the way from the output numerical coordinates to the input image. It is possible to train a coordinate regression model without this property, such as by matching predicted heatmaps to target heatmaps generated from the ground truth locations. However, this approach cannot be used in architectures where the numerical coordinates are learned implicitly as intermediate values, including the prominent example of Spatial Transformer Networks <ref type="bibr" target="#b7">[8]</ref>.</p><p>There are many CNN-based solutions to other computer vision tasks, such as classification and semantic segmentation, which exhibit both spatial generalization and end-toend differentiability. However, existing solutions for coordinate regression sacrifice one property or the other.</p><p>The most successful existing coordinate regression approach is to apply a loss directly to output heatmaps rather than numerical coordinates <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. Synthetic heatmaps are generated for each training example by rendering a spherical 2D Gaussian centered on the ground truth coordinates. The model is trained to produce output images which resemble the synthetic heatmaps using mean-square-error loss. During inference, numerical coordinates are obtained from the model's output by computing the argmax of pixel values, which is a non-differentiable operation. Although this approach has good spatial generalization, it does have a few disadvantages. Most notably, gradient flow begins at the heatmap rather than the numerical coordinates ( <ref type="figure" target="#fig_0">Figure 1a</ref>). This leads to a disconnect between the loss function being optimized (similarity between heatmaps) and the metric we are actually interested in (the distance between predicted coordinates and ground truth). Only the brightest pixel is used to calculate numerical coordinates at inference time, but all of the pixels contribute to the loss during training. Making predictions based on the argmax also introduces quantization issues, since the coordinates have their precision tied to the heatmap's resolution.</p><p>Another coordinate regression approach is to add a fully connected layer which produces numerical coordinates <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>. An attractive (and sometimes required) property of this approach is that it is possible to backpropagate all the way from the predicted numerical coordinates to the input image. However, the weights of the fully-connected layer are highly dependent on the spatial distribution of the inputs during training. To illustrate this point, consider an extreme situation where the training set consists entirely of coordinates located within the left-hand half of the image. Many of the fully connected layer's input activations will be useless, and as a result weights corresponding to the right-hand side of the image will not be trained properly. So although the convolutional part of the model is spatially invariant, the model as a whole will not generalize well to objects on the right-hand side of the image. This is an inefficient usage of the training data, and causes particularly bad performance on small datasets.</p><p>We propose our differentiable spatial to numerical transform (DSNT) layer as an alternative to existing approaches. The DSNT layer may be used to adapt existing CNN architectures, such as a pretrained ResNet <ref type="bibr" target="#b11">[12]</ref>, to coordinate regression problems. Our technique fully preserves the spatial generalization and end-to-end differentiability of the model, without introducing additional parameters. <ref type="figure" target="#fig_0">Figure 1</ref>  trates how the DSNT layer fits into the model as a whole in comparison to fully connected and heatmap matching approaches. <ref type="table">Table 1</ref> summarizes the features that DSTN poses which selectively appear in fully connected (FC) and heatmap matching (HM) based approaches. We find that DSNT is able to consistently outperform the accuracy of heatmap matching and fully connected approaches across a variety of architectures on the MPII human pose dataset <ref type="bibr" target="#b12">[13]</ref>, and is therefore a suitable replacement in most situations. Our experiments show that stateof-the-art stacked hourglass models <ref type="bibr" target="#b3">[4]</ref> achieve higher accuracy when heatmap matching is replaced with DSNT. For ResNet-34 models, DSNT outperforms heatmap matching by 90.5% with 7 × 7 pixel heatmaps, and by 2.0% with 56 × 56 pixel heatmaps. Since accuracy at low heatmap resolution is much better with DSNT, a wider variety of efficient architectures may be considered for coordinate regression. For instance, a simple ResNet-50 network with DSNT is comparable in accuracy to an 8-stack hourglass network, but exhibits triple the speed and half of the memory usage during inference.</p><p>The DSNT layer presented in this paper is very similar to the soft-argmax operation of Luvizon et al. <ref type="bibr" target="#b13">[14]</ref>, which was developed in parallel with our own work. The softargmax has also been applied to different problem domains prior to this <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. However, we extend the idea further by proposing a regularization strategy which increases prediction accuracy. Additionally, we conduct a comprehensive set of experiments exploring configurations and properties of the operation, and the trade-off between accuracy and inference speed in the context of complete pose estimation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Heatmap matching and fully connected layers are prevalent in existing solutions to problems including human pose estimation and Spatial Transformer Networks. As such, the following section describes how existing coordinate regression approaches are applied in those contexts. Although this paper focuses on pose estimation as an exemplar of the DSNT layer's capability, our approach is broadly applica-ble to any coordinate regression problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human pose estimation</head><p>DeepPose <ref type="bibr" target="#b10">[11]</ref> is one of the earliest CNN-based models to perform well on the human pose estimation task, and helped pioneer the current dominance of deep learning in this area. In order to predict pose joint locations, DeepPose uses a multi-stage cascade of CNNs with fully connected outputs. The first stage of the cascade predicts the absolute coordinates of the joint locations, and subsequent stages refine the predictions by producing relative position deltas. The authors argue that the cascade arrangement enables reasoning about human pose at a higher level, since later stages are able to analyze global structure.</p><p>Shortly after DeepPose was published, Tompson et al. <ref type="bibr" target="#b8">[9]</ref> proposed a higher accuracy model which uses heatmap matching to calculate loss. Heatmap matching has since become overwhelmingly dominant amongst human pose estimation models, including the state-of-the-art stacked hourglass architecture <ref type="bibr" target="#b3">[4]</ref> which is fundamental to current leaders of the MPII single person pose estimation challenge <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Each "hourglass" in a stacked hourglass network uses the first half of its layers to downsample activations, and the second half to upsample back to the original size. By stacking multiple hourglasses together, the network is able to process data in a repeated bottom-up, top-down fashion, achieving an effect similar to DeepPose's cascade. Skip layers are used extensively throughout the architecture, both within and across individual hourglasses, which makes the model easier to train with backpropagation.</p><p>Very recent research suggests that adversarial training <ref type="bibr" target="#b19">[20]</ref> aids in the prediction of likely joint positions by having a discriminator learn the difference between coherent and nonsensical poses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Although we do not conduct such experiments in this paper, we observe that adversarial training is orthogonal to our findings and could be combined with our DSNT layer as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial Transformer Networks</head><p>The internal Localisation Network component of Spatial Transformer Networks <ref type="bibr" target="#b7">[8]</ref> uses a fully connected layer to predict translation transformation parameters, which are effectively just 2D location coordinates. It is not possible to use heatmap matching in such a model, as gradients must be passed backwards through the coordinate calculations. In contrast, our DSNT layer could be used as a drop-in replacement for calculating the translation parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main idea</head><p>We introduce a new differentiable layer for adapting fully convolutional networks (FCNs) to coordinate regression. FCNs are a broad class of CNNs which rely solely on spatially invariant operations to produce their outputs <ref type="bibr" target="#b5">[6]</ref>, and are hence naturally spatially generalizable. Most CNNs with fully connected output layers can be converted into FCNs by simply removing the fully connected layers. FCNs are already spatially generalizable and end-to-end differentiable, so we design our new layer in such a way that these two desirable properties are preserved. This new layer-which we call the DSNT layer-is placed at the output of the FCN and transforms spatial heatmaps into numerical coordinates.</p><p>Activations are represented spatially throughout an FCN, which is very useful for tasks like semantic segmentation <ref type="bibr" target="#b1">[2]</ref> where the output is intended to be spatial. However, for coordinate regression tasks like human pose estimation the output needs to be coordinate pairs. This begs the question: how do we transform spatial activations into numerical coordinates such that we can still effectively train the model?</p><p>Consider the case of locating a person's neck in the input image. This location may be represented spatially as a heatmap <ref type="figure" target="#fig_1">(Figure 2b</ref>), and can be learned by an FCN since it is simply a single-channel image. The purpose of the DSNT layer is to transform such a heatmap into numerical coordinates, which is the form of output we require for coordinate regression. However, we have to be careful about how we approach designing the DSNT, since we want the layer to be part of an end-to-end trainable model. For example, if we simply take the location of the brightest pixel then we cannot calculate meaningful gradients during training. Therefore, we design the DSNT layer such that it is able to propagate smooth gradients back through all heatmap pixels from the numerical coordinates.</p><p>In contrast to heatmap matching techniques, we do not require applying a loss directly to the heatmap output by the FCN to make it resemble <ref type="figure" target="#fig_1">Figure 2b</ref>. Instead, the heatmap is learned indirectly by optimizing a loss applied to the predicted coordinates output by the model as a whole. This means that during training the heatmap will evolve to produce accurate coordinates via the DSNT layer. An example of an implicitly learned heatmap is shown in <ref type="figure" target="#fig_1">Figure 2c</ref>.  </p><formula xml:id="formula_0">Z X Y x = Ẑ , X F =   0.1 × 0.4 + 0.1 × 0.0 + 0.6 × 0.4 + 0.1 × 0.8 + 0.1 × 0.4   = 0.4 y = Ẑ , Y F =   0.1 × −0.4 + 0.1 × 0.0 + 0.6 × 0.0 + 0.1 × 0.0 + 0.1 × 0.4   = 0.0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Differentiable Spatial to Numerical Transform</head><p>In this section we describe the technical details of our differentiable spatial to numerical transform (DSNT) layer. The DSNT layer has no trainable parameters, is fully differentiable, and generalizes spatially. Accordingly, it is possible to use our layer as part of a CNN model to enable numerical coordinate outputs without sacrificing end-to-end learning with backpropagation.</p><p>The input to the DSNT is a single-channel normalized heatmap,Ẑ, represented as an m × n matrix where m and n correspond to the heatmap resolution. By "normalized" we mean that all elements ofẐ are non-negative and sum to one-the same conditions which must be fulfilled by a probability distribution. Using such a normalized heatmap guarantees that predicted coordinates will always lie within the spatial extent of the heatmap itself. The unnormalized heatmap output of an FCN, Z, can be normalized by applying a heatmap activation functionẐ = φ(Z). Suitable choices for φ(Z) are discussed in Section 4.1.</p><p>Let X and Y be m×n matrices, where</p><formula xml:id="formula_1">X i,j = 2j−(n+1) n and Y i,j = 2i−(m+1) m</formula><p>. That is, each entry of X and Y contains its own x-or y-coordinate respectively, scaled such that the top-left corner of the image is at (−1, −1) and bottom-right is at (1, 1).</p><p>By taking a probabilistic interpretation ofẐ we can represent the coordinates, c, as a discrete bivariate random vector with mass function p(c) defined as</p><formula xml:id="formula_2">Pr(c = X i,j Y i,j ) =Ẑ i,j</formula><p>for all i = 1 . . . m, j = 1 . . . n.</p><p>In the heatmap matching approach to coordinate regression, the predicted numerical coordinates are analogous to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Rectification PCKh</p><formula xml:id="formula_3">Softmax Z i,j = exp(Z i,j ) 86.81% Abs Z i,j = |Z i,j | 86.48% ReLU Z i,j = max(0, Z i,j ) 86.69% Sigmoid Z i,j = (1 + exp(−Z i,j )) −1 86.71%</formula><p>the mode of c. For the DSNT layer we instead take our prediction to be the mean of c, denoted µ = E[c]. Unlike the mode, the mean can a) have its derivative calculated, allowing us to backpropagate through the DSNT layer; and b) predict coordinates with sub-pixel precision. Equation 1 details how the expectation is calculated, and hence defines the DSNT operation. We use ·, · F to denote the Frobenius inner product, which is equivalent to taking the scalar dot product of vectorized matrices. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the DSNT operation with an example. Notice how the symmetrical off-center values of the heatmap cancel each other out in the calculations. In practice, this property tends to cause the network to learn heatmaps which are roughly symmetrical about the predicted location.</p><formula xml:id="formula_4">DSNT(Ẑ) = µ = Ẑ , X F Ẑ , Y F<label>(1)</label></formula><p>One seemingly apparent flaw with using the mean instead of the mode is that the predicted coordinates will be affected adversely by outliers in the heatmap. However, it is important to keep in mind that the heatmap itself is learned with the objective of optimizing coordinate accuracy. Therefore, during training the model is encouraged to threshold its activations such that outliers are simply not placed in the heatmap at all. That is, the network is specifically punished for polluting the heatmap with low confidence outliers because they would adversely affect results, and hence the model can simply learn to avoid such situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Heatmap activation</head><p>As mentioned earlier, a heatmap activation function φ(Z) is required to normalize the heatmap before applying the DSNT. Here we will describe several choices for this function by decomposing the activation into two parts. Firstly, each element of the input image Z undergoes rectification to produce a non-negative output. The rectified image Z is then normalized using the L 1 norm so that the elements sum to one (i.e.Ẑ = ( Z i,j ) −1 Z ). <ref type="table" target="#tab_1">Table 2</ref> shows some possible options for the rectification function, along with validation set PCKh accuracy measurements on the MPII human pose dataset. These results were gathered using ResNet-34 models pretrained on ImageNet, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect prediction MSE = 16</head><p>Ground truth Argmax prediction <ref type="figure">Figure 4</ref>: When heatmap matching, it is possible for predictions to worsen despite the pixel-wise MSE improving.</p><p>dilated to produce a heatmap resolution of 28 × 28 pixels.</p><p>No regularization was used. Although the choice of rectification function does not appear to have a large impact on results, our experiments indicate that softmax works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Loss function</head><p>Since the DSNT layer outputs numerical coordinates, it is possible to directly calculate the two-dimensional Euclidean distance between the prediction µ and ground truth p. We take advantage of this fact to formulate the core term of our loss function (Equation 2).</p><formula xml:id="formula_5">L euc (µ, p) = p − µ 2<label>(2)</label></formula><p>The Euclidean loss function has the advantage of directly optimizing the metric we are interested in: the distance between the predicted and actual locations.</p><p>Contrast this with the mean-square-error (MSE) loss used in heatmap matching, which optimizes the pixel-wise similarity between the output and a synthetic heatmap generated from ground truth locations. The pixel-wise MSE loss is a much less direct way of optimizing the metric that we actually care about. During training, the model is completely ignorant of the fact that coordinate predictions are based solely on the brightest heatmap pixel. Another way to put this is that despite the Euclidean loss having a global minimum when the MSE loss is zero, we aren't guaranteed that an optimization step which improves MSE loss will improve our results. <ref type="figure">Figure 4</ref> illustrates an example situation where improving the MSE loss degrades the predictive accuracy of the model. In this case we see that the output with a single pixel at the correct location has worse MSE but better location prediction than an almost perfectly matching heatmap with the brightest pixel placed incorrectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Regularization</head><p>There are many different possible heatmaps that will lead to the same coordinates being output from the DSNT layer. For example, the spread of the heatmap has no effect on the output-blobs resembling 2D Gaussians with large variance and small variance can produce identical coordinates. Although such freedom may be viewed as beneficial, a potential drawback is that the model does not have strongly supervised pixel-wise gradients through the heatmap during training. Experimentally, we find that providing such supervision via regularization can yield marked performance improvements over vanilla DSNT. <ref type="bibr">Equation 3</ref> shows how regularization is incorporated into the DSNT loss function. A regularization coefficient, λ, is used to set the strength of the regularizer, L reg .</p><formula xml:id="formula_6">L(Ẑ, p) = L euc (DSNT(Ẑ), p) + λL reg (Ẑ)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Variance regularization</head><p>By expanding upon the probabilistic interpretation of the DSNT layer (Section 4), we can calculate the variance of coordinates. This is described for x-coordinates in Equation 4 (y-coordinates are handled similarly). The calculated variance represents the "spread" of the blob in the heatmap, which is analogous to the size of the synthetic 2D Gaussian drawn in the heatmap matching approach.</p><formula xml:id="formula_7">Var[c x ] = E[(c x − E[c x ]) 2 ] (4) = Ẑ , (X − µ x ) (X − µ x ) F</formula><p>We are now able to introduce a variance regularization term, Equation 5. The "spread" of the learned heatmaps is controlled by a hyperparameter, the target variance, σ 2 t . Note that this regularization term does not directly constrain the specific shape of learned heatmaps.</p><formula xml:id="formula_8">L var (Ẑ) = (Var[c x ] − σ 2 t ) 2 + (Var[c y ] − σ 2 t ) 2 (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Distribution regularization</head><p>Alternatively, we can impose even stricter regularization on the appearance of the heatmap to directly encourage a certain shape. More specifically, to force the heatmap to resemble a spherical Gaussian, we can minimize the divergence between the generated heatmap and an appropriate target normal distribution. Equation 6 defines the distribution regularization term, where D(·||·) is a divergence measure (e.g. Jensen-Shannon divergence).</p><formula xml:id="formula_9">L D (Ẑ, p) = D(p(c)||N (p, σ 2 t I 2 ))<label>(6)</label></formula><p>Adding a regularization term of this form is similar to incorporating the usual heatmap matching objective into the DSNT loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting the best regularization</head><p>In order to determine the best performing regularization term, we conducted a series of experiments on the MPII human pose dataset with a ResNet-34@28px model.  Firstly, we compared different options for the regularization function, L reg : variance regularization, and distribution regularization with Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences. The pose estimation results in <ref type="table" target="#tab_2">Table 3</ref> indicate that JS distribution regularization achieves the highest accuracy. The sample heatmap images shown in <ref type="figure" target="#fig_5">Figure 5</ref> illustrate how dramatically the choice of regularization term can change the appearance of heatmaps. For example, distribution regularization (using either KL or JS divergence) very effectively encourages the production of distinctly Gaussian-shaped blobs. In contrast, variance reg- ularization with σ t = 2 results in an interesting strategy of splitting the heatmap into four blobs around the joint.</p><p>We conducted further experiments to determine the optimal regularization hyperparameters ( <ref type="figure" target="#fig_6">Figure 6</ref>). The accuracy of the model was found to be quite robust with respect to the regularization strength, λ (Equation 3). In terms of the target Gaussian standard deviation, σ t , values in the range of half a pixel to one pixel were found to work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model base</head><p>We conducted experiments using two different fully convolutional model architectures for the CNN portion of the coordinate regression network (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>ResNet The ResNet architecture <ref type="bibr" target="#b11">[12]</ref> is well-known for performing extremely well in classification tasks. We converted ImageNet-pretrained ResNet models into fully convolutional networks (FCNs) by removing the final fully connected classification layer. Such models produce 7 × 7 px spatial heatmap outputs. However, we were able to adjust the heatmap resolution of the FCN using dilated convolutions, as proposed by Yu and Koltun <ref type="bibr" target="#b20">[21]</ref>. More specifically, we change the convolution stride from 2 to 1 in one or more downsampling stages, then use dilated convolutions in subsequent layers to preserve the receptive field size. For each downsampling stage modified in this way, the heatmap resolution increases by a factor of two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stacked hourglass</head><p>The stacked hourglass architecture <ref type="bibr" target="#b3">[4]</ref> is currently state-of-the-art for human pose estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. The heatmap resolution of this architecture is 64 × 64 px.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Output strategy</head><p>Heatmap matching (HM) We follow the specific technique used by Newell et al. <ref type="bibr" target="#b3">[4]</ref>. MSE pixel-wise loss is applied directly to the output of the FCN. During inference, numeric coordinates are calculated based on the brightest pixel of the heatmap, with small adjustments to the location made based on the brightness of adjacent pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCKh total</head><p>Stacks HM DSNT DSNTr <ref type="figure">Figure 9</ref>: Varying output strategy and stack count for hourglass <ref type="bibr" target="#b3">[4]</ref> models.</p><p>Fully connected (FC) A softmax heatmap activation is applied to the output of the FCN, followed by a fully connected layer which produces numerical coordinates. The model is trained with Euclidean loss.</p><p>DSNT Same as fully connected, but with our DSNT layer instead of the fully connected layer.</p><p>DSNT with regularization (DSNTr) Same as DSNT, but with the inclusion of a regularization term in the loss function. The method of regularization we selected was Jensen-Shannon divergence with σ t = 1 and λ = 1, which empirically performed best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Dataset and training</head><p>We use the MPII human pose dataset <ref type="bibr" target="#b12">[13]</ref> to evaluate the effectiveness of our DSNT layer on an important real-world task. The dataset contains images of 28,883 people with up to 16 joint annotations each, along with approximate person location and scale labels to facilitate the cropping of singleperson poses.</p><p>Samples from the dataset were augmented during training time using the same scheme as Newell et al. <ref type="bibr" target="#b3">[4]</ref>, which consists of horizontal flips, 75%-125% scaling, ±30 degree rotation, and 60%-140% channel-wise pixel value scaling. Since the test set labels are not public, we evaluate on the fixed validation set used in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b3">[4]</ref>.</p><p>The models were optimized with RMSProp <ref type="bibr" target="#b22">[23]</ref> using an initial learning rate of 2.5 × 10 −4 . Each model was trained for 120 epochs, with the learning rate reduced by a factor of 10 at epochs 60 and 90 (an epoch is one complete pass over the training set). Training was completed on single Maxwell-architecture NVIDIA Titan X GPUs.</p><p>Our ResNet-based networks were trained using minibatches of 32 samples each, with the exception of highly memory-intensive configurations (e.g. ResNet-101@28px). The stacked hourglass models were trained using minibatches of 6 samples each. Our implementation code for DSNT, written in PyTorch, is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results</head><p>The PCKh performance metric is the percentage of joints with predicted locations that are no further than half of the head segment length from the ground truth. As per the evaluation code provided by MPII, we exclude the pelvis and thorax joints from the average total PCKh. In order to compare the different approaches to coordinate regression, we conducted a series of experiments with a ResNet-34-based network <ref type="figure" target="#fig_7">(Figure 7)</ref>. The heatmap matching achieved a very low PCKh of 44% at 7 × 7 px heatmap resolution, which falls outside the bounds of the figure. As the resolution increases, the performance of heatmap matching improves relative to the other approaches, which is evidence of the quantization effects inherent to calculating coordinates via a pixel-wise argmax. This demonstrates that heatmap matching is not suitable for models which generate low-resolution heatmaps, whereas DSNT is largely robust to heatmap size. At higher resolutions, the fully connected approach performs worst. Our DSNT approach exhibits good performance across all resolutions-even 7 × 7 px-due to the predictions produced by DSNT not having precision dependent on pixel size.</p><p>Regularization improves DSNT accuracy in all cases except the lowest resolution, where boundary effects come into play (i.e. a 1 pixel standard deviation Gaussian drawn in a 7 × 7 px image is likely to clip heavily, which adversely affects the DSNT calculation). Fully connected output was found to be worse than heatmap matching at higher resolutions, and worse than DSNT in general.</p><p>We conducted further experiments with ResNet-based <ref type="bibr" target="#b11">[12]</ref> models to evaluate the impact that depth has on performance. The results in <ref type="figure">Figure 8</ref> suggest that higher heatmap resolution is beneficial at any depth. However, the trade-off is that increasing resolution with dilations has a large impact on memory consumption and computational cost. For this  reason, we could not train ResNet-101@56px. PCKh was found to increase significantly with depth up until ResNet-50, with only a slight gain observed when increasing the depth even further to ResNet-101. In addition to ResNet, we also trained stacked hourglass networks <ref type="bibr" target="#b3">[4]</ref>. Even though the stacked hourglass architecture was developed using heatmap matching, we found that models trained using DSNT with regularization achieved consistently better results ( <ref type="figure">Figure 9</ref>). Analysis of misclassified examples revealed that DSNT was less accurate for predicting edge case joints that lie very close to the image boundary, which is expected due to how the layer works. <ref type="figure" target="#fig_0">Figure 10</ref> directly compares stacked hourglass networks trained with heatmap matching and our ResNet-based networks trained with DSNT and regularization. Although the 8-stack hourglass network was found to have the highest overall accuracy, the ResNet-based models were found to be much faster with only modest concessions in terms of accuracy. For instance, ResNet-50@28px has 8% fewer parameters, requires less than half of the memory during training, and is over 3× faster at inference than HG8, whilst still achieving~99% of the PCKh score.</p><p>Spatial generalization was tested by training models with a restricted training set size and no data augmentation. ure 11 shows that fully connected output exhibits very poor spatial generalization, achieving the extremely low PCKh score of 22% when trained on 1024 samples. On the other hand, both DSNT and heatmap matching perform much better with fewer samples, indicating better generalization.</p><p>Finally, we evaluated our ResNet-50@28px DSNTr model on the test set. The results in <ref type="table" target="#tab_3">Table 4</ref> show that our solution, using a much smaller and simpler model (ResNet-50), was able to achieve accuracy competitive with more complex models. A consequence of using a smaller model is that ResNet-50@28px infers significantly faster and uses less memory than all other methods shown in the table. Note that we determined the running time and memory usage of the other methods by downloading pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>There are multiple possible approaches to using CNNs for numerical coordinate regression tasks, each of which affects the behavior of the model in different ways-a fully connected output layer reduces spatial generalization, and heatmap matching introduces issues with differentiability and quantization. In contrast, our proposed DSNT layer can be used to adapt fully convolutional networks for coordinate regression without introducing these problems. We have shown that models built with DSNT can achieve com-petitive results on real human pose data without complex task-specific architectures, forming a strong baseline. Such models also offer a better accuracy to inference speed tradeoff when compared with stacked hourglass models.</p><p>Interesting directions for future work are to integrate DSNT with complex pose estimation approaches (e.g. adversarial training <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>), or to use DSNT as an internal layer for models where intermediate coordinate prediction is required (e.g. Spatial Transformer Networks <ref type="bibr" target="#b7">[8]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of coordinate regression model architectures. The arrows indicate inference (black) and gradient flow (dashed red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Spatial representations of an example neck location. Image (b) is a 2D Gaussian rendered at the ground truth location, whereas (c) is learned freely by a model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Coordinate calculation using the differentiable spatial to numerical transform (DSNT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Heatmap appearance for models trained with different regularization terms (red = right wrist, blue = left wrist).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Varying the Gaussian size and regularization strength for JS regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Varying output resolution and strategy for ResNet-34 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Validation accuracy vs inference time, closer to the top-left is better. Labels show heatmap resolution (ResNet models) or stack count (hourglass models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Varying number of training samples (no augmentation) for ResNet-34@28px models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Heatmap activation functions and their corresponding human pose estimation results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Pose estimation results for different regularization terms, using a ResNet-34@28px model.</figDesc><table><row><cell>Regularization</cell><cell>λ</cell><cell>Validation PCKh σ t = 1 σ t = 2</cell></row><row><cell cols="2">None Variance Kullback-Leibler 1 N/A 100</cell><cell>86.86% 84.58% 85.88% 84.67% 84.15%</cell></row><row><cell cols="2">Jensen-Shannon 1</cell><cell>87.59% 86.71%</cell></row><row><cell>Predicted pose (for σ t = 1)</cell><cell cols="2">σ t = 1 Wrist heatmaps σ t = 2</cell></row><row><cell>No reg.</cell><cell></cell><cell></cell></row><row><cell>Variance reg.</cell><cell></cell><cell></cell></row><row><cell>KL reg.</cell><cell></cell><cell></cell></row><row><cell>JS reg.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>MPII human pose test set PCKh accuracies and inference-time efficiency results. Method Head Shoul. Elbow Wrist Hip Knee Ankle Total Time (ms)* Memory* Tompson et al. [22] 96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 --Rafi et al. [24] 97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3 27.6±0.</figDesc><table><row><cell>1</cell><cell>2768 MiB</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/anibali/dsntnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-toend training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>COURSERA: Neural networks for machine learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
