<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A 3D Convolutional Approach to Spectral Object Segmentation in Space and Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Burceanu</surname></persName>
							<email>eburceanu@bitdefender.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
							<email>marius.leordeanu@cs.pub.ro</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A 3D Convolutional Approach to Spectral Object Segmentation in Space and Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate object segmentation in video as a spectral graph clustering problem in space and time, in which nodes are pixels and their relations form local neighbourhoods. We claim that the strongest cluster in this pixel-level graph represents the salient object segmentation. We compute the main cluster using a novel and fast 3D filtering technique that finds the spectral clustering solution, namely the principal eigenvector of the graph's adjacency matrix, without building the matrix explicitly -which would be intractable. Our method is based on the power iteration which we prove is equivalent to performing a specific set of 3D convolutions in the space-time feature volume. This allows to avoid creating the matrix and have a fast parallel implementation on GPU. We show that our method is much faster than classical power iteration applied directly on the adjacency matrix. Different from other works, ours is dedicated to preserving object consistency in space and time at the level of pixels. In experiments, we obtain consistent improvement over the top state of the art methods on DAVIS-2016 dataset. We also achieve top results on the well-known SegTrackv2 dataset.</p><p>1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Elements from a video are interconnected in space and time and have an intrinsic graph structure <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Most existing approaches use higher-level components, such as objects, super-pixels or features, at a significantly lower resolution. Considering this graph structure in space-time, explicitly at the dense pixel-level, is an extremely expensive problem. Our proposed solution to video object segmentation, Spectral Filtering Segmentation (SFSeg), is based on transforming an expensive eigenvalue problem inspired from spectral clustering, into 3D convolutions on the space-time volume. This makes it fast, while keeping the properties of spectral clustering. We are the first, to our best knowledge, to propose a practical spectral clustering approach to video object segmentation at the pixel level, in space and time. * Contact Author Most state of the art algorithms for this task do not use the time constraint, and when they do, they take little advantage of it. Time plays a fundamental factor in how objects move and change in the world, but computer vision does not yet exploit it sufficiently. Consequently, the segmentation outputs of current state of the art algorithms is not always consistent over time. Our work comes to address precisely this aspect and our contribution is demonstrated through solid experiments on DAVIS-2016 and SegTrackv2 datasets on which we improve over state of the art methods.</p><p>We demonstrate in experiments that the eigenvector of the graph's adjacency matrix is a good solution for salient object segmentation. Once our filtering-based optimization converges, the segmentation map is spatio-temporally consistent, with a smooth transition between frames: noise coming from other objects is removed and missing parts of the object are added back. Through multiple iterations, the relevant information is propagated step by step to farther away neighbourhoods in space and time, acting like a diffusion.</p><p>Our contribution is two-fold. Besides formulating the segmentation problem in video as an eigenvalue problem on the adjacency matrix of the graph in space-time, we also provide a very fast optimization algorithm that computes the required eigenvector (which represents the desired segmentation) without explicitly creating or using the huge adjacency matrix. We prove theoretically and in practice that our algorithm reaches the same solution as a standard routine for eigenvector computation. We also show in experiments that the values in the final eigenvector, with one element per video pixel, confirm the spectral clustering assumption and provide an improved soft-segmentation of the main object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most state of the art methods for video object segmentation are using CNNs architectures, pre-trained for object segmentation on other large image datasets. They have a strong image-based backbone and are not designed from scratch with both space and time dimensions in mind. Many solutions <ref type="bibr" target="#b1">[Khoreva et al., 2017]</ref> adapt image segmentation methods by adding an additional branch to the architecture for incorporating the time axis: motion branch (previous frames or optical flow as) or previous masks branch (for mask propagation). Other methods are based on one-shot learning strategies and fine tune the model on the first video frame, followed by some post-processing refinement . Approaches derived from OSVOS <ref type="bibr">[2017]</ref> do not take the time axis into account. Our method comes to better address the natural space-time relationship, which is why it is effective when combined with frame-based segmentation algorithms.</p><p>Graph representations. Graph methods are suitable for segmentation and can have different representations, where the nodes can be pixels, super-pixels, voxels or image/video regions. Graph edges are usually undirected, modeled as symmetric similarity functions. The choice of the representation influences both accuracy and runtime. Specifically, pixel-level representations are computationally extremely expensive, making the problem intractable for high resolution videos. Our fast solution implicitly uses a pixel-level graph representation: we make a first-order Taylor approximation of the Gaussian kernel (usually used for pairwise affinities) and rewrite it as a sequence of 3D convolutions in the video directly. Thus, we get the desired outcome without explicitly working with the graph. We describe it in detail in Sec. 3.</p><p>Spectral clustering. Computing eigenvectors of matrices extracted from data is a classic approach for clustering. There are several choices in the literature for choosing those matrices, the most popular being the Laplacian matrix <ref type="bibr" target="#b4">[Ng et al., 2001]</ref>, normalized <ref type="bibr" target="#b7">[Shi and Malik, 2000]</ref> or unnormalized. Other methods use the random walk matrix or directly the unnormalized adjacency matrix. Most methods are based on finding the eigenvectors corresponding to the smallest eigenvalues, while others, including our approach, require the leading eigenvectors. Graph Cuts are a popular class of spectral clustering algorithms, with many variants: normalized, average, min-max, mean cut and topological cut.</p><p>CRFs. Discriminative graphical models <ref type="bibr" target="#b1">[Kumar and Hebert, 2003]</ref> are often applied over the segmentation of images and videos <ref type="bibr">(denseCRF [Krähenbühl and Koltun, 2011]</ref>). CRFs are effective as they incorporate the observed data both at the level of nodes as well as edges. But they have a strict probabilistic interpretation and use inference algorithms that are significantly more expensive than the simpler eigenvector power iteration that we use for optimizing our non-probabilistic objective score.</p><p>Image segmentation. Graph cuts have been used in image segmentation <ref type="bibr" target="#b7">[Shi and Malik, 2000]</ref>. They are expensive in practice, as they require the computation of eigenvectors of smallest eigenvalues for very large Laplacian matrices. Fast graph-based algorithm for image segmentation exist, such as <ref type="bibr" target="#b0">[Felzenszwalb and Huttenlocher, 2004]</ref>, which is linear in the number of edges and it is based on an heuristic for building the minimum spanning tree. It is still used as staring point by current methods. Another approach <ref type="bibr" target="#b5">[Pourian et al., 2015]</ref> is to learn image regions with spectral graph partitioning and formulate segmentation as a convex optimization problem.</p><p>Video Segmentation. Many video segmentation methods adapt existing image segmentation. In <ref type="bibr" target="#b7">[Yu et al., 2015]</ref> a parametric graph partitioning model over superpixels is proposed. Hierarchical graph-based segmentation over RGBD video sequences <ref type="bibr" target="#b0">[Hickson et al., 2014]</ref> also groups pixels into regions. The problem is solved using bipartite graph matching and minimizing the spanning tree. In <ref type="bibr" target="#b2">[Li et al., 2018]</ref>, an efficient graph cut method is applied on a subset of pixels. To our best knowledge, all of the efficient methods group pixels into superpixels, regions from a grid or object proposals to handle the computational and memory burden. However, the hard initial grouping of pixels comes with a risk and could carry errors into the final solution, as it misses details available only at the original pixel resolution.</p><p>Our formulation is most related to <ref type="bibr" target="#b1">[Leordeanu and Hebert, 2005;</ref><ref type="bibr" target="#b4">Meila and Shi, 2001]</ref>. Our solution is the leading eigenvector of M (the adjacency matrix), computed fast and stably with power iteration as explained in Sec. 3. Note that using the unnormalized adjacency matrix in combination with power iteration is the least expensive spectral approach and the only one that can be factored into simple and fast 3D convolutions. This possibility gives our algorithm efficiency and speed (Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>We formulate salient object segmentation in video as a graph partitioning problem (foreground vs background), where the graph is both spatial and temporal. Each node i represents a pixel in the space-time volume, which has N = N f × H × W pixels. N f is the number of frames and (H, W ) the frame size. Each edge captures the similarity between two pixels and is defined by the pairwise function M i,j . The pairwise connections between pixels i and j, in space and time are symmetric and always non-negative, defining a N × N adjacency matrix M. We take into account only the local connections in space-time, so M is sparse.</p><p>Let s and f be feature vectors of size N × 1 with a feature value for each node. They will be used in defining the similarity function M ij (Eq. 1). For now we consider the simplest case when (s i , f i ) represent single channel features (e.g. they could be soft masks, grey level values, edge or motion cues, or any pre-trained features). Later on we show how we can easily adapt the formulation to the multi-channel feature case. We define the edge similarity M i,j using a Gaussian kernel:</p><formula xml:id="formula_0">M i,j = s p i s p j e −α(fi−fj ) 2 −βdist 2 i,j = s p i s p j e −α(fi−fj ) 2 G i,j<label>(1)</label></formula><formula xml:id="formula_1">M i,j ≈ s p i s p j unary terms [1 − α(f i − f j ) 2 ]G i,j pairwise terms .<label>(2)</label></formula><p>In graph methods, it is common to use two types of terms for representing the model over the graph. Unary terms are about individual node properties, while pairwise terms describe relations between pairs of nodes. In our case, s i , s j describe individual node properties, whereas f i , f j are used to define the pairwise similarity kernel between the two nodes. Note that in Eq. 2 we approximate the Gaussian kernel with its first-order Taylor expansion. The approximation is crucial in making our filtering approach possible, as shown next. Hyperparameters p and α control the importance of those terms. To partition the space-time graph of video pixels, we want to find the strongest cluster in this graph. We first represent a segmentation solution (i.e., cluster in the space-time graph) with an indicator vector x, that has one element for each node in the 3D space-time volume, such that x i = 1 if node (pixel) i is in the video segmentation cluster (foreground) and x i = 0 otherwise (background). We define the clustering score to be the sum over all pairwise similarity terms M ij between the nodes inside the cluster. The higher this score, the stronger the sum of connections and the cluster. The segmentation score can be written compactly in matrix form as S(x) = x T Mx. Similar to other spectral approaches in graph matching <ref type="bibr" target="#b1">[Leordeanu and Hebert, 2005]</ref>, we find the segmentation solution x s that maximizes S(x) under the relaxed constraints x 2 = 1. Fixing the L2 norm of x is needed since only relative soft segmentation values matter. Thus, our optimization problem become one of maximizing the Raleigh quotient:</p><formula xml:id="formula_2">x s = argmax x (x T Mx/ x 2 ).</formula><p>( <ref type="formula">3)</ref> The global optimum solution is the principal eigenvector of M. M is symmetric and has non-negative values, so the solution will also have non-negative elements, by Perron-Frobenius theorem <ref type="bibr" target="#b0">[Frobenius, 1907]</ref>. The final segmentation could be simply obtained by thresholding. However, matrix M, even for a small video has 20 million nodes (50 frames of 480 × 854), making the problem of finding the leading eigenvector with standard procedures intractable (Sec 4.2).</p><p>Next we show how to take advantage of the first-order expansion of the pairwise terms defining M and break power iteration into several very fast 3D convolutions in space and time, directly on the feature maps, without explicitly using the very big adjacency matrix. Our method receives as input pixel level feature maps and returns a final segmentation, as the solution x s to problem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Power iteration with pixel-wise iterations</head><p>We apply power iteration algorithm to compute the eigenvector. At iteration k + 1, we have Eq. 4:</p><formula xml:id="formula_3">x k+1 i ← j∈N (i) M i,j x k j ,<label>(4)</label></formula><p>where, after each iteration, the solution is normalized to unit norm and N (i) is the set of neighbors pixels with i, in space and time. Expanding M i,j (Eq. 2), Eq. 4 becomes:</p><formula xml:id="formula_4">x k+1 i ← αs p i j∈N (i) s p j [α −1 − f 2 i − f 2 j + 2f i f j ]G i,j x k j , (5) x k+1 i ← αs p i (α −1 − f 2 i ) j∈N (i) s p j G i,j x k j − αs p i j∈N (i) s p j f 2 j G i,j x k j + 2αs p i f i j∈N (i) s p j f j G i,j x k j .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Power iteration using 3D convolutions</head><p>In Eq. 6 we observe that the links between the nodes are local (M is sparse) and we can replace the sums over neighbours with local 3D convolutions in space and time. Thus, we rewrite Eq. 6 as a sum of convolutions in 3D:</p><formula xml:id="formula_5">X crt ← S p · (α −1 1 − F 2 ) · G 3D * (S p · X k )− S p · G 3D * (F 2 · S p · X k )+ 2S p · F · G 3D * (F · S p · X k ),<label>(7)</label></formula><formula xml:id="formula_6">X k+1 ← X crt / X crt 2 ,<label>(8)</label></formula><p>where * is a convolution over a 3D space-time volume with a 3D Gaussian filter (G 3D ), · is an element-wise multiplication, 3D matrices X k , S, F have the original video shape (N f × H × W ) and 1 is a 3D matrix with all values 1. We transformed the standard form of power iteration in Eq. 4 in several very fast matrix operations: 3 convolutions and 13 element-wise matrix operations (multiplications and additions), which are local operations that can be parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple feature channels</head><p>Our approach in Eq. 7 can easily accommodate multiple feature channels if we rewrite M i,j from Eq. 2 and propagate it through Eq. 7, the final multi-channel solution is obtained by summing over the final solution for each channel:</p><formula xml:id="formula_7">X multi crt = N f eat m=1 X crt (F c ),<label>(9)</label></formula><p>where F c is one (3D) channel feature matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head><p>We present the version of our algorithm (Alg. 1) that has a single channel feature map, but can be easily adapted to the multi-channel version, using Eq. 9. We first initialize the solution X with a uniform vector or with a soft-segmentation provided by another method, if it is available. We also initialize feature maps S and F, which could be of any kind: lower-level (optical flow, edges, gray-level values) or higherlevel pre-trained semantic features (deep features or initial soft/hard segmentation maps). At each iteration, we select a time frame around the current one. In Step 2, we multiply the corresponding matrices, apply the convolutions, compose the results and obtain the new segmentation mask for pixels in current frame, using the space-time operations (as in Eq. 7). Since the solution needs to be binary at the end (for evaluation), after each iteration (Step 3, line 14 in Alg. 1), we project the solution in a more discrete space (see Sec. 4.1).  </p><formula xml:id="formula_8">S w , X w , F w ← T OF (S, X, F)[i − w : i + w]</formula><formula xml:id="formula_9">T1 ← (α −1 1 − F 2 w ) · G 3D * (S p w · X w ) 8: T2 ← −G 3D * (F 2 w · S p w · X w ) 9: T3 ← 2F w · G 3D * (F w · S p w · X w )</formula><p>10: X ← project binary(X) 15: end for</p><formula xml:id="formula_10">X new [i] ← S p w · (T1 + T2 + T3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Binarization -Spectral vs Discrete space</head><p>At the end, we need to have a hard segmentation map for the object of interest. Over the iterations, a spectral method makes the solution continuous. It was previously observed that in graph matching optimization, where the solution is relaxed <ref type="bibr" target="#b1">[Leordeanu et al., 2009]</ref>, keeping it close to the initial discrete domain comes with a better final performance, even though the optimum in the spectral space is affected. So we integrated the binarization in the iterative optimization. After a few iterations in the continuous space, we start projecting the solution on an almost discrete space through a sigmoid (which continuously approximates a step function) and initialize the next iteration with this projection. After the last iteration, we apply a hard threshold on a solution much closer to the discrete space than before. This way, the transition is smoother compared with a simple sharp thresholding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Numerical Analysis</head><p>We compare the standard power iteration eigenvector computation with our filtering formulation, both from qualitative and quantitative (speedup) points of view.</p><p>Computational Complexity. Lanczos <ref type="bibr">[1950]</ref> method for sparse matrices has O(kN f N p N i ) complexity for computing the leading eigenvector, where k is the number of neighbours for each node, N f the number of frames in video, N p the number of pixels per frame and N i the number of iterations. Our full iteration algorithm has also O(kN f N p N i ) complexity, but with highly parallelizable operations, comparing to Lanczos. The Gaussian filters are separable, so the 3D convolutions can be broken into a sequence of three vector-wise convolutions, reducing the complexity O(k) for filtering to 3O(k   We compare three solutions: a) Lanczos for the principal eigenvector for Eq. 1 b) Lanczos for the approximate adjacency matrix as in Eq. 2 c) our 3D convolutions approach. For a small graph of 4000 nodes (a video with 10 frames of 20 × 20 pixels), a) and b) have 0.15 sec/iter and our 3D filtering formulation has 0.02 sec/iter <ref type="figure" target="#fig_2">(Fig. 2)</ref>. Our approach scales better, having a huge advantage when working with videos with millions of nodes because we do not explicitly build the adjacency matrix and filtering is parallelized on GPU.</p><p>Qualitative analysis. We perform tests on synthetic data, in order to study the differences between the original spectral solution using the exponential pairwise scores (1) and the one obtained after our first-order Taylor approximation trick (2). In <ref type="figure" target="#fig_1">Fig. 3</ref> we see qualitative comparisons between the solutions obtained by three implementations: our SFSeg, power iteration with original pairwise scores and numpy eigenvector with original pairwise scores. The output is almost identical. In the synthetic experiments, the input is noisy, but all spectral solutions manage to reconstruct the initial segmentation.</p><p>Quantitative analysis. We analyze the numerical differences between the original eigenvector and our approximation (SFSeg). We plot the angle (in degrees) and the IoU (Jaccard) between SFSeg (first-order approximation of pairwise functions, optimized with 3D convolutions) and the original eigenvector (exponential pairwise functions in the adjacency matrix), over multiple SFSeg iterations in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note that in these experiments we intentionally start from a far away solution (70 degrees difference between the SFSeg initial segmentation vector and the original eigenvector) to better show that SFSeg indeed converges to practically the same eigenvector. Such comparisons can be performed only on synthetic data with relatively small videos, for which the computation of the adjacency matrix needed for the original eigenvector is tractable. The results clearly show that SFSeg, with first order approximations of the pairwise functions on edges and optimization based on 3D filters, reaches the same theoretical solution, while being orders of magnitude faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Analysis</head><p>Experiments on <ref type="bibr">DAVIS-2016</ref><ref type="bibr">. DAVIS-2016</ref><ref type="bibr">[Perazzi et al., 2016</ref>] is a densely annotated video object segmentation dataset. It contains 50 high-resolution video sequences (30 train/20 valid), with a total of 3455 annotated frames of realworld scenes. The benchmark comes with two tasks: the unsupervised one, where the solutions do not have access to the first frame of the video and the semi-supervised one, where the methods use the ground-truth from the first frame. In both setups, the methods can train the model on the training set and report their performance on the validation set. Our results are reported on the validation set, but we do not use the training set. For optical flow we used the Pytorch implementation of Flownet2 <ref type="bibr" target="#b6">[Reda et al., 2017]</ref>. Experimental Setup. We test SFSeg with input from precomputed segmentations of the video produced by top methods from DAVIS-2016, on both tasks. For the features maps, we initialized S with the pre-computed input segmentation values. For F, we used two channels: the magnitude for the direct optical flow and for the reverse optical flow. We set: N i = 5; α = 1 and p = 0.1 for unsupervised task and p = 0.2 for the semi-supervised one. The algorithm is implemented as in Alg. 1 with the multi-channels as in Eq. 9.</p><p>In Tab. 1 we show the results of our method, SFSeg, when combined with top methods on DAVIS-2016, semisupervised and unsupervised tasks. For a better understanding of the results, we also show the effect of applying SFSeg Average Boost +1.1% 80% <ref type="table">Table 1</ref>: Improvement over top segmentation methods on DAVIS-2016 tasks, validation set. SFSeg has the same hyper-parameters per task. We also included results for other competitive (non-SOTA) inputs. 2 nd column: Jaccard score for the input method; 3 rd column: score after applying SFSeg over the input method; 4 th column: the percentage of videos when the performance is improved after using SFSeg. The average SFSeg boost is 1.1% in Jaccard score and on average SFSeg raises performance for 80% of videos. Input methods : <ref type="bibr" target="#b7">[Voigtlaender and Leibe, 2017;</ref><ref type="bibr" target="#b2">Luiten et al., 2018;</ref><ref type="bibr" target="#b3">M. Siam, 2019;</ref><ref type="bibr" target="#b7">Song et al., 2018;</ref><ref type="bibr" target="#b1">Koh and Kim, 2017;</ref><ref type="bibr" target="#b7">Tokmakov et al., 2017;</ref><ref type="bibr" target="#b0">Jain et al., 2017;</ref><ref type="bibr" target="#b0">Cheng et al., 2018;</ref><ref type="bibr" target="#b0">Faktor and Irani, 2014</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Frames</head><p>Input Mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SFSeg Iter2</head><p>SFSeg Convergence over other competitive, non-SOTA methods. We noted that the improvement is not related with the quality measure of the input. In some cases the improvement is stronger when input comes from stronger methods. Nevertheless, we consistently improve over the input method, whose segmentation mask we use to initialize the segmentation X 0 . In <ref type="figure" target="#fig_5">Fig. 5</ref> we show the iterative effect of SFSeg. Each example starts from the initial RGB frame and its initial segmentation (as produced by top DAVIS methods), and presents the Input PReMVOS SFSeg GT Input OnAVOS SFSeg GT Input ARP SFSeg GT <ref type="figure">Figure 6</ref>: We show the output of SFSeg (col 3) over the input masks (col 2) received from top DAVIS-2016 solutions in various video frames (col 1). We see how the quality of the masks is increasing, bringing the input masks closer to ground truth (col 4). 1. PReMVOS -3 rd place on semi-supervised (motocross-jump); 2. OnAVOS -1 st place on semi-supervised (breakdance); 3. ARP -2 nd place on unsupervised (dog).</p><p>Method Score <ref type="formula">(</ref>  Experiments on SegTrackv2. SegTrackv2 <ref type="bibr" target="#b1">[Li et al., 2013]</ref> is a video object segmentation dataset, containing 14 videos, with multiple objects per frame. The purpose for video object segmentation task is to find the segmentation for all the objects in the frame (also split in two tasks: using the first frame or not). We use our standalone method, SFSeg, applied over the soft output of a competitive Backbone (BB): UNet over ResNet34 pretrained features, fine tuned 40 epochs on salient object segmentation in images on DUTS dataset  (with RectifiedAdam as optimizer). In Tab. 2 we show comparative results of our standalone method and other top solutions on the SegTrackv2 dataset.</p><p>SFSeg vs denseCRF. We compare SFSeg with denseCRF <ref type="bibr" target="#b1">[Krähenbühl and Koltun, 2011]</ref>, which is one of the most used refinement method in video object segmentation <ref type="bibr" target="#b7">[Song et al., 2018]</ref>. When applied over the same Backbone presented above, we observe that SFSeg brings a stronger improvement than denseCRF on both DAVIS-2016 and SegTrackv2 (Tab. 3). More, the two are complementary: in combination, the performance is boosted by the largest margin.</p><p>Running Time. The algorithm scales well, the running time being linear in the number of video pixels, as detailed  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We formulate video object segmentation as clustering in the space-time graph of pixels. We introduce an efficient spectral algorithm, Spectral Filtering Segmentation (SFSeg), in which the standard power iteration for computing the principal eigenvector of the graph adjacency matrix is transformed into a set of 3D convolutions applied on 3D feature maps in the video volume. Our original theoretical contribution makes the initial intractable problem possible. We validate experimentally that our solution based on a first-order Taylor approximation of the original pairwise potential used in spectral clustering is practically equivalent to the original one. In experiments, SFSeg consistently improves (for 80% of videos) over top published video object segmentation methods, at a small additional computational cost. Moreover, our method also achieves top performance in combination with other backbone networks (not necessarily state of the art). The consistent improvements in practice indicate that our spectral approach brings a new and complementary dimension to clustering in space-time, which is not fully addressed by current solutions. In the immediate future we will explore ways to learn more powerful features end-to-end, within our spectral clustering formulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We see the video as a locally connected graph of pixels in space-time. The strength and the number of connections are enforcing the pixel membership to the salient video object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 3 )</head><label>3</label><figDesc>: 3*7*7=147 vs 3+7+7=17 for a 3x7x7 kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Total runtime in logarithmic scale for 100 iterations, including the time for building the adjacency matrix for power iteration. Our filtering formulation scales with the number of nodes, in contrast to power iteration, having an exponentially better time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A toy examples for qualitative comparisons with soft masks for a six frames video. Starting with a very noisy input segmentation mask and showing: SFSeg segmentations after 5 iters; Power Iteration after 5 iters; the real main eigenvector. The results are almost identical, proving that SFSeg is a good approximation. More, for other methods, this is tractable only on toy examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The angle and the IoU between real eigenvector and our SFSeg solution. The evolution of those metrics is monitored over multiple SFSeg iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>We present the evolution of SFSeg, over several iterations. Using the input segmentation mask (column 2) from top methods on DAVIS: ARP, FSEG and LVO, we show the intermediate value of the mask at Iter2 (column 3) and Iter4 (column 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Power iteration with 3D convolutions algorithm. At each iteration we pass through the whole video and compute the updated soft-segmentation X. First, we select a time window around current frame [i − w, i + w]. Secondly we compute the eigenvector with convolutions. Then, after each iteration, we binarize the solution (see Sec. 4.1).</figDesc><table><row><cell cols="2">S -unary feature maps for video</cell></row><row><cell cols="2">F -defines pairwise feature maps for video</cell></row><row><cell cols="2">X -salient object segmentation</cell></row><row><cell cols="2">1: X ← S</cell></row><row><cell cols="2">2: for iter in [1..N i ] do</cell></row><row><cell>3:</cell><cell>for i in [1..N f ] do</cell></row><row><cell>4:</cell><cell>STEP 1. Take a temporal window around frame i:</cell></row><row><cell>5:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparative results on SegTrackv2. Our standalone solution, Backbone + SFSeg + denseCRF, obtains the best results among the other top methods in the literature. Input methods: [Tokmakov et al., 2017; Jain et al., 2017; Caelles et al., 2017; Faktor and Irani, 2014; Khoreva et al., 2017]. segmentation at an intermediate iteration and the final one, when SFSeg reaches convergence.InFig. 6we show qualitative examples of our spectral method, SFSeg, applied over the initial mask, received as input from highly qualitative segmentation solutions on DAVIS-2016. The new masks show significant improvement without using other new means of supervision.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Refinement Comparison. We compare SFSeg with dense-CRF when applied to a competitive end-to-end Backbone (as detailed above). While SFSeg outperforms denseCRF when used individually, the two methods prove to be not only different, but also complementary, since combining them boosts the Jaccard score.in Sec. 4. For a frame of 480 × 854 pixels, it takes 0.055 sec per iteration, compared with 0.8 sec for denseCRF. Filtering takes 60% of time, the rest of 40% being used on other auxiliary operations (copying tensors). The time penalty of adding SFSeg is minor for most methods, which takes several seconds per frame (e.g. 4.5 sec per frame, 13 sec per frame<ref type="bibr" target="#b2">[Luiten et al., 2018]</ref>). We tested on a GTX Titan X MaxwellGPU, in Pytorch [Paszke et al., 2017].</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">About a Fundamental Theorem of Group Theory. II. Session Reports of the Royal Prussian Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caelles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
	<note>CVPR. Learning to combine motion and appearance for fully automatic segmention of generic objects in videos. arXiv</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lanczos, 1950] C. Lanczos. An iteration method for the solution of the eigenvalue i problem . of linear differential and integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient inference in fully connected crfs with gaussian edge potentials. arXiv</title>
		<imprint>
			<publisher>Kumar and Hebert</publisher>
			<date type="published" when="1950" />
		</imprint>
	</monogr>
	<note>An integer projected fixed point method for graph matching and MAP inference. Video segmentation by tracking many figureground segments. ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
	<note>Premvos: Proposal-generation, refinement and merging for video object segmentation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video object segmentation using teacher-student adaptation in a human robot interaction (hri) setting. ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal M Elhoseiny M Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang ; Maninis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
	<note>Video object segmentation without temporal information</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Automatic differentiation in pytorch. Perazzi et al., 2016] F. Perazzi, J. Pont-Tuset</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised graph based semantic segmentation by learning communities of image-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung ; N. Pourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
	<note>A benchmark dataset and evaluation methodology for video object segmentation. CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">flownet2-pytorch: Pytorch implementation of flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Efficient video object segmentation via network modulation. Efficient video segmentation using parametric graph partitioning. ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
