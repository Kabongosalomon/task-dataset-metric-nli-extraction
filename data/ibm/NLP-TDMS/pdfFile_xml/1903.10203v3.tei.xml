<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Variational Generation for Low Shot Heterogeneous Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Hu</surname></persName>
							<email>yibo.hu@cripac.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
							<email>huaibo.huang@cripac.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nlpr</forename><forename type="middle">&amp;amp;</forename><surname>Cripac</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casia</forename></persName>
						</author>
						<title level="a" type="main">Dual Variational Generation for Low Shot Heterogeneous Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. The related code is available at https://github.com/BradyFU/DVG.</p><p>Recently, the great progress of high-quality face synthesis <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> has made "recognition via generation" possible. TP- GAN [16]  and CAPG- GAN [13]  introduce face synthesis to improve the quantitative performance of large pose face recognition. For HFR, [32] proposes a two-path model to synthesize VIS images from NIR images.</p><p>[36] utilizes a GAN based multi-stream feature fusion technique to generate VIS images from polarimetric thermal faces. However, all these methods are * Equal Contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of deep learning, face recognition has made significant progress <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2]</ref> in recent years. However, in many real-world applications, such as video surveillance, facial authentication on mobile devices and computer forensics, it is still a great challenge to match heterogeneous face images in different modalities, including sketch images <ref type="bibr" target="#b36">[37]</ref>, near infrared images <ref type="bibr" target="#b23">[24]</ref> and polarimetric thermal images <ref type="bibr" target="#b35">[36]</ref>. Heterogeneous face recognition (HFR) has attracted much attention in the face recognition community. Due to the large domain gap, one challenge is that the face recognition model trained on VIS data often degrades significantly for HFR. Therefore, lots of cross domain feature matching methods <ref type="bibr" target="#b9">[10]</ref> are introduced to reduce the large domain gap between heterogeneous face images. However, since it is expensive and time-consuming to collect a large number of heterogeneous face images, there is no public large-scale heterogeneous face database. With the limited training data, CNNs trained for HFR often tend to be overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional translation</head><p>Unconditional dual variational generation Translation <ref type="figure">Figure 1</ref>: The diversity comparisons between the conditional image-to-image translation <ref type="bibr" target="#b31">[32]</ref> (left part, the above is the input NIR image and the below is the corresponding translated VIS image) and our unconditional DVG (right part, all paired heterogeneous images are generated via noise). For the conditional image-to-image translation methods, given one NIR image, a generator only synthesizes one new VIS image with same attributes (e.g., the pose and the expression) except for the spectral information. Differently, DVG generates massive new paired images with rich intra-class diversity from noise. based on conditional image-to-image translation framework, leading to two potential challenges: 1) Diversity: Given one image, a generator only synthesizes one new image of the target domain <ref type="bibr" target="#b31">[32]</ref>. It means such conditional image-to-image translation methods can only generate limited number of images. In addition, as shown in the left part of <ref type="figure">Fig. 1</ref>, two images before and after translation have same attributes (e.g., the pose and the expression) except for the spectral information, which means it is difficult for such conditional image-to-image translation methods to promote intra-class diversity. In particular, these problems will be very prominent in the low-shot heterogeneous face recognition, i.e., learning from few heterogeneous data. 2) Consistency: When generating large-scale samples, it is challenging to guarantee that the synthesized face images belong to the same identity of the input images. Although identity preserving loss <ref type="bibr" target="#b12">[13]</ref> constrains the distances between features of the input and synthesized images, it does not constraint the intra-class and inter-class distances of the embedding space.</p><p>To tackle the above challenges, we propose a novel unconditional Dual Variational Generation (DVG) framework (shown in <ref type="figure">Fig. 3</ref>) that generates large-scale paired heterogeneous images with the same identity from noise. Unconditional generative models can generate new images (generate single image per time) from noise <ref type="bibr" target="#b20">[21]</ref>, but since these images do not have identity labels, it is difficult to use these images for recognition networks. DVG makes use of the property of generating new images of the unconditional generative model <ref type="bibr" target="#b20">[21]</ref>, and adopts a dual generation manner to get paired heterogeneous images with the same identity every time. This enables DVG to generate large-scale images, and make the generated images can be used to optimize recognition networks. Meanwhile, DVG also absorbs the various intra-class changes of the training database, leading to the generated paired images have abundant intra-class diversity. For instance, as presented in the right part of <ref type="figure">Fig. 1</ref>, the first four paired images have different poses, and the fifth paired images have different expressions. Furthermore, DVG only pays attention to the identity consistency of the paired heterogeneous images rather than the identity whom the paired heterogeneous images belong to, which avoids the consistency problem of previous methods. Specifically, we introduce a dual variational autoencoder to learn a joint distribution of paired heterogeneous images. In order to constrain the generated paired images to belong to the same identity, we impose both a distribution alignment in the latent space and a pairwise identity preserving in the image space. New paired images are generated by sampling and copying a noise from a standard Gaussian distribution, as displayed in the left part of <ref type="figure">Fig. 3</ref>. These generated paired images are used to optimize the HFR network by a pairwise distance constraint, aiming at reducing the domain discrepancy.</p><p>In summary, the main contributions are as follows:</p><p>• We provide a new insight into the problems of HFR. That is, we consider HFR as a dual generation problem, and propose a novel dual variational generation framework. This framework generates new paired heterogeneous images with abundant intra-class diversity to reduce the domain gap of HFR. • In order to guarantee that the generated paired images belong to the same identity, we constrain the consistency of paired images in both latent space and image space. These allow new images sampled from the noise can be used for recognition networks. • We can sample large-scale diverse paired heterogeneous images from noise. By constraining the pairwise feature distances of the generated paired images in the HFR network, the domain discrepancy is effectively reduced.</p><p>• Experiments on the CASIA NIR-VIS 2.0, the Oulu-CASIA NIR-VIS, the BUAA-VisNir and the IIIT-D Viewed Sketch databases demonstrate that our method can generate photo-realistic images, and significantly improve the performance of recognition.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heterogeneous Face Recognition</head><p>Lots of researchers pay their attention to Heterogeneous Face Recognition (HFR). For the feature-level learning, <ref type="bibr" target="#b21">[22]</ref> employs HOG features with sparse representation for HFR. <ref type="bibr" target="#b6">[7]</ref> utilizes LBP histogram with Linear Discriminant Analysis to obtain domain-invariant features. <ref type="bibr" target="#b9">[10]</ref> proposes Invariant Deep Representation (IDR) to disentangle representations into two orthogonal subspaces for NIR-VIS HFR. Further, <ref type="bibr" target="#b10">[11]</ref> extends IDR by introducing Wasserstein distance to obtain domain invariant features for HFR. For the image-level learning, the common idea is to transform heterogeneous face images from one modality into another one via image synthesis. <ref type="bibr" target="#b18">[19]</ref> utilizes joint dictionary learning to reconstruct face images for boosting the performance of face matching. <ref type="bibr" target="#b22">[23]</ref> proposes a cross-spectral hallucination and low-rank embedding to synthesize a heterogeneous image in a patch way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Models</head><p>Variational autoencoders (VAEs) <ref type="bibr" target="#b20">[21]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b5">[6]</ref> are the most prominent generative models. VAEs consist of an encoder network q φ (z|x) and a decoder network p θ (x|z). q φ (z|x) maps input images x to the latent variables z that match to a prior p(z), and p θ (x|z) samples images x from the latent variables z. The evidence lower bound objective (ELBO) of VAEs:</p><formula xml:id="formula_0">log p θ (x) ≥ E qφ(z|x) log p θ (x|z) − D KL (q φ (z|x)||p(z)).<label>(1)</label></formula><p>The two parts in ELBO are a reconstruction error and a Kullback-Leibler divergence, respectively.</p><p>Differently, GANs adopt a generator G and a discriminator D to play a min-max game. G generates images from a prior p(z) to confuse D, and D is trained to distinguish between generated data and real data. This adversarial rule takes the form:</p><formula xml:id="formula_1">min G max D E x∼pdata(x) [log D(x)] + E z∼pz(z) [log(1 − D(G(z)))] .</formula><p>(2) They have achieved remarkable success in various applications, such as unconditional image generation that generates images from noise <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref>, and conditional image generation that synthesizes images according to the given condition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16]</ref>. According to <ref type="bibr" target="#b14">[15]</ref>, VAEs have nice manifold representations, while GANs are better at generating sharper images.</p><p>Another work to address the similar problem of our method is CoGAN <ref type="bibr" target="#b24">[25]</ref>, which uses a weightsharing manner to generate paired images in two different modalities. However, CoGAN neither explicitly constrains the identity consistency of paired images in the latent space nor in the image space. It is challenging for the weight-sharing manner of CoGAN to generate paired images with the same identity, as shown in <ref type="figure">Fig. 4</ref>.</p><formula xml:id="formula_2">! " Copy Standard Gaussian Noise #̂" HFR Net Domain Gap Reduction z % &amp; ' &amp; # &amp; # ( Distribution Alignment ) &amp; * &amp; ) ( * ( % ( ! " # " Concat + ,-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise Identity Preserving</head><p>' ( Reconstructed Generated <ref type="figure">Figure 3</ref>: The purpose (left part) and training model (right part) of our unconditional DVG framework. DVG generates large-scale new paired heterogeneous images with the same identity from standard Gaussian noise, aiming at reducing the domain discrepancy for HFR. In order to achieve this purpose, we elaborately design a dual variational autoencoder. Given a pair of heterogeneous images from the same identity, the dual variational autoencoder learns a joint distribution in the latent space. In order to guarantee the identity consistency of the generated paired images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we will introduce our method in detail, including the dual variational generation and heterogeneous face recognition. Note that we specifically discuss the NIR-VIS images for better presentation. Other heterogeneous images are also applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dual Variational Generation</head><p>As shown in the right part of <ref type="figure">Fig. 3</ref>, DVG consists of a feature extractor F ip , and a dual variational autoencoder: two encoder networks and a decoder network, all of which play the same roles of VAEs <ref type="bibr" target="#b20">[21]</ref>. Specifically, F ip extracts the semantic information of the generated images to preserve the identity information. The encoder network E N maps NIR images x N to a latent space z N = q φ N (z N |x N ) by a reparameterization trick: z N = u N + σ N , where u N and σ N denote mean and standard deviation of NIR images, respectively. In addition, is sampled from a multi-variate standard Gaussian and denotes the Hadamard product. The encoder network E V has the same manner as E N :</p><formula xml:id="formula_3">z V = q φ V (z V |x V )</formula><p>, which is for VIS images x V . After obtaining the two independent distributions, we concatenate z N and z V to get the joint distribution z I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution Learning</head><p>We utilize VAEs to learn the joint distribution of the paired NIR-VIS images. Given a pair of NIR-VIS images {x N , x V }, we constrain the posterior distribution q φ N (z N |x N ) and q φ V (z V |x V ) by the Kullback-Leibler divergence:</p><formula xml:id="formula_4">L kl = D KL (q φ N (z N |x N )||p(z N )) + D KL (q φ V (z V |x V )||p(z V )),<label>(3)</label></formula><p>where the prior distributions p(z N ) and p(z V ) are both the multi-variate standard Gaussian distributions. Like the original VAEs, we require the decoder network p θ (x N , x V |z I ) to be able to reconstruct the input images x N and x V from the learned distribution:</p><formula xml:id="formula_5">L rec = −E q φ N (z N |x N )∪q φ V (z V |x V ) log p θ (x N , x V |z I ).<label>(4)</label></formula><p>Distribution Alignment We expect a pair of NIR-VIS images {x N , x V } to be projected into a common latent space by the encoders E N and E V , i.e., the NIR distribution p(z (i) N ) is the same as the VIS distribution p(z (i) V ), where i denotes the identity information. That means we maintain the identity consistency of the generated paired images in the latent space. Explicitly, we align the NIR and VIS distributions by minimizing the Wasserstein distance between the two distributions. Given two Gaussian distributions p(z V ) is simplified <ref type="bibr" target="#b9">[10]</ref> as:</p><formula xml:id="formula_6">L dist = 1 2 ||u (i) N − u (i) V || 2 2 + ||σ (i) N − σ (i) V || 2 2 .<label>(5)</label></formula><p>Pairwise Identity Preserving In previous image-to-image translation works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>, identity preserving is usually introduced to maintain identity information. The traditional approach uses a pre-trained feature extractor to enforce the features of the generated images to be close to the target ones. However, since the lack of intra-class and inter-class constraints, it is challenge to guarantee the synthesized images to belong to the specific categories of the target images. Considering that DVG generates a pair of heterogeneous images per time, we only need to consider the identity consistency of the paired images.</p><p>Specifically, we adopt Light CNN <ref type="bibr" target="#b33">[34]</ref> as the feature extractor F ip to constrain the feature distance between the reconstructed paired images:</p><formula xml:id="formula_7">L ip-pair = ||F ip (x N ) − F ip (x V )|| 2 2 ,<label>(6)</label></formula><p>where F ip (·) means the normalized output of the last fully connected layer of F ip . In addition, we also use F ip to make the features of the reconstructed images and the original input images close enough as previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>:</p><formula xml:id="formula_8">L ip-rec = ||F ip (x N ) − F ip (x N )|| 2 2 + ||F ip (x V ) − F ip (x V )|| 2 2 ,<label>(7)</label></formula><p>wherex N andx V denote the reconstructions of the input paired images x N and x V , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversity Constraint</head><p>In order to further increase the diversity of the generated images, we also introduce a diversity loss <ref type="bibr" target="#b26">[27]</ref>. In the sampling stage, when two sampled noise z I1 are z I2 are close, the generated images x I1 and x I2 are going to be similar. We maximize the following loss to encourage the decoder D I to generate more diverse images:</p><formula xml:id="formula_9">L div = max D I |F ip (x I1 ) − F ip (x I2 )| |z I1 − z I2 | .<label>(8)</label></formula><p>Overall Loss Moreover, in order to increase the sharpness of our generated images, we also adopt an adversarial loss L adv as <ref type="bibr" target="#b30">[31]</ref>. Hence, the overall loss to optimize the dual variational autoencoder can be formulated as</p><formula xml:id="formula_10">L gen = L rec + L kl + L adv + λ 1 L dist + λ 2 L ip-pair + λ 3 L ip-rec + λ 4 L div ,<label>(9)</label></formula><p>where λ 1 , λ 2 , λ 3 and λ 4 are the trade-off parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous Face Recognition</head><p>For the heterogeneous face recognition, our training data contains the original limited labeled data x i (i ∈ {N, V }) and the large-scale generated unlabeled paired NIR-VIS datax i (i ∈ {N, V }). Here, we define a heterogeneous face recognition network F to extract features f i = F (x i ; Θ), where i ∈ {N, V } and Θ is the parameters of F . For the original labeled NIR and VIS images, we utilize a softmax loss:</p><formula xml:id="formula_11">L cls = i∈{N,V } softmax(F (x i ; Θ), y),<label>(10)</label></formula><p>where y is the label of identity.</p><p>For the generated paired heterogeneous images, since they are generated from noise, there are no specific classes for the paired images. But as mentioned in section 3.1, DVG ensures that the generated paired images belong to the same identity. Therefore, a pairwise distance loss between the paired heterogeneous samples is formulated as follows:</p><formula xml:id="formula_12">L pair = ||F (x N ; Θ) − F (x V ; Θ)|| 2 2 ,<label>(11)</label></formula><p>In this way, we can efficiently minimize the domain discrepancy by generating large-scale unlabeled paired heterogeneous images. As stated above, the final loss to optimize for the heterogeneous face recognition network can be written as</p><formula xml:id="formula_13">L hfr = L cls + α 1 L pair ,<label>(12)</label></formula><p>where α 1 is the trade-off parameter.  (a) The quantitative comparisons of different methods. MD (lower is better) means the mean feature distance between the generated paired NIR and VIS images. FID (lower is better) is measured based on the features of LightCNN-9, instead of the traditional Inception model. (b) The ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Databases and Protocols</head><p>Three NIR-VIS heterogeneous face databases and one Sketch-Photo heterogeneous face database are used to evaluate our proposed method. For the NIR-VIS face recognition, following <ref type="bibr" target="#b34">[35]</ref>, we report Rank-1 accuracy and verification rate (VR)@false accept rate (FAR) for the CASIA NIR-VIS 2.0 <ref type="bibr" target="#b23">[24]</ref>, the Oulu-CASIA NIR-VIS <ref type="bibr" target="#b17">[18]</ref> and the BUAA-VisNir Face <ref type="bibr" target="#b13">[14]</ref> databases. Note that, for the Oulu-CASIA NIR-VIS database, there are only 20 subjects are selected as the training set. In addition, the IIIT-D Viewed Sketch database <ref type="bibr" target="#b0">[1]</ref> is employed for the Sketch-Photo face recognition. Due to the few number of images in the IIIT-D Viewed Sketch database, following the protocols of <ref type="bibr" target="#b2">[3]</ref>, we use the CUHK Face Sketch FERET (CUFSF) <ref type="bibr" target="#b36">[37]</ref> as the training set and report the Rank-1 accuracy and VR@FAR=1% for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details</head><p>For the dual variational generation, the architectures of the encoder and decoder networks are the same as <ref type="bibr" target="#b14">[15]</ref>, and the architecture of our discriminator is the same as <ref type="bibr" target="#b30">[31]</ref>. These networks are trained using Adam optimizer with a fixed rate of 0.0002. Other parameters λ 1 , λ 2 , λ 3 and λ 4 in Eq. (9) are set to 50, 5, 1000 and 0.2, respectively. For the heterogeneous face recognition, we utilize both LightCNN-9 and LightCNN-29 <ref type="bibr" target="#b33">[34]</ref> as the backbones. The models are pre-trained on the MS-Celeb-1M database <ref type="bibr" target="#b8">[9]</ref> and fine-tuned on the HFR training sets. All the face images are aligned to 144 × 144 and randomly cropped to 128 × 128 as the input for training. Stochastic gradient descent (SGD) is used as the optimizer, where the momentum is set to 0.9 and weight decay is set to 5e-4. The learning rate is set to 1e-3 initially and reduced to 5e-4 gradually. The batch size is set to 64 and the dropout ratio is 0.5. The trade-off parameters α 1 in Eq. (12) is set to 0.001 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Analyses</head><p>In this section, we analyze three metrics, including identity consistency, distribution consistency and visual quality, to demonstrate the effectiveness of DVG. The compared methods include CoGAN <ref type="bibr" target="#b24">[25]</ref> and VAE <ref type="bibr" target="#b20">[21]</ref>. For VAE model, the input is the concatenated NIR-VIS images.</p><p>Identity Consistency. In order to analyze the identity consistency, we measure the feature distance between the generated paired images on the CASIA NIR-VIS 2.0 database. Specifically, we first use a pre-trained Light CNN-9 <ref type="bibr" target="#b33">[34]</ref> to extract features and then measure the mean distance (MD) of the paired images. The results are reported in <ref type="table" target="#tab_1">Table 1a</ref>. MD is computed from 50K generated image pairs and the MD value of the original database is 0.26. We can clearly see that the MD value of DVG is even smaller than the original database, which means that our method can effectively guarantee the identity consistency of the generated paired images. The recognition performance of different methods is also reported in <ref type="table" target="#tab_1">Table 1a</ref>. We can see that DVG correspondingly achieves the best results.</p><p>Distribution Consistency. On the CASIA NIR-VIS 2.0 database, we take Fréchet Inception Distance (FID) <ref type="bibr" target="#b11">[12]</ref> to measure the Fréchet distance of two distributions in the feature space, reflecting the distribution consistency. We first measure the FID between the generated VIS images and the real VIS images, and the FID between the generated NIR images and the real NIR images, respectively. Then we calculate the mean FID as the final results, which are reported in <ref type="table" target="#tab_1">Table 1a</ref>. Considering that the face recognition network can better extract features of face images, we use a LightCNN-9 to extract features for calculating FID instead of the traditional Inception model. Similarly, FID results are computed from 50K generated image pairs. As shown in <ref type="table" target="#tab_1">Table 1a</ref>, DVG achieves best results, demonstrating that DVG has really learned the distributions of two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DVG CoGAN VAE</head><p>Visual Quality. In <ref type="figure">Fig. 4</ref>, we compare the dual generation results (128 × 128 resolution) of different methods on the CASIA NIR-VIS 2.0 database. Our visual results are obviously better than CoGAN and VAE. Moreover, we can observe that the generated paired images of VAE and CoGAN are not similar, which leads to worse Rank-1 accuracy during optimizing HFR network (see <ref type="table" target="#tab_1">Table 1a</ref>). More dual generation results of DVG are shown in <ref type="figure" target="#fig_0">Fig. 2 (256 × 256 resolution)</ref> and <ref type="figure">Fig. 5</ref>.</p><p>Ablation Study. <ref type="table" target="#tab_1">Table 1b</ref> presents the comparison results of our DVG and its three variants on the CASIA NIR-VIS 2.0 database. We observe that the recognition performance will decrease if one component is not adopted. Particularly, the accuracy drops significantly when the distribution alignment loss L dist or the pairwise identity preserving loss L ip-pair are not used. These results suggest that every component is crucial in our model.</p><p>Moreover, we analyze how the number of generated samples influence the HFR network on the Oulu-CASIA NIR-VIS database that only contains 20 identities with about 1,000 images for training. We generate 1K, 5K, 10K and 50K pairs of heterogeneous images via DVG, and we obtain 68.7%, 85.9%, 89.5% and 89.4% on VR@FAR=0.1% by LightCNN-9, respectively. The results have been significantly improved with the increasing number of the generated pairs, suggesting that DVG can boost the performance of the low-shot heterogeneous face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons with State-of-the-art Methods</head><p>The recognition performance of our proposed DVG is demonstrated in this section on four heterogeneous face recognition databases. The performance of state-of-the-art methods, such as IDNet <ref type="bibr" target="#b28">[29]</ref>, HFR-CNN <ref type="bibr" target="#b29">[30]</ref>, Hallucination <ref type="bibr" target="#b22">[23]</ref>, DLFace <ref type="bibr" target="#b27">[28]</ref>, TRIVET <ref type="bibr" target="#b25">[26]</ref>, IDR <ref type="bibr" target="#b9">[10]</ref>, W-CNN <ref type="bibr" target="#b10">[11]</ref>, RCN <ref type="bibr" target="#b3">[4]</ref>, MC-CNN <ref type="bibr" target="#b2">[3]</ref> and DVR <ref type="bibr" target="#b34">[35]</ref> is compared in  <ref type="bibr" target="#b28">[29]</ref> 87.1 ± 0.9 74.5 --------HFR-CNN <ref type="bibr" target="#b29">[30]</ref> 85.9 ± 0.9 78.0 --------Hallucination <ref type="bibr" target="#b22">[23]</ref> 89.6 ± 0.9 ---------DLFace <ref type="bibr" target="#b27">[28]</ref> 98.68 ---------TRIVET <ref type="bibr" target="#b25">[26]</ref> 95.7 ± 0.5 91.  To further analyze the effectiveness of the proposed DVG for low-shot heterogeneous face recognition, we evaluate DVG on the Oulu-CASIA NIR-VIS and the IIIT-D Viewed Sketch Face databases. As mentioned in section 4.1, there are fewer identities or images in these two databases. Expect for the above commonly used NIR-VIS and Sketch-Photo, we further explore other potential applications, including the face recognition under different resolutions on the NJU-ID database <ref type="bibr" target="#b16">[17]</ref> and different poses on the Multi-PIE database <ref type="bibr" target="#b7">[8]</ref>. The NJU-ID database consists of 256 identities with one ID card image (102 × 126 resolution) and one camera image (640 × 480 resolution) per identity. Considering the few number of images in the NJU-ID database, we use our collected ID-Photo database (1000 identities) as the training set and the NJU-ID database as the testing set. The Multi-PIE database contains 337 subjects with different poses. We use profiles (±75 o , ±90 o ) and frontal faces as different modalities. 200 persons are used as the training set and the rest 137 persons are the testing set (Setting 2 of <ref type="bibr" target="#b12">[13]</ref>). On the NJU-ID database, we improve Rank-1 by 5.5% (DVG 96.8% -Baseline 91.3%) and VR@FAR=1% by 6.2% (DVG 96.7% -Baseline 90.5%) over the baseline LightCNN-29. On the Multi-PIE database, the Rank-1 of ±90 o and ±75 o is increased by 18.5% (DVG 83.9% -Baseline 65.4%) and 4.3% (DVG 97.3% -Baseline 93.0%), respectively. We will continue to explore more applications in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper has developed a novel dual variational generation framework that generates large-scale new paired heterogeneous images with abundant intra-class diversity from noise, providing a new insight into the problems of HFR. A dual variational autoencoder is first proposed to learn a joint distribution of paired heterogeneous images. Then, both the distribution alignment in the latent space and the pairwise distance constraint in the image space are utilized to ensure the identity consistency of the generated image pairs. Finally, DVG generates diverse paired heterogeneous images with the same identity from noise to boost HFR network. Extensive qualitative and quantitative experimental results on four databases have shown the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The dual generation results via noise (256 × 256 resolution). For each pair, the left is NIR and the right is the paired VIS image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 ) 2 )</head><label>22</label><figDesc>and p(z (i) V ) = N (u (i) V , σ (i) V , the 2-Wasserstein distance between p(z (i) N ) and p(z (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visual comparisons of dual image generation results on the CASIA NIR-VIS 2.0 database. The generated paired images of DVG are more similar than those of CoGAN and VAE.OuluBUAA CUFSF The dual generation results on the Oulu-CASIA NIR-VIS, the BUAA-VisNir and the CUHK Face Sketch FERET (CUFSF) databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental analyses on the CASIA NIR-VIS 2.0 database. The backbone is LightCNN-9.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. In addition, LightCNN-9 and LightCNN-29 are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with other state-of-the-art deep HFR methods on the CASIA NIR-VIS 2.0, the Oulu-CASIA NIR-VIS, the BUAA-VisNir and the IIIT-D Viewed Sketch databases. The ROC curves on the CASIA NIR-VIS 2.0, the Oulu-CASIA NIR-VIS and the BUAA-VisNir databases, respectivelyFor the most challenging CASIA NIR-VIS 2.0 database, it is obvious that DVG outperforms other state-of-the-art methods. We first employ LightCNN-9 as the backbone to perform DVG, which obtains 99.2% on Rank-1 accuracy and 98.8% on VR@FAR=0.1%. Further, when backbone changed to more powerful LightCNN-29, DVG obtains 99.8% on Rank-1 accuracy and 99.8% on VR@FAR=0.1%. Moreover, for BUAA-VisNir Face database, DVG obtains 99.3% on Rank-1 accuracy and 97.3% on VR@FAR=0.1%, which outperforms our baseline LightCNN-29 and other state-of-the-art methods.</figDesc><table><row><cell></cell><cell>1.000</cell><cell>(a) CASIA NIR-VIS 2.0 ROC</cell><cell></cell><cell>1.0</cell><cell>(b) Oulu-CASIA NIR-VIS ROC</cell><cell>1.000</cell><cell>(c) BUAA-VisNir ROC</cell></row><row><cell></cell><cell>0.975</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.975</cell></row><row><cell>True Positive Rate</cell><cell cols="2">10 5 10 4 10 3 10 2 10 1 10 0 False Positive Rate 0.800 0.825 0.850 0.875 0.900 0.925 0.950 DVG DVR TRIVET IDR WCNN</cell><cell>True Positive Rate</cell><cell cols="2">10 5 10 4 10 3 10 2 10 1 10 0 False Positive Rate 0.3 0.4 0.5 0.6 0.7 0.8 DVG DVR TRIVET IDR WCNN</cell><cell>True Positive Rate</cell><cell>False Positive Rate 10 5 10 4 10 3 10 2 10 1 10 0 0.800 0.825 0.850 0.875 0.900 0.925 0.950 DVG DVR TRIVET IDR WCNN</cell></row><row><cell cols="2">Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>presents the performance of DVG on these two challenging low-shot HFR databases. For the Oulu-CASIA NIR-VIS database, we observe that DVG with LightCNN-29 significantly boosts the performance from 84.9%<ref type="bibr" target="#b34">[35]</ref> to 92.9% on VR@FAR=0.1%. Besides, for the IIIT-D Viewed Sketch Face database, DVG also obtains 96.99% on Rank-1 accuracy and 97.86% on VR@FAR=1%, which significantly outperform our baseline lightCNN-29 and state-of-the-art methods including RCN and MC-CNN by a large margin.Fig. 6 presents the ROC curves, including TRIVET, IDR, W-CNN, DVR, and the proposed DVG. To better demonstrate the results, we only perform ROC curves of DVG trained on LightCNN-29. It is obvious that DVG outperforms other state-of-the-art methods, especially on the low shot heterogeneous databases such as the Oulu-CASIA NIR-VIS database.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is funded by the National Natural Science Foundation of China (Grants No. 61622310) and Beijing Natural Science Foundation (Grants No. JQ18017).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Memetic approach for matching sketches with digital face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Himanshu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vatsa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual component convolutional neural networks for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Residual compensation networks for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High fidelity face manipulation with extreme pose and expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12003</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Qian Zhang, and Ran He</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of face recognition system in heterogeneous environments (visible vs nir)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaditya</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ho</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning invariant deep representation for NIR-VIS face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wasserstein CNN: learning invariant features for NIR-VIS face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ran He, and Zhenan Sun. Pose-guided photorealistic face rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The BUAA-VisNir face database instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introvae: Introspective variational autoencoders for photographic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition by margin-based cross-modality metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning mappings for face synthesis from near infrared to visual light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nir-vis heterogeneous face recognition via crossspectral joint dictionary learning and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipan</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Not afraid of the dark: Nir-vis face recognition via cross-spectral hallucination and low-rank embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The casia nir-vis 2.0 face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transferring deep representation for nir-vis heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dlface: Deep local descriptor for cross-modality face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seeing the forest from the trees: A holistic approach to near-infrared heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesung</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Sahasrabudhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rıza Alp Güler, Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial discriminative heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TIFS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Disentangled variational representation for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Synthesis of high-quality visible faces from polarimetric thermal faces using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">S</forename><surname>Riggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuowen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coupled information-theoretic encoding for face photosketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards pose invariant face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karlekar</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiri</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Junliang Xing, Shuicheng Yan, and Jiashi Feng. 3d-aided deep pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayashree</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiri</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
