<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Architecture Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-22">22 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Sreekumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Vishnu</forename><forename type="middle">Naresh</forename><surname>Boddeti</surname></persName>
						</author>
						<title level="a" type="main">Neural Architecture Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-22">22 Mar 2021</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Networks</term>
					<term>Neural Architecture Search</term>
					<term>AutoML</term>
					<term>Transfer Learning</term>
					<term>Evolutionary Algorithms</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has emerged as a promising avenue for automatically designing task-specific neural networks. Existing NAS approaches require one complete search for each deployment specification of hardware or objective. This is a computationally impractical endeavor given the potentially large number of application scenarios. In this paper, we propose Neural Architecture Transfer (NAT) to overcome this limitation. NAT is designed to efficiently generate task-specific custom models that are competitive under multiple conflicting objectives. To realize this goal we learn task-specific supernets from which specialized subnets can be sampled without any additional training. The key to our approach is an integrated online transfer learning and many-objective evolutionary search procedure. A pre-trained supernet is iteratively adapted while simultaneously searching for task-specific subnets. We demonstrate the efficacy of NAT on 11 benchmark image classification tasks ranging from large-scale multi-class to small-scale fine-grained datasets. In all cases, including ImageNet, NATNets improve upon the state-of-the-art under mobile settings (≤ 600M Multiply-Adds). Surprisingly, small-scale fine-grained datasets benefit the most from NAT. At the same time, the architecture search and transfer is orders of magnitude more efficient than existing NAS methods. Overall, experimental evaluation indicates that, across diverse image classification tasks and computational objectives, NAT is an appreciably more effective alternative to conventional transfer learning of fine-tuning weights of an existing network architecture learned on standard datasets. Code is available at https://github.com/human-analysis/neural-architecture-transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE classification is a fundamental task in computer vision, where given a dataset and, possibly, multiple objectives to optimize, one seeks to learn a model to classify images. Solutions to address this problem fall into two categories: (a) Sufficient Data: A custom convolutional neural network architecture is designed and its parameters are trained from scratch using variants of stochastic gradient descent, and (b) Insufficient Data: An existing architecture designed on a large scale dataset, such as ImageNet <ref type="bibr" target="#b0">[1]</ref>, along with its pre-trained weights (e.g., VGG <ref type="bibr" target="#b1">[2]</ref>, ResNet <ref type="bibr" target="#b2">[3]</ref>), is finetuned for the task at hand. These two approaches have emerged as the mainstays of present day computer vision.</p><p>Success of the aforementioned approaches is primarily attributed to architectural advances in convolutional neural networks. Initial efforts at designing neural architectures relied on human ingenuity. Steady advances by skilled practitioners has resulted in designs, such as AlexNet <ref type="bibr" target="#b3">[4]</ref>, VGG <ref type="bibr" target="#b1">[2]</ref>, GoogLeNet <ref type="bibr" target="#b4">[5]</ref>, ResNet <ref type="bibr" target="#b2">[3]</ref>, DenseNet <ref type="bibr" target="#b5">[6]</ref> and many more, which have led to performance gains on the ImageNet Large Scale Visual Recognition Challenge <ref type="bibr" target="#b0">[1]</ref>. In most other cases, a recent large scale study <ref type="bibr" target="#b6">[7]</ref> has shown that, across many tasks, transfer learning by finetuning ImageNet pre-trained networks outperforms networks that are trained from scratch on the same data.</p><p>Moving beyond manually designed network architectures, Neural Architecture Search (NAS) <ref type="bibr">[8]</ref> seeks to automate this process and find not only good architectures, but also their associated weights for a given image classification task. This goal has led to notable improvements in convolutional neural network Z. Lu is with Southern University of Science and Technology, Shenzhen, China. The majority of this work was done when Z. <ref type="bibr">Lu</ref>  architectures on standard image classification benchmarks, such as ImageNet, CIFAR-10 <ref type="bibr" target="#b8">[9]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref> etc., in terms of predictive performance, computational complexity and model size. However, apart from transfer learning by fine-tuning the weights, current NAS approaches have failed to deliver new models for both weights and topology on custom non-standard datasets. The key barrier to realizing the full potential of NAS is the large data and computational requirements for employing existing NAS algorithms on new tasks.</p><p>In this paper, we introduce Neural Architecture Transfer (NAT) to breach this barrier. Given an image classification task, NAT obtains custom neural networks (both topology and weights), optimized for possibly many conflicting objectives, and does so without the steep computational burden of running NAS for each new task from scratch. A single run of NAT efficiently obtains multiple custom neural networks spanning the entire trade-off front of objectives.</p><p>Our solution builds upon the concept of a supernet <ref type="bibr" target="#b9">[10]</ref> which comprises of many subnets. All subnets are trained simultaneously through weight sharing, and can be sampled very efficiently. This procedure decouples the network training and the search phases of NAS. A many-objective 1 search can then be employed on top of the supernet to find all network architectures that provide the best trade-off among the objectives. However, training such supernets for each task from scratch is very computationally and data intensive. The key idea of NAT is to leverage an existing supernet and efficiently transfer it into a task-specific supernet, whilst simultaneously searching for architectures that offer the best tradeoff between the objectives of interest. Therefore, unlike standard supernet-based NAS, we combine supernet transfer learning with the search process. At the conclusion of this process, NAT returns <ref type="figure">Fig. 1</ref>: Overview: Given a dataset and objectives to optimize, NAT designs custom architectures spanning the objective trade-off front. NAT comprises of two main components, supernet adaptation and evolutionary search, that are iteratively executed. NAT also uses an online accuracy predictor model to improve its computational efficiency.</p><p>(i) subnets that span the entire objective trade-off front, and (ii) a task-specific supernet. The latter can now be utilized for all future deployment-specific NAS, i.e., new and different hardware or objectives, without any additional training.</p><p>The core of NAT's efficiency lies in only adapting the subnets of the supernet that will lie on the efficient trade-off front of the new dataset, instead of all possible subnets. But, the structure of the corresponding subnets is unknown before adaptation. We resolve this "chicken-and-egg problem" by adopting an online procedure that alternates between the two primary stages of NAT: (a) supernet adaptation of subnets that are at the current trade-off front, and (b) evolutionary search for subnets that span the manyobjective trade-off front. A pictorial overview of the entire NAT method is shown in <ref type="figure">Fig.1</ref>.</p><p>In the adaptation stage, we first construct a layer-wise empirical distribution from the promising subnets returned by evolutionary search. Then, subnets sampled from this distribution are fine-tuned. In the search stage, to improve the efficiency of the search, we adopt a surrogate model to quickly predict the objectives of any sampled subnet without a full-blown and costly evaluation. Furthermore, the predictor model itself is also learned online from previously evaluated subnets. We alternate between these two stages until our computational budget 2 is exhausted.</p><p>The key contributions of this paper are:</p><p>-We introduce Neural Architecture Transfer as a NAS-powered alternative to fine-tuning based transfer learning. NAT is powered by a simple, yet highly effective online supernet fine-tuning and online accuracy predicting surrogate model.</p><p>-We demonstrate the scalability and practicality of NAT on multiple datasets corresponding to different scenarios; large-scale multi-class (ImageNet <ref type="bibr" target="#b0">[1]</ref>, CINIC-10 <ref type="bibr" target="#b11">[12]</ref>), medium-scale multiclass (CIFAR-10, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>), small-scale multi-class (STL-10 <ref type="bibr" target="#b12">[13]</ref>), large-scale fine-grained (Food-101 <ref type="bibr" target="#b13">[14]</ref>), medium-scale fine-grained (Stanford Cars <ref type="bibr" target="#b14">[15]</ref>, FGVC Aircraft <ref type="bibr" target="#b15">[16]</ref>) and smallscale fine-grained (DTD <ref type="bibr" target="#b16">[17]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref>, Oxford Flowers102 <ref type="bibr" target="#b18">[19]</ref>) datasets.</p><p>-Under mobile settings (≤ 600M MAdds), NATNets lead to state-of-the-art performance across all these tasks. For instance, on ImageNet, NATNet achieves a Top-1 accuracy of 80.5% at 600M MAdds. <ref type="bibr" target="#b1">2</ref>. We manually set the computational budget to a maximum of 1 day on a 8-GPU (NVIDIA 2080Ti) server. This is equivalent to the computational resources available to a small lab.</p><p>-We also demonstrate the utility of NAT in searching for a backbone for semantic segmentation, a dense prediction task. On Cityscapes <ref type="bibr" target="#b19">[20]</ref>, NAT matches the mIoU performance of Auto-DeepLab <ref type="bibr" target="#b20">[21]</ref> while using 4× fewer MAdds.</p><p>-Finally we demonstrate the scalability and utility of NAT across many objectives and on dense image prediction. Optimizing accuracy, model size and one of MAdds, CPU or GPU latency, NATNets dominate MobileNetV3 <ref type="bibr" target="#b21">[22]</ref> across all objectives. We also consider a 12 objective problem of finding a common architecture across eleven datasets while minimizing MAdds. The best trade-off NATNet dominates all models across these datasets under mobile settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recent years have witnessed growing interests in neural architecture search. The promise of being able to automatically search for task-dependent network architectures is particularly appealing as deep neural networks are widely deployed in diverse applications and computational environments. Early methods <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> made efforts to simultaneously evolve the topology of neural networks along with weights and hyperparameters. These methods perform competitively with hand-crafted networks on simple control tasks with shallow fully connected networks. Recent efforts <ref type="bibr" target="#b34">[35]</ref> primarily focus on designing deep convolutional neural network architectures.</p><p>The development of NAS largely happened in two phases. Starting from NASNet <ref type="bibr">[8]</ref>, the focus of the first wave of methods was primarily on improving the predictive accuracy of CNNs including Block-QNN <ref type="bibr" target="#b35">[36]</ref>, Hierarchical NAS <ref type="bibr" target="#b36">[37]</ref>, and AmoebaNet <ref type="bibr" target="#b37">[38]</ref>, etc. These methods relied on Reinforcement Learning (RL) or Evolutionary Algorithm (EA) to search for an optimal modular structure that is repeatedly stacked together to form a network architecture. The search was typically carried out on relatively small-scale datasets (e.g. CIFAR-10/100 <ref type="bibr" target="#b8">[9]</ref>), following which the best architectures were transferred to ImageNet for validation. A steady stream of improvements over state-of-the-art on numerous datasets were reported. The focus of the second wave of NAS methods was on improving the search efficiency.</p><p>A few methods have also been proposed to adapt NAS to other scenarios. These include meta-learning based approaches <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> with application to few-shot learning tasks. XferNAS <ref type="bibr" target="#b40">[41]</ref> and EAT-NAS <ref type="bibr">[42]</ref> illustrate how architectures can be transferred between similar datasets or from smaller to larger datasets. Some approaches <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> proposed RL-based NAS methods that  <ref type="bibr">[23]</ref> SBMO C10 DARTS <ref type="bibr" target="#b23">[24]</ref> gradient C10 LEMONADE <ref type="bibr" target="#b24">[25]</ref> EA C10, C100, ImageNet64 ProxylessNAS <ref type="bibr" target="#b25">[26]</ref> RL / gradient search on multiple tasks during training and transfer the learned search strategy, as opposed to searched networks, to new tasks at inference. Next, we provide short overviews on methods that are closely related to the technical approach in this paper. <ref type="table" target="#tab_1">Table 1</ref> provides a comparative overview of NAT to existing NAS approaches.</p><p>Performance Prediction: Evaluating the performance of an architecture requires a computationally intensive process of iteratively optimizing model weights. To alleviate this computational burden, regression models have been learned to predict an architecture's performance without actually training it. Baker et al. <ref type="bibr" target="#b44">[45]</ref> use a radial basis function to estimate the final accuracy of architectures from its accuracy in the first 25% of training iterations. PNAS <ref type="bibr">[23]</ref> uses a multilayer perceptron (MLP) and a recurrent neural network to estimate the expected improvement in accuracy if the current modular structure (which is later stacked together to form a network) is expanded with a new branch. Conceptually, both of these methods seek to learn a prediction model that extrapolate (rather than interpolate), resulting in poor correlation in prediction.</p><p>OnceForAll <ref type="bibr" target="#b30">[31]</ref> also uses a MLP to predict accuracy from architecture encoding. However, the model is trained offline for the entire search space, thereby requiring a large number of samples for learning (16K samples -&gt; 2 GPU-days for just constructing the surrogate model). Instead of using uniformly sampled architectures to train the prediction model to approximate the entire landscape, ChamNet <ref type="bibr" target="#b28">[29]</ref> trains many architectures through full SGD and selects only 300 samples of high accuracy with diverse efficiency (Multiply-adds, Latency, Energy) to train a prediction model offline. In contrast, NAT learns a prediction model in an online fashion only on the samples at the current trade-off front as we explore the search space. Such an approach only needs to interpolate over a much smaller space of architectures constituting the current trade-off front. Consequently, this procedure significantly improves both the accuracy and the sample complexity of constructing the prediction model.</p><p>Weight Sharing: Approaches in this category involve training a supernet that contains all searchable architectures as its subnets. They can be broadly classified into two categories depending on whether the supernet training is coupled with architecture search or decoupled into a two-stage process. Approaches of the former kind <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref> are computationally efficient but return suboptimal models. Numerous studies <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> allude to weak correlation between performance at the search and final evaluation stages. Methods of the latter kind <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b49">[50]</ref> use performance of subnets (obtained by sampling the trained supernet) as a metric to select architectures during search. However, training a supernet beforehand for each new task is computationally prohibitive. In this work, we take an integrated approach where we train a supernet on large-scale datasets (e.g. ImageNet) once and couple it with our architecture search to quickly adapt it to a new task. An elaborated discussion connecting our method to existing approaches is provided in Section A.</p><p>Multi-Objective NAS: Methods that consider multiple objectives for designing hardware specific models have also been developed. The objectives are optimized either through (i) scalarization, or (ii) Pareto-based solutions. The former include, ProxylessNAS <ref type="bibr" target="#b25">[26]</ref>, MnasNet <ref type="bibr" target="#b26">[27]</ref>, ChamNet <ref type="bibr" target="#b28">[29]</ref>, MobileNetV3 <ref type="bibr" target="#b21">[22]</ref>, and FBNetV2 <ref type="bibr" target="#b31">[32]</ref> which use a scalarized objective or an additional constraint to encourage high accuracy and penalize compute inefficiency at the same time, e.g., maximize Acc * (Latency/T arget) −0.07 . Conceptually, the search of architectures is still guided by a single objective and only one architecture is obtained per search. Empirically, multiple runs with different weighting of the objectives are needed to find an architecture with the desired trade-off, or multiple architectures with different complexities. Methods in the latter category include <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> and aim to approximate the entire Pareto-efficient frontier simultaneouslyi.e. multiple architectures with different complexities are obtained in a single run. These approaches rely on heuristics (e.g., EA) to efficiently navigate the search space allowing practitioners to visualize the trade-off between the objectives and to choose a suitable network a posteriori to the search. NAT falls into the latter category and uses an accuracy prediction model and weight sharing for efficient architecture transfer to new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>Neural Architecture Transfer consists of three main components: an accuracy predictor, an evolutionary search routine, and a supernet. NAT starts with an archive A of architectures (subnets) created by uniform sampling from our search space. We evaluate the performance f i of each subnet (a i ) using weights inherited from the supernet. The accuracy predictor is then constructed from (a i , f i ) pairs which (jointly with any additional objectives provided by the user) drives the subsequent many-objective evolutionary search towards optimal architectures. Promising architec-tures at the conclusion of the evolutionary process are added to the archive A. The (partial) weights of the supernet corresponding to the top-ranked subnets in the archive are fine-tuned. NAT repeats this process for a pre-specified number of iterations. At the conclusion, we output both the archive and the task-specific supernet. Networks that offer the best trade-off among the objectives can be post-selected from the archive. Detailed descriptions of each component of NAT are provided in the following subsections. <ref type="figure">Figure 1</ref> and Algorithm 1 provide an overview of our entire approach.   Sw ← Adapt(Sw, A, Dtrn, E) ⊳ Algo. 5 <ref type="bibr" target="#b13">14</ref> t ← t + 1 15 end 16 // optional in case of no preferences from users. <ref type="bibr" target="#b16">17</ref> A * ← choose a subset of archs from A based on trade-offs by method presented in Section C. <ref type="bibr" target="#b17">18</ref> Return Sw, A, A * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The problem of neural architecture search for a target dataset D = {D trn , D vld , D tst } with many objectives can be formulated as the following bilevel optimization problem <ref type="bibr" target="#b54">[55]</ref>, minimize F (a) = f 1 (a; w * (a)), . . . , f m (a; w * (a)) T , subject to w * (a) ∈ arg min L(w; a), a ∈ Ω a , w ∈ Ω w , (1) where the upper-level variable a defines a candidate architecture, and the lower-level variable w(a) denotes its associated weights. L(w; a) is the cross-entropy loss on the training data D trn for an architecture a. F : Ω → R m constitutes m (user-) desired, possibly competing, objectives-e.g., predictive performance on validation data D vld , number of parameters (#Params), multiplyadds (#MAdds), latency / power consumption / memory footprint on specific hardware etc.</p><p>The bi-level optimization is typically solved in an iterative fashion, with an inner optimization loop over the weights of the network for a given architecture, and an outer optimization loop over the network architectures themselves. The computational challenge of solving this problem stems from both the upper and lower level optimization. Learning optimal weights of a network in the lower level necessitates costly iterations of stochastic gradient descent over multiple epochs. Similarly, searching the optimal architecture on the upper level is prohibitive due to the discrete nature of the architecture description, size of search space and our desire to optimize many, possibly conflicting, objectives.  R 192 · · · 224 · · · 256 E 0 · · · 8 · · · 16</p><formula xml:id="formula_0">W 1.0 1.2 E 0 1 Li Skip E = 3, K = 3 E = 3, K = 5 . . . E = 6, K = 7 E 0 1 2 . . . 9 (b) Encoding Fig. 2:</formula><p>The architectures in our search space are variants of MobileNetV2 family of models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b55">[56]</ref>. (a) Each networks consists of five stages. Each stage has two to four layers. Each layer is an inverted residual bottleneck block. The search space includes, input image resolution (R), width multiplier (W), the number of layers in each stage, the # of output channels (expansion ratio E) of the first 1 × 1 convolution and the kernel size (K) of the depth-wise separable convolution in each layer. <ref type="bibr">(b)</ref> Networks are represented as 22-integer strings, where the first two correspond to resolution and width multiplier, and the rest correspond to the layers. Each value indicates a choice, e.g. the third integer (L 1 ) takes a value of "1" corresponds to using expansion ratio of 3 and kernel size of 3 in layer 1 of stage 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Space and Encoding</head><p>The search for optimal network architectures can be performed over many different search spaces. The generality of the chosen search space has a major influence on the quality of results that are feasible. We adopt a modular design for overall structure of the network, consisting of a stem, multiple stages and a tail (see <ref type="figure">Fig. 2a</ref>). The stem and tail are common to all networks and not searched. Each stage in turn comprises of multiple layers, and each layer itself is an inverted residual bottleneck structure <ref type="bibr" target="#b55">[56]</ref>.</p><p>-Network: We search for the input image resolution and the width multiplier (a factor that scales the # of output channels of each layer uniformly <ref type="bibr" target="#b56">[57]</ref>). Following previous work <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, we segment the CNN architecture into five sequentially connected stages. The stages gradually reduce the feature map size and increase the number of channels ( <ref type="figure">Fig. 2a</ref> Left).</p><p>-Stage: We search over the number of layers, where only the first layer uses stride 2 if the feature map size decreases, and we allow each block to have minimum of two and maximum of four layers ( <ref type="figure">Fig. 2a</ref> Middle).</p><p>-Layer: We search over the expansion ratio (between the # of output and input channels) of the first 1 × 1 convolution and the kernel size of the depth-wise separable convolution ( <ref type="figure">Fig. 2a</ref> Right).  Overall, we search over four primary hyperparameters of CNNs i.e., the depth (# of layers), the width (# of channels), the kernel size, and the input resolution. The resulting volume of our search space is approximately 3.5 × 10 19 for each combination of image resolution and width multiplier.</p><p>To encode these architectural choices, we use an integer string of length 22, as shown in <ref type="figure">Fig. 2b</ref>. The first two values represent the input image resolution and width multiplier, respectively. The remaining 20 values denote the expansion ratio and kernel size settings for each of the 20 layers. The available options for expansion ratio and kernel size are <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> and <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, respectively. It is worth noting that we sort the layer settings in ascending #MAdds order, which is beneficial to the mutation operator used in our evolutionary search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accuracy Predictor</head><p>The main computational bottleneck of NAS arises from the nested nature of the bi-level optimization problem. The inner optimization requires the weights of the subnets to be thoroughly learned prior to evaluating its performance. Methods like weight-sharing <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref> allow sampled subnets to inherit weights among themselves or from a supernet, avoiding the time-consuming process (typically requiring hours) of learning weights through SGD. However, standalone weight-sharing still requires inference on validation data (typically requiring minutes) to assess performance. Therefore, simply having to evaluate the subnets can still render the overall process computationally prohibitive for methods <ref type="bibr">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref> that sample thousands of architectures during search.</p><p>To mitigate the computational burden of fully evaluating the subnets, we adopt a surrogate accuracy predictor that regresses the performance of a sampled subnet without performing training or inference. By learning a functional relation between the integerstrings (subnets in the encoded space) and the corresponding performance, this approach decouples the evaluation of an architecture from data-processing (including both SGD and inference). Consequently, the evaluation time reduces from hours/minutes to seconds. We illustrate this concept in <ref type="figure" target="#fig_4">Fig. 3</ref>. The effectiveness of this idea, however, is critically dependent on the quality of the surrogate model. Below we identify three desired properties of such a model: 1) Reliable prediction: high rank-order correlation 3 between predicted and true performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Accuracy Predictor (RBF Ensemble)</head><p>Input : Training data X, training targets Y , ensemble size K 1 k ← 0 // initialize an counter. 2 pool ← ∅ // initialize a pool to store all models. 3 while k &lt; K do 4 (X,Ỹ ) ← randomly create a subset of the training data. <ref type="bibr" target="#b4">5</ref> idx ← randomly pick a subset of the features in training data. <ref type="bibr" target="#b5">6</ref> rbf ← fit a RBF model fromX[:, idx] andỸ . <ref type="bibr" target="#b6">7</ref> pool ← pool ∪ (rbf, idx) // append the fitted model to the pool. <ref type="bibr">8</ref> k ← k + 1 9 end 10 Return a pool of K RBF models. Current approaches <ref type="bibr">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref> that use surrogate based accuracy predictors, however, do not satisfy property <ref type="formula">(1)</ref> and <ref type="formula">(3)</ref> simultaneously. For instance, PNAS [23] uses 1,160 subnets to build the surrogate but only achieves a rank-order correlation of 0.476. Similarly, OnceForAll <ref type="bibr" target="#b30">[31]</ref> uses 16,000 subnets to build the surrogate. The poor sample complexity and rank-order correlation of these approaches, is due to the offline learning of the surrogate model. Instead of focusing on models that are at the trade-off front of the objectives, these surrogate models are built for the entire search space. Consequently, these methods require a significantly larger and more complex surrogate model.</p><p>We overcome the aforementioned limitation by restricting the surrogate model to the search space that constitutes the current objective trade-off. Such a solution significantly reduces the sample complexity of the surrogate and increases the reliability of its predictions. We adopt four low-complexity predictors, namely, Gaussian Process (GP) <ref type="bibr" target="#b28">[29]</ref>, Radial Basis Function (RBF) <ref type="bibr" target="#b44">[45]</ref>, Multilayer Perceptron (MLP) <ref type="bibr">[23]</ref>, and Decision Tree (DT) <ref type="bibr" target="#b57">[58]</ref>. Empirically, we observe that RBFs are consistently better than the other three models if the # of training samples is more than 100. To further improve RBF's performance, especially under a high sample efficiency regime, we construct an ensemble of RBF models. As outlined in Algorithm 2, each RBF model is constructed with a subset of samples and features randomly selected from the training instances. The correlation between predicted accuracy and true accuracy from an ensemble of 500 RBF models outperforms all</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3: Evolutionary Search</head><p>Input : Accuracy predictor S f , additional objectivesf , archive of archs A, max. # of generations G, population size K, crossover probability pc, mutation probability pm. 1 g ← 0 // initialize an generation counter. <ref type="bibr" target="#b1">2</ref> f ← S f (A) // compute accuracy of all archs in archive. 3 P ← Selection(A, f,f (A), K) // initialize the parent population with top-K ranked archs from A. 4 while g &lt; G do 5 // choose parents through tournament selection for mating. <ref type="bibr" target="#b5">6</ref> P ← Binary Tournament Selection(P ) 7 // create offspring population by crossover between parents. <ref type="bibr">8</ref> Q ← Crossover(P, pc) 9 // induce randomness to offspring population through mutation. <ref type="bibr" target="#b9">10</ref> Q ← Mutation(Q, pm) <ref type="bibr" target="#b10">11</ref> R ← P ∪ Q // merge parent and offspring population. <ref type="bibr" target="#b11">12</ref> // survive the top-K archs to next generation.</p><formula xml:id="formula_1">13 P ← Selection(R, S f (R),f (R), K) 14 g ← g + 1 15 end 16 Return parent population P .</formula><p>other models across all regimes. <ref type="figure" target="#fig_5">Fig. 4</ref> compares the performance of the different surrogate models we considered. Practically, we observed that the RBF ensemble can be learned under a minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Many-Objective Evolutionary Search</head><p>Given the accuracy predictor, we employ a customized evolutionary algorithm (EA) to search for optimal architectures that offer the best trade-off between many objectives. The EA is an iterative process in which initial architectures, selected from the archive of previously explored architectures, are gradually improved as a group, referred to as a population. In every generation (iteration), a group of offspring (i.e., new architectures) are created by applying variations through crossover and mutation (described below) operations on the most promising architectures, also known as parents, found so far in the population. Every member of the population, i.e., both parents and offspring, competes for survival and reproduction (becoming a parent) in each generation. See <ref type="figure">Fig. 1</ref> (bottom right shaded in green) for a pictorial overview, and Algorithm 3 for the pseudocode.</p><p>Crossover exchanges information between two or more population members to create two or more new members. Designing an effective crossover between non-standard solution representations can be difficult and has been largely ignored by existing EA-based NAS algorithms <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Here we adopt a customized, homogeneous crossover that uniformly picks integers from parent architectures to create offspring architectures. This crossover operator offers two properties: (1) it preserves common integers shared between parents; and (2) it is free of additional hyperparameters. <ref type="figure" target="#fig_6">Fig. 5a</ref> visualizes our implementation of the crossover operation. We generate two offspring architectures with each crossover, and an offspring population of the same size as the parent population is generated in each generation.</p><p>Mutation is a local operator that perturbs a solution to produce a new solution in its vicinity. In this work, we use a discretized version of the polynomial mutation (PM) operator <ref type="bibr" target="#b59">[60]</ref> and apply it to every solution created by the crossover operator. For a given architecture a, PM is carried out integer-wise with probability p m , and the mutated i th integer, a i , of the mutated offspring is:</p><formula xml:id="formula_2">a ′ i =    ai + ((2u) 1/(1+ηm ) − 1)(ai − a (L) i ), for u ≤ 0.5, ai + (1 − 2(1 − u) 1/(1+ηm) )(a (U ) i − ai), for u &gt; 0.5 (2) where u is a uniform random number in the interval [0, 1]. a (L) i and a (U) i</formula><p>are the lower and upper bounds of a i , respectively. Each mutated value in an offspring is rounded to the nearest integer. The PM operator inherits the parent-centric convention, in which the offspring are intentionally created around the parents. The centricity is controlled via an index hyperparameter η m . In particular, high-values of η m tend to create mutated offspring around the parent, and low-values encourage mutated offspring to be further away from the parent architecture. See <ref type="figure" target="#fig_6">Fig. 5b</ref> for a visualization of the effect of η m . It is the worth noting that the PM operator was originally proposed for continuous optimization where distances between variable values are naturally defined. In contrast, in context of our encoding, our variables are categorical in nature, indicating a particular layer hyperparameter. So we sort the searched subnets in ascending order of #MAdds, such that η m now controls the difference in #MAdds between the parent and the mutated offspring.</p><p>We apply PM to every member in the offspring population (created from crossover). We then merge the mutated offspring population with the parent population and select the top half using many-objective selection operator described in Algorithm 4. This procedure creates the parent population for the next generation. We repeat this overall process for a pre-specified number of generations and output the parent population at the conclusion of the evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Many-Objective Selection</head><p>In addition to high predictive accuracy, real-world applications demand NAS algorithms to simultaneously balance a few other conflicting objectives that are specific to the deployment scenarios. For instance, mobile or embedded devices often have restrictions in terms of model size, multiply-adds, latency, power consumption, and memory footprint. With no prior assumption on the correlation among these objectives, a scalable (to the number of objectives) selection is required to drive the search towards the high dimensional Pareto front. In this work, we adopt the reference point guided selection originally proposed in NSGA-III <ref type="bibr" target="#b10">[11]</ref>, which has been shown to be effective in handling problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4: Reference Point Based Selection</head><p>Input : A set of archs R, their objectives F , number of archs to select N , reference directions Z. 1 // put archs into different fronts (rank levels) based on domination.</p><formula xml:id="formula_3">2 (F 1 , F 2 , . . .) ← non dominated sort(F ) 3 S ← ∅, i ← 1 4 while |S|+|F i |&lt; N do S ← S ∪ F i ; i ← i + 1; 5 F L ← F i // next front is the split front where we cannot accommodate all archs associated with it. 6 if |S|+|F L |= N then S ← S ∪ F L ; 7 else 8 (S,F L ) ← Normalize(S, F L ) // normalize the objectives based</formula><p>the ideal and nadir points derived from R. <ref type="bibr" target="#b8">9</ref> d ← compute orthogonal dist to Z i for each i 10 ρ ← count #associated solutions for Z i based on d for each i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>// remaining archs from F L to fill up S. <ref type="figure">Fig. 6</ref>: (a) An example (assuming minimization of all objectives) of the selection process in Algo 4: We first create reference directions Z by joining reference points with the ideal solution (origin). Then through non dominated sort, three non-dominated solutions are identified, associated with reference directions Z (1) , Z <ref type="bibr" target="#b2">(3)</ref> and Z <ref type="bibr" target="#b4">(5)</ref> . We then select the remaining solutions by the orthogonal distances to the reference directions with no associated solutionsi.e. Z <ref type="bibr" target="#b1">(2)</ref> and Z <ref type="bibr" target="#b3">(4)</ref> . This selection is scalable to larger # of objectives. A tri-objective example is shown in <ref type="bibr">(b)</ref>.</p><formula xml:id="formula_4">12 S ← S ∪ Niching(F L , N − |S|, ρ, d) 13 end 14 Return S. (a) (b)</formula><p>with as many as 15 objectives. In the remainder of this section, we provide an overview of NSGA-III procedure and refer readers to the original publication for more details.</p><p>Domination is a widely-used partial ordering concept for comparing two objective vectors. For a generic many-objective optimization problem: min a {f 1 (a), . . . , f m (a)}, where f i (·) are the objectives (say, loss functions) to be optimized and a is the representation of a neural network architecture. For two given solutions a 1 and a 2 , solution a 1 is said to dominate a 2 (i.e., a 1 a 2 ) if following conditions are satisfied:</p><formula xml:id="formula_5">1) a 1 is no worse than a 2 for all objectives (f i (a 1 ) ≤ f i (a 2 ), ∀i ∈ {0, .</formula><p>. . , m}), and 2) a 1 is strictly better than a 2 in at least one objective</p><formula xml:id="formula_6">∃ i ∈ {0, . . . , m} | f i (a 1 ) &lt; f i (a 2 )).</formula><p>A solution a i is said to be non-dominated if these conditions hold against all the other solutions a j (with j = i) in the entire search space of a.</p><p>With the above definition, we can sort solutions to different ranks of domination, where solutions in the same rank are nondominated to each other, and there exists at least one solution in lower rank that dominates any solution in the higher rank. Thus, a lower non-dominated ranked set is lexicographically better than a higher ranked set. This process is referred as non dominated sort, and it is the first step in the selection process. During the manyobjective selection process, the lower ranked sets are chosen one at a time until no more sets can be included to maintain the population size. The final accepted set may have to be split to choose only a part. For this purpose, we choose the most diverse subset based on a diversity-maintaining mechanism. We first create a set of reference directions from a set of uniformly distributed (in (m − 1)-dimensional space) reference points in the unit simplex by using Das-and-Dennis method <ref type="bibr" target="#b60">[61]</ref>. Then we associate each solution to a reference direction based on orthogonal distance of the solution from the direction. Then, for every reference direction, we choose the closest associated solution in a systematic manner by adaptively computing a niche count ρ so that every reference direction gets an equal opportunity to choose a representative closest solution in the selected population. The domination and diversity-preserving procedures are easily scalable to any number of objectives and importantly are free from any user-defined hyperparameter. See Algorithm 4 for the pseudocode and <ref type="figure">Fig. 6</ref> for a graphical illustration. A more elaborated discussion on the necessity of the reference point based selection is provided in Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Supernet Adaptation</head><p>Instead of training every architectures sampled during search from scratch, NAS with weight sharing <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref> inherits weights from previously-trained networks or from a supernet. Directly inheriting the weights obviates the need to optimize the weights from scratch and speeds up the search from thousands of GPU days to only a few. In this work, we focus on the supernet approach <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>. It involves first training a large network model (in which searchable architectures become subnets) prior to the search. Then the performance of the subnets, evaluated with the inherited weights, is used to guide the selection of architectures during search. The key to the success of this approach is that the performance of the subnets with the inherited weights be highly correlated with the performance of the same subnet when thoroughly trained from scratch. Satisfying this desideratum necessitates that the supernet weights be learned in such a way that all subnets are optimized simultaneously.</p><p>Existing methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b52">[53]</ref> attempt to achieve the above goal by imposing fairness in training the supernet, where the probabilities of training any particular subnet for each batch of data is uniform in expectation. However, we argue that simultaneously training all the subnets in the search space is practically not feasible and, more importantly, not necessary. Firstly, it is evident from existing NAS approaches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b61">[62]</ref> that different objectives (#Params, #MAdds, latency on different hardware, etc.) require Based on the aforementioned observations, we propose a simple yet effective supernet training routine that only focuses on training the subnets recommended by the evolutionary search algorithm in Section 3.5. Specifically, we seek to exploit the knowledge gained from the search process so far. Recall that our algorithm uses an archive to keep track of the promising architectures explored so far. For each value in our architecture encoding, we construct a categorical distribution from architectures in the archive, where the probability for i th integer taking on the j value is computed as:</p><formula xml:id="formula_7">p(X i = j) =</formula><p># of architectures with option j at i th integer total # of architectures in the archive (3) In each training step (batch of data), we sample an integer-string from the above distribution <ref type="bibr" target="#b4">5</ref> . We then activate the sub parts of the supernet corresponding to the architecture decoded from the integer-string. Only weights corresponding to the activated sub parts in the supernet will be updated in each step. See Algorithm 5 for pseudocode. A more in-depth discussion connecting our proposed approach to the existing supernet-based NAS approaches is provided in Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>In this section, we present experimental results to evaluate the efficacy of Neural Architecture Transfer on multiple image classification tasks. In addition, we also investigate the scalability of our approach to more than two objectives. For all the experiments in this section, we use the same set of hyperparmaters (see <ref type="table" target="#tab_5">Table 2</ref>) for the different components of NAT. These choices were guided by the ablation studies described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We consider eleven image classification datasets for evaluation with sample size varying from 2,040 to 180,000 images (20 to 18,000 images per class; <ref type="table" target="#tab_6">Table 3</ref>). These datasets span a wide variety of image classification tasks, including superordinate-level recognition (ImageNet <ref type="bibr" target="#b0">[1]</ref>, CIFAR-10 <ref type="bibr" target="#b8">[9]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, CINIC-10 <ref type="bibr" target="#b11">[12]</ref>, STL-10 <ref type="bibr" target="#b12">[13]</ref>); fine-grained recognition (Food-101 <ref type="bibr" target="#b13">[14]</ref>, Stanford Cars <ref type="bibr" target="#b14">[15]</ref>, FGVC Aircraft <ref type="bibr" target="#b15">[16]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b3">4</ref>. For example, AmoebaNet <ref type="bibr" target="#b37">[38]</ref> samples a large number of 27K architectures which is still only about 10 −13 % of its search space. 5. A visualization of such distributions is shown in 20c. multi-class 1,281,167 50,000 1,000 CINIC-10 <ref type="bibr" target="#b11">[12]</ref> 180,000 9,000 10 CIFAR-10 <ref type="bibr" target="#b8">[9]</ref> 50,000 10,000 10 CIFAR-100 <ref type="bibr" target="#b8">[9]</ref> 50,000 10,000 10 STL-10 <ref type="bibr" target="#b12">[13]</ref> 5,000 8,000 10</p><p>Food-101 <ref type="bibr" target="#b13">[14]</ref> fine-grained 75,750 25,250 101 Stanford Cars <ref type="bibr" target="#b14">[15]</ref> 8,144 8,041 196 FGVC Aircraft <ref type="bibr" target="#b15">[16]</ref> 6,667 3,333 100 DTD <ref type="bibr" target="#b16">[17]</ref> 3,760 1,880 47 Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref> 3,680 3,369 37 Oxford Flowers102 <ref type="bibr" target="#b18">[19]</ref> 2,040 6,149 102</p><p>Oxford Flowers102 <ref type="bibr" target="#b18">[19]</ref>); and texture classification (DTD <ref type="bibr" target="#b16">[17]</ref>). We use the ImageNet dataset for training the supernet, and use the other ten datasets for architecture transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supernet Preparation</head><p>Our supernet is constructed by setting the architecture encoding at the maximum value, i.e. four layers in each stage and every layer uses expand ratio of six and kernel size of seven. Adapting subnets of a supernet with randomly initialized weights leads to training instability and large variance in its performance. Therefore, we warm-up the supernet weights on ImageNet following the progressive shrinking algorithm <ref type="bibr" target="#b30">[31]</ref>, where the supernet is first trained at full-scale, with subnets corresponding to different options (expand ratio, kernel size, # of layers) being gradually activated during the training process. This procedure, which takes about 6 days on a server with eight V100 GPUs, is optimized with only the cross-entropy loss i.e., a single objective. We note that supernet preparation expense is a one-time cost that amortizes over any subsequent transfer to different datasets and objective combinations we show in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ImageNet Classification</head><p>Before we evaluate our approach for architecture transfer to other datasets, we first validate its effectiveness on the ImageNet-1K dataset. This experiment evaluates the effectiveness of NAT in adapting and searching for architectures that span trade-off between two objectives. For this experiment, we consider accuracy and #MAdds as the two objective of interest. We randomly sample 50,000 images from the original ImageNet training set as the validation set to guide the architecture search. We run NAT for 30 iterations, and from the final archive of architectures, we select four models ranging from 200M MAdds to 600M MAdds (for high-end mobile devices). Following <ref type="bibr" target="#b30">[31]</ref>, we fine-tune 6 each model to further boost the performance. Our fine-tune training largely follows <ref type="bibr" target="#b26">[27]</ref>: RMSProp optimizer with decay 0.9 and momentum 0.9; batch normalization momentum 0.99; weight decay 1e-5. We use a batch size of 512 and an initial learning rate of 0.012 that gradually reduces to zero following the cosine annealing schedule. Our regularization settings are similar as in <ref type="bibr" target="#b27">[28]</ref>: we use augmentation policy <ref type="bibr" target="#b62">[63]</ref>, drop connect ratio 0.2, and dropout ratio 0.2. <ref type="table" target="#tab_7">Table 4</ref> shows the performance of NAT models obtained through bi-objective optimization of maximizing accuracy and minimizing #MAdds. Our models, referred to as NAT-M{1,2,3,4}, are in ascending order of #MAdds ( <ref type="figure" target="#fig_7">Fig. 7</ref>). <ref type="figure" target="#fig_8">Fig. 8</ref> shows the full #MAdds-accuracy trade-off curve comparison between NAT and existing NAS methods. <ref type="bibr" target="#b5">6</ref>. Section 5.5 studies the impact of this fine-tuning step.  <ref type="bibr" target="#b0">[1]</ref>: NATNets comparison with manual and automated design of efficient convolutional neural networks. Models are grouped into sections for better visualization. Our results are underlined and the best result in each section is in bold. CPU latency (batchsize=1) is measured on Intel i7-8700K and GPU latency (batchsize=64) is measured on 1080Ti. "WS" stands for weight sharing. All methods are under single crop and single model condition, without any additional data.  Results indicate that NATNets completely dominate (i.e. better in both #MAdds and accuracy) all existing designs, both manual and from other NAS algorithms, under mobile settings (≤ 600M MAdds). Compared to manually. designed networks, NAT is noticeably more efficient. NAT-M1 is 2.3% and 1.5% more accurate than MobileNetV3 <ref type="bibr" target="#b21">[22]</ref> and FBNetV2-F4 <ref type="bibr" target="#b31">[32]</ref> respectively, while being equivalent in efficiency (i.e. #MAdds, CPU and GPU latency). Furthermore, NATNets are consistently 6% more accurate than MobileNetV2 <ref type="bibr" target="#b55">[56]</ref> scaled by width multiplier from 200M to 600M #MAdds. Our largest model, NAT-M4, achieves a new state-of-the-art ImageNet top-1 accuracy of 80.5% under mobile settings (≤ 600M #MAdds). Interestingly, even though this experiment did not explicitly optimize for CPU or GPU latency, NATNets are faster than those (MobileNet-V3 <ref type="bibr" target="#b21">[22]</ref>, MNasNet <ref type="bibr" target="#b26">[27]</ref>) that explicitly do optimize for latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scalability to Datasets</head><p>Existing NAS approaches are rarely applied to datasets beyond standard ones (i.e. CIFAR-10 [9] and ImageNet <ref type="bibr" target="#b0">[1]</ref>), where the classification task is at superordinate-level and the # of training images are sufficiently large. Instead, they adopt a conventional transfer learning setup <ref type="bibr" target="#b6">[7]</ref>, in which the architectures found by searching on standard benchmark datasets are transferred as is, with weights fine-tuned to new datasets. We argue that such a process is conceptually contradictory to the goal of NAS. The architectures transferred from standard datasets are sub-optimal either with respect to accuracy, efficiency or both. On the other hand, by transferring both architecture and weights NAT can indeed design bespoke models for each dataset.</p><p>We evaluated NAT on ten image classification datasets (see <ref type="table" target="#tab_6">Table 3</ref>) that present different challenges in terms of diversity in classes (superordinate vs. fine-grained) and size of training set (large vs small). For each dataset, we run NAT with two objectives: maximize top-1 accuracy on validation data (20% randomly separated from the training set) and minimize #MAdds. We start from the supernet trained on ImageNet (which is created once before all experiments; see Section 4.2) and adapt it to the new dataset. During this procedure, the last linear layer is reset depending on the number of categories in the new dataset. NAT is now applied for a total of 30 iterations. In each iteration the supernet is adapted for 5 epochs using SGD with a momentum of 0.9. The learning rate is initialized at 0.01 and annealed to zero in 150 epochs (30 iterations with five epochs in each). All hyperparameters are set at default values from <ref type="table" target="#tab_5">Table 2</ref>. For each dataset, the overall NAT process takes slightly under a day on a server with eight 2080Ti GPUs. <ref type="figure" target="#fig_9">Fig. 9</ref> shows the accuracy and #MAdds trade-off for each dataset across a wide range of models, including NATNets, existing NAS and hand-designed models. Across all datasets, NATNets consistently achieve better accuracy while being an order of magnitude more efficient (#MAdds) than existing models, suggesting that searching directly on the targeted datasets is a more effective alternative to the conventional transfer learning that fine-tunes weights of architectures learned on standard datasets (i.e. ImageNet and CIFAR-10). Under mobile settings (≤ 600M), NATNets achieve the state-of-the-art on these datasets, and a new state-of-the-art accuracy on both STL-10 <ref type="bibr" target="#b12">[13]</ref> and CINIC-10 7 <ref type="bibr" target="#b11">[12]</ref> datasets. Surprisingly, on small scale datasets e.g. Oxford Flowers102 <ref type="bibr" target="#b18">[19]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref>, DTD <ref type="bibr" target="#b16">[17]</ref> and STL-10 <ref type="bibr" target="#b12">[13]</ref>, we observe that NATNets are significantly more effective than conventional fine-tuning. Even on fine-grained datasets such as Stanford Cars and FGVC aircraft, where conventional finetuning did not improve upon training from scratch, NATNets improve accuracy while also being significantly more efficient. <ref type="figure" target="#fig_10">Fig. 10</ref> shows a visualization of architectures with 350M MAdds for each dataset. The lack of similarity in the networks suggest that different datasets require different architectures to be efficient in accuracy-MAdds, and NAT is able to generate these 7. According to <ref type="bibr" target="#b69">[70]</ref> for STL-10, and <ref type="bibr" target="#b70">[71]</ref> for CINIC-10. customized networks for each dataset. Additional visualization of architectures searched on all datasets is provided in Section E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scalability to Objectives</head><p>Practical applications of NAS can rarely be considered from the point of view of a single objective, and most often, they must be evaluated from many different, possibly competing, objectives. We demonstrate the scalability of NAT to more than two objectives, and evaluate its effectiveness.</p><p>We use NAT to simultaneously optimize for three objectivesnamely, model accuracy on ImageNet, model size (#params), and model computational efficiency. We consider three different metrics to quantify computational efficiency-#MAdds, CPU latency, and GPU latency. In total, we run three instances of three-objective search-i.e. maximize accuracy, minimize #params, and minimize one of #MAdds, CPU latency or GPU latency. We follow the settings from the ImageNet experiment in Section 4.3, except the fine-tuning step.</p><p>After obtaining the non-dominated (trade-off) solutions, we first visualize the objectives in <ref type="figure" target="#fig_11">Fig. 11</ref>. We observe that Pareto surfaces emerge at higher model complexity regime (i.e. high #params, #MAdds, etc.), shown in the 3D scatter plot in the top row, suggesting that trade-offs exist between model size (#params) and model efficiency (#MAdds and latency). In other words, #params and {#MAdds, CPU, GPU latency} are not completely correlated-e.g. a model with a fewer #params is not necessarily more efficient in #MAdds or latency than another model with more #params. This is one of the advantages of using a many-objective optimization algorithm compared to optimizing a single scalarized objective (such, as a weighted-sum of objectives <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>). <ref type="figure" target="#fig_11">Fig. 11</ref> visualizes, in 2D, the top-1 accuracy as a trade-off with each one of the four considered efficiency metrics in the bottom row. The 2D projection is obtained by ignoring the third objective. For better visualization we only show the architectures that are close to the performance trade-off of MobilNetV3 <ref type="bibr" target="#b21">[22]</ref>. NATNets obtained directly from the three-objective search i.e., before any fine-tuning of their weights, consistently outperform MobileNetV3 on ImageNet along all the objectives (top-1 accuracy, #params, #MAdds, CPU and GPU latency). Additionally, we compare to MUXNets <ref type="bibr" target="#b53">[54]</ref> which are also obtained from a three-objective NAS optimizing {top-1 accuracy, #params, and #MAdds}. However, MUXNets adopt a search space that is specifically tailored for reducing model size. Therefore, in comparison to MUXNets, we observe that NATNets perform favourably on all the remaining three efficiency metrics, except for #params. Primarily driven by curiosity in terms of pushing the scalability of our approach with respect to number of objectives, we provide an application to 12 objective problem in Section F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Utility on Dense Image Prediction</head><p>Dense image prediction is another series of important computer vision tasks, that assigns a label to each pixel in the input image <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>. Success in these tasks relies on both feature extraction via a backbone CNN, e.g. ResNet <ref type="bibr" target="#b2">[3]</ref>, and feature aggregation, e.g. FPN <ref type="bibr" target="#b73">[74]</ref>, at multiple scales. In this section, we use NAT to design efficient backbone feature extractors for semantic segmentation, to demonstrate its utility beyond image classification.</p><p>Similar to previous studies, we start from the supernet trained on ImageNet (which is created once before all experiments; see Section 4.2). We remove the last classification layer and pair it with the BiSeNet segmentation heads <ref type="bibr" target="#b74">[75]</ref>, a lightweight semantic segmentation framework for real-time performance. We modify the searched input resolutions from [192, . . ., 256] to [512, . . ., 1280] and keep other searched options the same as before. NAT is applied to minimize #MAdds and maximize mIoU on validation data (20% randomly sampled from the training set) for 20 iterations. In each iteration, the supernet is adapted for 2K iterations using SGD with a momentum of 0.9 and weight decay of 5 × 10 −4 . We use a batch size of eight for each GPU. We use an initial learning rate of 0.01 and follow the "poly" learning rate schedule from the original BiSeNet <ref type="bibr" target="#b74">[75]</ref>, in which the initial learning rate is multiplied by (1 − iter max iter ) 0.9 in each iteration. All other hyperparameters are set at default values from <ref type="table" target="#tab_5">Table 2</ref>. On the Cityscapes dataset <ref type="bibr" target="#b19">[20]</ref>, the overall NAT process takes a day on a server with six Titan RTX GPUs.  Empirically, we observe that NAT based backbones consistently outperform the original BiSeNets, which are based on ResNets. To realize the full potential of the searched NATNets, we further finetune the obtained models for 4K iterations. As shown in <ref type="table" target="#tab_4">Table 5</ref>, the resulting NAT model yields comparable performance against state-of-the-art methods, including PSPNet <ref type="bibr" target="#b75">[76]</ref>, DeepLabv3 <ref type="bibr" target="#b76">[77]</ref>, Auto-DeepLab-S <ref type="bibr" target="#b20">[21]</ref>, while being 4x -28x more efficient in #Madds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY</head><p>In this section, we provide additional experiments towards quantifying the impacts of the main components introduced in NAT and hyperparameter analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Accuracy Predictor Performance</head><p>In this subsection, we assess the effectiveness of different accuracy predictor models. We first uniformly sampled 350 architectures from our search space and trained them using SGD for 150 epochs on ImageNet. Each one of them is fine-tuned for 50 epochs on the other ten datasets <ref type="table" target="#tab_6">(Table 3)</ref>. From the 350 pairs of architectures and top-1 accuracy computed on each dataset, we reserved a subset (randomly chosen) of 50 pairs for testing, and the remaining 300 pairs are then available for training the predictor models. <ref type="figure" target="#fig_5">Fig. 4</ref> compares the mean (over 11 datasets) Spearman rank correlation between the predicted and the true accuracy for each accuracy predictor as the training sample size is varied from 50 to 300. Empirically, we observe that radial basis function (RBF) has higher Spearman rank correlation compared to the other three models. The proposed RBF ensemble model further improves performance over the standalone RBF model across all training sample size regimes. <ref type="figure" target="#fig_2">Fig. 13</ref> shows a visualization of the comparative performance of predictor models on different datasets. From the trade-off perspective of minimizing number of training examples (which reduces the overall computational cost) and maximizing Spearman rank correlation in prediction (which improves the accuracy in ranking architectures during search), we chose the RBF ensemble as our accuracy predictor model and a training size of 100 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Search Efficiency</head><p>The overall computation cost consumed by a NAS algorithm can be factored into three phases: (1) Prior-search: Cost incurred prior to architecture search, e.g. training supernet in case of one-shot approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref> or constructing accuracy predictor <ref type="bibr" target="#b28">[29]</ref>, etc; (2) During-search: Cost associated with measuring the performance of candidate architectures sampled during search through inference. It also includes the cost of training the supernet in case it is coupled with the search, like in <ref type="bibr" target="#b23">[24]</ref> and NAT; (3) Post-search: Cost associated with choosing a final architecture, and/or fine-tuning/re-training the final architectures from scratch. For comparison, we select representative NAS algorithms, including those based on reinforcement learning (RL), gradients, evolutionary algorithm (EA), and weight sharing (WS). <ref type="table">Table 6</ref> shows results for ImageNet and CIFAR-10. The former is the dataset on which the supernet is trained and the latter is a proxy for transfer learning to a non-standard dataset. NAT consistently achieves better performance, both in terms of top-1 accuracy and model efficiency (e.g. #MAdds), compared to the baselines while computational cost is similar or lower. The primary computational cost of NAT is the prior-search training of supernet for 1200 hours. We emphasize, again, that it is a one-time cost that is amortized across all subsequent deployment scenarios (e.g. 10 additional datasets in Section 4.4).</p><p>Comparing the search phase contribution to the success of different NAS algorithms is challenging and ambiguous due to substantial disparities in search spaces and training procedures. So, we conduct the following controlled experiment where we replace only the evolutionary search component in the NAT pipeline with (1) a random search that uniformly samples (with possible repetition) from the search space, and (2) NSGANet <ref type="bibr" target="#b50">[51]</ref>, another multiobjective EA-based NAS algorithm. This experiment is under a biobjective setup: maximize top-1 accuracy and minimize #MAdds. We run each method five times on three datasets to capture the variance in performance due to inherent stochasticity in the optimization initialization. We use hypervolume <ref type="bibr" target="#b77">[78]</ref>, a widelyused metric for comparing algorithms under multiple objectives, as the evaluation metric. <ref type="figure" target="#fig_0">Fig. 14</ref> shows the mean and the standard deviation of the hypervolume achieved by each method. The evolutionary search component in NAT is 3× -5× more sample efficient than the baselines for the same hypervolume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Crossover</head><p>Crossover is a standard operator in evolutionary algorithms, but has largely been avoided by existing EA-based NAS methods <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b58">[59]</ref>. But as we demonstrate here, a carefully designed crossover operation can significantly improve search efficiency. <ref type="figure" target="#fig_2">Fig. 13</ref>: Top row: Spearman rank correlation between predicted accuracy and true accuracy of different surrogate models across many datasets. Each accuracy predictor is constructed from 250 samples (trained architectures). Error bars show mean and standard deviation over ten runs. Bottom row: Goodness of fit visualization of RBF ensemble, the best accuracy predictor. TABLE 6: Comparing the relative search efficiency of NAT to other methods. "-" denotes for not applicable, "WS" stands for weight sharing and "SMBO" stands for sequential model-based optimization <ref type="bibr" target="#b79">[79]</ref>. † is taken from <ref type="bibr" target="#b31">[32]</ref>, ‡ estimate based on the # of models evaluated during search (20K in <ref type="bibr">[8]</ref>, 1.2K in <ref type="bibr">[23]</ref>, 27K in <ref type="bibr" target="#b37">[38]</ref>). * denotes re-ranking stage where top 100-250 models undergo extended training and evaluation for 300 epochs before selecting the final model. We run the evolutionary search of NAT with and without the crossover operator on four datasets; ImageNet <ref type="bibr" target="#b0">[1]</ref>, CIFAR-10 [9], Oxford Flowers102 <ref type="bibr" target="#b18">[19]</ref>, and Stanford Cars <ref type="bibr" target="#b14">[15]</ref>. The hyperparameters that we compare are: 1) w/ crx: crossover probability of 0.9; mutation probability of 0.1; mutation index η m of 3.</p><p>2) w/o crx: crossover probability of 0.0; mutation probability of 0.2; mutation index η m of 3.</p><p>We double the mutation probability when crossover is not used to compensate for the reduced exploration ability of the search. On each dataset, we run each setting to maximize the top-1 accuracy 11 times and report the median performance as a function of the number of architecture sampled in <ref type="figure" target="#fig_6">Fig 15a.</ref> On all four datasets, the crossover operator significantly improves the efficiency of the evolutionary search algorithm. To further validate, we sweep over the probability of crossover while maintaining the rest of the settings. The median performance (over 11 runs) deteriorates as the crossover probability is reduced from 0.9 to 0.2 (see <ref type="figure" target="#fig_6">Fig. 15b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Mutation Hyperparameters</head><p>The mutation operator used in NAT is controlled via two hyperparameters-namely, the mutation probability p m and mutation index η m . To identify the optimal hyperparameter values, we conduct the following parameter sweep experiments. Setting the rest of the hyperparameters to their default values (see <ref type="table" target="#tab_5">Table 2</ref>), we sweep the value of p m from 0.1 to 0.8, and η m from 1.0 to 20. And for each setting, we run NAT eleven times on four datasets (same as the crossover experiment) to maximize the top-1 accuracy. <ref type="figure">Figs. 16a and 16b</ref> show the effect of mutation probability p m and mutation index η m , respectively. We observe that increasing the mutation probability has an adverse effect on performance. Similarly, low values of η m , which encourages the mutated offspring to be further away from parent architectures, improves the performance. Based on these observations, we set the mutation probability p m and mutation index η m parameters to 0.1 and 1.0, respectively, for all our experiments in Section 4. For each study, we run NAT eleven times on four datasets to maximize top-1 accuracy and report the median performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effectiveness of Supernet Adaptation</head><p>Recall that NAT adopts any supernet trained on a large-scale dataset, e.g. ImageNet, and seeks to efficiently transfer to a task-specific supernet on a given dataset. Here, we compare this procedure to a more conventional approach of adapting every subnet (candidate architectures in search) directly. Specifically, we consider the following, 1) Supernet Adaptation: fine-tune supernet for 5 epochs in each iteration and use accuracy from inherited weights (without further training) to select architectures during search (adopted in NAT). 2) Subnet Adaptation: fine-tune each subnet for 5 epochs from the inherited weights, then measure the accuracy.</p><p>We apply these two approaches to a bi-objective search of maximizing top-1 accuracy and minimizing #MAdds on four datasets, including CIFAR-10, CIFAR-100, Oxford Flowers102, and STL-10. <ref type="figure" target="#fig_7">Figure 17</ref> compares the final Pareto fronts. Adapting the supernet yields significantly better performance than adapting individual subnets. Furthermore, we select a subset of searched subnets after subnet adaptation and fine-tune their weights for an additional 150 epochs. We refer to this as additional finetuning in <ref type="figure" target="#fig_7">Fig. 17</ref>. Empirically, we observe that further fine-tuning can match the performance of supernet adaptation on datasets with larger training samples per class (e.g. 4,000 in CIFAR-10). On datasets with fewer samples per class (e.g. 20 in Flowers 102), there is still a large performance gap between supernet adaptation and additional fine-tuning. Overall the results suggest that supernet adaptation is more effective on tasks with limited training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Towards Quantifying Architectural Advancement</head><p>Comparing the architectural contribution to the success of different NAS algorithms can be difficult and ambiguous due to substantial differences in training procedures, e.g. data augmentation, training hyperparameters, etc. Therefore, to quantify the architectural advancement made by NAT alone, we train NAT-M1 from randomly initialized weights (instead of inheriting them from the supernet) with standard training hyperparameters (see <ref type="table" target="#tab_11">Table 7</ref>). We then compare the outcome to two other recently proposed efficient models, MobileNetV3 <ref type="bibr" target="#b21">[22]</ref> and FBNetV2 <ref type="bibr" target="#b31">[32]</ref>. The results are summarized in <ref type="table" target="#tab_12">Table 8</ref>, where we observe that the NAT searched model, NAT-M1, is 0.5 -1.0% more accurate on ImageNet than compared models using similar or less #MAdds. Step LR w/ Decay + Linear Warm-up <ref type="bibr" target="#b80">[80]</ref> Advance + Random Augmentation <ref type="bibr" target="#b81">[81]</ref> + Random Erase Pixel <ref type="bibr" target="#b82">[82]</ref> + Drop path <ref type="bibr" target="#b83">[83]</ref> To further quantify the architectural advancement made by NAT, we use NAT-M1 as a drop-in replacement of the backbone feature extractor for three dense image prediction tasks, including object detection, semantic segmentation, and instance segmentation. More specifically, we replace the EfficientNet-B0 <ref type="bibr" target="#b27">[28]</ref> in EfficientDet-D0 <ref type="bibr" target="#b84">[84]</ref> for object detection; the ResNet-18 <ref type="bibr" target="#b2">[3]</ref> in BiSeNet <ref type="bibr" target="#b74">[75]</ref> for semantic segmentation; and the ResNet-50 <ref type="bibr" target="#b2">[3]</ref> in YOLACT <ref type="bibr" target="#b85">[85]</ref> for instance segmentation. For comparison, we apply the same procedure to both MobileNetV3 and FBNetV2 as well. The results are reported in <ref type="table" target="#tab_12">Table 8</ref>. In general, our NAT searched model, NAT-M1, is consistently better than peer competitors across all tasks and datasets using similar or less #MAdds. Specifically, NAT-M1 is better than the compared models on all three datasets for semantic segmentation, achieving 1.0 -2.3 higher mIoU. Finally, we break down the effect of different training settings and additional fine-tuning for the Top-1 accuracy of the searched models in <ref type="table" target="#tab_13">Table 9</ref>. The advance setting in <ref type="table" target="#tab_11">Table 7</ref> also uses knowledge distillation <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper considered the problem of designing custom neural network architectures that trade-off multiple objectives for a given image classification task. We introduced Neural Architecture Transfer (NAT), a practical and effective approach for this purpose. We described our efforts to harness the concept of a supernet and an evolutionary search algorithm for designing taskspecific neural networks trading-off accuracy and computational complexity. We also showed how to use an online regressor, as a surrogate model to predict the accuracy of subnets in the supernet. Experimental evaluation on eleven benchmark image classification datasets, ranging from large-scale multi-class to small-scale finegrained tasks, showed that networks obtained by NAT outperform conventional fine-tuning based transfer learning, while being orders of magnitude more efficient under mobile settings (≤ 600M Multiply-Adds). NAT was especially effective for small-scale finegrained tasks where fine-tuning pre-trained ImageNet models is ineffective. Finally, we also demonstrated the utility of NAT in optimizing up to twelve objectives with a subsequent trade-off analysis procedure for identifying a single preferred solution.</p><p>Overall, NAT is the first large scale demonstration of manyobjective neural architecture search for designing custom taskspecific models on diverse image classification datasets. shallower networks with more channels in each layer, from the latency perspective <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b61">[62]</ref>.</p><p>To overcome the aforementioned limitations of existing oneshot approaches, we propose NAT. The key difference is that NAT trains the supernet online. Instead of randomly sampling subnets to train the supernet all at once, NAT estimates the distribution (in the variable space) of the optimal subnets from the subnets returned by a many-objective search algorithm, and trains the supernet in correspondence to the estimated distribution. NAT does so in a progressive manner, where the estimated distribution and supernet training are gradually refined through iterations (see <ref type="figure" target="#fig_9">Fig. 19</ref>). We argue that our approach is conceptually more scalable and efficient than existing one-shot approaches since the supernet training now can focus on the promising task-specific subnets recommended by the search algorithm, instead of on all subnets globally.</p><p>To visualize the difference between the existing approach of disentangling supernet training from architecture search, and our approach that use architecture search to guide the supernet training, let us consider the following problem of minimizing a two-variable Rosenbrock function <ref type="bibr" target="#b89">[89]</ref>:</p><formula xml:id="formula_8">minimize f (x 1 , x 2 ) = (1 − x 1 ) 2 + 100(x 2 − x 2 1 ) 2 , x 1 , x 2 ∈ [−2.048, 2.048].<label>(4)</label></formula><p>The objective landscape (contour) of the above two-variable Rosenbrock function is shown in <ref type="figure" target="#fig_17">Fig. 20a</ref>. Let's also assume that each function evaluation of f (x 1 , x 2 ) in Eq (4) is expensive and hence extensively probing the true value is prohibitive (as in the case of NAS). To efficiently optimize this problem, we may learn a meta-model,f (x 1 , x 2 ), to interpolate the landscape (from limited true evaluations). The meta-model should be quick to compute, and hence can be called extensively by an optimization algorithm (as in the case of one-shot NAS). One way is to spend all the true evaluation budget on randomly sampled (from a uniform distribution) solutions at the beginning to learn a meta-model; then the optimization is carried out on the meta-model (as in the case of existing one-shot NAS approaches <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b52">[53]</ref>). See <ref type="figure" target="#fig_17">Fig. 20b</ref> for a visualization. Another way is to adaptively learn a metamodel in an online fashion. Instead of uniformly exhausting all the true evaluation budget at the beginning, the online approach (as in the case of NAT) constructs an initial coarse meta-model from uniformly sampled solutions using partial budget, then a gradual refinement is applied using the solutions optimized based on the current meta-model. See <ref type="figure" target="#fig_17">Fig. 20c</ref> for a visualization. As shown in <ref type="figure" target="#fig_18">Fig. 21</ref>, the online approach allows the meta-model to focus on local regions where potential optimal solutions are more likely to reside, eventually leading to a better solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B MANY-OBJECTIVE SELECTION CONTINUED</head><p>Recall from Section 3.5 in the main paper that domination is a widely-adopted partial ordering concept to compare solutions with two or more objectives. It is used to sort solutions into different ranks of importance, where solutions in lower rank are lexicographically better than solutions in higher rank; and solutions in the same rank are non-dominated, i.e. equally good. However, as well recognized by the evolutionary many-objective optimization community <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b90">[90]</ref>, an increasing larger fraction of randomly generated solutions becomes non-dominated as the number of objectives increases (see <ref type="figure" target="#fig_19">Fig. 22</ref> for a visualization). As a result, the selection pressure provided from domination diminishes quickly as the number of objectives increases, leading to a slow convergence towards the Pareto front. Offline surrogate modelling approach (adopted by existing oneshot NAS methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b52">[53]</ref>): the objective landscape is interpolated through uniformly sampled solutions, then the optimization is carried out on the interpolated landscape. (c) Online surrogate modelling approach (ours): a coarse interpolation of the objective landscape is firstly learned using partial budget, then the landscape is gradually refined by adding the optimization outcome on the current landscape to the interpolation. See <ref type="figure" target="#fig_18">Fig. 21</ref> for comparison on the obtained results.  <ref type="figure" target="#fig_17">Fig. 20b</ref>). Bottom row visualizes the evaluated solutions by the two approaches. Even though the offline approach of uniformly sampling provides a better global interpolation of the landscape (i.e. sub- <ref type="figure">figure (b)</ref>), the online approach achieves a better local interpolation around the optimum (i.e. sub-figure (a) Right). The true landscape is shown in <ref type="figure" target="#fig_17">Fig. 20a</ref>.</p><p>To compensate for the degradation in selection pressure from domination alone, many recently proposed many-objective optimization algorithms <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b90">[90]</ref>, <ref type="bibr" target="#b91">[91]</ref>, <ref type="bibr" target="#b92">[92]</ref> opt for the route of reference point based selection, including this work. The reference points serve as a set of pre-defined targets to aid the selection whenever domination concept finds two solutions indistinguishable, i.e. non-dominated. To demonstrate the effectiveness of the reference point based selection, we select the DTLZ1 problem <ref type="bibr" target="#b93">[93]</ref>, a benchmark problem that is scalable in number of objectives, and compare the IGD metric 9 <ref type="bibr" target="#b95">[95]</ref>, a widely-used performance assessment indicator for comparing many-objective optimization algorithms. We vary the number of objectives in DTLZ1 from 3 to 15 and perform 31 independent runs for 9. Note that Hypervolume, another multi-objective performance metric that is used in the main paper, is computationally infeasible to calculate under large numbers of objectives <ref type="bibr" target="#b94">[94]</ref>. each selection method. The mean IGD values along with the standard deviations are plotted in <ref type="figure" target="#fig_4">Fig. 23</ref>. The consistently lower IGD values across different numbers of objectives confirm the effectiveness of the reference point based selection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C CHOOSING BEST TRADE-OFF SOLUTION</head><p>The proposed many-objective EA is expected to produce N (population size) solutions trading-off all m objectives. These solutions are guaranteed to have one property: a gain in one objective between i-th and j-th solutions comes only from a loss in at least one other objective between them. We calculate the trade-off of i-th solution as the average loss per unit average  Avg.Loss(i, j) Avg.Gain(i, j)</p><p>where</p><formula xml:id="formula_10">Avg.Loss(i, j) = m k=1 max (0, f k (j) − f k (i)) m k=1 {1|f k (j) &gt; f k (j)} Avg.Gain(i, j) = M k=1 max (0, f k (i) − f k (j)) m k=1 {1|f k (i) &gt; f k (j)</formula><p>} Thereafter, the solutions having the highest trade-off value indicates that it causes the largest average loss in some objectives to make a unit average gain in other objectives to choose any of its neighbors. If this highest trade-off value is much larger statistically than other solutions, then the highest trade-off solution is the preferred choice, in case of no preferences provided from users. <ref type="figure" target="#fig_5">Figure 24</ref> visualizes the #MAdds-accuracy trade-off curve, where our NATNets achieve better top-1 accuracy with much fewer #MAdds than other CNN models. Notably, NAT-M1 is more accurate, and 20x more efficient in #MAdds than ResNet-50 <ref type="bibr" target="#b2">[3]</ref>;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D COMPARISON TO EXISTING CONVNETS</head><p>NAT-M4 is more accurate, and 21x more efficient in #MAdds than Inception-ResNet-v2 <ref type="bibr" target="#b68">[69]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E ARCHITECTURE VISUALIZATION</head><p>One of the main advantages of multi-objective optimization is that it generates a set of non-dominated solutions in a single run. These non-dominated solutions are special in the sense that one has to sacrifice on one objective to gain on another. Thereby, "mining" on these non-dominated solutions oftentimes yields important design principles for the task at hand, in this case, to efficiently construct an architecture specific to the objectives and dataset. To demonstrate this concept, we visualize the non-dominated architectures (to maximize top-1 accuracy and minimize #MAdds) resulting from NAT on a diverse set of datasets in <ref type="figure" target="#fig_6">Fig. 25</ref>. Each sub-figure is a heat map showing the distribution of the searched, input image resolutions, width multipliers, and layer settings.</p><p>It is clear from <ref type="figure" target="#fig_6">Fig. 25</ref> that even under the same objectives, the optimal architectures for different datasets are different. For example, the most frequent input image resolution is 192 (the lowest value in our searched options) for Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref> and STL-10 <ref type="bibr" target="#b12">[13]</ref>. While on FGVC Aircraft <ref type="bibr" target="#b15">[16]</ref> and Food-101 <ref type="bibr" target="#b13">[14]</ref>, the most frequent choice of resolution is 256, which is the highest value in our searched option. Similar observations can be made in case of width multiplier and layer settings. This example provides empirical evidence necessary for finding dataset-specific optimal architectures, as opposed to conventional transfer learning. And as demonstrated in the main paper, our proposed NAT presents an efficient and effective way to achieve this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F SCALABILITY TO OBJECTIVES CONTINUED</head><p>To further validate the scalability of NAT to a large number of objectives, we consider the top-1 accuracy on each of the 11 datasets shown in <ref type="table" target="#tab_6">Table 3</ref> (main paper) along with #MAdds, as separate objectives, resulting in a 12-objective optimization problem. Not only is such a large-scale optimization plausible   <ref type="figure" target="#fig_9">Fig. 9</ref> in main paper.</p><p>Flowers102 <ref type="bibr" target="#b18">[19]</ref> Oxford-IIIT Pets <ref type="bibr" target="#b17">[18]</ref> DTD <ref type="bibr" target="#b16">[17]</ref> STL10 <ref type="bibr" target="#b12">[13]</ref> FGVC Aircraft <ref type="bibr" target="#b15">[16]</ref> #Params  with NAT, it also reveals important information, which a lowdimensional optimization may not. During search, the accuracy on each dataset is computed by inheriting weights from the dataset-specific supernets generated from previous experiments (Section 4.4 in the main paper). Since the supernets are already adapted to each dataset, we exclude the supernet adaptation step in NAT for this experiment. <ref type="figure" target="#fig_23">Fig. 26</ref> (Left) shows the 12 objective values for all 45 nondominated architectures obtained by NAT in a parallel coordinate plot (PCP), where each vertical bar is an objective and each line connecting all 12 vertical bars is an architecture. We now apply the trade-off decision analysis presented in Section A and observe that the highest trade-off solution is more than (µ + 3σ) trade-off away from the rest of 44 solutions. This solution is highlighted in dark blue in <ref type="figure" target="#fig_23">Fig. 26 (Left)</ref>. Its intermediate performance in all objectives indicate that this best trade-off solution makes a good compromise on all 12 objectives among all 45 obtained solutions. In <ref type="figure" target="#fig_23">Fig. 26 (Right)</ref>, we compare this solution with different baseline models that are fine-tuned to each dataset separately. Notably, our NATNet achieves better accuracy on all datasets with similar or less #MAdds than EfficientNet-B0 <ref type="bibr" target="#b27">[28]</ref>, MobileNetV2 <ref type="bibr" target="#b55">[56]</ref>, NASNet-A <ref type="bibr">[8]</ref>, and ResNet-50 <ref type="bibr" target="#b2">[3]</ref>, making our highest trade-off solution a preferred one.</p><p>The above analysis alludes to a computational mechanism for choosing a single preferred trade-off solution from the Pareto solutions obtained by a many-objective optimization algorithm. If such an overwhelmingly high trade-off solution exists in the Pareto front, it becomes one of the best choices and can outperform solutions found by a single-objective optimization algorithm. Without resorting to a many-objective optimization to find multiple tradeoff solutions, identification of such a high trade-off solution is very challenging. where each vertical bar is an objective and each line is a non-dominated architectures achieved by NAT from a 12-obj optimization of minimizing #MAdds and maximizing accuracy on the 11 datasets. The model with the best trade-off (see Section A for details) is highlighted in dark blue. Right: 1-on-1 comparison between the selected NATNet (top-ranked in trade-off) and representative peer models on top-1 accuracy on various datasets. Method with larger area is better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 4 /</head><label>14</label><figDesc>Neural Architecture Transfer Input : Training data Dtrn, validation data D vld , additional objectivesf , supernet Sw, archive size N , # of iterations T , # of epochs E, # of generations G. 1 t ← 0 // initialize an iteration counter. 2 A ← randomly initialize an archive of archs with a size of N . 3 while t &lt; T do / compute accuracy by inheriting weights and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 f 6 // construct the accuracy predictor. 7 S 8 / 9 Pt 10 / 12 /</head><label>567891012</label><figDesc>← Sw(A, D vld ) f ← Accuracy Predictor(A, f ) ⊳ Algo. 2 / find promising archs by evolutionary search. ← Evolutionary Search(S f ,f , A, G) ⊳ Algo. 3 / keep the top-N ranked archs in archive. 11 A ← Selection(A ∪ Pt, N ) ⊳ Algo. 4 / fine tune supernet to promising archs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>13</head><label>13</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Stage</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Top Path: A typical process of evaluating an architecture in NAS algorithms. Bottom Path: Accuracy predictor aims to bypass the timeconsuming components for evaluating a network's performance by directly regressing its accuracy f from a (architecture in the encoded space).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Accuracy predictor performance as a function of training samples. For each model, we show the mean and standard deviation of the Spearman rank correlation on 11 datasets(Table 3). The size of RBF ensemble is 500.2) Consistent prediction: the quality of the prediction should be consistent across different datasets.3) Sample efficiency: minimizing the number of training examples necessary to construct an accurate predictor model, since each training sample requires costly training and evaluation of a subnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Crossover Operator: new offspring architectures are created by recombining integers from two parent architectures. The probability of choosing from either one of the parents is equal. (b) Mutation Operator: histograms showing the probabilities of mutated values with current value at 5 under different hyperparameter ηm settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>ImageNet Architectures from Trade-Off Front.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>MAdds vs. ImageNet Accuracy. NATNets outperform other models in both objectives. In particular, NAT-M4 achieves a new state-of-the-art top-1 accuracy of 80.5% under mobile setting (≤ 600M MAdds). NAT-M1 improves MobileNetV3 top-1 accuracy by 2.3% with similar #MAdds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>MAdds vs. Accuracy trade-off curves comparing NAT and existing architectures on a diverse set of datasets. The datasets are arranged in ascending order of training set size. Methods shown in the legend pre-train on ImageNet and fine-tune the weights on the target dataset. Methods with names annotated in sub-figures train from scratch or use external training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Efficient architectures (350M MAdds) obtained by NAT on ten diverse image classification datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Top row: NATNets obtained from tri-objective search to maximize ImageNet top-1 accuracy, minimize model size (#Params), and minimize {#MAdds, CPU latency, GPU latency} from left to right. Pareto surfaces emerge at higher model complexity regime (i.e. top right corner) suggesting that trade-offs exist between model size (#params) and model efficiency (#MAdds and latency). Bottom row: 2D projections from above 3D scatter, showing top-1 accuracy vs. each of the four efficiency related measurements. The first two 2D projections are from the first 3D scatter, and the remaining two 2D projections are from the second and third 3D scatters, respectively. To better visualize (the comparison with MobileNetV3<ref type="bibr" target="#b21">[22]</ref> and MUXNet<ref type="bibr" target="#b53">[54]</ref>), partial solutions from the non-dominated frontiers are shown. All top-1 accuracy shown are without fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>MAdds vs. Cityscapes mIoU. NAT obtained backbone feature extractors (green curve) significantly outperform the original BiSeNet, which are based on ResNets (R18 -R152). With further fine-tuning of 4K iterations, NAT achieves the state-of-the-art performance (red curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12</head><label>12</label><figDesc>compares the mIoU-MAdds trade-off obtained by NAT and the original BiSeNet [75] on the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>( a )Fig. 15 :Fig. 16 :</head><label>a1516</label><figDesc>Effect of Crossover (b) Effect of Crossover Probability Ablation study on the crossover operator: (a) the median performance from eleven runs of our evolutionary algorithm with and without the crossover operator. (b) the median performance deteriorates as the crossover probability reduces from 0.9 to 0.2. (a) Effect of Mutation Probability (b) Effect of Mutation Hyperparameter ηm Hyperparameter study on (a) mutation probability pm and (b) mutation index parameter ηm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 :</head><label>17</label><figDesc>Comparing the performance of adapting supernet, adapting subnet and additional fine-tuning under a bi-objective search setup on four datasets. Details are provided in Section 5.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 :</head><label>19</label><figDesc>Overview of our proposed NAT. The distribution of optimal subnets is estimated from the promising architectures returned by architecture search. Then it is used to guide the training of the supernet. The "per iteration" refers to the iteration in Algorithm 1 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 :</head><label>20</label><figDesc>(a) True objective landscape (contour) of a two-variable Rosenbrock function. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 21 :</head><label>21</label><figDesc>Top row compares the interpolated landscapes and the obtained optimum by (a) our online surrogate modeling (Fig. 20c) with initial, 3/4, and full budget from Left to Right, and (b) offline surrogate modeling (existing one-shot NAS approaches;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 22 :</head><label>22</label><figDesc>Mean ratio of non-dominated solutions from a set of randomly generated solutions. N is the sample size of the randomly generated solutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 23 :</head><label>23</label><figDesc>Performance comparison of reference point based (Algorithm 4 in the main paper) and domination based selections [96] on DTLZ1 problem [93]. gain among m nearest neighbors (B(i)) based on normalized Euclidean distance are used here), as follows [97]: Trade-off(i) = |B(i)| max j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 24 :</head><label>24</label><figDesc>MAdds vs. ImageNet Accuracy. Our NATNets significantly outperform other models from NAS algorithms and human experts. In particular, NAT-M4 achieves new state-of-the-art 80.5% top-1 accuracy under mobile setting (600M MAdds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 25 :</head><label>25</label><figDesc>Non-dominated architectures to {top-1 accuracy, #MAdds} obtained by NAT on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 26 :</head><label>26</label><figDesc>Left: Parallel Coordinate Plot (PCP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>was with Michigan State University. E-mail: luzc@sustech.edu.cn, G. Sreekumar, E. Goodman, W. Banzhaf, K. Deb, and V. N. Boddeti are with Michigan State University, East Lansing, MI, 48824 USA. E-mail: {sreekum1,goodman,banzhafw,kdeb,vishnu}@msu.edu.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Comparison of NAT and existing NAS methods. † indicates methods that scalarize multiple objectives into one composite objective or as an additional constraint, see text for details.</figDesc><table><row><cell>Methods</cell><cell>Search Method</cell><cell>Performance Prediction</cell><cell>Weight Sharing</cell><cell>Multiple Objective</cell><cell>Dataset Searched</cell></row><row><cell>NASNet [8]</cell><cell>RL</cell><cell></cell><cell></cell><cell></cell><cell>C10</cell></row><row><cell>PNAS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 5 :</head><label>5</label><figDesc>Adapt Supernet Input : Supernet Sw, archive of archs A, training data Dtrn, number of epochs E. 1 e ← 0 // initialize an epoch counter. Distr ← construct the distribution from A following Eq. (3). 3 while e &lt; E do 4 for each batch in Dtrn do 5 subnet ← sample from Distr. w ← set forward path of Sw according to subnet. L ← compute cross-entropy loss on data batch. ∇w ← compute the gradient by ∂L/∂w Sw ← one step of SGD.</figDesc><table><row><cell>10</cell><cell>end</cell></row><row><cell>11</cell><cell>e ← e + 1</cell></row><row><cell>12 end</cell><cell></cell></row><row><cell cols="2">13 Return supernet Sw.</cell></row></table><note>26789</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Hyperparameter Settings</figDesc><table><row><cell>Category</cell><cell>Parameter</cell><cell>Setting</cell></row><row><cell>Global</cell><cell>Archive size Number of iterations</cell><cell>300 30</cell></row><row><cell>Accuracy predictor</cell><cell>Train size Ensemble size</cell><cell>100 500</cell></row><row><cell></cell><cell>Population size</cell><cell>100</cell></row><row><cell></cell><cell>Number of generations per iteration</cell><cell>100</cell></row><row><cell>Evolutionary search</cell><cell>Crossover probability</cell><cell>0.9</cell></row><row><cell></cell><cell>Mutation probability</cell><cell>0.1</cell></row><row><cell></cell><cell>Mutation index ηm</cell><cell>1.0</cell></row><row><cell>Supernet</cell><cell>Number of epochs per iteration</cell><cell>5</cell></row><row><cell cols="3">different architectures in order to be efficient. In other words, not</cell></row><row><cell cols="3">all subnets are equally important for the task at hand. Secondly,</cell></row><row><cell cols="3">only a tiny fraction 4 of the search space can practically be explored</cell></row><row><cell>by a NAS algorithms.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Benchmark Datasets for Evaluation</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Train Size</cell><cell>Test Size</cell><cell>#Classes</cell></row><row><cell>ImageNet [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table /><note>ImageNet-1K Classification</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc>Cityscapes Semantic Segmentation<ref type="bibr" target="#b19">[20]</ref>: All results are based on single-scale inputs from validation set.</figDesc><table><row><cell>Method</cell><cell cols="2">#Params #Multi-Adds</cell><cell>mIoU (%)</cell></row><row><cell>BiSeNet [75]</cell><cell>13.4M</cell><cell>67B</cell><cell>74.8</cell></row><row><cell>PSPNet [76]</cell><cell>65.9M</cell><cell>2,017B</cell><cell>78.4</cell></row><row><cell>DeepLabv3+ [77]</cell><cell>43.5M</cell><cell>1,551B</cell><cell>79.6</cell></row><row><cell>Auto-DeepLab-S [21]</cell><cell>10.2M</cell><cell>333B</cell><cell>79.7</cell></row><row><cell>NAT + BiSeNet (ours)</cell><cell>8.8M</cell><cell>73B</cell><cell>79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7 :</head><label>7</label><figDesc>Details of training hyperparameter settings. Advance settings are in addition to standard settings.</figDesc><table><row><cell>Setting</cell><cell>Data Augmentation</cell><cell>Regularization</cell><cell>Optimizer</cell><cell>LR Schedule</cell></row><row><cell>Standard</cell><cell>Horizontal Flop + Crop</cell><cell>Drop out</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>RMSProp + Exponential</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Moving Averaging</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 :</head><label>8</label><figDesc>Comparison between NAT searched model and representative models on ImageNet classification under standard training setup, and as feature extractors on MS COCO<ref type="bibr" target="#b86">[86]</ref> object detection task, PASCAL VOC<ref type="bibr" target="#b87">[87]</ref> instance segmentation task and semantic segmentation tasks.</figDesc><table><row><cell>Backbone</cell><cell></cell><cell>MobileNetV3 [22]</cell><cell>FBNetV2 [32]</cell><cell>NAT-M1 (ours)</cell></row><row><cell>#MAdds</cell><cell></cell><cell>219M</cell><cell>238M</cell><cell>225M</cell></row><row><cell cols="2">ImageNet Top-1 Acc.</cell><cell>74.7</cell><cell>75.2</cell><cell>75.7</cell></row><row><cell>Object</cell><cell>AP</cell><cell>31.8</cell><cell>31.1</cell><cell>32.2</cell></row><row><cell>Detection</cell><cell>AP s/m/l</cell><cell>10.4 / 37.3 / 50.1</cell><cell cols="2">10.9 / 36.6 / 48.4 11.5 / 37.9 / 49.7</cell></row><row><cell>Instance</cell><cell>AP bbox</cell><cell>44.0</cell><cell>44.8</cell><cell>45.2</cell></row><row><cell>Segmentation</cell><cell>AP mask</cell><cell>43.6</cell><cell>43.9</cell><cell>44.3</cell></row><row><cell>Semantic Segmentation</cell><cell>Cityscapes [20] PASCAL VOC [87] COCO-Stuff [88]</cell><cell>73.0 73.8 28.5</cell><cell>72.6 73.6 28.5</cell><cell>74.0 75.9 29.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9 :</head><label>9</label><figDesc>Effect of different training setups. Details of the standard and advanced settings under Random Initialization are provided inTable 7.</figDesc><table><row><cell>Training</cell><cell cols="2">Random Initialization</cell><cell cols="2">Inherited from Supernet</cell></row><row><cell>Settings</cell><cell>standard</cell><cell>advanced</cell><cell>w/o fine-tune</cell><cell>w/ fine-tune</cell></row><row><cell>NAT-M1</cell><cell>75.7</cell><cell>77.1</cell><cell>75.9</cell><cell>77.5</cell></row><row><cell>NAT-M2</cell><cell>76.9</cell><cell>78.0</cell><cell>77.4</cell><cell>78.6</cell></row><row><cell>NAT-M3</cell><cell>78.2</cell><cell>79.1</cell><cell>78.9</cell><cell>79.9</cell></row><row><cell>NAT-M4</cell><cell>78.8</cell><cell>79.5</cell><cell>79.4</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 10 :</head><label>10</label><figDesc>NAT model performance corresponding to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 11 :</head><label>11</label><figDesc>Accuracy predictor model mean (standard deviation) performance corresponding toFig. 13in main paper.</figDesc><table><row><cell>Method</cell><cell>ImageNet [1]</cell><cell>CIFAR-10 [9]</cell><cell>CIFAR-100 [9]</cell><cell>Flowers102 [19]</cell><cell>Food-101 [14]</cell><cell>Oxford-IIIT Pets [18]</cell><cell>Aircraft [16]</cell><cell>Stanford Cars [15]</cell><cell>DTD [17]</cell><cell>STL-10 [13]</cell></row><row><cell>GP</cell><cell>0.606 (0.09)</cell><cell>0.969 (0.01)</cell><cell>0.693 (0.13)</cell><cell>0.918 (0.02)</cell><cell>0.980 (0.01)</cell><cell>0.945 (0.02)</cell><cell>0.551 (0.17)</cell><cell>0.964 (0.01)</cell><cell>0.467 (0.11)</cell><cell>0.973 (0.11)</cell></row><row><cell>RBF</cell><cell>0.705 (0.11)</cell><cell>0.969 (0.01)</cell><cell>0.806 (0.08)</cell><cell>0.932 (0.03)</cell><cell>0.981 (0.01)</cell><cell>0.967 (0.01)</cell><cell>0.693 (0.08)</cell><cell>0.977 (0.01)</cell><cell>0.653 (0.06)</cell><cell>0.979 (0.01)</cell></row><row><cell>MLP</cell><cell>0.635 (0.09)</cell><cell>0.851 (0.06)</cell><cell>0.562 (0.10)</cell><cell>0.766 (0.06)</cell><cell>0.775 (0.09)</cell><cell>0.798 (0.05)</cell><cell>0.658 (0.15)</cell><cell>0.717 (0.10)</cell><cell>0.490 (0.09)</cell><cell>0.899 (0.06)</cell></row><row><cell>DT</cell><cell>0.625 (0.11)</cell><cell>0.974 (0.01)</cell><cell>0.736 (0.11)</cell><cell>0.940 (0.02)</cell><cell>0.990 (0.01)</cell><cell>0.961 (0.01)</cell><cell>0.629 (0.14)</cell><cell>0.986 (0.01)</cell><cell>0.590 (0.14)</cell><cell>0.976 (0.01)</cell></row><row><cell>RBF Ensemble</cell><cell>0.866 (0.04)</cell><cell>0.959 (0.02)</cell><cell>0.858 (0.05)</cell><cell>0.931 (0.01)</cell><cell>0.967 (0.03)</cell><cell>0.943 (0.01)</cell><cell>0.870 (0.07)</cell><cell>0.975 (0.01)</cell><cell>0.890 (0.04)</cell><cell>0.964 (0.02)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Low mean square error is also desirable, but not necessary since the selection of architectures in the subsequent evolutionary search compares relative performance between architectures.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A RELATION TO EXISTING ONE-SHOT NAS</head><p>Most existing one-shot NAS approaches follow a two-step process, where the supernet training and the architecture search are disentangled into two sequential stages. This process starts with training a supernet (in which searchable architectures become subnets) offline as a one-time process prior to the search. Then the performance of the subnets, evaluated with the inherited weights, is used to guide the selection of architectures during search. Early one-shot approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b52">[53]</ref> follow a conventional (rather naïve) way to train the supernet, i.e. train a randomly chosen subpart (subnet) of the supernet directly from randomly initialized weights for each mini-batch (see <ref type="figure">Fig. 18a</ref>). Consequently, the searched subnets need to be re-trained thoroughly from scratch as the performance evaluated with inherited weights are far below the true performance and can only be used as a proxy indicator to compare the relative difference between subnets.</p><p>The progressive shrinking algorithm proposed in OnceForAll <ref type="bibr" target="#b30">[31]</ref> also trains the supernet in an offline fashion, but differs in three aspects-(i) it pre-trains the supernet at full scale before sampling subnets; (ii) it gradually adds the searched dimensions (kernel size, depth, width) into the search space; and (iii) it uses the full-scale supernet to supervise the training of subnets. However, the supernet weights update is still based on randomly sampled subnets. See <ref type="figure">Fig. 18b</ref> for a visualization. Empirically, OnceForAll shows that the supernet trained with progressive shrinking enables subnets with inherited weights to be directly deployed without re-training.</p><p>Despite the success shown in OnceForAll, we argue that such an offline training process of supernet is fundamentally limited by the fact that it requires all subnets to be learned simultaneously. To elaborate, without prior knowledge on the distribution of the optimal subnets for the tasks at hand, the supernet training has to cover the search space of subnets globally as the training is performed prior to the search as a one-time process. However, training the supernet weights in such a way that all subnets are optimized simultaneously is practically infeasible. For instance, progressive shrinking <ref type="bibr" target="#b30">[31]</ref> sampled roughly 634K 8 subnets during supernet training, which is less than 10 −12 % of the its total subnet volume. Any additional options added to the search space (one more kernel size and expand ratio choice) will require 100x more training epochs (100K vs 1K) to cover the same volume of subnets, which is obviously not scalable. Moreover, we argue that simultaneously training all subnets is also unnecessary as not all subnets are equally important for the tasks at hand. Specifically, existing NAS works have shown that different hardware requires different architectures to be efficient, e.g. CPU favors deeper networks with fewer channels in each layer, while GPU favors</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better?&quot; in CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: Solving problems with box constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="601" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03505</idno>
		<title level="m">Cinic-10 is not imagenet or cifar-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chamnet: Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards fast adaptation of neural architectures with meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning of neural architectures for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Staffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Xfernas: Transfer neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08307</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Eat-nas: Elastic architecture transfer for accelerating largescale neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transfer learning with neural automl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sbaiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bartok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05781</idno>
		<title level="m">Fast task-aware architecture inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10823</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SMASH: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">NSGA-Net: Neural architecture search using multiobjective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dpp-net: Device-aware progressive search for pareto-optimal neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MUXConv: Information multiplexing in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mathematical programs with optimization problems in the constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bracken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Mcgill</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/169087" />
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Surrogateassisted evolutionary deep learning using an end-to-end random forestbased performance predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simulated binary crossover for continuous search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="148" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Normal-boundary intersection: A new method for generating the pareto surface in nonlinear multicriteria optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
		<idno type="DOI">10.1137/S1052623496307510</idno>
		<ptr target="https://doi.org/10.1137/S1052623496307510" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="657" />
			<date type="published" when="1998-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Atomnas: Fine-grained end-to-end neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural architecture search for lightweight non-local networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Blockwisely supervised neural architecture search with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mixconv: Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big single-stage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Enaet: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Xnas: Neural architecture search with expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multiobjective optimization using evolutionary algorithms -a comparative case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ppsn V</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Schoenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Problem Solving from Nature</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwefel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="292" to="301" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">COCO-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An automatic method for finding the greatest or least value of a function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rosenbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">An evolutionary manyobjective optimization algorithm based on dominance and decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="694" to="716" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Moea/d: A multiobjective evolutionary algorithm based on decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="712" to="731" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A reference vector guided evolutionary algorithm for many-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sendhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="773" to="791" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Scalable test problems for evolutionary multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laumanns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary multiobjective optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="105" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On the complexity of computing the hypervolume indicator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Ibanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paquete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vahrenhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1075" to="1082" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sierra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican international conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: NSGA-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Multi-objective optimization using evolutionary algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$</forename><surname>%&amp;amp;&amp;apos;( ; *+</surname></persName>
		</author>
		<idno>/012&quot;3+</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<title level="m">//%*,&quot;+.$) 58)+7.)!&quot;#$%&amp; &apos;()#*+ 9:;):&apos;6%3</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;lt;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--=)</forename></persName>
		</author>
		<title level="m">+*#-/=)&gt;? !.@+3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$</forename><surname>%&amp;amp;$&amp;apos; (%)*&amp;amp;*&amp;amp;+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">%</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">A typical process that most early One-Shot NAS approaches follow</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$</forename><surname>%&amp;amp;&amp;apos;( ; *+</surname></persName>
		</author>
		<idno>/012&quot;3+4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<idno>3*8</idno>
		<title level="m">//%*,&quot;+.$) 69)+8.)!&quot;#$%&amp; &apos;()#*+ :-%;)&lt;</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">=</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--&amp;gt;)</forename></persName>
		</author>
		<title level="m">+*#-/? @2+,%#&quot;&apos;)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">+,%# 2.3)1&quot;+*8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">It pre-trains the supernet at full scale before subnet sampling and use the supernet at full scale to supervise the training of subnets. *And the searched dimensions are gradually added to the search space, i.e. kernel size -&gt; kernel size + depth -&gt; kernel size + depth + width. Fig. 18: Overview of existing one-shot NAS approaches</title>
		<imprint/>
	</monogr>
	<note>The Progressive Shrinking algorithm proposed in OnceForAll. which decouples the supernet training and architecture search to two sequential steps</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$</forename><surname>%&amp;amp;&amp;apos;(&amp;amp;)*+%</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;$</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,-.)$</forename><surname>%&amp;amp;# ; #*+</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$&amp;quot;</forename><surname>%&amp;amp;&amp;apos;(</surname></persName>
		</author>
		<title level="m">$123456&amp;.%)7&amp;</title>
		<editor>&amp;$</editor>
		<editor>.+</editor>
		<imprint>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
				<title level="m">$%&amp; &quot;&amp;;&apos;5$%.+ &quot;&amp;;&apos;)%&amp;</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">$</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!&amp;quot;#$%#&amp;amp; ; *$</forename><surname>%&amp;amp;+*</surname></persName>
		</author>
		<title level="m">&amp;+/%/0*1&amp;#+</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*/+)+</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)*$</forename><surname>%&amp;amp;+*</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0&amp;*01&apos;0&apos;&quot;#$%#&amp;&apos;( 53%&apos;0*.&amp;+/%/</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
