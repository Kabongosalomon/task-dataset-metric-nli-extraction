<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prescribed Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-11">October 11, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J R</forename><surname>Ruiz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
						</author>
						<title level="a" type="main">Prescribed Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-11">October 11, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generative adversarial networks</term>
					<term>entropy regularization</term>
					<term>log-likelihood evaluation</term>
					<term>mode collapse</term>
					<term>diverse image generation</term>
					<term>deep generative models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support-a phenomenon known as mode collapse-and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational auto-encoders (VAEs). 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b22">(Goodfellow et al., 2014)</ref> are a family of generative models that have shown great promise. They achieve state-of-the-art performance in the image domain; for example image generation <ref type="bibr" target="#b30">(Karras et al., 2019;</ref><ref type="bibr" target="#b10">Brock et al., 2018)</ref>, image super-resolution <ref type="bibr" target="#b35">(Ledig et al., 2017)</ref>, and image translation <ref type="bibr" target="#b27">(Isola et al., 2017)</ref>.  <ref type="bibr">GAN</ref> and PresGAN on a toy two-dimensional experiment. The ground truth is a uniform mixture of 10 Gaussians organized on a ring. Given the right set of hyperparameters, a GAN could perfectly fit this target distribution. In this example we chose the GAN hyperparameters such that it collapses-here 4 out of 10 modes are missing. We then fit the PresGAN using the same hyperparameters as the collapsing GAN. The PresGAN is able to correct the collapsing behavior of the GAN and learns a good fit for the target distribution. GANs learn densities by defining a sampling procedure. A latent variable z is sampled from a prior p(z) and a samplex(z; θ ) is generated by taking the output of a neural network with parameters θ , called a generator, that takes z as input. The density p θ (x) implied by this sampling procedure is implicit and undefined <ref type="bibr" target="#b45">(Mohamed and Lakshminarayanan, 2016)</ref>. However, GANs effectively learn the parameters θ by introducing a classifier D φ -a deep neural network with parameters φ, called discriminator-that distinguishes between generated samplesx(z; θ ) and real data x, with distribution p d (x). The parameters θ and φ are learned jointly by optimizing the GAN objective, <ref type="bibr">GAN</ref> (θ , φ) = x∼p d (x) log D φ (x) + z∼p(z) log 1 − D φ (x(z; θ )) .</p><p>(1) GANs iteratively maximize the loss in Eq. 1 with respect to φ and minimize it with respect to θ .</p><p>In practice, the minimax procedure described above is stopped when the generator produces realistic images. This is problematic because high perceptual quality does not necessarily correlate with goodness of fit to the target density. For example, memorizing the training data is a trivial solution to achieving high perceptual quality. Fortunately, GANs do not merely memorize the training data <ref type="bibr" target="#b1">Arora et al., 2017)</ref>.</p><p>However GANs are able to produce images indistinguishable from real images while still failing to fully capture the target distribution <ref type="bibr" target="#b10">(Brock et al., 2018;</ref><ref type="bibr" target="#b30">Karras et al., 2019)</ref>. Indeed GANs suffer from an issue known as mode collapse. When mode collapse happens, the generative distribution p θ (x) is degenerate and of low support <ref type="bibr" target="#b1">(Arora et al., 2017</ref><ref type="bibr" target="#b2">(Arora et al., , 2018</ref>. Mode collapse causes GANs, as density estimators, to fail both qualitatively and quantitatively. Qualitatively, mode collapse causes lack of diversity in the generated samples. This is problematic for certain applications of GANs, e.g. data augmentation. Quantitatively, mode collapse causes poor generalization to new data. This is because when mode collapse happens, there is a (support) mismatch between the learned distribution p θ (x) and the data distribution. Using annealed importance sampling with a kernel density estimate of the likelihood, <ref type="bibr" target="#b67">Wu et al. (2016)</ref> report significantly worse log-likelihood scores for GANs when compared to variational auto-encoders (VAEs). Similarly poor generalization performance was reported by <ref type="bibr" target="#b23">Grover et al. (2018)</ref>.</p><p>A natural way to prevent mode collapse in GANs is to maximize the entropy of the generator <ref type="bibr" target="#b5">(Belghazi et al., 2018)</ref>. Unfortunately the entropy of GANs is unavailable. This is because the existence of the generative density p θ (x) is not guaranteed <ref type="bibr" target="#b45">(Mohamed and Lakshminarayanan, 2016;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017)</ref>.</p><p>In this paper, we propose a method to alleviate mode collapse in GANs resulting in a new family of GANs called prescribed GANs (PresGANs). PresGANs prevent mode collapse by explicitly maximizing the entropy of the generator. This is done by augmenting the loss in Eq. 1 with the negative entropy of the generator, such that minimizing Eq. 1 with respect to θ corresponds to fitting the data while also maximizing the entropy of the generative distribution. The existence of the generative density is guaranteed by adding noise to the output of a density network <ref type="bibr" target="#b39">(MacKay, 1995;</ref><ref type="bibr" target="#b15">Diggle and Gratton, 1984)</ref>. This process defines the generative distribution p θ (x), not as an implicit distribution as in standard GANs, but as an infinite mixture of well-defined densities as in continuous VAEs <ref type="bibr" target="#b32">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref>. The generative distribution of PresGANs is therefore very flexible.</p><p>Although the entropy of the generative distribution of PresGANs is well-defined, it is intractable. However, fitting a PresGAN to data only involves computing the gradients of the entropy and not the entropy itself. PresGANs use unbiased Monte Carlo estimates of these gradients.</p><p>An illustrative example. To demonstrate how PresGANs alleviate mode collapse, we form a target distribution by organizing a uniform mixture of K = 10 twodimensional Gaussians on a ring. We draw 5,000 samples from this target distribution. We first fit a GAN, setting the hyperparameters so that the GAN suffers from mode collapse 2 . We then use the same settings for PresGAN to assess whether it can correct the collapsing behavior of the GAN. <ref type="figure" target="#fig_0">Figure 1</ref> shows the collapsing behavior of the GAN, which misses 4 modes of the target distribution. The PresGAN, on the other hand, recovers all the modes. Section 5 provides details about the settings of this synthetic experiment.</p><p>Contributions. This paper contributes to the literature on the two main open problems in the study of GANs: preventing mode collapse and evaluating loglikelihood.</p><p>• How can we perform entropy regularization of the generator of a GAN so as to effectively prevent mode collapse? We achieve this by adding noise to the output of the generator; this ensures the existence of a density p θ (x) and makes its entropy well-defined. We then regularize the GAN loss to encourage densities p θ (x) with high entropy. During training, we form unbiased estimators of the (intractable) gradients of the entropy regularizer. We show how this prevents mode collapse, as expected, in two sets of experiments (see Section 5). The first experiment follows the current standard for measuring mode collapse in the GAN literature, which is to report the number of modes recovered by the GAN on MNIST (10 modes) and STACKEDMNIST (1,000 modes) and the Kullback-Leibler (KL) divergence between the true label distribution and the one induced by the GAN. We conducted a second experiment which sheds light on another way mode collapse can occur in GANs, which is when the data is imbalanced.</p><p>• How can we measure log-likelihood in GANs? Evaluating log-likelihood for GANs allows assessing how they generalize to new data. Existing measures focus on sample quality, which is not a measure of generalization. This inability to measure predictive log-likelihood for GANs has restricted their use to domains where one can use perceptual quality measures (e.g., the image domain). Existing methods for evaluating log-likelihood for GANs either use a proxy to log-likelihood <ref type="bibr" target="#b56">(Sánchez-Martín et al., 2019)</ref> or define the likelihood of the generator only at test time, which creates a mismatch between training and testing <ref type="bibr" target="#b67">(Wu et al., 2016)</ref>, or assume invertibility of the generator of the GAN <ref type="bibr" target="#b23">(Grover et al., 2018)</ref>. Adding noise to the output of the generator immediately makes tractable predictive log-likelihood evaluation via importance sampling.</p><p>Outline. The rest of the paper is organized as follows. In Section 2 we set the notation and provide desiderata for deep generative modeling. In Section 3 we describe PresGANs and how we compute their predictive log-likelihood to assess generalization. In Section 4 we discuss related work. We then assess the performance of PresGANs in terms of mode collapse, sample quality, and log-likelihood in Section 5. Finally, we conclude and discuss key findings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prologue</head><p>In this paper, we characterize a deep generative model (DGM) by its generative process and by the loss used to fit its parameters. We denote by p θ (x) the generative distribution induced by the generative process-it is parameterized by a deep neural network with parameters θ . The loss, that we denote by (θ , φ), often requires an additional set of parameters φ that help learn the model parameters θ . We next describe choices for p θ (x) and (θ , φ) and then specify desiderata for deep generative modeling.</p><p>The generative distribution. Recent DGMs define the generative distribution either as an implicit distribution or as an infinite mixture <ref type="bibr" target="#b22">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b32">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref>.</p><p>Implicit generative models define a density using a sampling procedure. This is the approach of GANs <ref type="bibr" target="#b22">(Goodfellow et al., 2014)</ref>. A latent variable z is sampled from a prior p(z), usually a standard Gaussian or a uniform distributon, and a sample is generated by taking the output of a neural network that takes z as input. The density p θ (x) implied by this sampling procedure is undefined. Any measure that relies on an analytic form of the density p θ (x) is therefore unavailable; e.g., the log-likelihood or the entropy.</p><p>An alternative way to define the generative distribution is by using the approach of VAEs <ref type="bibr" target="#b32">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref>. They define p θ (x) as an infinite mixture,</p><formula xml:id="formula_0">p θ (x) = p θ (x | z) p(z) dz.<label>(2)</label></formula><p>Here the mixing distribution is the prior p(z). The conditional distribution p θ (x | z) is an exponential family distribution, such as a Gaussian or a Bernoulli, parameterized by a neural network with parameters θ . Although both the prior p(z) and p θ (x | z) are simple tractable distributions, the generative distribution p θ (x) is highly flexible albeit intractable. Because p θ (x) in Eq. 2 is well-defined, the log-likelihood and the entropy are also well-defined (although they may be analytically intractable).</p><p>The loss function. Fitting the models defined above requires defining a learning procedure by specifying a loss function. GANs introduce a classifier D φ , a deep neural network parameterized by φ, to discriminate between samples from the data distribution p d (x) and the generative distribution p θ (x). The auxiliary parameters φ are learned jointly with the model parameters θ by optimizing the loss in Eq. 1. This training procedure leads to high sample quality but often suffers from mode collapse <ref type="bibr" target="#b1">(Arora et al., 2017</ref><ref type="bibr" target="#b2">(Arora et al., , 2018</ref>).</p><p>An alternative approach to learning θ is via maximum likelihood. This requires a well-defined density p θ (x) such as the one in Eq. 2. Although well-defined, p θ (x) is intractable, making it difficult to learn the parameters θ by maximum likelihood. VAEs instead introduce a recognition network-a neural network with parameters φ that takes data x as input and outputs a distribution over the latent variables z-and maximize a lower bound on log p θ (x) with respect to both θ and φ,</p><formula xml:id="formula_1">VAE (θ , φ) = E p d (x) E q φ (z | x) log p θ (x, z) q φ (z | x) = −KL(q φ (z | x)p d (x)||p θ (x, z)). (3)</formula><p>Here KL(·||·) denotes the KL divergence. Maximizing VAE (θ , φ) is equivalent to minimizing this KL which leads to issues such as latent variable collapse <ref type="bibr" target="#b9">(Bowman et al., 2015;</ref><ref type="bibr" target="#b13">Dieng et al., 2018b)</ref>. Furthermore, optimizing Eq. 3 may lead to blurriness in the generated samples because of a property of the reverse KL known as zero-forcing <ref type="bibr" target="#b43">(Minka et al., 2005)</ref>.</p><p>Desiderata. We now outline three desiderata for DGMs. We next introduce a new family of GANs that fulfills all the desiderata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prescribed Generative Adversarial Networks</head><p>PresGANs generate data following the generative distribution in Eq. 2. Note that this generative process is the same as for standard VAEs <ref type="bibr" target="#b32">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref>. In particular, PresGANs set the prior p(z) and the likelihood p θ (x | z) to be Gaussians,</p><formula xml:id="formula_2">p(z) = (z | 0, I) and p θ (x | z) = (x | µ θ (z), Σ θ (z)) .<label>(4)</label></formula><p>The mean µ θ (z) and covariance Σ θ (z) of the conditional p θ (x | z) are given by a neural network that takes z as input.</p><p>In general, both the mean µ θ (z) and the covariance Σ θ (z) can be functions of z.</p><p>For simplicity, in order to speed up the learning procedure, we set the covariance matrix to be diagonal with elements independent from z, i.e., Σ θ (z) = diag σ 2 , and we learn the vector σ together with θ . From now on, we parameterize the mean with η, write µ η (z), and define θ = (η, σ) as the parameters of the generative distribution.</p><p>To fit the model parameters θ , PresGANs optimize an adversarial loss similarly to <ref type="bibr">GANs.</ref> In doing so, they keep GANs' ability to generate samples with high perceptual quality. Unlike GANs, the entropy of the generative distribution of PresGANs is well-defined, and therefore PresGANs can prevent mode collapse by adding an entropy regularizer to Eq. 1. Furthermore, because PresGANs define a density over their generated samples, we can measure how they generalize to new data using predictive log-likelihood. We describe the entropy regularization in Section 3.1 and how to approximate the predictive log-likelihood in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Avoiding mode collapse via entropy regularization</head><p>One of the major issues that GANs face is mode collapse, where the generator tends to model only some parts or modes of the data distribution <ref type="bibr" target="#b1">(Arora et al., 2017</ref><ref type="bibr" target="#b2">(Arora et al., , 2018</ref>. PresGANs mitigate this problem by explicitly maximizing the entropy of the generative distribution,</p><formula xml:id="formula_3">PresGAN (θ , φ) = GAN (θ , φ) − λ (p θ (x)) .<label>(5)</label></formula><p>Here (p θ (x)) denotes the entropy of the generative distribution. It is defined as</p><formula xml:id="formula_4">(p θ (x)) = − p θ (x) [log p θ (x)] .<label>(6)</label></formula><p>The loss GAN (θ , φ) in Eq. 5 can be that of any of the existing GAN variants. In Section 5 we explore the standard deep convolutional generative adversarial network Algorithm 1: Learning with Prescribed Generative Adversarial Networks (PresGANs) input : Data x, entropy regularization level λ Initialize parameters η, σ, φ</p><formula xml:id="formula_5">for iteration t = 1, 2, . . . do Draw minibatch of observations x 1 , . . . , x b , . . . , x B for b = 1, 2, . . . , B do Get noised data: ε b ∼ (0, I) and x b = x b + σ ε b Draw latent variable z b ∼ (0, I) Generate data: s b ∼ (0, I) andx b =x b (z b , s b ; θ ) = µ η (z b ) + σ s b end Compute ∇ φ PresGAN (θ , φ) (Eq. 16</formula><p>) and take a gradient step for φ Initialize an HMC sampler using z b</p><formula xml:id="formula_6">Drawz (m) b ∼ p θ (z |x b ) for m = 1, . . . , M and b = 1, . . . , B using that sampler Compute ∇ η PresGAN ((η, σ), φ) (Eq. 14)</formula><p>and take a gradient step for η Compute ∇ σ PresGAN ((η, σ), φ) (Eq. 15) and take a gradient step for σ Truncate σ in the range [σ low , σ high ] end (DCGAN) <ref type="bibr" target="#b50">(Radford et al., 2015)</ref> and the more recent StyleGAN <ref type="bibr" target="#b30">(Karras et al., 2019)</ref> architectures.</p><p>The constant λ in Eq. 5 is a hyperparameter that controls the strength of the entropy regularization. In the extreme case when λ = 0, the loss function of PresGAN coincides with the loss of a GAN, where we replaced its implicit generative distribution with the infinite mixture in Eq. 2. In the other extreme when λ = ∞, optimizing PresGAN (θ , φ) corresponds to fitting a maximum entropy generator that ignores the data. For any intermediate values of λ, the first term of PresGAN (θ , φ) encourages the generator to fit the data distribution, whereas the second term encourages to cover all of the modes of the data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The entropy</head><p>(p θ (x)) is intractable because the integral in Eq. 6 cannot be computed. However, fitting the parameters θ of PresGANs only requires the gradients of the entropy. In Section 3.2 we describe how to form unbiased Monte Carlo estimates of these gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fitting Prescribed Generative Adversarial Networks</head><p>We fit PresGANs following the same adversarial procedure used in GANs. That is, we alternate between updating the parameters of the generative distribution θ and the parameters of the discriminator φ. The full procedure is given in Algorithm 1. We now describe each part in detail.</p><p>Fitting the generator. We fit the generator using stochastic gradient descent. This requires computing the gradients of the PresGAN loss with respect to θ ,</p><formula xml:id="formula_7">∇ θ PresGAN (θ , φ) = ∇ θ GAN (θ , φ) − λ∇ θ (p θ (x)) .<label>(7)</label></formula><p>We form stochastic estimates of ∇ θ GAN (θ , φ) based on reparameterization <ref type="bibr" target="#b32">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014;</ref><ref type="bibr" target="#b61">Titsias and Lázaro-Gredilla, 2014)</ref>; this requires differentiating Eq. 1. Specifically, we introduce a noise variable ε to reparameterize the conditional from Eq. 4, 3</p><formula xml:id="formula_8">x(z, ε; θ ) = µ η (z) + σ ε,<label>(8)</label></formula><p>where θ = (η, σ) and ε ∼ (0, I).</p><p>Here µ η (z) and σ denote the mean and standard deviation of the conditional p θ (x | z), respectively. We now write the first term of Eq. 7 as an expectation with respect to the latent variable z and the noise variable ε and push the gradient into the expectation,</p><formula xml:id="formula_9">∇ θ GAN (θ , φ) = p(z)p(ε) ∇ θ log 1 − D φ (x(z, ε; θ )) .<label>(9)</label></formula><p>In practice we use an estimate of Eq. 9 using one sample from p(z) and one sample from p(ε),</p><formula xml:id="formula_10">∇ θ GAN (θ , φ) = ∇ θ log 1 − D φ (x(z, ε; θ )) .<label>(10)</label></formula><p>The second term in Eq. 7, corresponding to the gradient of the entropy, is intractable. We estimate it using the same approach as <ref type="bibr" target="#b62">Titsias and Ruiz (2018)</ref>. We first use the reparameterization in Eq. 8 to express the gradient of the entropy as an expectation,</p><formula xml:id="formula_11">∇ θ (p θ (x)) = −∇ θ p θ (x) [log p θ (x)] = −∇ θ p(ε)p(z) log p θ (x) x=x(z,ε;θ ) = − p(ε)p(z) ∇ θ log p θ (x) x=x(z,ε;θ ) = − p(ε)p(z) ∇ x log p θ (x) x=x(z,ε;θ ) ∇ θ x(z, ε; θ ) ,</formula><p>where we have used the score function identity p θ (x) [∇ θ log p θ (x)] = 0 on the second line. We form a one-sample estimator of the gradient of the entropy as</p><formula xml:id="formula_12">∇ θ (p θ (x)) = −∇ x log p θ (x) x=x(z,ε;θ ) × ∇ θ x(z, ε; θ ).<label>(11)</label></formula><p>In Eq. 11, the gradient with respect to the reparameterization transformation ∇ θ x(z, ε; θ ) is tractable and can be obtained via back-propagation. We now derive</p><formula xml:id="formula_13">∇ x log p θ (x), ∇ x log p θ (x) = ∇ x p θ (x) p θ (x) = ∇ x p θ (x, z)dz p θ (x) = ∇ x p θ (x | z) p θ (x | z) p θ (x, z) p θ (x) dz = ∇ x log p θ (x | z)p θ (z | x)dz = p θ (z | x) [∇ x log p θ (x | z)] .</formula><p>While this expression is still intractable, we can estimate it. One way is to use selfnormalized importance sampling with a proposal learned using moment matching with an encoder <ref type="bibr" target="#b14">(Dieng and Paisley, 2019)</ref>. However, this would lead to a biased (albeit asymptotically unbiased) estimate of the entropy. In this paper, we form an unbiased estimate of ∇ x log p θ (x) using samples z (1) , . . . , z (M ) from the posterior,</p><formula xml:id="formula_14">∇ x log p θ (x) = 1 M M m=1 ∇ x log p θ (x | z (m) ), z (m) ∼ p θ (z | x).<label>(12)</label></formula><p>We obtain these samples using Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b47">(Neal et al., 2011)</ref>. Crucially, in order to speed up the algorithm, we initialize the HMC sampler at stationarity. That is, we initialize the HMC sampler with the sample z that was used to produce the generated sample x(z, ε; θ ) in Eq. 8, which by construction is an exact sample from p θ (z | x). This implies that only a few HMC iterations suffice to get good estimates of the gradient <ref type="bibr" target="#b62">(Titsias and Ruiz, 2018)</ref>. We also found this holds empirically; for example in Section 5 we use 2 burn-in iterations and M = 2 HMC samples to form the Monte Carlo estimate in Eq. 12.</p><p>Finally, using Eqs. 7 and 10 to 12 we can approximate the gradient of the entropyregularized adversarial loss with respect to the model parameters θ ,</p><formula xml:id="formula_15">∇ θ PresGAN (θ , φ) = ∇ θ log 1 − D φ (x(z, ε; θ )) + λ M M m=1 ∇ x log p θ (x | z (m) ) x=x(z (m) ,ε;θ ) × ∇ θ x z (m) , ε; θ . (13)</formula><p>In particular, the gradient with respect to the generator's parameters η is unbiasedly approximated by</p><formula xml:id="formula_16">∇ η PresGAN (θ , φ) = ∇ η log 1 − D φ (x(z, ε; θ )) − λ M M m=1 x(z (m) , ε; θ ) − µ η z (m) σ 2 ∇ η µ η (z (m) ),<label>(14)</label></formula><p>and the gradient estimator with respect to the standard deviation σ is</p><formula xml:id="formula_17">∇ σ PresGAN (θ , φ) = ∇ σ log 1 − D φ (x(z, ε; θ )) − λ M M m=1 x(z (m) , ε; θ ) − µ η z (m) σ 2 · ε.<label>(15)</label></formula><p>These gradients are used in a stochastic optimization algorithm to fit the generative distribution of PresGAN.</p><p>Fitting the discriminator. Since the entropy term in Eq. 5 does not depend on φ, optimizing the discriminator of a PresGAN is analogous to optimizing the discriminator of a GAN,</p><formula xml:id="formula_18">∇ φ PresGAN (θ , φ) = ∇ φ GAN (θ , φ).<label>(16)</label></formula><p>To prevent the discriminator from getting stuck in a bad local optimum where it can perfectly distinguish between real and generated data by relying on the added noise, we apply the same amount of noise to the real data x as the noise added to the generated data. That is, when we train the discriminator we corrupt the real data according to</p><formula xml:id="formula_19">x = x + σ ε,<label>(17)</label></formula><p>where σ is the standard deviation of the generative distribution and x denotes the real data. We then let the discriminator distinguish between x and x(z, ε; θ ) from Eq. 8.</p><p>This data noising procedure is a form of instance noise <ref type="bibr" target="#b58">(Sønderby et al., 2016)</ref>. However, instead of using a fixed annealing schedule for the noise variance as <ref type="bibr" target="#b58">Sønderby et al. (2016)</ref>, we let σ be part of the parameters of the generative distribution and fit it using gradient descent according to Eq. 15.</p><p>Stability. Data noising stabilizes the training procedure and prevents the discriminator from perfectly being able to distinguish between real and generated samples using the background noise. We refer the reader to Huszár (2016) for a detailed exposition.</p><p>When fitting PresGANs, data noising is not enough to stabilize training. This is because there are two failure cases brought in by learning the variance σ 2 using gradient descent. The first failure mode is when the variance gets very large, leading to a generator completely able to fool the discriminator. Because of data noising, the discriminator cannot distinguish between real and generated samples when the variance of the noise is large.</p><p>The second failure mode is when σ 2 gets very small, which makes the gradient of the entropy in Eq. 14 dominate the overall gradient of the generator. This is problematic because the learning signal from the discriminator is lost.</p><p>To stabilize training and avoid the two failure cases discussed above we truncate the variance of the generative distribution, σ low ≤ σ ≤ σ high (we apply this truncation element-wise). The limits σ low and σ high are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enabling tractable predictive log-likelihood approximation</head><p>Replacing the implicit generative distribution of GANs with the infinite mixture distribution defined in Eq. 2 has the advantage that the predictive log-likelihood can be tractably approximated. Consider an unseen datapoint x * . We estimate its log marginal likelihood log p θ (x * ) using importance sampling,</p><formula xml:id="formula_20">log p θ (x * ) ≈ log 1 S S s=1 p θ x * | z (s) · p z (s) r z (s) | x * ,<label>(18)</label></formula><p>where we draw S samples z (1) , . . . , z (S) from a proposal distribution r(z | x * ).</p><p>There are different ways to form a good proposal r(z | x * ), and we discuss several alternatives in Section 7.1 of the appendix. In this paper, we take the following approach. We define the proposal as a Gaussian distribution,</p><formula xml:id="formula_21">r(z | x * ) = (µ r , Σ r ).<label>(19)</label></formula><p>We set the mean parameter µ r to the maximum a posteriori solution, i.e., µ r = arg max z (log p θ (x * | z) + log p (z)). We initialize this maximization algorithm using the mean of a pre-fitted encoder, q γ (z | x * ). The encoder is fitted by minimizing the reverse KL divergence between q γ (z | x) and the true posterior p θ (z | x) using the training data. This KL is</p><formula xml:id="formula_22">KL q γ (z | x)||p θ (z | x) = log p θ (x) − q γ (z | x) log p θ (x | z)p(z) − log q γ (z | x) .<label>(20)</label></formula><p>Because the generative distribution is fixed at test time, minimizing the KL here is equivalent to maximizing the second term in Eq. 20, which is the evidence lower bound (ELBO) objective of VAEs.</p><p>We set the proposal covariance Σ r as an overdispersed version 4 of the encoder's covariance matrix, which is diagonal. In particular, to obtain Σ r we multiply the elements of the encoder's covariance by a factor γ. In Section 5 we set γ to 1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>GANs <ref type="bibr" target="#b22">(Goodfellow et al., 2014)</ref> have been extended in multiple ways, using alternative distance metrics and optimization methods (see, e.g., <ref type="bibr" target="#b36">Li et al., 2015;</ref><ref type="bibr" target="#b19">Dziugaite et al., 2015;</ref><ref type="bibr" target="#b48">Nowozin et al., 2016;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b51">Ravuri et al., 2018;</ref><ref type="bibr" target="#b21">Genevay et al., 2017)</ref> or using ideas from VAEs <ref type="bibr" target="#b40">(Makhzani et al., 2015;</ref><ref type="bibr" target="#b41">Mescheder et al., 2017;</ref><ref type="bibr" target="#b18">Dumoulin et al., 2016;</ref><ref type="bibr" target="#b17">Donahue et al., 2016;</ref><ref type="bibr" target="#b63">Tolstikhin et al., 2017;</ref><ref type="bibr" target="#b65">Ulyanov et al., 2018;</ref><ref type="bibr" target="#b54">Rosca et al., 2017)</ref>.</p><p>Other extensions aim at improving the sample diversity of GANs. For example, <ref type="bibr" target="#b60">Srivastava et al. (2017)</ref> use a reconstructor network that reverses the action of the generator. <ref type="bibr" target="#b37">Lin et al. (2018)</ref> use multiple observations (either real or generated) as an input to the discriminator to prevent mode collapse. <ref type="bibr" target="#b3">Azadi et al. (2018)</ref> and <ref type="bibr" target="#b64">Turner et al. (2018)</ref> use sampling mechanisms to correct errors of the generative distribution. <ref type="bibr" target="#b68">Xiao et al. (2018)</ref> relies on identifying the geometric structure of the data embodied under a specific distance metric. Other works have combined adversarial learning with maximum likelihood <ref type="bibr" target="#b23">(Grover et al., 2018;</ref><ref type="bibr" target="#b69">Yin and Zhou, 2019)</ref>; however, the low sample quality induced by maximum likelihood still occurs. Finally, <ref type="bibr" target="#b11">Cao et al. (2018)</ref> introduce a regularizer for the discriminator to encourage diverse activation patterns in the discriminator across different samples. In contrast to these works, PresGANs regularize the entropy of the generator to prevent mode collapse.</p><p>The idea of entropy regularization has been widely applied in many problems that involve estimation of unknown probability distributions. Examples include approximate Bayesian inference, where the variational objective contains an entropy penalty <ref type="bibr" target="#b29">(Jordan, 1998;</ref><ref type="bibr" target="#b7">Bishop, 2006;</ref><ref type="bibr" target="#b66">Wainwright et al., 2008;</ref><ref type="bibr" target="#b8">Blei et al., 2017)</ref>; reinforcement learning, where the entropy regularization allows to estimate more uncertain and explorative policies <ref type="bibr" target="#b57">(Schulman et al., 2015;</ref><ref type="bibr" target="#b44">Mnih et al., 2016)</ref>; statistical learning, where entropy regularization allows an inferred probability distribution to avoid collapsing to a deterministic solution <ref type="bibr" target="#b20">(Freund and Schapire, 1997;</ref><ref type="bibr" target="#b59">Soofi, 2000;</ref><ref type="bibr" target="#b28">Jaynes, 2003)</ref>; or optimal transport <ref type="bibr" target="#b53">(Rigollet and Weed, 2018)</ref>. More recently, <ref type="bibr" target="#b34">Kumar et al. (2019)</ref> have developed maximum-entropy generators for energy-based models using mutual information as a proxy for entropy.</p><p>Another body of related work is about how to quantitatively evaluate GANs. Inception scores measure the sample quality of GANs and are used extensively in the GAN literature <ref type="bibr" target="#b55">(Salimans et al., 2016;</ref><ref type="bibr" target="#b24">Heusel et al., 2017;</ref><ref type="bibr" target="#b6">Bińkowski et al., 2018)</ref>. However, sample quality measures only assess the quality of GANs as data generators and not as density estimators. Density estimators are evaluated for generalization to new data. Predictive log-likelihood is a measure of goodness of fit that has been used to assess generalization; for example in VAEs. Finding ways to evaluate predictive log-likelihood for GANs has been an open problem, because GANs do not define a density on the generated samples. <ref type="bibr" target="#b67">Wu et al. (2016)</ref> use a kernel density estimate <ref type="bibr" target="#b49">(Parzen, 1962)</ref> and estimate the log-likelihood with annealed importance sampling <ref type="bibr" target="#b46">(Neal, 2001)</ref>. <ref type="bibr" target="#b4">Balaji et al. (2018)</ref> show that an optimal transport GAN with entropy regularization can be viewed as a generative model that maximizes a variational lower bound on average sample likelihoods, which relates to the approach of VAEs <ref type="bibr" target="#b32">(Kingma and Welling, 2013)</ref>. <ref type="bibr" target="#b56">Sánchez-Martín et al. (2019)</ref> propose EvalGAN, a method to estimate the likelihood. Given an observation x , EvalGAN first finds the closest observation x that the GAN is able to generate, and then it estimates the likelihood p(x ) by approximating the proportion of samples z ∼ p(z) that lead to samples x that are close to x. EvalGAN requires selecting an appropriate distance metric for each problem and evaluates GANs trained with the usual implicit generative distribution. Finally, <ref type="bibr" target="#b23">Grover et al. (2018)</ref> assume invertibility of the generator to make log-likelihood tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Study</head><p>Here we demonstrate PresGANs' ability to prevent mode collapse and generate high-quality samples. We also evaluate its predictive performance as measured by log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">An Illustrative Example</head><p>In this section, we fit a GAN to a toy synthetic dataset of 10 modes. We choose the hyperparameters such that the GAN collapses. We then apply these same hyperparameters to fit a PresGAN on the same synthetic dataset. This experiment demonstrates the PresGAN's ability to correct the mode collapse problem of a GAN.</p><p>We form the target distribution by organizing a uniform mixture of K = 10 twodimensional Gaussians on a ring. The radius of the ring is r = 3 and each Gaussian has standard deviation 0.05. We then slice the circle into K parts. The location of the centers of the mixture components are determined as follows. Consider the k th mixture component. Its coordinates in the 2D space are center x = r · cos k · 2π K and center y = r · sin k · 2π K .</p><p>We draw 5,000 samples from the target distribution and fit a GAN and a PresGAN.</p><p>We set the dimension of the latent variables z used as the input to the generators to 10. We let both the generators and the discriminators have three fully connected layers with tanh activations and 128 hidden units in each layer. We set the minibatch size to 100 and use Adam for optimization <ref type="bibr" target="#b31">(Kingma and Ba, 2014)</ref>, with a learning rate of 10 −3 and 10 −4 for the discriminator and the generator respectively. The Adam hyperparameters are β 1 = 0.5 and β 2 = 0.999. We take one step to optimize the generator for each step of the discriminator. We pick a random minibatch at each iteration and run both the GAN and the PresGAN for 500 epochs.</p><p>For PresGAN we set the burn-in and the number of HMC samples to 2. We choose a standard number of 5 leapfrog steps and set the HMC learning rate to 0.02. The acceptance rate is fixed at 0.67. The log-variance of the noise of the generative distribution of PresGAN is initialized at 0.0. We put a threshold on the variance to a minimum value of σ low = 10 −2 and a maximum value of σ high = 0.3. The regularization parameter λ is 0.1. We fit the log-variance using Adam with a learning rate of 10 −4 . <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates how the PresGAN alleviates mode collapse. The distribution learned by the regular GAN misses 4 modes of the target distribution. The PresGAN is able to recover all the modes of the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Assessing mode collapse</head><p>In this section we evaluate PresGANs' ability to mitigate mode collapse on real datasets. We run two sets of experiments. In the first set of experiments we adopt the current experimental protocol for assessing mode collapse in the GAN literature. That is, we use the MNIST and STACKEDMNIST datasets, for which we know the true number of modes, and report two metrics: the number of modes recovered by the PresGAN and the KL divergence between the label distribution induced by the PresGAN and the true label distribution. In the second set of experiments we demonstrate that mode collapse can happen in GANs even when the number of modes is as low as 10 but the data is imbalanced.</p><p>Increased number of modes. We consider the MNIST and STACKEDMNIST datasets. MNIST is a dataset of hand-written digits, 5 in which each 28 × 28 × 1 image corresponds to a digit. There are 60,000 training digits and 10,000 digits in the test set. MNIST has 10 modes, one for each digit. STACKEDMNIST is formed by concatenating triplets of randomly chosen MNIST digits along the color channel to form images of size 28 × 28 × 3 <ref type="bibr" target="#b42">(Metz et al., 2017)</ref>. We keep the same size as the original MNIST, 60,000 training digits for 10,000 test digits. The total number of modes in STACKEDMNIST is 1,000, corresponding to the number of possible triplets. The true total number of modes is 10. All methods capture all the 10 modes. The KL captures a notion of discrepancy between the labels of real versus generated images. PresGAN generates images whose distribution of labels is closer to the data distribution, as evidenced by lower KL scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Modes KL DCGAN <ref type="bibr" target="#b50">(Radford et al., 2015)</ref> 10 ± 0.0 0.902 ± 0.036 VEEGAN <ref type="bibr" target="#b60">(Srivastava et al., 2017)</ref> 10 ± 0.0 0.523 ± 0.008 PACGAN <ref type="bibr" target="#b37">(Lin et al., 2018)</ref> 10 ± 0.0 0.441 ± 0.009 PresGAN (this paper) 10 ± 0.0 0.003 ± 0.001  <ref type="bibr" target="#b50">(Radford et al., 2015)</ref> 392.0 ± 7.376 8.012 ± 0.056 VEEGAN <ref type="bibr" target="#b60">(Srivastava et al., 2017)</ref> 761.8 ± 5.741 2.173 ± 0.045 PACGAN <ref type="bibr" target="#b37">(Lin et al., 2018)</ref> 992.0 ± 1.673 0.277 ± 0.005 PresGAN (this paper) 999.6 ± 0.489 0.115 ± 0.007</p><p>We consider DCGAN as the base architecture and, following <ref type="bibr" target="#b50">Radford et al. (2015)</ref>, we resize the spatial resolution of images to 64 × 64 pixels.</p><p>To measure the degree of mode collapse we form two diversity metrics, following <ref type="bibr" target="#b60">Srivastava et al. (2017)</ref>. Both of these metrics require to fit a classifier to the training data. Once the classifier has been fit, we sample S images from the generator. The first diversity metric is the number of modes captured, measured by the number of classes that are captured by the classifier. We say that a class k has been captured if there is at least one generated sample for which the probability of being assigned to class k is the largest. The second diversity metric is the KL divergence between two discrete distributions: the empirical average of the (soft) output of the classifier on generated images, and the empirical average of the (soft) output of the classifier on real images from the test set. We choose the number of generated images S to match the number of test samples on each dataset. That is, S = 10,000 for both MNIST and STACKEDMNIST. We expect the KL divergence to be zero if the distribution of the generated samples is indistinguishable from that of the test samples.</p><p>We measure the two mode collapse metrics described above against DCGAN <ref type="bibr" target="#b50">(Radford et al., 2015)</ref> (the base architecture of PresGAN for this experiment). We also compare against other methods that aim at alleviating mode collapse in GANs, namely, VEEGAN <ref type="bibr" target="#b60">(Srivastava et al., 2017)</ref> and PACGAN <ref type="bibr" target="#b37">(Lin et al., 2018)</ref>. For PresGAN we set the entropy regularization parameter λ to 0.01. We chose the variance thresholds to be σ low = 0.001 and σ high = 0.3.</p><p>Tables 1 and 2 show the number of captured modes and the KL for each method. Assessing the impact of the entropy regularization parameter λ on mode collapse on MNIST and STACKEDMNIST. When λ = 0 (i.e., no entropy regularization is applied to the generator), then mode collapse occurs as expected. When entropy regularization is applied but the value of λ is very small (λ = 10 −6 ) then mode collapse can still occur as the level of regularization is not enough. When the value of λ is appropriate for the data then mode collapse does not occur. Finally, when λ is too high then mode collapse can occur because the entropy maximization term dominates and the data is poorly fit.</p><p>MNIST STACKEDMNIST λ Modes KL Modes KL 0 10 ± 0.0 0.050 ± 0.0035 418.2 ± 7.68 4.151 ± 0.0296 10 −6 10 ± 0.0 0.005 ± 0.0008 989.8 ± 1.72 0.239 ± 0.0059 10 −2 10 ± 0.0 0.003 ± 0.0006 999.6 ± 0.49 0.115 ± 0.0074 5 × 10 −2 10 ± 0.0 0.004 ± 0.0008 999.4 ± 0.49 0.099 ± 0.0047 10 −1 10 ± 0.0 0.005 ± 0.0004 999.4 ± 0.80 0.102 ± 0.0032 5 × 10 −1 10 ± 0.0 0.006 ± 0.0011 907.0 ± 9.27 0.831 ± 0.0209 The figures show the number of modes captured (higher is better) and the KL divergence (lower is better) under increasingly imbalanced settings. The maximum number of modes in each case is 10. All methods suffer from mode collapse as the level of imbalance increases except for the PresGAN which is robust to data imbalance.</p><p>The results are averaged across 5 runs. All methods capture all the modes of MNIST. This is not the case on STACKEDMNIST, where the PresGAN is the only method that can capture all the modes. Finally, the proportion of observations in each mode of PresGAN is closer to the true proportion in the data, as evidenced by lower KL divergence scores.</p><p>We also study the impact of the entropy regularization by varying the hyperparameter λ from 0 to 0.5. <ref type="table" target="#tab_3">Table 3</ref> illustrates the results. Unsurprisingly, when there is no entropy regularization, i.e., when λ = 0, then mode collapse occurs. This is also the case when the level of regularization is not enough (λ = 10 −6 ). There is a whole range of values for λ such that mode collapse does not occur (λ ∈ {0.01, 0.05, 0.1}). Finally, when λ is too high for the data and architecture under study, mode collapse can still occur. This is because when λ is too high, the entropy regularization term dominates the loss in Eq. 5 and in turn the generator does not fit the data as well. This is also evidenced by the higher KL divergence score when λ = 0.5 vs. when 0 &lt; λ &lt; 0.5.</p><p>Increased data imbalance. We now show that mode collapse can occur in GANs when the data is imbalanced, even when the number of modes of the data distribution is small. We follow <ref type="bibr" target="#b12">Dieng et al. (2018a)</ref> and consider a perfectly balanced version of MNIST as well as nine imbalanced versions. To construct the balanced dataset we used 5,000 training examples per class, totaling 50,000 training examples. We refer to this original balanced dataset as D 0 . Each additional training set D k leaves only 5 training examples for each class j ≤ k, and 5,000 for the rest. (See the Appendix for all the class distributions.)</p><p>We used the same classifier trained on the unmodified MNIST but fit each method on each of the 9 new MNIST distributions. We chose λ = 0.1 for PresGAN. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the results in terms of both metrics-number of modes and KL divergence. DCGAN, VEEGAN, and PACGAN face mode collapse as the level of imbalance increases. This is not the case for PresGAN, which is robust to imbalance and captures all the 10 modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Assessing sample quality</head><p>In this section we assess PresGANs' ability to generate samples of high perceptual quality. We rely on perceptual quality of generated samples and on Fréchet Inception distance (FID) scores <ref type="bibr" target="#b24">(Heusel et al., 2017)</ref>. We also consider two different GAN architectures, the standard DCGAN and the more recent StyleGAN, to show robustness of PresGANs vis-a-vis the underlying GAN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCGAN.</head><p>We use DCGAN <ref type="bibr" target="#b50">(Radford et al., 2015)</ref> as the base architecture and build PresGAN on top of it. We consider four datasets: MNIST, STACKEDMNIST, CIFAR-10, and CelebA. CIFAR-10 <ref type="bibr" target="#b33">(Krizhevsky et al., 2009</ref>) is a well-studied dataset of 32 × 32 images that are classified into one of the following categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. CelebA <ref type="bibr" target="#b38">(Liu et al., 2015)</ref> is a large-scale face attributes dataset. Following <ref type="bibr" target="#b50">Radford et al. (2015)</ref>, we resize all images to 64 × 64 pixels. We use the default DCGAN settings. We refer the reader to the code we used for DCGAN, which was taken from https: //github.com/pytorch/examples/tree/master/dcgan. We set the seed to 2019 for reproducibility.</p><p>There are hyperparameters specific to PresGAN. These are the noise and HMC hyperparameters. We set the learning rate for the noise parameters σ to 10 −3 and constrain its values to be between 10 −3 and 0.3 for all datasets. We initialize log σ to −0.5. We set the burn-in and the number of HMC samples to 2. We choose a standard number of 5 leapfrog steps and set the HMC learning rate to 0.02. The acceptance rate is fixed at 0.67. We found that different λ values worked better for different datasets. We used λ = 5 × 10 −4 for CIFAR-10 and CELEBA λ = 0.01 for MNIST and STACKEDMNIST.</p><p>We found the PresGAN's performance to be robust to the default settings for most of these hyperparameters. However we found the initialization for σ and its learning rate to play a role in the quality of the generated samples. The hyperparameters mentioned above for σ worked well for all datasets. <ref type="table" target="#tab_4">Table 4</ref> shows the FID scores for DCGAN and PresGAN across the four datasets. We can conclude that PresGAN generates images of high visual quality. In addition, the FID scores are lower because PresGAN explores more modes than DCGAN. Indeed, when the generated images account for more modes, the FID sufficient statistics (the mean and covariance of the Inception-v3 pool3 layer) of the generated data get closer to the sufficient statistics of the empirical data distribution.</p><p>We also report the FID for VEEGAN and PACGAN in <ref type="table" target="#tab_4">Table 4</ref>. VEEGAN achieves better FID scores than DCGAN on all datasets but CELEBA. This is because VEEGAN collapses less than DCGAN as evidenced by <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. PACGAN achieves better FID scores than both DCGAN and VEEGAN on all datasets but on STACKEDMNIST where it achieves a significantly worse FID score. Finally, PresGAN outperforms all of these methods on the FID metric on all datasets signaling its ability to mitigate mode collapse while preserving sample quality.</p><p>Besides the FID scores, we also assess the visual quality of the generated images. In Section 7.3 of the appendix, we show randomly generated (not cherry-picked) images from DCGAN, VEEGAN, PACGAN, and PresGAN. For PresGAN, we show the mean of the conditional distribution of x given z. The samples generated by PresGAN have high visual quality; in fact their quality is comparable to or better than the DCGAN samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN.</head><p>We now consider a more recent GAN architecture (StyleGAN) <ref type="bibr" target="#b30">(Karras et al., 2019)</ref> and a higher resolution image dataset (FFHQ). FFHQ is a diverse dataset of faces from Flickr 6 introduced by <ref type="bibr" target="#b30">Karras et al. (2019)</ref>. The dataset contains 70,000 high-quality PNG images with considerable variation in terms of age, ethnicity, and image background. We use a resolution of 128 × 128 pixels.</p><p>StyleGAN feeds multiple sources of noise z to the generator. In particular, it adds Gaussian noise after each convolutional layer before evaluating the nonlinearity. Building PresGAN on top of StyleGAN therefore requires to sample all noise variables z through HMC at each training step. To speed up the training procedure, we only sample the noise variables corresponding to the input latent code and condition on all the other Gaussian noise variables. In addition, we do not follow the progressive growing of the networks of <ref type="bibr" target="#b30">Karras et al. (2019)</ref> for simplicity.</p><p>For this experiment, we choose the same HMC hyperparameters as for the previous experiments but restrict the variance of the generative distribution to be σ high = 0.2. We set λ = 0.001 for this experiment. <ref type="figure" target="#fig_2">Figure 3</ref> shows cherry-picked images generated from StyleGAN and PresGAN. We can observe that the PresGAN maintains as good perceptual quality as the base architecture. In addition, we also observed that the StyleGAN tends to produce some redundant images (these are not shown in <ref type="figure" target="#fig_2">Figure 3</ref>), something that we did not observe with the PresGAN. This lack of diversity was also reflected in the FID scores which were 14.72 ± 0.09 for StyleGAN and 12.15 ± 0.09 for PresGAN. These results suggest that entropy regularization effectively reduces mode collapse while preserving sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Assessing held-out predictive log-likelihood</head><p>In this section we evaluate PresGANs for generalization using predictive log-likelihood. We use the DCGAN architecture to build PresGAN and evaluate the log-likelihood on two benchmark datasets, MNIST and CIFAR-10. We use images of size 32 × 32.</p><p>We compare the generalization performance of the PresGAN against the VAE <ref type="bibr" target="#b32">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref> by controlling for the architecture and the evaluation procedure. In particular, we fit a VAE that has the same decoder architecture as the PresGAN. We form the VAE encoder by using the same architecture as the DCGAN discriminator and getting rid of the output layer. We used linear maps to get the mean and the log-variance of the approximate posterior.</p><p>To measure how PresGANs compare to traditional GANs in terms of log-likelihood, we also fit a PresGAN with λ = 0.</p><p>Evaluation. We control for the evaluation procedure and follow what's described in Section 3.3 for all methods. We use S = 2,000 samples to form the importance sampling estimator. Since the pixel values are normalized in [−1, +1], we use a truncated Gaussian likelihood for evaluation. Specifically, for each pixel of the test image, we divide the Gaussian likelihood by the probability (under the generative model) that the pixel is within the interval [−1, +1]. We use the truncated Gaussian likelihood at test time only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings.</head><p>For the PresGAN, we use the same HMC hyperparameters as for the previous experiments. We constrain the variance of the generative distribution using σ low = 0.001 and σ high = 0.2. We use the default DCGAN values for the remaining hyperparameters, including the optimization settings. For the CIFAR-10 experiment, we choose λ = 0.001. We set all learning rates to 0.0002. We set the dimension of the latent variables to 100. We ran both the VAE and the PresGAN for a maximum of 200 epochs. For MNIST, we use the same settings as for CIFAR-10 but use λ = 0.0001 and ran all methods for a maximum of 50 epochs. <ref type="table" target="#tab_5">Table 5</ref> summarizes the results. Here GAN denotes the PresGAN fitted using λ = 0. The VAE outperforms both the GAN and the PresGAN on both MNIST and CIFAR-10. This is unsurprising given VAEs are fitted to maximize log-likelihood. The GAN's performance on CIFAR-10 is particularly bad, suggesting it suffered from mode collapse. The PresGAN, which mitigates mode collapse achieves significantly better performance than the GAN on CIFAR-10. To further analyze the generalization performance, we also report the log-likelihood on the training set in <ref type="table" target="#tab_5">Table 5</ref>. We can observe that the difference between the training log-likelihood and the test log-likelihood is very small for all methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Epilogue</head><p>We introduced the PresGAN, a variant of GANs that addresses two of their limitations. PresGANs prevent mode collapse and are amenable to predictive log-likelihood evaluation. PresGANs model data by adding noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise stabilizes training, renders approximation of predictive log-likelihoods tractable, and enables unbiased estimators for the gradients of the entropy of the generative distribution. We evaluated PresGANs on several image datasets. We found they effectively prevent mode collapse and generate samples of high perceptual quality. We further found that PresGANs reduce the gap in performance between GANs and VAEs in terms of predictive log-likelihood.</p><p>We found the level of entropy regularization λ plays an important role in mode collapse. We leave as future work the task of finding the optimal λ. We now discuss some insights that we concluded from our empirical study in Section 5. Implicit distributions and sample quality. It's been traditionally observed that GANs generate samples with higher perceptual quality than VAEs. This can be explained by looking at the two ways in which GANs and VAEs differ; the generative distribution and the objective function. VAEs use prescribed generative distributions and optimize likelihood whereas GANs use implicit generative distributions and optimize an adversarial loss. Our results in Section 5 suggest that the implicit generators of traditional GANs are not the key to high sample quality; rather, the key is the adversarial loss. This is because PresGANs use the same prescribed generative distributions as VAEs and achieve similar or sometimes better sample quality than GANs.</p><p>Mode collapse, diversity, and imbalanced data. The current literature on measuring mode collapse in GANs only focuses on showing that mode collapse happens when the number of modes in the data distribution is high. Our results show that mode collapse can happen not only when the number of modes of the data distribution is high, but also when the data is imbalanced; even when the number of modes is low. Imbalanced data are ubiquitous. Therefore, mitigating mode collapse in GANs is important for the purpose of diverse data generation. GANs and generalization. The main method to evaluate generalization for density estimators is predictive log-likelihood. Our results agree with the current literature that GANs don't generalize as well as VAEs which are specifically trained to maximize log-likelihood. However, our results show that entropy-regularized adversarial learning can reduce the gap in generalization performance between GANs and VAEs. Methods that regularize GANs with the maximum likelihood objective achieve good generalization performance when compared to VAEs but they sacrifice sample quality when doing so <ref type="bibr" target="#b23">(Grover et al., 2018)</ref>. In fact we also experienced this tension between sample quality and high log-likelihood in practice.</p><p>Why is there such a gap in generalization, as measured by predictive log-likelihood, between GANs and VAEs? In our empirical study in Section 5 we controlled for the architecture and the evaluation procedure which left us to compare maximizing likelihood against adversarial learning. Our results suggest mode collapse alone does not explain the gap in generalization performance between GANs and VAEs. Indeed <ref type="table" target="#tab_5">Table 5</ref> shows that even on MNIST, where mode collapse does not happen, the VAE achieves significantly better log-likelihood than a GAN.</p><p>We looked more closely at the encoder fitted at test time to evaluate log-likelihood for both the VAE and the GAN (not shown in this paper). We found that the encoder implied by a fitted GAN is very underdispersed compared to the encoder implied by a fitted VAE. Underdispersed proposals have a negative impact on importance sampling estimates of log-likelihood. We tried to produce a more overdispersed proposal using the procedure described in Section 3.3. However we leave as future work learning overdispersed proposals for GANs for the purpose of log-likelihood evaluation.</p><p>by n q η (z n | x n ) log p θ (x n , z n ) − log q η (z n | x n ) . We set the proposal r(z | x * ) = q η (z | x * ). (Alternatively, the encoder can be used to initialize a sampler.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Assessing mode collapse under increased data imbalance</head><p>In the main paper we show that mode collapse can happen not only when there are increasing number of modes, as done in the GAN literature, but also when the data is imbalanced. We consider a perfectly balanced version of MNIST by using 5,000 training examples per class, totalling 50,000 training examples. We refer to this original balanced dataset as D1. We build nine additional training sets from this balanced dataset. Each additional training set Dk leaves only 5 training examples for each class j &lt; k. See <ref type="table">Table 6</ref> for all the class distributions. <ref type="table">Table 6</ref>: Class distributions using the MNIST dataset. There are 10 class-one class for each of the 10 digits in MNIST. The distribution D1 is uniform and the other distributions correspond to different imbalance settings as given by the proportions in the table. Note these proportions might not sum to one exactly because of rounding.</p><p>Dist 0 1 2 3 4 5 6 7 8 9 D1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 D2 10 −3 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 D3 10 −3 10 −3 0.12 0.12 0.12 0.12 0.12 0.12 0.12 0.12 D4 10 −3 10 −3 10 −3 0.14 0.14 0.14 0.14 0.14 0.14 0.14 D5 10 −3 10 −3 10 −3 10 −3 0.17 0.17 0.17 0.17 0.17 0.17 D6 10 −3 10 −3 10 −3 10 −3 10 −3 0.20 0.20 0.20 0.20 0.20 D7 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 0.25 0.25 0.25 0.25 D8 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 0.33 0.33 0.33 D9 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 0.49 0.49 D10 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 0.99</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Sample quality</head><p>Here we show some sample images generated by DCGAN and PresGAN, together with real images from each dataset. These images were not cherry-picked, we randomly selected samples from all models. For PresGAN, we show the mean of the generator distribution, conditioned on the latent variable z. In general, we observed the best image quality is achieved by the entropy-regularized PresGAN.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Density estimation with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Assessing mode collapse under increased data imbalance on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Generated images on FFHQ for StyleGAN (left) and PresGAN (right). The PresGAN maintains the high perceptual quality of the StyleGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Real and generated images on CELEBA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Assessing mode collapse on MNIST.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Assessing mode collapse on STACKEDMNIST. The true total number of modes is 1,000. All methods suffer from collapse except PresGAN, which captures nearly all the modes of the data distribution. Furthermore, PresGAN generates images whose distribution of labels is closer to the data distribution, as evidenced by lower KL scores.</figDesc><table><row><cell>Method</cell><cell>Modes</cell><cell>KL</cell></row><row><cell>DCGAN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Fréchet Inception distance (FID) (lower is better). PresGAN has lower FID scores than DCGAN, VEEGAN, and PACGAN. This is because PresGAN mitigates mode collapse while preserving sample quality.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>FID</cell></row><row><cell>DCGAN (Radford et al., 2015)</cell><cell>MNIST</cell><cell>113.129 ± 0.490</cell></row><row><cell>VEEGAN (Srivastava et al., 2017)</cell><cell>MNIST</cell><cell>68.749 ± 0.428</cell></row><row><cell>PACGAN (Lin et al., 2018)</cell><cell>MNIST</cell><cell>58.535 ± 0.135</cell></row><row><cell>PresGAN (this paper)</cell><cell>MNIST</cell><cell>42.019 ± 0.244</cell></row><row><cell>DCGAN</cell><cell>STACKEDMNIST</cell><cell>97.788 ± 0.199</cell></row><row><cell>VEEGAN</cell><cell>STACKEDMNIST</cell><cell>86.689 ± 0.194</cell></row><row><cell>PACGAN</cell><cell>STACKEDMNIST</cell><cell>117.128 ± 0.172</cell></row><row><cell>PresGAN</cell><cell>STACKEDMNIST</cell><cell>23.965 ± 0.134</cell></row><row><cell>DCGAN</cell><cell>CIFAR-10</cell><cell>103.049 ± 0.195</cell></row><row><cell>VEEGAN</cell><cell>CIFAR-10</cell><cell>95.181 ± 0.416</cell></row><row><cell>PACGAN</cell><cell>CIFAR-10</cell><cell>54.498 ± 0.337</cell></row><row><cell>PresGAN</cell><cell>CIFAR-10</cell><cell>52.202 ± 0.124</cell></row><row><cell>DCGAN</cell><cell>CELEBA</cell><cell>39.001 ± 0.243</cell></row><row><cell>VEEGAN</cell><cell>CELEBA</cell><cell>46.188 ± 0.229</cell></row><row><cell>PACGAN</cell><cell>CELEBA</cell><cell>36.058 ± 0.212</cell></row><row><cell>PresGAN</cell><cell>CELEBA</cell><cell>29.115 ± 0.218</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Generalization performance as measured by negative log-likelihood (lower is better) on MNIST and CIFAR-10. Here the GAN denotes a PresGAN fitted without entropy regularization (λ = 0). The PresGAN reduces the gap in performance between the GAN and the VAE on both datasets.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell><cell cols="2">CIFAR-10</cell></row><row><cell></cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell>Test</cell></row><row><cell>VAE</cell><cell cols="4">−3483.94 −3408.16 −1978.91 −1665.84</cell></row><row><cell>GAN</cell><cell cols="2">−1410.78 −1423.39</cell><cell>−572.25</cell><cell>−569.17</cell></row><row><cell cols="5">PresGAN −1418.91 −1432.50 −1050.16 −1031.70</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: The code for this paper can be found at https://github.com/adjidieng/PresGANs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A GAN can perfectly fit this distribution when choosing the right hyperparameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">With this reparameterization we use the notation x(z, ε; θ ) instead ofx(z; θ ) to denote a sample from the generative distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In general, overdispersed proposals lead to better importance sampling estimates.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See http://yann.lecun.com/exdb/mnist.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See https://github.com/NVlabs/ffhq-dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ian Goodfellow, Andriy Mnih, Aaron Van den Oord, and Laurent Dinh for their comments. Francisco J. R. Ruiz is supported by the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No. 706760. Adji B. Dieng is supported by a Google PhD Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 7.1 Other Ways to Compute Predictive Log-Likelihood</head><p>Here we discuss different ways to obtain a proposal in order to approximate the predictive log-likelihood. For a test instance x * , we estimate the marginal loglikelihood log p θ (x * ) using importance sampling,</p><p>where we draw the S samples z (1) , . . . , z (S) from a proposal distribution r(z | x * ). We next discuss different ways to form the proposal r(z | x * ).</p><p>One way to obtain the proposal is to set r(z | x * ) as a Gaussian distribution whose mean and variance are computed using samples from an HMC algorithm with</p><p>That is, the mean and variance of r(z | x * ) are set to the empirical mean and variance of the HMC samples.</p><p>The procedure above requires to run an HMC sampler, and thus it may be slow. We can accelerate the procedure with a better initialization of the HMC chain. Indeed, the second way to evaluate the log-likelihood also requires the HMC sampler, but it is initialized using a mapping z = g η (x ). The mapping g η (x ) is a network that maps from observed space x to latent space z. The parameters η of the network can be learned at test time using generated data. In particular, η can be obtained by generating data from the fitted generator of PresGAN and then fitting g η (x ) to map x to z by maximum likelihood. This is, we first sample M pairs (z m , x m ) M m=1 from the learned generative distribution and then we obtain η by minimizing M m=1 ||z m − g η (x m )|| 2 2 . Once the mapping is fitted, we use it to initialize the HMC chain.</p><p>A third way to obtain the proposal is to learn an encoder network q η (z | x) jointly with the rest of the PresGAN parameters. This is effectively done by letting the discriminator distinguish between pairs (x, z) ∼ p d (x) · q η (z | x) and (x, z) ∼ p θ (x, z) rather than discriminate x against samples from the generative distribution. These types of discriminator networks have been used to learn a richer latent space for GAN <ref type="bibr" target="#b17">(Donahue et al., 2016;</ref><ref type="bibr" target="#b18">Dumoulin et al., 2016)</ref>. In such cases, we can use the encoder network q η (z | x) to define the proposal, either by setting r(z | x * ) = q η (z | x * ) or by initializing the HMC sampler at the encoder mean.</p><p>The use of an encoder network is appealing but it requires a discriminator that takes pairs (x, z). The approach that we follow in the paper also uses an encoder network but keeps the discriminator the same as for the base DCGAN. We found this approach to work better in practice. More in detail, we use an encoder network q η (z | x); however the encoder is fitted at test time by maximizing the variational ELBO, given</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (GANs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Do gans learn the distribution? some theory and empirics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06758</idno>
		<title level="m">Discriminator rejection sampling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Entropic GANs meet VAEs: A statistical approach to compute sample likelihoods in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04147</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmd</forename><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving gan training via binarized representation entropy (bre) regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03644</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Learning with reflective likelihoods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04863</idno>
		<title level="m">Avoiding latent variable collapse with generative skip models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reweighted expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Diggle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Gratton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monte Carlo methods of inference for implicit statistical models</title>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
		<title level="m">Training generative neural networks via maximum mean discrepancy optimization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00292</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flow-gan: Combining maximum likelihood and adversarial learning in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Instance noise: a trick for stabilising gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<ptr target="https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probability theory: The logic of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pacgan: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian neural networks and density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2391" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Microsoft Research</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<title level="m">Learning in implicit generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning implicit generative models with the method of learned moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11006</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Entropic optimal transport is maximum-likelihood deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathématique</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="1228" to="1235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04987</idno>
		<title level="m">Variational approaches for auto-encoding generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sánchez-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pérez-Cruz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09557</idno>
		<title level="m">Out-of-sample testing for gans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Principal information theoretic approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Soofi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">452</biblScope>
			<biblScope unit="page" from="1349" to="1353" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3308" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational bayes for non-conjugate inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ruiz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02078</idno>
		<title level="m">Unbiased implicit variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saatci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11357</idno>
		<title level="m">Metropolis-hastings generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">It takes (only) two: Adversarial generator-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">On the quantitative analysis of decoder-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04273</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">BourGAN: Generative networks with metric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12659</idno>
		<title level="m">Semi-implicit generative model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">On the discriminationgeneralization tradeoff in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02771</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
