<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WEAKLY SUPERVISED VIDEO ANOMALY DETECTION VIA CENTER-GUIDED DISCRIMINATIVE LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WEAKLY SUPERVISED VIDEO ANOMALY DETECTION VIA CENTER-GUIDED DISCRIMINATIVE LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly detection</term>
					<term>weak supervision</term>
					<term>multiple-instance learning</term>
					<term>center loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in surveillance videos is a challenging task due to the diversity of anomalous video content and duration. In this paper, we consider video anomaly detection as a regression problem with respect to anomaly scores of video clips under weak supervision. Hence, we propose an anomaly detection framework, called Anomaly Regression Net (AR-Net), which only requires video-level labels in training stage. Further, to learn discriminative features for anomaly detection, we design a dynamic multiple-instance learning loss and a center loss for the proposed AR-Net. The former is used to enlarge the inter-class distance between anomalous and normal instances, while the latter is proposed to reduce the intra-class distance of normal instances. Comprehensive experiments are performed on a challenging benchmark: Shang-haiTech. Our method yields a new state-of-the-art result for video anomaly detection on ShanghaiTech dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video anomaly detection is an important yet challenging task in computer vision, and it is widely used in crime warning, intelligent video surveillance and evidence collection. According to the study <ref type="bibr">[1]</ref>, there are two kinds of paradigms: unary classification and binary classification, for weakly-supervised video anomaly detection. Anomalies are usually defined as the video content patterns that are different from usual patterns in previous works [2] <ref type="bibr" target="#b0">[3]</ref>  <ref type="bibr" target="#b1">[4]</ref> [5] <ref type="bibr" target="#b3">[6]</ref>. Based on this definition, the unary classification paradigm-based methods only model usual patterns with normal training samples. However, it is impossible to collect all kinds of normal samples in a training set. Consequently, normal videos being different from training ones may tend to be false alarmed under this paradigm.</p><p>To address this issue, the binary classification paradigm was introduced, in which training data contains both anomalous and normal videos. Following the binary classification  <ref type="bibr" target="#b4">[7]</ref>. (b) Complementary inner bag loss in <ref type="bibr" target="#b6">[9]</ref>. (c) Temporal ranking loss in <ref type="bibr" target="#b5">[8]</ref>. (d) Ours.</p><p>paradigm, some studies on anomaly detection [1] <ref type="bibr" target="#b4">[7]</ref> [8] <ref type="bibr" target="#b6">[9]</ref> have been published. In [1], video anomaly detection was formulated as a fully-supervised learning task under noise labels. As a correction, a Graph Convolutional Network (GCN) was proposed to train an action classifier. The GCN and the action classifier were optimized alternately. In this paper, we formulate video anomaly detection as a weakly-supervised learning problem following binary classification paradigm, where only video-level labels are involved in training stage. Recently, Multiple-Instance Learning (MIL) has become a major technique in several computer vision tasks including weakly-supervised temporal activity localization and classification <ref type="bibr" target="#b7">[10]</ref>  <ref type="bibr" target="#b8">[11]</ref> and weakly-supervised object detection <ref type="bibr" target="#b9">[12]</ref>. There are also some MIL-based video anomaly detection studies <ref type="bibr" target="#b4">[7]</ref> [8] <ref type="bibr" target="#b6">[9]</ref>. Each training video is treated as a bag, and clips of videos are regarded as instances in these methods. An anomalous video is treated as a positive bag, and a normal one is presented as a negative bag. Sultani et al. <ref type="bibr" target="#b4">[7]</ref> proposed a deep MIL ranking model with features extracted by C3D network <ref type="bibr" target="#b10">[13]</ref> as input. The deep MIL ranking loss was proposed for separating the anomaly scores of anomalous and normal instances. Zhang et al. <ref type="bibr" target="#b6">[9]</ref> proposed a complementary inner bag loss to reduce intra-class distances and enlarge inter-class distances of instances simultaneously. The highest and lowest anomaly scores of anomalous and normal videos were required. Zhu and Newsam <ref type="bibr" target="#b5">[8]</ref> proposed a temporal ranking loss calculated according to anomaly scores. The anomaly score of a video is the weighted sum of a video anomaly score vector and an attention vector. However, as shown in <ref type="figure" target="#fig_0">Fig 1,</ref> these methods adopted pair-wisely calculated losses, based on which the detection ability of models partly depend on batch size. In other words, the detection performance is partly limited by graphics memory. In this work, we look into a method that maximizes the inter-class distances and minimizes intra-class distances without instances from pair videos.</p><p>We propose a framework, termed Anomaly Regression Network (AR-Net), and two novel losses to learn discriminative features under video-level weak supervision. As illustrated in <ref type="figure" target="#fig_0">Fig 1-(d)</ref>, the dynamic multiple-instance learning loss (L DMIL ) is proposed to make features more separable. L DMIL is acquired by calculating the cross entropy between the anomaly scores of video clips and their corresponding video labels. The center loss is designed as the distances between anomaly scores of video clips and their corresponding average anomaly score c i in each training normal video. By minimizing the two losses, a discriminative feature representation can be obtained for video anomaly detection.</p><p>Comprehensive experiments are conducted on a benchmark: ShanghaiTech <ref type="bibr" target="#b11">[14]</ref>. Our approach yields a new stateof-the-art result and obtains an absolute gain of 4.94% in terms of Area Under the Curve (AUC) on ShanghaiTech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>In this section, we first define the notations and problem statement. Then we describe the proposed feature extraction network. Finally, we present our AR-Net followed by a detailed description of the proposed losses. Problem Statement: A training set consisting of n videos is denoted by Ï‡ = {x i } n i=1 in an anomaly detection dataset. The temporal duration of the dataset is defined as</p><formula xml:id="formula_0">T = {t i } n i=1 , where t i is the clip number of the i-th video. The video anomaly label set is denoted as Y = {y i } n i=1 , where y i = {0, 1}.</formula><p>In the testing stage, the predicted anomaly score vector of a video x is denoted as s = {s j } t j=1 , where s j âˆˆ [0, 1], and s j is anomaly score of the j-th video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Extraction</head><p>To make use of both the appearance and motion information of the videos, Inflated 3D (I3D) <ref type="bibr" target="#b12">[15]</ref>, pretrained on the Kinetics <ref type="bibr" target="#b12">[15]</ref> dataset, is used as the feature extraction network. An input video is divided into non-overlapped clips, each of which contains 16 consecutive frames. The RGB and Optical-Flow versions of I3D are denoted by I3D RGB and I3D Optical-Flow respectively. The former takes RGB frames as input while the latter takes Optical-Flow frames. We concatenate features from penultimate layer of the I3D RGB and I3D Optical-Flow as our final feature representation of video clips.</p><p>The feature matrix</p><formula xml:id="formula_1">X i shown in Fig 2 is composed of features from the training video x i .The dimension of X i is F Ã— t i ,</formula><p>where F is the dimension of clip features. X i rather than raw video clips is fed to our AR-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anomaly Regression Network</head><p>The architecture of AR-Net is shown in <ref type="figure" target="#fig_1">Fig 2.</ref> The Fully Connected Layer (FC-Layer) and Anomaly Regression Layer (AR-Layer) in AR-Net require only video-level labels for video anomaly detection. We adopt ReLU <ref type="bibr" target="#b13">[16]</ref> as the activation function of FC-Layer. To avoid overfitting, Dropout <ref type="bibr" target="#b14">[17]</ref> is introduced to FC-Layer and can be formalized as follows:</p><formula xml:id="formula_2">X FC i = D(max(0, W FC X i + b FC )) (1) where D(Â·) denotes Dropout, W FC âˆˆ R F Ã—F and b FC âˆˆ R F Ã—1</formula><p>are learnable parameters to be optimized from the training data, and X FC i âˆˆ R F Ã—ti is the i-th video feature output from output features of the FC-Layer.</p><p>We establish a mapping function between the representations X FC i and anomaly score vectors s i by AR-Layer, which is a fully connected layer. The AR-Layer can be represented as follows:</p><formula xml:id="formula_3">s i = 1 1 + exp(W AR X FC i + b AR )<label>(2)</label></formula><p>where W AR âˆˆ R 1Ã—F , b AR âˆˆ R 1 are learnable parameters and s i âˆˆ R 1Ã—ti . The anomaly score vectors s i represent the probabilities that instances are classified as anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dynamic Multiple-Instance Learning Loss</head><p>As discussed in Section 1, the video anomaly detection is treated as a MIL task in this paper. In MIL, a positive bag contains at least one positive instance and a negative bag contains no positive instances, i.e., an abnormal video contains at least one anomalous event and a normal video contains no anomalous events. To enlarge the inter-class distance between anomalous and normal instances under a weak supervision, inspired by the k-max MIL loss in <ref type="bibr" target="#b7">[10]</ref> [11], we propose a Dynamic Multiple-Instance learning (DMIL) loss that takes the diversity of video duration into consideration. Different from the max-selection method involved in MIL-based loss function used in <ref type="bibr" target="#b4">[7]</ref> [8] <ref type="bibr" target="#b6">[9]</ref>, we introduce kmax selection method, which is used in [10] <ref type="bibr" target="#b8">[11]</ref>, to obtain the k-max anomaly scores. The k is determined based on the number of clips in a video. Specifically,</p><formula xml:id="formula_4">k i = t i Î±<label>(3)</label></formula><p>where Î± is a hyperparameter. Thus, the k-max anomaly scores of the i-th video can be represented as,</p><formula xml:id="formula_5">p i = sort(s i ) S i = {p j i | j = 1, 2, ..., k i }<label>(4)</label></formula><p>where s i is anomaly score vector of the i-th video, sort(Â·) is a descending sort operator and p i is the sorted s i . Thus, S i consists of top-k i elements in s i . The DMIL loss can then be represented as follows:</p><formula xml:id="formula_6">L DMIL = 1 k i s j i âˆˆSi [âˆ’y i log(s j i ) +(1 âˆ’ y i )log(1 âˆ’ s j i )]<label>(5)</label></formula><p>where y i = {0, 1} is the video anomaly label. Furthermore, instead of calculating the cross-entropy between the average of selected k scores and the video label in <ref type="bibr" target="#b7">[10]</ref> [11], we calculate the cross-entropy between each of the selected k scores and the video label as the instance loss respectively. Noise labels will affect anomaly scores of the sample features from which an average anomaly score is calculated. While our DMIL loss focuses on individual anomaly scores rather than an average one. Thus, this loss keeps errors brought by noise labels from propagating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Center loss for Anomaly Scores Regression</head><p>The objective of the DMIL loss is to enlarge the inter-class distance of instances. However, both the max and k-max selection method inevitably produce wrong label assignment, since the anomaly scores of normal clips and abnormal clips in the abnormal video are similar in early training stage. As a result, the intra-class distance of normal instances is unfortunately enlarged by the DMIL loss, and this will reduce detection accuracy in testing stage. Inspired by the center loss in <ref type="bibr" target="#b15">[18]</ref>, we propose a novel center loss for anomaly score regression to address the abovementioned issue. In <ref type="bibr" target="#b15">[18]</ref>, the center loss learns the feature center of each class and penalizes the distance between the feature representations and their corresponding class centers. In our case, the center loss proposed for anomaly score regression gathers the anomaly scores of normal video clips.</p><p>Our center loss for anomaly score regression can be represented as,</p><formula xml:id="formula_7">L c = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ 1 t i ti j=1 s j i âˆ’ c i 2 2 , if y i = 0 0, otherwise<label>(6)</label></formula><formula xml:id="formula_8">c i = 1 t i ti j=1 s j i<label>(7)</label></formula><p>where c i is the center of anomaly score vector s i of the i-th video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Optimization</head><p>The total loss function of the AR-Net can be represented as follows:</p><formula xml:id="formula_9">L = L DMIL + Î»L c<label>(8)</label></formula><p>To achieve a balance between the two losses in training stage, we empirically set Î» = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiments Setup</head><p>Datasets: The proposed AR-Net is evaluated on a challenging dataset containing untrimmed videos with variable scenes, contents and durations. ShanghaiTech <ref type="bibr" target="#b11">[14]</ref> is a dataset that contains 437 videos with 130 anomalies on 13 scenes. However, this dataset is proposed for unary-classification, so all the training videos are normal <ref type="bibr" target="#b11">[14]</ref>. To make binaryclassification available, we adopt the split version proposed in <ref type="bibr">[1]</ref>. Specifically, there are 238 training videos and 199 testing videos. Evaluation Metric: Similar to previous works [1] <ref type="bibr" target="#b11">[14]</ref> [7], we use an Area Under of Curve (AUC) of the frame-level Receiver Operating Characteristics (ROC) and False Alarm Rate (FAR) with threshold 0.5 as the evaluation metrics. In the video anomaly detection task, the higher AUC demonstrates, the better the model performs, and lower FAR on a normal video implies stronger robustness of an anomaly detection method. Implementation Details:</p><p>We combine I3D RGB and I3D Optical-Flow as our feature extractor, denoted as I3D Conc . The feature of I3D Conc is the concatenation of features from I3D RGB and I3D Optical-Flow . Besides, our feature extractor is not fine-tuned. The Optical-Flow frames of each clip are generated based on TV-L1 algorithm <ref type="bibr" target="#b16">[19]</ref>. Empirically, we set Î± = 4 for ShanghaiTech dataset. The weights of the AR-Net are initialized by Xavier method <ref type="bibr" target="#b17">[20]</ref>, and the Dropout probability for FC-Layer is 0.7. We adopt the Adam optimizer <ref type="bibr" target="#b18">[21]</ref> with a batch size of 60, in which 30 normal videos and 30 abnormal ones are randomly selected from the training set. The learning rate is always 10 âˆ’4 in our experiments.  <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of our method against existing approaches <ref type="bibr" target="#b4">[7]</ref> [1] <ref type="bibr" target="#b6">[9]</ref> on the ShanghaiTech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison Results</head><p>In order to present a comparison against MIL-based works on ShanghaiTech, we reproduced the method in <ref type="bibr" target="#b6">[9]</ref> and adopted the open source code provided by Sultani et al. <ref type="bibr" target="#b4">[7]</ref> to conduct the anomaly detection. The above two models are obtained by pretrained C3D. As shown in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="bibr" target="#b6">[9]</ref> achieves a frame-level AUC of 82.50%. Meanwhile, <ref type="bibr" target="#b4">[7]</ref> performs a frame-level AUC of 86.30%, outperforming the best existing method [1]. Our method substantially exceeds both Sultani et al. <ref type="bibr" target="#b4">[7]</ref> and Zhong et al. <ref type="bibr">â€ </ref> [1] with a frame-level AUC of 91.24%. Furthermore, our approach is the only one surpassing 90% in terms of AUC on ShanghaiTech.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>The comparison results by using different loss functions in <ref type="table" target="#tab_1">Table 2</ref> illustrate the boost brought by the proposed L c and L DMIL in our AR-Net. AR-Net that involves k-max selectionbased MIL loss (L k-max MIL ) is treated as the baseline in our ablation study. It achieves a frame-level AUC of 86.50% on ShanghaiTech. While the proposed DMIL loss-based AR-Net boosts the performance by obtaining a frame-level AUC of 89.10% on ShanghaiTech. Besides, with the help of the proposed L c , the FAR on ShanghaiTech is reduced to 1/9 of those achieved by baseline.</p><p>To demonstrate the performance brought by video appearance and motion information, we compare the anomaly detection results based on different feature extractors. As show in <ref type="table" target="#tab_2">Table 3</ref>, AR-Net with I3D RGB achieves a frame-level AUC of 85.38%. And the I3D Optical-Flow based AR-Net achieves a frame-level AUC of 82.34%. The AR-Net with I3D Conc boosts the performance with a frame-level AUC of 91.24%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Qualitative Analysis</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method surpasses most of the recent MIL-based works. In ShanghaiTech, sometimes anomaly makes up a tiny part of a whole video. In segmented-based methods <ref type="bibr" target="#b4">[7]</ref>  <ref type="bibr" target="#b6">[9]</ref>, although each video is divided into 32 nonoverlapped segments, the anomalous events in each segment (a) 01 0177 (b) 01 0015 <ref type="figure">Fig. 3</ref>. Comparison visualization of testing results between <ref type="bibr" target="#b4">[7]</ref> and ours. may still account for little parts. In other words, the features of anomalous frames to be overwhelmed by normal ones in a segment. As a result, segments containing anomaly tend to be regarded as normal patterns, i.e., the anomaly detection model trained by these segments lacks capability to detect short-term anomaly. While in clip-based methods like [10] <ref type="bibr" target="#b8">[11]</ref> and ours, videos are divided into clips with fixed number of frames. Anomaly detection models using this strategy avoids anomaly being overwhelmed by normal frames and are able to recognize short-term anomaly. As shown in <ref type="figure">Fig.3-(a)</ref>, the model in <ref type="bibr" target="#b4">[7]</ref> (illustrated in the 1st row) does not recognize illegal bicycling while ours (in the 2nd row) does.</p><p>As shown in the 1st row of <ref type="figure">Fig.3-(b)</ref>, both of <ref type="bibr" target="#b4">[7]</ref> (illustrated in the 1st row) and our method (in the 2nd row) fail to detect abnormal events in '01 0015'. The reasons are: 1) the anomaly, marked with a red box, only takes up a local part of the video scene, while methods with global input are prone to ignoring local anomalies 2) the anomaly of skateboarding on the sidewalk are not visually distinguishable from normal behaviors, hence the anomalous clips and non-ones are not separable. In conclusion, video anomaly detection in these kinds of scenes is still a big challenge for current models.</p><p>In order to gain insight into the hyperparameter Î±, we perform experiments using the I3D Conc feature extractor with different values of Î±, as shown in <ref type="figure" target="#fig_2">Fig.4</ref>. In fact, the Î± determines the proportion of noisy-label instances in training stage. Although higher Î± results in a smaller proportion of noisy-label instances, some anomalous instances will be ignored in training stage. This leads to insufficient diversity of training anomalous instances, which reduces the frame-level AUC of the AR-Net. There is at most 2.44% decrease on frame-level AUC when Î± is set to 8, 16, 32, or 64. Moreover, the lower Î± causes more normal instances being labeled as the anomalous ones in training stage. This also reduces the frame-level AUC of the AR-Net. <ref type="figure" target="#fig_2">Fig 4 shows</ref> that AR-Net suffers 0.37% decrease on frame-level AUC when Î± is lower than 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a MIL-based anomaly regression network for video anomaly detection. Besides, we design a dynamic loss L DMIL to learn separable features and a center loss L c to correct the anomaly scores output by our AR-Net. By optimizing the parameters of AR-Net under weak supervision, the dynamic multiple-instance learning loss avoids false alarm caused by interference between clip features. While the center regression loss suppresses label noise by smoothing the distribution of anomaly scores. In addition, the clip-based instance generation strategy benefits to short-term anomaly detection. Experiments on a challenging datasets clearly demonstrate the effectiveness of our approach for video anomaly detection. In the future, we will investigate to model temporal relation between instances to obtain more robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REFERENCES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>This work was supported in part by the National Natural Science Foundation of China under Grant 61822109 and the Fok Ying Tung Education Foundation under Grant 161061. Comparison of loss functions for MIL-based anomaly detection methods. Float numbers in colored circles stand for anomaly scores of segments or clips, while those in colored triangles represent anomaly scores of videos. Binary numbers in black squares are video-level labels. (a) MIL ranking loss in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our model (AR-Net) with two proposed loss terms (DMIL, center), feature extractor and legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>AUC of different Î± values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[ 1 ]</head><label>1</label><figDesc>Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, and Ge Li, "Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection," in IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 1237-1246. [2] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao, "Future frame prediction for anomaly detection-a new baseline," in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6536-6545.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUC and FAR of the proposed method against 3 existing methods. The Â§ , â€  and â€¡ indicate the anomaly detection model based on C3D, TSN RGB and TSN Optical-Flow in [1], respectively.</figDesc><table><row><cell>Methods</cell><cell cols="2">ShanghaiTech AUC(%) FAR(%)</cell></row><row><cell>Sultani et al. [7]</cell><cell>86.30</cell><cell>0.15</cell></row><row><cell>Zhang et al. [9]</cell><cell>82.50</cell><cell>0.10</cell></row><row><cell cols="2">Zhong et al.  Â§ [1] 76.44</cell><cell>âˆ’</cell></row><row><cell cols="2">Zhong et al.  â€  [1] 84.44</cell><cell>âˆ’</cell></row><row><cell cols="2">Zhong et al.  â€¡ [1] 84.13</cell><cell>âˆ’</cell></row><row><cell>AR-Net</cell><cell>91.24</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>AUC and FAR of different loss functions.</figDesc><table><row><cell>Losses</cell><cell cols="2">ShanghaiTech AUC(%) FAR(%)</cell></row><row><cell>Baseline:L k-max MIL</cell><cell>86.50</cell><cell>0.93</cell></row><row><cell>Ours:L DMIL</cell><cell>89.10</cell><cell>0.21</cell></row><row><cell>Ours:L DMIL + Lc</cell><cell>91.24</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>AUC and FAR of different feature extractors.</figDesc><table><row><cell>Feature extractor</cell><cell cols="2">ShanghaiTech AUC(%) FAR(%)</cell></row><row><cell>I3D RGB</cell><cell>85.38</cell><cell>0.27</cell></row><row><cell>I3D Optical-Flow</cell><cell>82.34</cell><cell>0.37</cell></row><row><cell>I3D Conc</cell><cell>91.24</cell><cell>0.10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object-centric autoencoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realworld anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion-aware feature for improved video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><forename type="middle">D</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4030" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">W-TALC: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3C-Net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8679" to="8687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">C-MIL: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large displacement optical flow computation withoutwarping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>SteinbrÃ¼cker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1609" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
