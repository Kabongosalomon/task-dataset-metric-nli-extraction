<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Provably Powerful Graph Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Serviansky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Provably Powerful Graph Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the 1-WL test <ref type="bibr" target="#b49">Xu et al., 2019)</ref>. Unfortunately, many simple instances of graphs are indistinguishable by the 1-WL test. In search for more expressive graph learning models we build upon the recent k-order invariant and equivariant graph neural networks (Maron et al., 2019a,b)   and present two results: First, we show that such k-order networks can distinguish between non-isomorphic graphs as good as the k-WL tests, which are provably stronger than the 1-WL test for k &gt; 2. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors. Second, setting our goal at building a provably stronger, simple and scalable model we show that a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable 3-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed 3-WL expressiveness, strictly stronger than message passing models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are an important data modality which is frequently used in many fields of science and engineering. Among other things, graphs are used to model social networks, chemical compounds, biological structures and high-level image content information. One of the major tasks in graph data analysis is learning from graph data. As classical approaches often use hand-crafted graph features that are not necessarily suitable to all datasets and/or tasks (e.g., <ref type="bibr" target="#b23">Kriege et al. (2019)</ref>), a significant research effort in recent years is to develop deep models that are able to learn new graph representations from raw features (e.g., <ref type="bibr" target="#b12">Gori et al. (2005)</ref>; <ref type="bibr" target="#b9">Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b37">Niepert et al. (2016)</ref>; <ref type="bibr" target="#b21">Kipf and Welling (2016)</ref>; <ref type="bibr" target="#b45">Veličković et al. (2017)</ref>; <ref type="bibr" target="#b30">Monti et al. (2017)</ref>; <ref type="bibr" target="#b15">Hamilton et al. (2017a)</ref>; <ref type="bibr" target="#b34">Morris et al. (2018)</ref>; <ref type="bibr" target="#b49">Xu et al. (2019)</ref>).</p><p>Currently, the most popular methods for deep learning on graphs are message passing neural networks in which the node features are propagated through the graph according to its connectivity structure <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref>. In a successful attempt to quantify the expressive power of message passing models, <ref type="bibr" target="#b34">Morris et al. (2018)</ref>; <ref type="bibr" target="#b49">Xu et al. (2019)</ref> suggest to compare the model's ability to distinguish between two given graphs to that of the hierarchy of the Weisfeiler-Lehman (WL) graph isomorphism tests <ref type="bibr" target="#b13">(Grohe, 2017;</ref><ref type="bibr" target="#b2">Babai, 2016)</ref>. Remarkably, they show that the class of message passing models has limited expressiveness and is not better than the first WL test (1-WL, a.k.a. color refinement). For example, <ref type="figure" target="#fig_0">Figure 1</ref> depicts two graphs (i.e., in blue and in green) that 1-WL cannot distinguish, hence indistinguishable by any message passing algorithm. The goal of this work is to explore and develop GNN models that possess higher expressiveness while maintaining scalability, as much as possible. We present two main contributions. First, establishing a baseline for expressive GNNs, we prove that the recent k-order invariant GNNs <ref type="bibr">(Maron et al., 2019a,b)</ref> offer a natural hierarchy of models that are as expressive as the k-WL tests, for k ≥ 2. Second, as k-order GNNs are not practical for k &gt; 2 we develop a simple, novel GNN model, that incorporates standard MLPs of the feature dimension and a matrix multiplication layer. This model, working only with k = 2 tensors (the same dimension as the graph input data), possesses the expressiveness of 3-WL. Since, in the WL hierarchy, 1-WL and 2-WL are equivalent, while 3-WL is strictly stronger, this model is provably more powerful than the message passing models. For example, it can distinguish the two graphs in <ref type="figure" target="#fig_0">Figure 1</ref>. As far as we know, this model is the first to offer both expressiveness (3-WL) and scalability (k = 2).</p><p>The main challenge in achieving high-order WL expressiveness with GNN models stems from the difficulty to represent the multisets of neighborhoods required for the WL algorithms. We advocate a novel representation of multisets based on Power-sum Multi-symmetric Polynomials (PMP) which are a generalization of the well-known elementary symmetric polynomials. This representation provides a convenient theoretical tool to analyze models' ability to implement the WL tests.</p><p>A related work to ours that also tried to build graph learning methods that surpass the 1-WL expressiveness offered by message passing is <ref type="bibr" target="#b34">Morris et al. (2018)</ref>. They develop powerful deep models generalizing message passing to higher orders that are as expressive as higher order WL tests. Although making progress, their full model is still computationally prohibitive for 3-WL expressiveness and requires a relaxed local version compromising some of the theoretical guarantees. Experimenting with our model on several real-world datasets that include classification and regression tasks on social networks, molecules, and chemical compounds, we found it to be on par or better than state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Deep learning on graph data. The pioneering works that applied neural networks to graphs are <ref type="bibr" target="#b12">Gori et al. (2005)</ref>; <ref type="bibr" target="#b40">Scarselli et al. (2009)</ref> that learn node representations using recurrent neural networks, which were also used in <ref type="bibr" target="#b27">Li et al. (2015)</ref>. Following the success of convolutional neural networks <ref type="bibr" target="#b24">(Krizhevsky et al., 2012)</ref>, many works have tried to generalize the notion of convolution to graphs and build networks that are based on this operation. <ref type="bibr" target="#b4">Bruna et al. (2013)</ref> defined graph convolutions as operators that are diagonal in the graph laplacian eigenbasis. This paper resulted in multiple follow up works with more efficient and spatially localized convolutions <ref type="bibr" target="#b17">(Henaff et al., 2015;</ref><ref type="bibr" target="#b7">Defferrard et al., 2016;</ref><ref type="bibr" target="#b21">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b26">Levie et al., 2017)</ref>. Other works define graph convolutions as local stationary functions that are applied to each node and its neighbours (e.g., <ref type="bibr" target="#b9">Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b1">Atwood and Towsley (2016)</ref>; <ref type="bibr" target="#b37">Niepert et al. (2016)</ref>; <ref type="bibr" target="#b16">Hamilton et al. (2017b)</ref>; <ref type="bibr" target="#b45">Veličković et al. (2017)</ref>; <ref type="bibr" target="#b31">Monti et al. (2018)</ref>). Many of these works were shown to be instances of the family of message passing neural networks <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref>: methods that apply parametric functions to a node and its neighborhood and then apply some pooling operation in order to generate a new feature for each node. In a recent line of work, it was suggested to define graph neural networks using permutation equivariant operators on tensors describing k-order relations between the nodes. <ref type="bibr" target="#b22">Kondor et al. (2018)</ref> identified several such linear and quadratic equivariant operators and showed that the resulting network can achieve excellent results on popular graph learning benchmarks. <ref type="bibr" target="#b28">Maron et al. (2019a)</ref> provided a full characterization of linear equivariant operators between tensors of arbitrary order. In both cases, the resulting networks were shown to be at least as powerful as message passing neural networks. In another line of work, <ref type="bibr" target="#b35">Murphy et al. (2019)</ref> suggest expressive invariant graph models defined using averaging over all permutations of an arbitrary base neural network.</p><p>Weisfeiler Lehman graph isomorphism test. The Weisfeiler Lehman tests is a hierarchy of increasingly powerful graph isomorphism tests <ref type="bibr" target="#b13">(Grohe, 2017)</ref>. The WL tests have found many applications in machine learning: in addition to <ref type="bibr" target="#b49">Xu et al. (2019)</ref>; <ref type="bibr" target="#b34">Morris et al. (2018)</ref>, this idea was used in <ref type="bibr" target="#b41">Shervashidze et al. (2011)</ref> to construct a graph kernel method, which was further generalized to higher order WL tests in <ref type="bibr" target="#b32">Morris et al. (2017)</ref>. <ref type="bibr" target="#b25">Lei et al. (2017)</ref> showed that their suggested GNN has a theoretical connection to the WL test. WL tests were also used in <ref type="bibr" target="#b53">Zhang and Chen (2017)</ref> for link prediction tasks. In a concurrent work, <ref type="bibr" target="#b33">Morris and Mutzel (2019)</ref> suggest constructing graph features based on an equivalent sparse version of high-order WL achieving great speedup and expressiveness guarantees for sparsely connected graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We denote a set by {a, b, . . . , c}, an ordered set (tuple) by (a, b, . . . , c) and a multiset (i.e., a set with possibly repeating elements) by { {a, b, . . . , c} }. We denote [n] = {1, 2, . . . , n}, and (a i | i ∈ [n]) = (a 1 , a 2 , . . . , a n ). Let S n denote the permutation group on n elements. We use multi-index i ∈ [n] k to denote a k-tuple of indices, i = (i 1 , i 2 , . . . , i k ). g ∈ S n acts on multi-indices i ∈ [n] k entrywise by g(i) = (g(i 1 ), g(i 2 ), . . . , g(i k )). S n acts on k-tensors X ∈ R n k ×a by (g · X) i,</p><formula xml:id="formula_0">j = X g −1 (i),j , where i ∈ [n] k , j ∈ [a].</formula><p>3.1 k-order graph networks <ref type="bibr" target="#b28">Maron et al. (2019a)</ref> have suggested a family of permutation-invariant deep neural network models for graphs. Their main idea is to construct networks by concatenating maximally expressive linear equivariant layers. More formally, a k-order invariant graph network is a composition</p><formula xml:id="formula_1">F = m • h • L d • σ • · · · • σ • L 1 , where L i : R n k i ×ai → R n k i+1 ×ai+1 , max i∈[d+1] k i = k, are equivariant linear layers, namely satisfy L i (g · X) = g · L i (X), ∀g ∈ S n , ∀X ∈ R n k i ×ai , σ is an entrywise non-linear activation, σ(X) i,j = σ(X i,j ), h : R n k d+1 ×a d+1 → R a d+2 is an invariant linear layer, namely satisfies h(g · X) = h(X), ∀g ∈ S n , ∀X ∈ R n k d+1 ×a d+1 ,</formula><p>and m is a Multilayer Perceptron (MLP). The invariance of F is achieved by construction (by propagating g through the layers using the definitions of equivariance and invariance):</p><p>F (g · X) = m(· · · (L 1 (g · X)) · · · ) = m(· · · (g · L 1 (X)) · · · ) = · · · = m(h(g · L d (· · · ))) = F (X).</p><p>When k = 2, <ref type="bibr" target="#b28">Maron et al. (2019a)</ref> proved that this construction gives rise to a model that can approximate any message passing neural network <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref> to an arbitrary precision; <ref type="bibr" target="#b29">Maron et al. (2019b)</ref> proved these models are universal for a very high tensor order of k = poly(n), which is of little practical value (an alternative proof was recently suggested in <ref type="bibr" target="#b20">Keriven and Peyré (2019)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Weisfeiler-Lehman graph isomorphism test</head><p>Let G = (V, E, d) be a colored graph where |V | = n and d : V → Σ defines the color attached to each vertex in V , Σ is a set of colors. The Weisfeiler-Lehman (WL) test is a family of algorithms used to test graph isomorphism. Two graphs G, G are called isomorphic if there exists an edge and color preserving bijection φ : V → V .</p><p>There are two families of WL algorithms: k-WL and k-FWL (Folklore WL), both parameterized by k = 1, 2, . . . , n. k-WL and k-FWL both construct a coloring of k-tuples of vertices, that is c : V k → Σ. Testing isomorphism of two graphs G, G is then performed by comparing the histograms of colors produced by the k-WL (or k-FWL) algorithms.</p><p>We will represent coloring of k-tuples using a tensor C ∈ Σ n k , where C i ∈ Σ, i ∈ [n] k denotes the color of the k-tuple v i = (v i1 , . . . , v i k ) ∈ V k . In both algorithms, the initial coloring C 0 is defined using the isomorphism type of each k-tuple. That is, two k-tuples i, i have the same isomorphism type (i.e., get the same color,</p><formula xml:id="formula_2">C i = C i ) if for all q, r ∈ [k]: (i) v iq = v ir ⇐⇒ v i q = v i r ; (ii) d(v iq ) = d(v i q ); and (iii) (v ir , v iq ) ∈ E ⇐⇒ (v i r , v i q ) ∈ E.</formula><p>Clearly, if G, G are two isomorphic graphs then there exists g ∈ S n so that g · C 0 = C 0 .</p><p>In the next steps, the algorithms refine the colorings C l , l = 1, 2, . . . until the coloring does not change further, that is, the subsets of k-tuples with same colors do not get further split to different color groups. It is guaranteed that no more than l = poly(n) iterations are required <ref type="bibr" target="#b8">(Douglas, 2011)</ref>.</p><p>The construction of C l from C l−1 differs in the WL and FWL versions. The difference is in how the colors are aggregated from neighboring k-tuples. We define two notions of neighborhoods of a k-tuple i ∈ [n] k :</p><formula xml:id="formula_3">N j (i) = (i 1 , . . . , i j−1 , i , i j+1 , . . . , i k ) i ∈ [n]</formula><p>(1)</p><formula xml:id="formula_4">N F j (i) = (j, i 2 , . . . , i k ), (i 1 , j, . . . , i k ), . . . , (i 1 , . . . , i k−1 , j) (2) N j (i), j ∈ [k]</formula><p>is the j-th neighborhood of the tuple i used by the WL algorithm, while N F j (i), j ∈ [n] is the j-th neighborhood used by the FWL algorithm. Note that N j (i) is a set of n k-tuples, while N F j (i) is an ordered set of k k-tuples. The inset to the right illustrates these notions of neighborhoods for the case k = 2: the top figure shows N 1 (3, 2) in purple and N 2 (3, 2) in orange. The bottom figure shows N F j (3, 2) for all j = 1, . . . , n with different colors for different j. The coloring update rules are:</p><formula xml:id="formula_5">WL: C l i = enc C l−1 i , { {C l−1 j | j ∈ N j (i)} } j ∈ [k]<label>(3)</label></formula><p>FWL:</p><formula xml:id="formula_6">C l i = enc C l−1 i , C l−1 j | j ∈ N F j (i) j ∈ [n]<label>(4)</label></formula><p>where enc is a bijective map from the collection of all possible tuples in the r.h.s. of Equations (3)-(4) to Σ.</p><formula xml:id="formula_7">When k = 1 both rules, (3)-(4), degenerate to C l i = enc C l−1 i , { {C l−1 j | j ∈ [n]</formula><p>} } , which will not refine any initial color. Traditionally, the first algorithm in the WL hierarchy is called WL, 1-WL, or the color refinement algorithm. In color refinement, one starts with the coloring prescribed with d. Then, in each iteration, the color at each vertex is refined by a new color representing its current color and the multiset of its neighbors' colors.</p><p>Several known results of WL and FWL algorithms <ref type="bibr" target="#b5">(Cai et al., 1992;</ref><ref type="bibr" target="#b13">Grohe, 2017;</ref><ref type="bibr" target="#b34">Morris et al., 2018;</ref><ref type="bibr" target="#b14">Grohe and Otto, 2015)</ref> are:</p><p>1. 1-WL and 2-WL have equivalent discrimination power. 2. k-FWL is equivalent to (k + 1)-WL for k ≥ 2. 3. For each k ≥ 2 there is a pair of non-isomorphic graphs distinguishable by (k + 1)-WL but not by k-WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Colors and multisets in networks</head><p>Before we get to the two main contributions of this paper we address three challenges that arise when analyzing networks' ability to implement WL-like algorithms: (i) Representing the colors Σ in the network; (ii) implementing a multiset representation; and (iii) implementing the encoding function.</p><p>Color representation. We will represent colors as vectors. That is, we will use tensors C ∈ R n k ×a to encode a color per k-tuple; that is, the color of the tuple i ∈ [n] k is a vector C i ∈ R a . This effectively replaces the color tensors Σ n k in the WL algorithm with R n k ×a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiset representation.</head><p>A key technical part of our method is the way we encode multisets in networks. Since colors are represented as vectors in R a , an n-tuple of colors is represented by a</p><formula xml:id="formula_8">matrix X = [x 1 , x 2 , . . . , x n ] T ∈ R n×a , where x j ∈ R a , j ∈ [n]</formula><p>are the rows of X. Thinking about X as a multiset forces us to be indifferent to the order of rows. That is, the color representing g · X should be the same as the color representing X, for all g ∈ S n . One possible approach is to perform some sort (e.g., lexicographic) to the rows of X. Unfortunately, this seems challenging to implement with equivariant layers.</p><p>Instead, we suggest to encode a multiset X using a set of S n -invariant functions called the Power-sum Multi-symmetric Polynomials (PMP) <ref type="bibr" target="#b3">(Briand, 2004;</ref><ref type="bibr" target="#b39">Rydh, 2007)</ref>. The PMP are the multivariate analog to the more widely known Power-sum Symmetric Polynomials,</p><formula xml:id="formula_9">p j (y) = n i=1 y j i , j ∈ [n]</formula><p>, where y ∈ R n . They are defined next. Let α = (α 1 , . . . , α a ) ∈ [n] a be a multi-index and for y ∈ R a we set y α = y α1</p><formula xml:id="formula_10">1 · y α2 2 · · · y αa a . Furthermore, |α| = a j=1 α j . The PMP of degree α ∈ [n] a is p α (X) = n i=1 x α i , X ∈ R n×a .</formula><p>A key property of the PMP is that the finite subset p α , for |α| ≤ n generates the ring of Multisymmetric Polynomials (MP), the set of polynomials q so that q(g · X) = q(X) for all g ∈ S n , X ∈ R n×a (see, e.g., <ref type="bibr" target="#b39">(Rydh, 2007)</ref> corollary 8.4). The PMP generates the ring of MP in the sense that for an arbitrary MP q, there exists a polynomial r so that q(X) = r (u(X)), where</p><formula xml:id="formula_11">u(X) := p α (X) |α| ≤ n .<label>(5)</label></formula><p>As the following proposition shows, a useful consequence of this property is that the vector u(X) is a unique representation of the multi-set X ∈ R n×a . Proposition 1. For arbitrary X, X ∈ R n×a : ∃g ∈ S n so that X = g · X if and only if u(X) = u(X ).</p><p>We note that Proposition 1 is a generalization of lemma 6 in <ref type="bibr" target="#b52">Zaheer et al. (2017)</ref> to the case of multisets of vectors. This generalization was possible since the PMP provide a continuous way to encode vector multisets (as opposed to scalar multisets in previous works). The full proof is provided in the supplementary material.</p><p>Encoding function. One of the benefits in the vector representation of colors is that the encoding function can be implemented as a simple concatenation: Given two color tensors C ∈ R n k ×a , C ∈ R n k ×b , the tensor that represents for each k-tuple i the color pair</p><formula xml:id="formula_12">(C i , C i ) is simply (C, C ) ∈ R n k ×(a+b) .</formula><p>5 k-order graph networks are as powerful as k-WL Our goal in this section is to show that, for every 2 ≤ k ≤ n, k-order graph networks <ref type="bibr" target="#b28">(Maron et al., 2019a)</ref> are at least as powerful as the k-WL graph isomorphism test in terms of distinguishing non-isomorphic graphs. This result is shown by constructing a k-order network model and learnable weight assignment that implements the k-WL test.</p><p>To motivate this construction we note that the WL update step, Equation 3, is equivariant (see proof in the supplementary material). Namely, plugging in g · C l−1 the WL update step would yield g · C l . Therefore, it is plausible to try to implement the WL update step using linear equivariant layers and non-linear pointwise activations.</p><formula xml:id="formula_13">Theorem 1. Given two graphs G = (V, E, d), G = (V , E , d ) that can be distinguished by the k-WL graph isomorphism test, there exists a k-order network F so that F (G) = F (G ).</formula><p>On the other direction for every two isomorphic graphs G ∼ = G and k-order network F ,</p><formula xml:id="formula_14">F (G) = F (G ).</formula><p>The full proof is provided in the supplementary material. Here we outline the basic idea for the proof. First, an input graph G = (V, E, d) is represented using a tensor of the form B ∈ R n 2 ×(e+1) , as follows. The last channel of B, namely B :,:,e+1 (':' stands for all possible values [n]) encodes the adjacency matrix of G according to E. The first e channels B :,:,1:e are zero outside the diagonal, and</p><formula xml:id="formula_15">B i,i,1:e = d(v i ) ∈ R e is the color of vertex v i ∈ V .</formula><p>Now, the second statement in Theorem 1 is clear since two isomorphic graphs G, G will have tensor representations satisfying B = g · B and therefore, as explained in Section 3.1, F (B) = F (B ).</p><p>More challenging is showing the other direction, namely that for non-isomorphic graphs G, G that can be distinguished by the k-WL test, there exists a k-network distinguishing G and G . The key idea is to show that a k-order network can encode the multisets { {B j | j ∈ N j (i)} } for a given tensor B ∈ R n k ×a . These multisets are the only non-trivial component in the WL update rule, Equation 3. Note that the rows of the matrix X = B i1,...,ij−1,:,ij+1,...,i k ,: ∈ R n×a are the colors (i.e., vectors) that define the multiset { {B j | j ∈ N j (i)} }. Following our multiset representation (Section 4) we would like the network to compute u(X) and plug the result at the i-th entry of an output tensor C.</p><p>This can be done in two steps: First, applying the polynomial function τ :</p><formula xml:id="formula_16">R a → R b , b = ( n+a a ) entrywise to B, where τ is defined by τ (x) = (x α | |α| ≤ n) (note that b</formula><p>is the number of multiindices α such that |α| ≤ n). Denote the output of this step Y. Second, apply a linear equivariant operator summing over the j-the coordinate of Y to get C, that is</p><formula xml:id="formula_17">C i,: := L j (Y) i,: = n i =1 Y i1,··· ,ij−1,i ,ij+1,...,i k ,: = j∈Nj (i) τ (B j,: ) = u(X), i ∈ [n] k ,</formula><p>where X = B i1,...,ij−1,:,ij+1,...,i k ,: as desired. Lastly, we use the universal approximation theorem <ref type="bibr" target="#b6">(Cybenko, 1989;</ref><ref type="bibr" target="#b18">Hornik, 1991)</ref> to replace the polynomial function τ with an approximating MLP m : R a → R b to get a k-order network (details are in the supplementary material). Applying m feature-wise, that is m(B) i,: = m(B i,: ), is in particular a k-order network in the sense of Section 3.1. In this section we describe a simple GNN model that has 3-WL discrimination power. The model has the form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A simple network with 3-WL discrimination power</head><formula xml:id="formula_18">F = m • h • B d • B d−1 · · · • B 1 ,<label>(6)</label></formula><p>where as in k-order networks (see Section 3.1) h is an invariant layer and m is an MLP. B 1 , . . . , B d are blocks with the following structure (see <ref type="figure" target="#fig_1">figure 2</ref> for an illustration). Let X ∈ R n×n×a denote the input tensor to the block. First, we apply three MLPs m 1 , m 2 :</p><formula xml:id="formula_19">R a → R b , m 3 : R a → R b to the input tensor, m l (X), l ∈ [3]</formula><p>. This means applying the MLP to each feature of the input tensor independently, i.e., m l (X) i1,i2,: := m l (X i1,i2,: ), l ∈ [3]. Second, matrix multiplication is performed between matching features, i.e., W :,:,j := m 1 (X) :,:,j · m 2 (X) :,:,j , j ∈ [b]. The output of the block is the tensor (m 3 (X), W).</p><p>We start with showing our basic requirement from GNN, namely invariance: Lemma 1. The model F described above is invariant, i.e., F (g · B) = F (B), for all g ∈ S n , and B.</p><p>Proof. Note that matrix multiplication is equivariant: for two matrices A, B ∈ R n×n and g ∈ S n one has (g · A) · (g · B) = g · (A · B). This makes the basic building block B i equivariant, and consequently the model F invariant, i.e., F (g · B) = F (B).</p><p>Before we prove the 3-WL power for this model, let us provide some intuition as to why matrix multiplication improves expressiveness. Let us show matrix multiplication allows this model to distinguish between the two graphs in <ref type="figure" target="#fig_0">Figure 1</ref>, which are 1-WL indistinguishable. The input tensor B representing a graph G holds the adjacency matrix at the last channel A := B :,:,e+1 . We can build a network with 2 blocks computing A 3 and then take the trace of this matrix (using the invariant layer h). Remember that the d-th power of the adjacency matrix computes the number of d-paths between vertices; in particular tr(A 3 ) computes the number of cycles of length 3. Counting shows the upper graph in <ref type="figure" target="#fig_0">Figure 1</ref> has 0 such cycles while the bottom graph has 12. The main result of this section is: Theorem 2. Given two graphs G = (V, E, d), G = (V , E , d ) that can be distinguished by the 3-WL graph isomorphism test, there exists a network F (equation 6) so that F (G) = F (G ). On the other direction for every two isomorphic graphs G ∼ = G and F (Equation 6), F (G) = F (G ).</p><p>The full proof is provided in the supplementary material. Here we outline the main idea of the proof. The second part of this theorem is already shown in Lemma 1. To prove the first part, namely that the model in Equation 6 has 3-WL expressiveness, we show it can implement the 2-FWL algorithm, that is known to be equivalent to 3-WL (see Section 3.2). As before, the challenge is in implementing the neighborhood multisets as used in the 2-FWL algorithm. That is, given an input tensor B ∈ R n 2 ×a we would like to compute an output tensor C ∈ R n 2 ×b where C i1,i2,: ∈ R b represents a color matching the multiset { {(B j,i2,: , B i1,j,: ) | j ∈ [n]} }. As before, we use the multiset representation introduced in section 4. Consider the matrix X ∈ R n×2a defined by</p><formula xml:id="formula_20">X j,: = (B j,i2,: , B i1,j,: ), j ∈ [n].<label>(7)</label></formula><p>Our goal is to compute an output tensor W ∈ R n 2 ×b , where W i1,i2,: = u(X).</p><formula xml:id="formula_21">Consider the multi-index set α | α ∈ [n] 2a , |α| ≤ n of cardinality b = n+2a 2a , and write it in the form {(β l , γ l ) | β, γ ∈ [n] a , |β l | + |γ l | ≤ n, l ∈ b}. Now define polynomial maps τ 1 , τ 2 : R a → R b by τ 1 (x) = (x β l | l ∈ [b]), and τ 2 (x) = (x γ l | l ∈ [b]). We apply τ 1 to the features of B, namely Y i1,i2,l := τ 1 (B) i1,i2,l = (B i1,i2,: ) β l ; similarly, Z i1,i2,l := τ 2 (B) i1,i2,l = (B i1,i2,: ) γ l . Now, W i1,i2,l := (Z :,:,l · Y :,:,l ) i1,i2 = n j=1 Z i1,j,l Y j,i2,l = n j=1 B β l j,i2,: B γ l i1,j,: = n j=1 (B j,i2,: , B i1,j,: ) (β l ,γ l ) , hence W i1,i2,: = u(X), where X is defined in Equation 7</formula><p>. To get an implementation with the model in Equation <ref type="formula" target="#formula_18">6</ref> we need to replace τ 1 , τ 2 with MLPs. We use the universal approximation theorem to that end (details are in the supplementary material).</p><p>To conclude, each update step of the 2-FWL algorithm is implemented in the form of a block B i applying m 1 , m 2 to the input tensor B, followed by matrix multiplication of matching features, W = m 1 (B) · m 2 (B). Since Equation 4 requires pairing the multiset with the input color of each k-tuple, we take m 3 to be identity and get (B, W) as the block output.</p><p>Generalization to k-FWL. One possible extension is to add a generalized matrix multiplication to k-order networks to make them as expressive as k-FWL and hence (k + 1)-WL. Generalized matrix multiplication is defined as follows. Given A 1 , . . . ,</p><formula xml:id="formula_22">A k ∈ R n k , then ( k i=1 A i ) i = n j=1 A 1 j,i2,...,i k A 2 i1,j,...,i k · · · A k i1,...,i k−1 ,j .</formula><p>Relation to . Our model offers two benefits over the 1-2-3-GNN suggested in the work of <ref type="bibr" target="#b34">Morris et al. (2018)</ref>, a recently suggested GNN that also surpasses the expressiveness of message passing networks. First, it has lower space complexity (see details below). This allows us to work with a provably 3-WL expressive model while <ref type="bibr" target="#b34">Morris et al. (2018)</ref> resorted to a local 3-GNN version, hindering their 3-WL expressive power. Second, from a practical point of view our model is arguably simpler to implement as it only consists of fully connected layers and matrix multiplication (without having to account for all subsets of size 3).</p><p>Complexity analysis of a single block. Assuming a graph with n nodes, dense edge data and a constant feature depth, the layer proposed in <ref type="bibr" target="#b34">Morris et al. (2018)</ref> has O(n 3 ) space complexity (number of subsets) and O(n 4 ) time complexity (O(n 3 ) subsets with O(n) neighbors each). Our layer (block), however, has O(n 2 ) space complexity as only second order tensors are stored (i.e., linear in the size of the graph data), and time complexity of O(n 3 ) due to the matrix multiplication. We note that the time complexity of <ref type="bibr" target="#b34">Morris et al. (2018)</ref> can probably be improved to O(n 3 ) while our time complexity can be improved to O(n 2.x ) due to more advanced matrix multiplication algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>Implementation details. We implemented the GNN model as described in Section 6 (see Equation 6) using the TensorFlow framework <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. We used three identical blocks B 1 , B 2 , B 3 , where in each block B i : R n 2 ×a → R n 2 ×b we took m 3 (x) = x to be the identity (i.e., m 3 acts as a skip connection, similar to its role in the proof of Theorem 2); m 1 , m 2 : R a → R b are chosen as d layer MLP with hidden layers of b features. After each block B i we also added a single layer MLP m 4 : R b+a → R b . Note that although this fourth MLP is not described in the model in Section 6 it clearly does not decrease (nor increase) the theoretical expressiveness of the model; we found it efficient for coding as it reduces the parameters of the model. For the first block, B 1 , a = e + 1, where for the other blocks b = a. The MLPs are implemented with 1 × 1 convolutions.  Parameter search was conducted on learning rate and learning rate decay, as detailed below. We have experimented with two network suffixes adopted from previous papers: (i) The suffix used in <ref type="bibr" target="#b28">Maron et al. (2019a)</ref> that consists of an invariant max pooling (diagonal and off-diagonal) followed by a three Fully Connected (FC) with hidden units' sizes of (512, 256, #classes); (ii) the suffix used in <ref type="bibr" target="#b49">Xu et al. (2019)</ref> adapted to our network: we apply the invariant max layer from <ref type="bibr" target="#b28">Maron et al. (2019a)</ref> to the output of every block followed by a single fully connected layer to #classes. These outputs are then summed together and used as the network output on which the loss function is defined.</p><p>Datasets. We evaluated our network on two different tasks: Graph classification and graph regression. For classification, we tested our method on eight real-world graph datasets from <ref type="bibr" target="#b50">(Yanardag and Vishwanathan, 2015)</ref>: three datasets consist of social network graphs, and the other five datasets come from bioinformatics and represent chemical compounds or protein structures. Each graph is represented by an adjacency matrix and possibly categorical node features (for the bioinformatics datasets). For the regression task, we conducted an experiment on a standard graph learning benchmark called the QM9 dataset <ref type="bibr" target="#b38">(Ramakrishnan et al., 2014;</ref><ref type="bibr" target="#b48">Wu et al., 2018)</ref>. It is composed of 134K small organic molecules (sizes vary from 4 to 29 atoms). Each molecule is represented by an adjacency matrix, a distance matrix (between atoms), categorical data on the edges, and node features; the data was obtained from the pytorch-geometric library <ref type="bibr" target="#b10">(Fey and Lenssen, 2019)</ref>. The task is to predict 12 real valued physical quantities for each molecule.</p><p>Graph classification results. We follow the standard 10-fold cross validation protocol and splits from <ref type="bibr" target="#b54">Zhang et al. (2018)</ref> and report our results according to the protocol described in <ref type="bibr" target="#b49">Xu et al. (2019)</ref>, namely the best averaged accuracy across the 10-folds. Parameter search was conducted on a fixed random 90%-10% split: learning rate in 5 · 10 −5 , 10 −4 , 5 · 10 −4 , 10 −3 ; learning rate decay in [0.5, 1] every 20 epochs. We have tested three architectures: (1) b = 400, d = 2, and suffix (ii); (2) b = 400, d = 2, and suffix (i); and (3) b = 256, d = 3, and suffix (ii). (See above for definitions of b, d and suffix). <ref type="table" target="#tab_0">Table 1</ref> presents a summary of the results (top part -non deep learning methods). The last row presents our ranking compared to all previous methods; note that we have scored in the top 3 methods in 6 out of 8 datasets.</p><p>Graph regression results. The data is randomly split into 80% train, 10% validation and 10% test. We have conducted the same parameter search as in the previous experiment on the validation set. We have used the network (2) from classification experiment, i.e., b = 400, d = 2, and suffix (i), with an absolute error loss adapted to the regression task. Test results are according to the best validation error. We have tried two different settings: (1) training a single network to predict all the output quantities together and (2) training a different network for each quantity. <ref type="table" target="#tab_1">Table 2</ref> compares the mean absolute error of our method with three other methods: 123-gnn  and <ref type="bibr" target="#b48">(Wu et al., 2018)</ref>; results of all previous work were taken from . Note that our method achieves the lowest error on 5 out of the 12 quantities when using a single network, and the lowest error on 9 out of the 12 quantities in case each quantity is predicted by an independent network. Equivariant layer evaluation. The model in Section 6 does not incorporate all equivariant linear layers as characterized in <ref type="bibr" target="#b28">(Maron et al., 2019a)</ref>. It is therefore of interest to compare this model to models richer in linear equivariant layers, as well as a simple MLP baseline (i.e., without matrix multiplication). We performed such an experiment on the NCI1 dataset <ref type="bibr" target="#b50">(Yanardag and Vishwanathan, 2015)</ref> comparing: (i) our suggested model, denoted Matrix Product (MP); (ii) matrix product + full linear basis from <ref type="bibr" target="#b28">(Maron et al., 2019a)</ref> (MP+LIN); (iii) only full linear basis (LIN); and (iv) MLP applied to the feature dimension.</p><p>Due to the memory limitation in <ref type="bibr" target="#b28">(Maron et al., 2019a)</ref> we used the same feature depths of b 1 = 32, b 2 = 64, b 3 = 256, and d = 2. The inset shows the performance of all methods on both training and validation sets, where we performed a parameter search on the learning rate (as above) for a fixed decay rate of 0.75 every 20 epochs. Although all methods (excluding MLP) are able to achieve a zero training error, the (MP) and (MP+LIN) enjoy better generalization than the linear basis of <ref type="bibr" target="#b28">Maron et al. (2019a)</ref>. Note that (MP) and (MP+LIN) are comparable, however (MP) is considerably more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We explored two models for graph neural networks that possess superior graph distinction abilities compared to existing models. First, we proved that k-order invariant networks offer a hierarchy of neural networks that parallels the distinction power of the k-WL tests. This model has lesser practical interest due to the high dimensional tensors it uses. Second, we suggested a simple GNN model consisting of only MLPs augmented with matrix multiplication and proved it achieves 3-WL expressiveness. This model operates on input tensors of size n 2 and therefore useful for problems with dense edge data. The downside is that its complexity is still quadratic, worse than message passing type methods. An interesting future work is to search for more efficient GNN models with high expressiveness. Another interesting research venue is quantifying the generalization ability of these models.</p><p>To show equivariance, it is enough to show that each entry of the r.h.s. tuple is equivariant. For its first entry: (g · C l−1 ) i = C l−1 g −1 (i) . For the other entries, consider w.l.o.g. B j i :</p><formula xml:id="formula_23">{ {(g · C l−1 ) j | j ∈ N j (i)} } = { {C l−1 g −1 (j) | j ∈ N j (i)} } = { {C l−1 j | j ∈ N j (g −1 (i))} } = B j g −1 (i) .</formula><p>We get that feeding k-WL update rule with g · C l−1 we get as output C l g −1 (i) = (g · C l ) i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 1</head><p>Proof. We will prove a slightly stronger claim: Assume we are given some finite set of graphs. For example, we can think of all combinatorial graphs (i.e., graphs represented by binary adjacency matrices) of n vertices . Our task is to build a k-order network F that assigns different output F (G) = F (G ) whenever G, G are non-isomorphic graphs distinguishable by the k-WL test.</p><p>Our construction of F has three main steps. First in Section C.1 we implement the initialization step. Second, Section C.2 we implement the coloring update rules of the k-WL. Lastly, we implement a histogram calculation providing different features to k-WL distinguishable graphs in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Input and Initialization</head><p>Input. The input to the network can be seen as a tensor of the form B ∈ R n 2 ×(e+1) encoding an input graph G = (V, E, d), as follows. The last channel of B, namely B :,:,e+1 (':' stands for all possible values [n]) encodes the adjacency matrix of G according to E. The first e channels B :,:,1:e are zero outside the diagonal, and B i,i,1:e = d(v i ) ∈ R e is the color of vertex v i ∈ V . Our assumption of finite graph collection means the set Ω ⊂ R n 2 ×(e+1) of possible input tensors B is finite as well. Next we describe the different parts of k-WL implementation with k-order network. For brevity, we will denote by B ∈ R n k ×a the input to each part and by C ∈ R n k ×b the output.</p><p>Initialization. We start with implementing the initialization of k-WL, namely computing a coloring representing the isomorphism type of each k-tuple. Our first step is to define a linear equivariant operator that extracts the sub-tensor corresponding to each multi-index i: let L : R n 2 ×(e+1) → R n k ×k 2 ×(e+2) be the linear operator defined by L(X) i,r,s,w = X ir,is,w , w ∈ [e + 1]</p><formula xml:id="formula_24">L(X) i,r,s,e+2 = 1 i r = i s 0 otherwise for i ∈ [n] k , r, s ∈ [k].</formula><p>L is equivariant with respect to the permutation action. Indeed, for w ∈ [e + 1], (g · L(X)) i,r,s,w = L(X) g −1 (i),r,s,w = X g −1 (ir),g −1 (is),w = (g · X) ir,is,w = L(g · X) i,r,s,w .</p><p>For w = e + 2 we have</p><formula xml:id="formula_25">(g·L(X)) i,r,s,w = L(X) g −1 (i),r,s,w = 1 g −1 (i r ) = g −1 (i s ) 0 otherwise = 1 i r = i s 0 otherwise = L(g·X) i,r,s,w .</formula><p>Since L is linear and equivariant it can be represented as a single linear layer in a k-order network. Note that L(B) i,:,:,1:(e+1) contains the sub-tensor of B defined by the k-tuple of vertices (v i1 , . . . , v i k ), and L(B) i,:,:,e+2 represents the equality pattern of the k-tuple i, which is equivalent to the equality pattern of the k-tuple of vertices (v i1 , . . . , v i k ). Hence, L(B) i,:,:,: represents the isomorphism type of the k-tuple of vertices (v i1 , . . . , v i k ). The first layer of our construction is therefore C = L(B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 k-WL update step</head><p>We next implement Equation 3. We achieve that in 3 steps. As before let B ∈ R n k ×a be the input tensor to the the current k-WL step.</p><p>First, apply the polynomial function τ : R a → R b , b = ( n+a a ) entrywise to B, where τ is defined by τ (x) = (x α ) |α|≤n (note that b is the number of multi-indices α such that |α| ≤ n). This gives</p><formula xml:id="formula_26">Y ∈ R n k ×b where Y i,: = τ (B i,: ) ∈ R b .</formula><p>Second, apply the linear operator</p><formula xml:id="formula_27">C j i,r := L j (Y) i,r = n i =1 Y i1,··· ,ij−1,i ,ij+1,...,i k ,r , i ∈ [n] k , r ∈ [b].</formula><p>L j is equivariant with respect to the permutation action. Indeed,</p><formula xml:id="formula_28">L j (g · Y) i,r = n i =1 (g·Y) i1,··· ,ij−1,i ,ij+1,...,r = n i =1 Y g −1 (i1)··· ,g −1 (ij−1),i ,g −1 (ij+1),...,r = L j (Y) g −1 (i),r = (g·L j (Y)) i,r .</formula><p>Now, note that</p><formula xml:id="formula_29">C j i,: = L j (Y) i,: = n i =1 τ (B i1,··· ,ij−1,i ,ij+1,...,i k ,: ) = j∈Nj (i) τ (B j,: ) = u(X),</formula><p>where X = B i1,...,ij−1,:,ij+1,...,i k ,: as desired.</p><p>Third, the k-WL update step is the concatenation: (B, C 1 , . . . , C k ).</p><p>To finish this part we need to replace the polynomial function τ with an MLP m : R a → R b . Since there is a finite set of input tensors Ω, there could be only a finite set Υ of colors in R a in the input tensors to every update step. Using MLP universality <ref type="bibr" target="#b6">(Cybenko, 1989;</ref><ref type="bibr" target="#b18">Hornik, 1991)</ref> , let m be an MLP so that τ (x) − m(x) &lt; for all possible colors x ∈ Υ. We choose sufficiently small so that for all possible</p><formula xml:id="formula_30">X = (B j | j ∈ N j (i)) ∈ R n×a , i ∈ [n] k , j ∈ [k], v(X) = i∈[n] m(x i ) satisfies the same properties as u(X) = i∈[n] τ (x i ) (see Proposition 1), namely v(X) = v(X ) iff ∃g ∈ S n so that X = g · X.</formula><p>Note that the 'if' direction is always true by the invariance of the sum operator to permutations of the summands. The 'only if' direction is true for sufficiently small . Indeed, v(X) − u(X) ≤ n max i∈[n] m(x i ) − τ (x i ) ≤ n , since x i ∈ Υ. Since this error can be made arbitrary small, u is injective and there is a finite set of possible X then v can be made injective by sufficiently small &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Histogram computation</head><p>So far we have shown we can construct a k-order equivariant network H = L d • σ • · · · • σ • L 1 implementing d steps of the k-WL algorithm. We take d sufficiently large to discriminate the graphs in our collection as much as k-WL is able to. Now, when feeding an input graph this equivariant network outputs H(B) ∈ R n k ×a which matches a color H(B) i,: (i.e., vector in R a ) to each k-tuple i ∈ [n] k .</p><p>To produce the final network we need to calculate a feature vector per graph that represents the histogram of its k-tuples' colors H(B). As before, since we have a finite set of graphs, the set of colors in H(B) is finite; let b denote this number of colors. Let m : R a → R b be an MLP mapping each color x ∈ R a to the one-hot vector in R b representing this color. Applying m entrywise after H, namely m <ref type="figure">(H(B)</ref>), followed by the summing invariant operator h :</p><formula xml:id="formula_31">R n k ×b → R b defined by h(Y) j = i∈[n] k Y i,j , j ∈ [b]</formula><p>provides the desired histogram. Our final k-order invariant network is</p><formula xml:id="formula_32">F = h • m • L d • σ • · · · • σ • L 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 2</head><p>Proof. The second claim is proved in Lemma 1. Next we construct a network as in Equation <ref type="formula" target="#formula_18">6</ref> distinguishing a pair of graphs that are 3-WL distinguishable. As before, we will construct the network distinguishing any finite set of graphs of size n. That is, we consider a finite set of input tensors Ω ⊂ R n 2 ×(e+2) .</p><p>Input. We assume our input tensors have the form B ∈ R n 2 ×(e+2) . The first e + 1 channels are as before, namely encode vertex colors (features) and adjacency information. The e + 2 channel is simply taken to be the identity matrix, that is B :,:,e+2 = I d .</p><p>Initialization. First, we need to implement the 2-FWL initialization (see Section 3.2). Namely, given an input tensor B ∈ R n 2 ×(e+1) construct a tensor that colors 2-tuples according to their isomorphism type. In this case the isomorphism type is defined by the colors of the two nodes and whether they are connected or not. Let A := B :,:,e+1 denote the adjacency matrix, and Y := B :,:,1:e the input vertex colors. Construct the tensor C ∈ R n 2 ×(4e+1) defined by the concatenation of the following colors matrices into one tensor: A · Y :,:,j , (11 T − A) · Y :,:,j , Y :,:,j · A, Y :,:,j · (11 T − A), j ∈ [e], and B :,:,e+2 . Note that C i1,i2,: encodes the isomorphism type of the 2-tuple sub-graph defined by v i1 , v i2 ∈ V , since each entry of C holds a concatenation of the node colors times the adjacency matrix of the graph (A) and the adjacency matrix of the complement graph (11 T −A); the last channel also contains an indicator if v i1 = v i2 . Note that the transformation B → C can be implemented with a single block B 1 .</p><p>2-FWL update step. Next we implement a 2-FWL update step, Equation 4, which for k = 2 takes the form C i = enc B i , (B j,i2 , B i1,j ) j ∈ [n]</p><p>, i = (i 1 , i 2 ), and the input tensor B ∈ R n 2 ×a . To implement this we will need to compute a tensor Y, where the coloring Y i encodes the multiset (B j,i2,: , B i1,j,: ) j ∈ [n] .</p><p>As done before, we use the multiset representation described in section 4. Consider the matrix X ∈ R n×2a defined by X j,: = (B j,i2,: , B i1,j,: ), j ∈ [n].</p><p>Our goal is to compute an output tensor W ∈ R n 2 ×b , where W i1,i2,: = u(X).</p><p>Consider the multi-index set α | α ∈ [n] 2a , |α| ≤ n of cardinality b = n+2a 2a , and write it in the form {(β l , γ l ) | β, γ ∈ [n] a , |β l | + |γ l | ≤ n, l ∈ b}. Now define polynomial maps τ 1 , τ 2 : R a → R b by τ 1 (x) = (x β l | l ∈ [b]), and τ 2 (x) = (x γ l | l ∈ [b]). We apply τ 1 to the features of B, namely Y i1,i2,l := τ 1 (B) i1,i2,l = (B i1,i2,: ) β l ; similarly, Z i1,i2,l := τ 2 (B) i1,i2,l = (B i1,i2,: ) γ l . Now, W i1,i2,l := (Z :,:,l · Y :,:,l ) i1,i2 = n j=1 Z i1,j,l Y j,i2,l = n j=1 τ 1 (B) j,i2,l τ 2 (B) i1,j,l = n j=1 B β l j,i2,: B γ l i1,j,: = n j=1 (B j,i2,: , B i1,j,: ) (β l ,γ l ) , hence W i1,i2,: = u(X), where X is defined in Equation 10.</p><p>To implement this in the network we need to replace τ i with MLPs m i , i = 1, 2. That is, W i1,i2,l := n j=1 m 1 (B) j,i2,l m 2 (B) i1,j,l = v(X),</p><p>where X ∈ R n×2a is defined in <ref type="figure" target="#fig_0">Equation 10</ref>.</p><p>As before, since input tensors belong to a finite set Ω ⊂ R n 2 ×(e+1) , so are all possible multisets X and all colors, Υ, produced by any part of the network. Similarly to the proof of Theorem 1 we can take (using the universal approximation theorem) MLPs m 1 , m 2 so that max x∈Υ,i=1,2 τ i (x) − m i (x) &lt; . We choose to be sufficiently small so that the map v(X) defined in Equation 11 maintains the injective property of u (see Proposition 1): It discriminates between X, X not representing the same multiset.</p><p>Lastly, note that taking m 3 to be the identity transformation and concatenating (B, m 1 (B) · m 2 (B)) concludes the implementation of the 2-FWL update step. The computation of the color histogram can be done as in the proof of Theorem 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two graphs not distinguished by 1-WL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Block structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Graph Classification Results on the datasets from<ref type="bibr" target="#b50">Yanardag and Vishwanathan (2015)</ref> </figDesc><table><row><cell>dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell>size</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>4127</cell><cell>5000</cell><cell>1000</cell><cell>1500</cell></row><row><cell>classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>3</cell></row><row><cell>avg node #</cell><cell>17.9</cell><cell>25.5</cell><cell>39.1</cell><cell>29.8</cell><cell>29.6</cell><cell>74.4</cell><cell>19.7</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GK (Shervashidze et al., 2009)</cell><cell>81.39±1.7</cell><cell>55.65±0.5</cell><cell>71.39±0.3</cell><cell>62.49±0.3</cell><cell>62.35±0.3</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>RW (Vishwanathan et al., 2010)</cell><cell>79.17±2.1</cell><cell>55.91±0.3</cell><cell>59.57±0.1</cell><cell>&gt; 3 days</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>PK (Neumann et al., 2016)</cell><cell>76±2.7</cell><cell>59.5±2.4</cell><cell>73.68±0.7</cell><cell>82.54±0.5</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>WL (Shervashidze et al., 2011)</cell><cell>84.11±1.9</cell><cell>57.97±2.5</cell><cell>74.68±0.5</cell><cell>84.46±0.5</cell><cell>85.12±0.3</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>FGSD (Verma and Zhang, 2017)</cell><cell>92.12</cell><cell>62.80</cell><cell>73.42</cell><cell>79.80</cell><cell>78.84</cell><cell>80.02</cell><cell>73.62</cell><cell>52.41</cell></row><row><cell>AWE-DD (Ivanov and Burnaev, 2018)</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell cols="4">NA 73.93±1.9 74.45 ± 5.8 51.54 ±3.6</cell></row><row><cell>AWE-FB (Ivanov and Burnaev, 2018)</cell><cell>87.87±9.7</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell cols="2">NA 70.99 ± 1.4</cell><cell cols="2">73.13 ±3.2 51.58 ± 4.6</cell></row><row><cell>DGCNN (Zhang et al., 2018)</cell><cell>85.83±1.7</cell><cell>58.59±2.5</cell><cell>75.54±0.9</cell><cell>74.44±0.5</cell><cell>NA</cell><cell>73.76±0.5</cell><cell>70.03±0.9</cell><cell>47.83±0.9</cell></row><row><cell>PSCN (Niepert et al., 2016)(k=10)</cell><cell>88.95±4.4</cell><cell>62.29±5.7</cell><cell>75±2.5</cell><cell>76.34±1.7</cell><cell>NA</cell><cell>72.6±2.2</cell><cell>71±2.3</cell><cell>45.23±2.8</cell></row><row><cell>DCNN (Atwood and Towsley, 2016)</cell><cell>NA</cell><cell>NA</cell><cell>61.29±1.6</cell><cell>56.61± 1.0</cell><cell>NA</cell><cell>52.11±0.7</cell><cell>49.06±1.4</cell><cell>33.49±1.4</cell></row><row><cell>ECC (Simonovsky and Komodakis, 2017)</cell><cell>76.11</cell><cell>NA</cell><cell>NA</cell><cell>76.82</cell><cell>75.03</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>DGK (Yanardag and Vishwanathan, 2015)</cell><cell>87.44±2.7</cell><cell>60.08±2.6</cell><cell>75.68±0.5</cell><cell>80.31±0.5</cell><cell>80.32±0.3</cell><cell>73.09±0.3</cell><cell>66.96±0.6</cell><cell>44.55±0.5</cell></row><row><cell>DiffPool (Ying et al., 2018)</cell><cell>NA</cell><cell>NA</cell><cell>78.1</cell><cell>NA</cell><cell>NA</cell><cell>75.5</cell><cell>NA</cell><cell>NA</cell></row><row><cell>CCN (Kondor et al., 2018)</cell><cell>91.64±7.2</cell><cell>70.62±7.0</cell><cell>NA</cell><cell>76.27±4.1</cell><cell>75.54±3.4</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell cols="3">Invariant Graph Networks (Maron et al., 2019a) 83.89±12.95 58.53±6.86</cell><cell cols="4">76.58±5.49 74.33±2.71 72.82±1.45 78.36±2.47</cell><cell cols="2">72.0±5.54 48.73±3.41</cell></row><row><cell>GIN (Xu et al., 2019)</cell><cell>89.4±5.6</cell><cell>64.6±7.0</cell><cell>76.2±2.8</cell><cell>82.7±1.7</cell><cell>NA</cell><cell>80.2±1.9</cell><cell>75.1±5.1</cell><cell>52.3±2.8</cell></row><row><cell>1-2-3 GNN (Morris et al., 2018)</cell><cell>86.1±</cell><cell>60.9±</cell><cell>75.5±</cell><cell>76.2±</cell><cell>NA</cell><cell>NA</cell><cell>74.2±</cell><cell>49.5±</cell></row><row><cell>Ours 1</cell><cell cols="2">90.55±8.7 66.17±6.54</cell><cell cols="4">77.2±4.73 83.19±1.11 81.84±1.85 80.16±1.11</cell><cell>72.6±4.9</cell><cell>50±3.15</cell></row><row><cell>Ours 2</cell><cell>88.88±7.4</cell><cell>64.7±7.46</cell><cell cols="4">76.39±5.03 81.21±2.14 81.77±1.26 81.38±1.42</cell><cell cols="2">72.2±4.26 44.73±7.89</cell></row><row><cell>Ours 3</cell><cell cols="2">89.44±8.05 62.94±6.96</cell><cell cols="4">76.66±5.59 80.97±1.91 82.23±1.42 80.68±1.71</cell><cell cols="2">73±5.77 50.46±3.59</cell></row><row><cell>Rank</cell><cell>3 rd</cell><cell>2 nd</cell><cell>2 nd</cell><cell>2 nd</cell><cell>2 nd</cell><cell>1 st</cell><cell>6 th</cell><cell>5 th</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Regression, the QM9 dataset.</figDesc><table><row><cell>Target</cell><cell>DTNN</cell><cell>MPNN</cell><cell cols="2">123-gnn Ours 1</cell><cell>Ours 2</cell></row><row><cell>µ</cell><cell>0.244</cell><cell>0.358</cell><cell>0.476</cell><cell>0.231</cell><cell>0.0934</cell></row><row><cell>α</cell><cell>0.95</cell><cell>0.89</cell><cell>0.27</cell><cell>0.382</cell><cell>0.318</cell></row><row><cell>homo</cell><cell cols="4">0.00388 0.00541 0.00337 0.00276</cell><cell>0.00174</cell></row><row><cell>lumo</cell><cell cols="4">0.00512 0.00623 0.00351 0.00287</cell><cell>0.0021</cell></row><row><cell>∆</cell><cell>0.0112</cell><cell>0.0066</cell><cell cols="2">0.0048 0.00406</cell><cell>0.0029</cell></row><row><cell>R 2</cell><cell>17</cell><cell>28.5</cell><cell>22.9</cell><cell>16.07</cell><cell>3.78</cell></row><row><cell cols="6">ZP V E 0.00172 0.00216 0.00019 0.00064 0.000399</cell></row><row><cell>U0</cell><cell>-</cell><cell>-</cell><cell>0.0427</cell><cell>0.234</cell><cell>0.022</cell></row><row><cell>U</cell><cell>-</cell><cell>-</cell><cell>0.111</cell><cell>0.234</cell><cell>0.0504</cell></row><row><cell>H</cell><cell>-</cell><cell>-</cell><cell>0.0419</cell><cell>0.229</cell><cell>0.0294</cell></row><row><cell>G</cell><cell>-</cell><cell>-</cell><cell>0.0469</cell><cell>0.238</cell><cell>0.024</cell></row><row><cell>Cv</cell><cell>0.27</cell><cell>0.42</cell><cell>0.0944</cell><cell>0.184</cell><cell>0.144</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the European Research Council (ERC Consolidator Grant, "LiftMatch" 771136) and the Israel Science Foundation (Grant No. 1830/17).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Proposition 1</head><p>Proof. First, if X = g · X, then p α (X) = p α (X ) for all α and therefore u(X) = u(X ). In the other direction assume by way of contradiction that u(X) = u(X ) and g · X = X , for all g ∈ S n . That is, X and X represent different multisets. Let [X] = {g · X | g ∈ S n } denote the orbit of X under the action of S n ; similarly denote [X ]. Let K ⊂ R n×a be a compact set containing [X], [X ], where [X] ∩ [X ] = ∅ by assumption.</p><p>By the Stone-Weierstrass Theorem applied to the algebra of continuous functions C(K, R) there exists a polynomial f so that f | <ref type="bibr">[X]</ref> ≥ 1 and f | [X ] ≤ 0. Consider the polynomial q(X) = 1 n! g∈Sn f (g · X).</p><p>By construction q(g · X) = q(X), for all g ∈ S n . Therefore q is a multi-symmetric polynomial. Therefore, q(X) = r(u(X)) for some polynomial r. On the other hand,</p><p>where we used the assumption that u(X) = u(X ). We arrive at a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of equivairance of WL update step</head><p>Consider the formal tensor B j of dimension n k with multisets as entries:</p><p>Then the k-WL update step (Equation 3) can be written as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">When is the algebra of multisymmetric polynomials generated by the elementary multisymmetric polynomials. Contributions to Algebra and Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Briand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="353" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The weisfeiler-lehman method and graph isomorphism testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for earning in raph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Descriptive complexity, canonisation, and definable graph structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pebble games and linear equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="844" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11921</idno>
		<title level="m">Anonymous walk embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<idno>abs/1905.04943</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<title level="m">Covariant compositional networks for learning graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morris</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dual-Primal Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glocalized weisfeiler-lehman graph kernels: Globallocal feature maps of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards a practical k-dimensional weisfeiler-leman algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01543</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02244</idno>
		<title level="m">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A minimal set of generators for the ring of multisymmetric functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rydh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;institut Fourier</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1741" to="1769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<title level="m">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
