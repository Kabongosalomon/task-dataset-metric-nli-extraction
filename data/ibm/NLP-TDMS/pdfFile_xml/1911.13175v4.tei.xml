<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CURL: Neural Curve Layers for Global Image Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Moran</surname></persName>
							<email>sean.j.moran@jpmchase.com</email>
							<affiliation key="aff0">
								<orgName type="institution">JP Morgan Chase &amp; Co</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
							<email>steven.mcdonagh@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
							<email>g.slabaugh@qmul.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CURL: Neural Curve Layers for Global Image Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel approach to adjust global image properties such as colour, saturation, and luminance using human-interpretable image enhancement curves, inspired by the Photoshop curves tool. Our method, dubbed neural CURve Layers (CURL), is designed as a multi-colour space neural retouching block trained jointly in three different colour spaces (HSV, CIELab, RGB) guided by a novel multi-colour space loss. The curves are fully differentiable and are trained end-to-end for different computer vision problems including photo enhancement (RGB-to-RGB) and as part of the image signal processing pipeline for image formation (RAW-to-RGB). To demonstrate the effectiveness of CURL we combine this global image transformation block with a pixel-level (local) image multi-scale encoder-decoder backbone network. In an extensive experimental evaluation we show that CURL produces stateof-the-art image quality versus recently proposed deep learning approaches in both objective and perceptual metrics, setting new state-of-the-art performance on multiple public datasets. Our code is publicly available at: https://github.com/sjmoran/CURL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image quality is of fundamental importance in any imaging system, especially DSLR and smartphone cameras. Modern smartphones for example, provide multiple camera sensors and sophisticated image signal processor (ISP) pipelines to produce images with good contrast, detail, colours, and dynamic range while at the same time mitigating against degradations. However, despite great advances in imaging hardware and ISP pipelines, there remains substantial room for improvement. Even professional photographers using DSLR will spend significant time using photo editing software to produce a quality digital photograph. This paper is inspired by the Photoshop curves tool 1 , which professional photographers use to modify global properties of an image through manual design of adjustment curves. However, most casual users lack artistic skills (or software) to retouch their photos in such a manner. In this context, several interesting research questions emerge: Is it possible to automatically estimate, and apply, adjustment curves through multiple colour spaces to improve image quality? What collection of curves are necessary? What order should image adjustments be applied? This paper not only precisely poses these research questions, but answers them using new neural *Work done while at Huawei Noah's Ark Lab. 1 https://www.cambridgeincolor.com/tutorials/photoshop-curves.htm CURve Layers dubbed CURL. We arrange a collection of such curve adjustments in a defined sequence operating in multiple colour spaces, resulting in a CURL block.</p><p>CURL can be used along with a deep backbone feature extractor to enhance images in an RGB-to-RGB mapping. An example is shown in <ref type="figure">Figure 1</ref>, where CURL adjusts an underexposed image to produce a result with better contrast and more pleasing colours than other methods such as the state-of-the-art DeepUPE <ref type="bibr" target="#b0">[1]</ref> approach.</p><p>Alternatively, a CURL block can be used as part of the RAW-to-RGB transformation modeling the entire ISP pipeline. In this configuration, first a pixel level block (modeled as an encoder/decoder network) is applied to perform local operations such as denoising and demosaicing, the latter estimating RGB colours at each pixel from the RAW data collected using a colour filter array. Then, a CURL block is used to transform the brightness, colour, and saturation to produce the final image. Modeling the entire ISP in this way, as a single neural network trained end-to-end, is very appealing as standard ISP pipelines consist of many different modules (e.g. denoising, demosiacing, automatic white balance, tone mapping, etc.) that are sequentially applied. Such ISPs are difficult and expensive to design, develop, and tune due to the complex dependencies between the modules.</p><p>Our contributions in this paper are three-fold:</p><p>• Multi-colour space neural retouching block: We introduce CURL, a neural retouching block, that learns a set of piece-wise linear scaling curves to globally adjust image properties in a human-interpretable manner, inspired by the Photoshop curves tool. • Multi-colour space loss function: We apply sequential differentiable transformations of the image in different colour spaces (Lab, RGB, HSV) guided by a novel multicolour space loss function. • Improved Encoder/Decoder for image enhancement:</p><p>CURL modifies images initially enhanced by a backbone network. We explore an effective backbone encoder/decoder architecture. Our Transformed Encoder-Decoder backbone (dubbed TED) has all but one U-Net skip connection removed, with the remaining skip connection endowed with a multi-scale neural processing block that enriches the information available to the decoder.</p><p>Input DeepUPE (16.85 dB) Ours (23.55 dB) Groundtruth <ref type="figure">Fig. 1</ref>. Underexposed image enhancement. Given a poorly exposed image, our method produces an image with pleasing contrast and colour better matching the groundtruth compared to the state-of-the-art DeepUPE model <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The CURL block, illustrated in <ref type="figure">Figure 2</ref>, is useful in two image enhancement scenarios, both of which we explore in this paper: a) Photo Enhancement (RGB-to-RGB mapping): taking an input RGB image and mapping that image to a visually pleasing output RGB image and b) RAW-to-RGB mapping: modelling the full ISP pipeline, taking RAW sensor input and producing an RGB output image.</p><p>Photo enhancement typically relates to brightness and colour adjustment. The state-of-the-art DeepUPE <ref type="bibr" target="#b0">[1]</ref> approach learns a luminance adjustment matrix that improves the exposure of an image. Hu et al. <ref type="bibr" target="#b1">[2]</ref> design a photo retouching approach (White-Box) using reinforcement learning and GANs to apply a sequence of filters to an image. Deep Photo Enhancer (DPE) <ref type="bibr" target="#b2">[3]</ref> is a GAN-based architecture with a U-Net-style generator for RGB image enhancement that produces state-of-the-art results on the popular MIT-Adobe5K benchmark dataset. Recently <ref type="bibr" target="#b3">[4]</ref> demonstrated an image enhancement method that does not require reference training images, and <ref type="bibr" target="#b4">[5]</ref> presented a content-preserving tone adjustment method. Arguably, the highest quality in digital photography is achieved with DSLR which, due to the large aperture and lenses, can produce higher quality photographs than a smartphone camera. Related work enhances smartphone images by learning a mapping between smartphone images and DSLR <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> photographs using deep learning. Our proposed CURL block differs from these methods in its exploitation of multiple colour spaces and the loss function which drives curve learning jointly in each colour space, outperforming <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref> on the RGB-to-RGB task.</p><p>Further combination of multiple ISP tasks (e.g. denoising, demosaicing, photo enhancement etc.) ultimately leads to modeling the entire ISP RAW-to-RGB transformation. Currently the literature is limited for deep learning methods that replace the full pipeline. The most relevant related work is DeepISP <ref type="bibr" target="#b7">[8]</ref>, which incorporates a low level network that performs local adjustment of the image including joint denoising and demosaicing, and a high level network that performs global image enhancement. Other recent work includes the Self-Guided Network (SGN) <ref type="bibr" target="#b8">[9]</ref> which relies extensively on pixel shuffling <ref type="bibr" target="#b9">[10]</ref> operations, and CameraNet <ref type="bibr" target="#b10">[11]</ref> that decomposes the problem into two subproblems of restoration and enhancement. Another related work is <ref type="bibr" target="#b11">[12]</ref> which also learns a RAW-to-RGB mapping, however for extremely dark images using a simple U-Net-style architecture <ref type="bibr" target="#b12">[13]</ref>. Nam and Kim <ref type="bibr" target="#b13">[14]</ref> present a deep model that mimicks the RAW-to-RGB ISP for different cameras. Despite this progress, it is an open question how far image quality can advance using deep RAWto-RGB networks. Similar to <ref type="bibr" target="#b11">[12]</ref>, our proposed pixel-level block has an encoder/decoder structure, however we replace standard skip connections with novel multi-scale contextual awareness (MSCA) connections, providing enriched features from the encoder to the decoder that boost performance. CURL includes global image transformations to adjust colours and brightness, similar to <ref type="bibr" target="#b7">[8]</ref>, however, we introduce neural curve layers, which more expressively adjust image properties in a controlled but human-interpretable way. Unlike <ref type="bibr" target="#b11">[12]</ref> or <ref type="bibr" target="#b7">[8]</ref>, the CURL block uses multiple colour spaces, producing results that outperform <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b11">[12]</ref> on the RAW-to-RGB mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TED + CURL FOR IMAGE ENHANCEMENT</head><p>CURL receives and processes convolutional features from a backbone network ( <ref type="figure">Figure 2</ref>). The backbone network, dubbed Transformed Encoder-Decoder (TED), is a multi-scale encoder-decoder neural network (Section III-A) for local pixel processing. The output of TED is passed to the CURL block (Section III-B), employing novel neural curve layers that globally adjust image properties. We argue that, as in previous work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, effective image enhancement requires both local (Section III-A) and global (Section III-B) adjustment. In Section III-A we describe TED, used for local pixel processing, while in Section III-B we describe CURL. The combination of both architectures is referred to as TED+CURL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformed Encoder-Decoder (TED) for Local Image Adjustment</head><p>Despite its popularity, in our experimental evaluation (Table I), we find that the standard U-Net backbone <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> is not entirely suitable for the image translation task, as it is important to consider global and mid-level context when making local pixel adjustments to reduce spatial inconsistencies in the predicted image. To address this, we redesign the encoder / decoder backbone with a skip connection ( <ref type="figure" target="#fig_1">Figure 3</ref> Parameters regressed by retouching block <ref type="figure">Fig. 2</ref>. Left: Placement of the CURL block with regards to an encoder-decoder backbone. The CURL block leverages neural curve layers for artist-inspired image refinement operations. The input to the backbone network is a RAW image and the output from the CURL block is a high quality RGB image with correct colour and brightness. Right: Illustration of a piece-wise linear neural curve from the CURL block that predicts a scaling factor that adjusts saturation based on hue.  features to enable cross-talk between image content at different scales. The MSCA-connection uses convolutional layers with dilation rate 2 and 4 to gain a larger receptive field and capture mid-level image context from the input. These midlevel feature maps are at the same spatial resolution as the input tensor to the block. Global image features are extracted using a series of convolutional layers with a stride 2, followed by a Leaky ReLU activation and then a max pooling operation. These layers are then followed by global average pooling and a fully connected layer, which outputs a fixed dimensional feature vector, tiled across the height and width dimensions of the input. The resulting feature maps are concatenated to the input and fused by a 1x1 convolution to produce a tensor with much fewer channels. This merges the local, mid-level and global contextual information for concatenation to the feature maps of the upsampling path at that particular level. We find that a MSCA-skip connection at level one of the encoder/decoder provides a good trade-off between parameter complexity and image quality, with all other skip connections removed ( <ref type="figure" target="#fig_1">Figure 3</ref>). Multi-scale neural blocks are not new, having been suggested in the literature before <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, to the best of our knowledge, we are the first to show that endowing a skip connection with a multi-scale processing block can dramatically reduce the number of parameters required while increasing the image quality (Table I, <ref type="figure">Figure 4</ref>) for RAW-to-RGB and RGB-to-RGB mapping.</p><p>Different to previous work, we evaluate our approach on both RAW-to-RGB and photo enhancement (RGB-to-RGB). The backbone encoder/decoder differs depending on the type of input data (RAW or RGB). Both architecture diagrams are presented in the supplementary material. For RAW-to-RGB experiments we modify the encoder architecture to facilitate the processing of RAW images represented by a colour filter array (CFA). The CFA tensor H×W ×1 is shuffled into a packed form using a pixel shuffle layer <ref type="bibr" target="#b9">[10]</ref> to give a (H/r)(W/r)×r 2 tensor, where we set r=2 in this work. This tensor is fed into the downsampling path of the backbone network. The output from the expanding path is a set of feature maps of half the width and height of the full RGB image. These feature maps are added to the input packed image (replicated 4× on channel dimension) through a long skip connection. A pixel shuffle upsampling operation <ref type="bibr" target="#b9">[10]</ref> produces a result with output shape H×W ×C, where C is the number of feature maps, and is passed to the retouching block for further processing. Our RGB-to-RGB network is broadly similar to the RAW-to-RGB network, however the pixel shuffling operations are removed. The long skip connection remains, but in this case the 3 channel input image is added to the output, rather than a shuffled version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CURL: Neural Curve Layers for Global Image Adjustment</head><p>This section describes CURL, a pluggable global image colour and luminance retouching block which forms the central contribution of this paper. The first three channels of the H×W ×C tensor from the pixel-level block are treated as the image to be globally adjusted, and the remaining channels serve as feature maps that are used as input to each neural curve layer. Our proposed neural curve layer block is shown in <ref type="figure">Figure 5</ref> and consists of, for each of the three colour spaces, a global feature extraction block followed by a fully connected layer that regresses the knot points of a piecewise-linear curve <ref type="figure">(Figure 2)</ref>. The curve adjusts the predicted image (Î i ∈ [0, 1]) by scaling pixels with the formula presented in Equation 1.</p><formula xml:id="formula_0">S(Î jl i ) = k 0 + M −1 m=0 (k m+1 − k m )δ(MÎ jl i − m),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">δ(x)=      0 x &lt; 0 x 0 ≤ x ≤ 1 1 x &gt; 1</formula><p>where M is the number of predicted knot points,Î jl i is the jth pixel value in the l-th colour channel of the i-th image, k m is the value of the knot point m. The neural curve outputs scale factors, so to apply the curve is a simple matter of multiplication of a pixel's value with its scale factor indicated by the curve. Example curves learnt by an instance of our model are shown in <ref type="figure">Figure 6</ref>. CURL regresses expressive curves, used to scale rather than remap colors in comparison to existing approaches, e.g. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b1">2</ref> The feature extraction block of the neural curve layer ( <ref type="figure">Figure 5</ref>) accepts a H×W ×C feature map, passing it to a group of blocks each consisting of a convolutional layer with 3×3 kernels of stride 2, and 2×2 maxpooling. We place a global average pooling layer and fully connected layer at the end. The neural curves are learnt in several colour spaces (RGB, Lab, HSV).</p><p>In <ref type="figure">Figure 5</ref>, we arrange the neural curve layers in a particular sequence, adjusting firstly luminance and the a, b chrominance channels (using three curves respectively) in CIELab space. Afterwards, we adjust the red, green, blue channels (using three curves respectively) in RGB space. Lastly hue is scaled based on hue, saturation based on saturation, saturation based on hue, and value based on value (using four curves respectively) in HSV space. This ordering of the colour spaces is found to be optimal based on a sweep on a validation dataset <ref type="bibr" target="#b2">3</ref> . The input to each neural curve layer consists of the concatenation of a H×W ×3 image converted to the given colour space, and a H×W ×C subset of the features from the pixel-level block (C =C−3). For the luminance neural curve layer, the fully connected layer regresses the parameters of the L, a and b channel scaling curves. The scaling curves scale the pixel values in the L, a, b channels using Equation 1. The adjusted CIELab image is then converted back to RGB. This H×W ×3 RGB image is concatenated with the pixellevel block H×W ×C feature map and fed into the second neural curve layer that learns three more curves, one for each channel of the RGB image. These curves are applied to the R, G, B channels to adjust the colours. Lastly, the H×W ×3 RGB image is converted to HSV space. HSV space separates the hue, saturation and value properties of an image. The HSV image is concatenated with the H×W ×C feature map and used by the final curve layer to predict the four HSV space adjustment curves. The four HSV curves are for saturationto-saturation mapping, hue-to-hue, value-to-value and hue-tosaturation mapping. The hue-to-saturation curve is particularly interesting as it permits the precise adjustment of the saturation of individual hues. These curves are applied to the HSV image and the HSV image is converted back to RGB space via a differentiable HSV to RGB conversion. A long skip connection links input and output. The differentiable colour spaces transformations required in the CURL block are readily implementable using e.g. the Kornia library <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The CURL Loss Function</head><p>The CURL loss function consists of three colour spacespecific terms which seek to optimise different aspects of the predicted image, including the hue, saturation, luminance and chrominance. The loss is designed to control each of the colour-space specific transformations in CURL. The loss is minimised over a set of N image pairs {(</p><formula xml:id="formula_2">I i ,Î i )} N i=1 , where I i is the reference image andÎ i is the predicted image. The CURL loss is presented in Equation 2 L = N i=1 L i hsv + L i lab + L i rgb + L i reg<label>(2)</label></formula><p>where L rgb , L lab , L hsv are the various colour space loss terms, and L reg is a curve regularization loss. These terms are defined in more detail below.</p><p>HSV loss, L hsv : given the hue (angle) H i ∈ [0, 2π), saturation S i ∈ [0, 1] and value V i ∈ [0, 1] for image I i , we compute L i hsv in the conical HSV colour space,</p><formula xml:id="formula_3">L i hsv = ω hsv ( Ŝ iVi cos(Ĥ i ) − S i V i cos(H i ) 1 + Ŝ iVi sin(Ĥ i ) − S i V i sin(H i ) 1 )<label>(3)</label></formula><p>HSV is advantageous as it separates colour into useful components (hue, saturation, intensity). We believe this is one of the first applications of a differentiable HSV transform and loss in a deep network for the purposes of image enhancement.</p><p>CIELab loss, L lab : we compute the L 1 distance between the Lab values of the groundtruth and predicted images (Equation 4). In particular, the multi-scale structural similarity (MS-SSIM) <ref type="bibr" target="#b17">[18]</ref> between the luminance channels of the ground truth and predicted images enforces a reproduction of the contrast, luminance and structure of the target image <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_4">L i lab = ω lab Lab(Î i ) − Lab(I i ) 1 + ω ms−ssim MS-SSIM(L(Î i ), L(I i ))<label>(4)</label></formula><p>where Lab(.) is a function that returns the CIELab Lab channels corresponding the RGB channels of the input image and L(.) is a function that returns the L channel of the image in CIELab colour space.</p><p>RGB loss, L rgb : this term consists of L 1 distance on RGB pixels between the predicted and groundtruth images and a cosine distance between RGB pixel vectors (where I j i is threeelement vector representing the RGB components of pixel j in the i-th image)</p><formula xml:id="formula_5">L i rgb = ω rgb Î i − I i 1 + ω cosine (1 − 1 HW HW j=1 (Î j i .I j i Î j i 2 I j i 2 ))<label>(5)</label></formula><p>Curve regularization loss, L reg : to mitigate overfitting we regularize by penalizing the curvature:</p><formula xml:id="formula_6">L i reg = ω reg R−1 r=0 M −3 m=0 (∆k r m+1 − ∆k r m ) 2<label>(6)</label></formula><p>where ∆k m is the gradient of the line segment defined by knot points k m , k m+1 , M is the number of knot points, and R denotes the number of curves.</p><p>Loss term weights: each loss term has an associated weight hyperparameter: ω rgb , ω hsv , ω lab , ω ms−ssim , ω reg ,ω cosine . We empirically find that only the ω mssim term is sensitive to the particular dataset. The supplementary material presents details on our training configuration. Our experimental evaluation L lab +L reg L lab +L hsv +L reg L lab +L hsv +L rgb,no−cos +L reg All terms Groundtruth <ref type="figure">Fig. 7</ref>. Qualitative effect of different combinations of terms in the CURL loss function on image quality. L rgb,no−cos is the RGB loss without the cosine distance term. All terms are most effective, with obvious artefacts and colour distortions removed. See Section III-C for more detail.</p><p>(Section IV) presents evidence towards the necessity and contribution of each term, in relation to output image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We evaluate CURL on three publicly available datasets: (i) Samsung S7 <ref type="bibr" target="#b7">[8]</ref> consists of 110, 12M pixel images of short-to-medium exposure RAW, RGB image pairs and medium-to-medium exposure RAW, RGB pairs. Following <ref type="bibr" target="#b7">[8]</ref> we divide the dataset into 90 images for training, 10 for validation and 10 for testing. (ii) MIT-Adobe5k-DPE <ref type="bibr" target="#b2">[3]</ref> contains 5,000 images taken on DSLR cameras and subsequently adjusted by an artist (Artist C). We follow the dataset pre-processing procedure of DeepPhotoEnhancer (DPE) <ref type="bibr" target="#b2">[3]</ref>. The training dataset consists of 2,250 RGB image pairs. The RAW input images are processed into RGB by Lightroom. The groundtruth RGB images are generated by applying the adjustments of Artist C to the input. The dataset is divided into 2,250 training images and 500 test images. We randomly sample 500 validation images from the 2,250 training images. The images are re-sized to have a long-edge of 500 pixels. (iii) MIT-Adobe5k-UPE <ref type="bibr" target="#b0">[1]</ref> MIT-Adobe5k dataset pre-processed as described in <ref type="bibr" target="#b0">[1]</ref> (DeepUPE). The training dataset consists of 4,000 images of RGB image input, groundtruth pairs. The groundtruth are from Artist C. The dataset is divided into 4,500 training images and 500 test images. We randomly sample 500 images to serve as our validation dataset. The images are not re-sized and range from 6M pixel to 25M pixel resolution. Evaluation Metrics: Our image quality metrics are PSNR, SSIM and the perceptual quality aware metric LPIPS <ref type="bibr" target="#b18">[19]</ref>. Hyperparameters were tuned on the held-out validation portion of each benchmark dataset. See supplementary material. Backbone: For the ablation studies in Tables IV-A we use the parameter heavier TED backbone network with an MSCAskip connection at every level. This variant mostly leads to the highest image quality. For the comparison to the state-ofthe-art in Section IV-B we use the parameter-light variant of TED with one MSCA-skip connection at the first level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CURL Ablation Studies</head><p>Colour Spaces of CURL <ref type="table" target="#tab_1">: Tables II, III</ref> present ablation studies on the inclusion of individual colour spaces and on the ordering of the colour spaces within the retouching block.</p><p>In <ref type="table" target="#tab_1">Table II</ref> we find that inclusion of all colour spaces (HSV, RGB, Lab) in the colour block leads to the best PSNR and SSIM, versus variants of the model that have only a single colour space <ref type="figure">(Figure 8</ref>). <ref type="table" target="#tab_1">Table III</ref> suggests that the ordering of the colour spaces is important, with the highest image quality attained with a Lab, RGB and then HSV adjustment. Loss function terms: Equation 2 presents the CURL loss function. We perform an ablation study in <ref type="table" target="#tab_1">Table IV</ref> on the various terms in the loss function. The highest image quality is attained with all loss function terms, demonstrating the need to constrain each of the three colour spaces appropriately with a dedicated loss term. Interestingly we find that coupling the the L 1 RGB loss with a cosine distance term gives a significant boost in image quality (25.45→27.09 dB, 0.756→0.793 SSIM) compared to the using just the L 1 RGB loss term. In addition the regularization term (L reg ) is important for highest image quality due to its role in constraining the flexibility of the neural retouching curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to state-of-the-art methods</head><p>We evaluate our method against competitive baseline models. Specifically our main state-of-the-art baselines are (i) U-Net: <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref> we use a U-Net architecture without the MSCA-connections, and broadly following the design of the generator architecture of DPE <ref type="bibr" target="#b2">[3]</ref>. (ii) DeepISP [8]: we follow the architecture and experimental procedure of <ref type="bibr" target="#b7">[8]</ref>. The thirty element RGB colour transformation matrix is initialized   Quantitative Comparison: (i) Samsung S7 dataset: We evaluate TED+CURL on the RAW-to-RGB mapping task for long-to-long exposure image pairs. TED+CURL is compared to U-Net <ref type="bibr" target="#b12">[13]</ref> and DeepISP <ref type="bibr" target="#b7">[8]</ref>. Results are presented in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper introduced CURL (CURve Layers), a novel neural block for image enhancement. CURL takes inspiration from artists/photographers and retouches images based on global image adjustment curves. The retouching curves are learnt automatically during training to adjust image properties by exploiting image representation in three different colours spaces (CIELab, HSV, RGB). The adjustments applied by these curves is moderated by novel multi-colour space loss function. In our experimental evaluation a encoder/decoder DeepISP (28.19 dB) TED+CURL (29.37 dB) Groundtruth backbone augmented with the pluggable CURL block significantly outperformed the state-of-the-art across a suite of benchmark datasets. Future research will investigate per-image adaptive ordering of adjustments in the CURL block.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) that we call multi-scale contextual awareness (MSCA) connection. Our Transformed Encoder-Decoder is dubbed TED. The MSCA-connection fuses multiple different contextual image features, combining global and mid-level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Transformed encoder/decoder (TED) backbone fusing multiple levels of image context to deliver more contextually relevant features for the expanding path (Section III-A). Main image: The multi-scale contextual awareness skip connection. Bottom right: the connection is placed at the first level skip connection. All other standard U-Net skip connections are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Examples images produced by DeepISP and TED+CURL on the Samsung S7 Medium Exposure dataset. DeepUPE (16.85 dB) TED+CURL (23.55 dB) Groundtruth Examples images produced by DeepUPE and TED+CURL on the MIT-Adobe-UPE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I BACKBONE</head><label>I</label><figDesc>NETWORK (TED) WITH MSCA-SKIP CONNECTION, IMPROVES IMAGE QUALITY WITH FEWER PARAMETERS (SAMSUNG S7 DATASET).</figDesc><table><row><cell>Architecture</cell><cell cols="3">PSNR SSIM # Parameters</cell></row><row><cell>TED (MSCA-skip, level 1)</cell><cell>26.56</cell><cell>0.781</cell><cell>1.3 M</cell></row><row><cell>TED (MSCA-skip, all levels)</cell><cell>26.39</cell><cell>0.793</cell><cell>3.3 M</cell></row><row><cell>U-Net</cell><cell>25.78</cell><cell>0.771</cell><cell>1.4 M</cell></row><row><cell>U-Net (large)</cell><cell>25.37</cell><cell>0.788</cell><cell>5.1 M</cell></row></table><note>Fig. 4. Left: U-Net. Middle: TED. Right: Groundtruth. TED improves colour reproduction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 5. CURL block refines the image from the backbone network to adjust luminance, colour and saturation. The image has luminance adjusted first, color (RGB) second, and saturation third using three piecewise linear curves. The output of this block is the final RGB image. C is number of feature maps, s stride, d dilation rate, H image height, W image width.Fig. 6. Examples of learnt neural global adjustment curves for different colour spaces (CIELab and HSV). Retouching is often a subtle operation, mildly adjusting colour and luminance properties of an image produced by the backbone network. A unique adjustment curve is predicted per image, with the magnitude of the adjustment depending on how much retouching is required for that image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H x W x 3</cell></row><row><cell></cell><cell></cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool(2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool (2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool (2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU,</cell><cell>Global average pooling</cell><cell>1 x 1 x C</cell><cell>Fully connected</cell><cell></cell><cell>x</cell><cell>Lab2Rgb</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scaling</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">H/128 x W/128 x C</cell><cell></cell><cell>Curve x3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H x W x 3</cell></row><row><cell>RGB2Lab</cell><cell></cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool(2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool(2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU, Maxpool(2x2,s=2)</cell><cell cols="2">Conv 3x3 (s=2, d=1), LReLU,</cell><cell>Global average pooling</cell><cell>1 x 1 x C</cell><cell>Fully connected</cell><cell>Scaling</cell><cell>x</cell><cell>Rgb2Hsv</cell><cell></cell></row><row><cell cols="2">H x W x C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">H/128 x W/128 x C</cell><cell></cell><cell></cell><cell>Curve x3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H x W x 3</cell></row><row><cell>3</cell><cell>C -3</cell><cell>Conv 3x3 (s=2, d=1),</cell><cell>LReLU, Maxpool(2x2,s=2)</cell><cell>Conv 3x3 (s=2, d=1),</cell><cell>LReLU, Maxpool(2x2,s=2)</cell><cell>Conv 3x3 (s=2, d=1),</cell><cell>LReLU, Maxpool(2x2,s=2)</cell><cell>Conv 3x3 (s=2, d=1),</cell><cell>LReLU</cell><cell>Global average pooling</cell><cell>1 x 1 x C</cell><cell>Fully connected</cell><cell></cell><cell>x</cell><cell>Hsv2Rgb</cell><cell>+</cell><cell>Output RGB image</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">H/128 x W/128 x C</cell><cell></cell><cell>Scaling Curve x4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY ON THE CURL GLOBAL IMAGE RETOUCHING LAYER. ALL COLOUR SPACES ARE NECESSARY IN CURL FOR BEST IMAGE QUALITY.</figDesc><table><row><cell>Architecture</cell><cell></cell><cell cols="4">PSNR↑ SSIM↑ LPIPS↓</cell></row><row><cell cols="2">TED+CURL (RGB only)</cell><cell cols="2">26.74</cell><cell>0.790</cell><cell>0.340</cell></row><row><cell cols="2">TED+CURL (HSV only)</cell><cell cols="2">25.88</cell><cell>0.780</cell><cell>0.308</cell></row><row><cell cols="2">TED+CURL (CIELab only)</cell><cell cols="2">26.98</cell><cell>0.788</cell><cell>0.323</cell></row><row><cell cols="4">TED+CURL (HSV, RGB, CIELab) 27.09</cell><cell>0.793</cell><cell>0.312</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="6">RESULTS FOR ALL PERMUTATIONS OF THE COLOUR SPACES IN</cell></row><row><cell></cell><cell cols="2">OUR CURL BLOCK.</cell><cell></cell><cell></cell></row><row><cell>Ordering</cell><cell>PSNR</cell><cell>SSIM</cell><cell cols="2">PSNR</cell><cell>PSNR</cell></row><row><cell></cell><cell>(test)↑</cell><cell>(test)↑</cell><cell cols="2">(valid)↑</cell><cell>(valid)↑</cell></row><row><cell>HSV→RGB→LAB</cell><cell>26.20</cell><cell>0.779</cell><cell cols="2">26.48</cell><cell>0.765</cell></row><row><cell>RGB→HSV→LAB</cell><cell>26.83</cell><cell>0.786</cell><cell cols="2">26.23</cell><cell>0.770</cell></row><row><cell>LAB→RGB→HSV</cell><cell>27.09</cell><cell>0.793</cell><cell cols="2">26.56</cell><cell>0.771</cell></row><row><cell>LAB→HSV→RGB</cell><cell>26.37</cell><cell>0.784</cell><cell cols="2">26.44</cell><cell>0.757</cell></row><row><cell>RGB→LAB→HSV</cell><cell>25.32</cell><cell>0.761</cell><cell cols="2">26.31</cell><cell>0.760</cell></row><row><cell>HSV→LAB→RGB</cell><cell>26.53</cell><cell>0.787</cell><cell cols="2">26.67</cell><cell>0.769</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Qualitative effect of different formulations of the CURL block. We keep only one colour space (either CIELab, RGB, HSV) in the CURL block and compare the output versus keeping all three colour spaces.</figDesc><table><row><cell>HSV (21.99 dB)</cell><cell>RGB (22.93 dB)</cell><cell>LAB (24.76 dB)</cell><cell>All (25.86 dB)</cell><cell>Groundtruth</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON THE SAMSUNG S7 DATASET [8] FOR THE VARIOUS TERMS IN THE CURL LOSS FUNCTION. rgb, no−cos IS EQUATION 5 without THE COSINE TERM. lab +L hsv +L rgb ) 26.32 0.768 TED+CURL (L lab +L hsv +L rgb,no−cos +Lreg) 25.45 0.756 TED+CURL (L lab +L hsv +L rgb +Lreg) 27.09 0.793</figDesc><table><row><cell>CURL Loss Terms</cell><cell>PSNR SSIM</cell></row><row><cell>TED+CURL (L lab +Lreg)</cell><cell>26.19 0.777</cell></row><row><cell>TED+CURL (L rgb +Lreg)</cell><cell>26.52 0.785</cell></row><row><cell>TED+CURL (L rgb,no−cos +Lreg)</cell><cell>25.65 0.768</cell></row><row><cell>TED+CURL (L hsv +Lreg)</cell><cell>25.90 0.777</cell></row><row><cell>TED+CURL (L hsv +L rgb +Lreg)</cell><cell>26.61 0.792</cell></row><row><cell>TED+CURL (L lab +L hsv +Lreg)</cell><cell>26.35 0.781</cell></row><row><cell>TED+CURL (L lab +L rgb +Lreg)</cell><cell>25.88 0.767</cell></row><row><cell>TED+CURL (L</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc>MEDIUM-TO-MEDIUM EXPOSURE RAW TO RGB MAPPING RESULTS ON THE HELD-OUT TEST IMAGES OF THE SAMSUNG S7DATASET<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Architecture</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell># Params</cell></row><row><cell cols="2">TED+ CURL 27.04</cell><cell>0.794</cell><cell>0.320</cell><cell>1.4 M</cell></row><row><cell>TED</cell><cell>26.56</cell><cell>0.781</cell><cell>0.339</cell><cell>1.3 M</cell></row><row><cell>U-Net [13]</cell><cell>25.90</cell><cell>0.783</cell><cell>0.340</cell><cell>5.1 M</cell></row><row><cell>DeepISP [8]</cell><cell>26.51</cell><cell>0.794</cell><cell>0.326</cell><cell>3.9 M</cell></row><row><cell cols="5">using linear regression. (iii) DPE: [3] we evaluate against the</cell></row><row><cell cols="5">supervised (paired data) version of DPE. (iv) DeepUPE [1]</cell></row><row><cell cols="3">(v) HDRNet [15] (vi) White-Box [2].</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V .</head><label>V</label><figDesc>TED+CURL produces higher-quality images for both exposure settings compared to U-Net and outperforms DeepISP in terms of PSNR and LPIPS metrics. This suggests TED+CURL is a competitive model for replacing the traditional RAW-to-RGB ISP pipeline. (ii) MIT-Adobe5k (DPE):Table VIpresents the results on this dataset. TED+CURL uses ∼2.5× fewer parameters than DPE, yet is able to maintain the same SSIM score while boosting PSNR by +0.17dB. (iii) MIT-Adobe5k (UPE):Table VIIdemonstrates that CURL is competitive with DeepUPE on this challenging high-resolution dataset. TED+CURL obtains a substantial +1.16dB boost in PSNR, a reduction in LPIPS (lower is better) while retaining a</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI PREDICTION</head><label>VI</label><figDesc>QUALITY OF PHOTOGRAPHER C RETOUCHINGS FOR THE MIT-ADOBE 5K [3] TEST IMAGES. RESULTS FOR OTHER ARCHITECTURES ARE EXTRACTED FROM [3]. SSIM RESULTS FOR PREDICTING THE RETOUCHING OF PHOTOGRAPHERS ON THE 500 TESTING IMAGES FROM THE MIT-ADOBE 5K DATASET. DATASET PRE-PROCESSED ACCORDING TO THE DEEPUPE PAPER [1]. THE PSNR AND SSIM OF OTHER ARCHITECTURES ARE EXTRACTED FROM [1]. Visual Comparison: results showing our method output in comparison to DeepISP and DeepUPE are shown in Figures 9-10. CURL produces images with more pleasing colour and luminance compared to DeepISP and DeepUPE. Additional visual examples are presented in the supplementary material.</figDesc><table><row><cell>Architecture</cell><cell cols="2">PSNR↑ SSIM↑</cell><cell cols="2">LPIPS↓ # Params</cell></row><row><cell>TED+CURL</cell><cell>24.04</cell><cell>0.900</cell><cell>0.583</cell><cell>1.4 M</cell></row><row><cell>DPED [6]</cell><cell>21.76</cell><cell>0.871</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">8RESBLK [20], [21] 23.42</cell><cell>0.875</cell><cell>-</cell><cell>-</cell></row><row><cell>FCN [22]</cell><cell>20.66</cell><cell>0.849</cell><cell>-</cell><cell>-</cell></row><row><cell>CRN [23]</cell><cell>22.38</cell><cell>0.877</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net [13]</cell><cell>22.13</cell><cell>0.879</cell><cell>-</cell><cell>-</cell></row><row><cell>DPE [3]</cell><cell>23.80</cell><cell>0.900</cell><cell>0.587</cell><cell>3.3 M</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell>AVERAGE PSNR, Architecture</cell><cell cols="4">PSNR↑ SSIM↑ LPIPS↓ # Params</cell></row><row><cell>TED+CURL</cell><cell>24.20</cell><cell>0.880</cell><cell>0.108</cell><cell>1.4 M</cell></row><row><cell>HDRNet [15]</cell><cell>21.96</cell><cell>0.866</cell><cell>-</cell><cell>-</cell></row><row><cell>DPE [3]</cell><cell>22.15</cell><cell>0.850</cell><cell>-</cell><cell>3.3 M</cell></row><row><cell>White-Box [2]</cell><cell>18.57</cell><cell>0.701</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Distort-and-Recover [24] 20.97</cell><cell>0.841</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepUPE [1]</cell><cell>23.04</cell><cell>0.893</cell><cell>0.158</cell><cell>1.0 M</cell></row><row><cell>competitive SSIM.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Thanks to the scaling, CURL can easily mix representations, for example mapping saturation as a function of hue, whereas previous approaches are restricted to mapping like to like (e.g. red channel to red channel).<ref type="bibr" target="#b2">3</ref> An experimental study on the ordering of the colour spaces is presented in Section IV.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exposure: A white-box photo post-processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zeroreference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Content-preserving tone adjustment for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cusao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dslrquality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepISP: Towards Learning an End-to-End Image Processing Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="912" to="923" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-guided network for fast image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cameranet: A two-stage framework for effective camera isp learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modelling the scene dependent imaging in cameras with a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for real-time image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expandnet: A deep convolutional neural network for high dynamic range expansion from low dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marnerides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bashford-Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hatchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiscale Structural Similarity for Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast image processing with fullyconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distort-and-recover: Color enhancement using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
