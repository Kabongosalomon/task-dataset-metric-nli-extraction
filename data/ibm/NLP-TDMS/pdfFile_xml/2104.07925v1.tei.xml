<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention! Stay Focus!</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Vo</surname></persName>
							<email>tuvovan@pukyong.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">BridgeAI Inc. Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention! Stay Focus!</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a deep convolutional neural networks (CNNs) to deal with the blurry artifacts caused by the defocus of the camera using dual-pixel images. Specifically, we develop a double attention network which consists of attentional encoders, triple locals and global local modules to effectively extract useful information from each image in the dual-pixels and select the useful information from each image and synthesize the final output image. We demonstrate the effectiveness of the proposed deblurring algorithm in terms of both qualitative and quantitative aspects by evaluating on the test set in the NTIRE 2021 Defocus Deblurring using Dual-pixel Images Challenge [1]. The code, and trained models are available at https://github.com/tuvovan/ATTSF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In general, the exposure of the image can be adjusted by two different ways. The first way is to change the shutter speed when the aperture is fixed to control the amount of light falling on the sensor. The other way is to keep the shutter speed unchanged while adjusting the aperture's size. While the former method may cause the motion blur if there is any possible object motion, the later method results in a shallow depth of field (DoF), causing defocus blur to occur in scene regions outside the DoF <ref type="bibr" target="#b1">[3]</ref> . Removing the defocus blur is critical as we can obtain an image which is captured using a wide aperture but still have everything in focus, which ensures a well-exposed image with sufficiently sharp image.</p><p>In theory, defocus blur is a result of a sharp region with a spatial point spread function (PSF) that use the neighborhood pixel in producing the blurry pixel <ref type="bibr" target="#b13">[14]</ref>. As a result, using the dual-pixel alone may not sufficiently enough to faithfully recover the original sharp pixel. However, we believe that by employing the large receptive fields provided by stacking convolution layers with maxpooling, a neural network will be able to produce the non-blurred outputs, given the dual-pixel inputs. Recently, Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> trained an Unet-like model which takes two images as input and produces a defocus blur-free output. Unet is an encoder-decoder framework which considers all pixel and channel equally which we think not suitable as the blurry pixels are distributed differently for each channel and each pixel location.</p><p>In this work, to employ different feature in both input images intentionally, we propose an attention deep convolutional neural networks (CNNs) to remove the defocus blur artifact which is built upon Encoder -Decoder architecture with the Dual Attention Modules. As mentioned above, we notice that every pixel and channel of the input images should be considered appropriately, make them contribute to the final output at different level. As a result, we redesigned the encoder to extract the useful information by adding the dual-attention module to the classical encoder module. Furthermore, the extracted features from the attention-encoder will be concatenated and put through the triple-local and global-non-local modules before being decoded by decoder modules and generate the sharp output image. We demonstrate the effectiveness of the proposed network through the NTIRE2021 Defocus Deblurring Challenge <ref type="bibr">[1]</ref>. Using the data provided by the competition, we trained a network and finally archived the average PSNR of 26.4243 dB, stands at the 9 th position in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Defocus Deblurring</head><p>While there are many previous works on deblurring field, we found that those methods that try to estimate the defocus map and deblurring are the closest methods as they all try to produce the sharp and deblurred output. The most common method for defocus deblurring task is to first estimate the deblurring kernel and then use that kernel as a guidance for deblurring. To find the deblurring kernel, Park et al. <ref type="bibr" target="#b11">[12]</ref> fed a combination of pretrained blur classification network to extract the deep blur feature along with the hand-crafted feature to a regression network to estimate the amount of blur in the pixel edge to later deblur it. Karaali et al. <ref type="bibr" target="#b7">[8]</ref> extracted the difference between gradient of the blurry image and the original one. And most recently, Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> introduced a deep learning model, which consists of encoder and decoder modules, use the dual-pixel data to directly solve for the defocus blur in a single step without estimating the defocus map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Modules in Deep Learning</head><p>Recently, attention mechanisms show the effectiveness in many computer vision fields including the image restoration. Thanks to its ability to facilitate deep neural networks to determine where to focus and improve the representation of interest <ref type="bibr" target="#b14">[15]</ref>. By observing that each pixel and each image channel should be considered separately, we end up adding the dual-attention <ref type="bibr" target="#b5">[6]</ref> module to the conventional encoder in our proposed network to tackle with the defocus deblurring challenge. By incorporating the attention mechanisms with the conventional encoder modules, every pixel and channel is dealt separately, make sure they contribute the useful information at different level before being merged and being decoded. With this mechanism, the proposed architecture yields high-quality results in both qualitative and quantitative perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Defocus Blur Dataset</head><p>There are several available datasets for the defocus deblurring task. Salvado et al. <ref type="bibr" target="#b4">[5]</ref> proposed the RTF dataset which has 22 image pairs of blurred image and its corresponding in-focus-image. The CUHK <ref type="bibr" target="#b12">[13]</ref> and DUT <ref type="bibr" target="#b16">[17]</ref> provide real-blurred images with the corresponding binary masks representing the blur/sharp regions. This dataset is more suitable for blur detection task. Recently, Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> proposed a large dataset with 500 different pairs of non-overlapped scenes. By using the dual-pixels, the dataset is extended to 2000 images with blur images and the corresponding sharp images. This dataset is used for NTIRE2021 Defocus Deblurring Challenge [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attention! Stay Focus! (ATTSF)</head><p>We design an attention encoder decoder network to effectively synthesize the blurry input images and generate a high-quality blur-free output. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of the proposed network, which takes two images (left blurry image and right blurry image) as input and then reconstructs a sharp image. In details, our proposed ATTSF consists of several attention encoders, triple local, globallocal blocks and decoder modules. The attention encoders are used to extract useful features from the blurry input images. The features generated from those encoders are concatenated together and being transferred to the triple-local and global non-local modules in parallel then finally being decoded to get the final sharp output image. To ensure the output image to have the useful feature from the input images, we use the skip-connection to connect the output feature of the encoders and decoders at every level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention Encoder (ATTE)</head><p>The conventional encoder usually consists of several convolution layers following by the activation layers and pooling layers. Encoder modules are good at extracting the high-level feature of the input image. However, all pixel and channel are treated equivalently which is not strong enough in this defocus deblurring challenge in our opinion. We observe that the defocus deblurring challenge, the blur level are not equally distributed both over image channels and image pixel. As a results, we employ the dual-attention mechanism, which is composed of channel attention and pixel attention or position attention. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of the dual-attention modules. To be specific, each ATTE block has its input to go through the dual-attention module to extract the high-level feature intentionally. Follow the dual-attention module is a couple of convolution layers with ReLU <ref type="bibr" target="#b10">[11]</ref> activation functions and MaxPooling layers, just similar to the conventional encoder.</p><p>Channel Attention As the input of the network is dual-pixel, each image should contribute different kind of information to the final output image. Base on this we employed the dual-attention module which includes the channel attention and pixel attention. The channel attention intentionally extract the feature across the channel dimension by calculating the channel attention map from the input feature. The channel attention applies the convolution layer following by the sigmoid function, which ensures that the attention map will range from 0.0 to 1.0, representing the amount of the information contribution of each feature channel to the output. By masking the input feature with the calculated attention mask, we get the output feature which consists of useful information from each channel of the input.</p><p>Pixel Attention In addition to channel attention, pixel attention a.k.a position attention is also crucial for this task. While channel attention works on channel domain, pixel attention, on the other hand, pays attention to the every pixel in the input feature. The module applies global average pooling(GAP) and global max pooling(GMP) in parallel on the input feature. The GAP and GMP feature are then concatenated together, follows by a 1 × 1 convolution with the sigmoid activation function to generate a pixel attention mask. The attention mask is then multiply equally with every input channel, resulting in the output feature.</p><p>Dual Attention Having the pixel attention and channel attention, the dual attention module takes the input feature and applies two 3 × 3 convolution layers follows by ReLU <ref type="bibr" target="#b10">[11]</ref> activation  function. The feature is then put through Pixel Attention and Channel Attention simultaneously and being concate-nated in the channel axis. Finally, the concatenated feature is 1 × 1 convoluted to match the dimension of the input fea-ture, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Attention Encoder As mentioned before, the proposed attention encoder is built based on the conventional encoder scheme by adding the dual attention module on top of it. Specifically, the input feature first goes through the dual attention module, then through the encoder part, which is composed of several 3×3 convolution layers and ReLU activation functions <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows the architecture of the triple local module. This is inspired by the inception modules, which has multiple convolution kernel with different size, in order to extract the feature of different levels. The small filter is able to extract local details of the features, and the large filter can cover larger regions of the receiving layers. All the features are concatenated in a channel-wise manner and compressed through a convolutional layer. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the architecture of the global local module. As we know, the convolution represents the local feature. In this task, although the local features are essential, we do not want to loose the global terms as it makes the whole image to be spatially consistent. Here we employed the idea from <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b2">[4]</ref> which calculate the correlation between two input signals of the whole image. The global local module cover large receptive fields so the network can ensure the spatial consistency, avoiding the hallucination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triple Local</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Local</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>In our implementation, each convolution in encoder module is followed by a Rectifier Linear Unit (ReLU) activation function <ref type="bibr" target="#b10">[11]</ref>, while each layer in decoder module is followed by a Leaky Rectifier Linear Unit (Leaky ReLU) <ref type="bibr" target="#b9">[10]</ref>. The reason behind using the Leaky ReLU instead of ReLU is to avoid the under-bound of the hidden layer's output, which may lead to unwanted reconstructed image. Every layer is initialized follow the He normal <ref type="bibr" target="#b6">[7]</ref>, and all convolution kernel in encoders or decoders are 3×3 . In each training batch, we apply several augmentation technique such as random rotation, horizontal and vertical flipping. All input images are normalized between 0.0 and 1.0.</p><p>We first trained the networks using the Adam optimizer <ref type="bibr" target="#b8">[9]</ref> with the learning rate of 1 × 10 −4 , and the batch size was set to 4 for 200 epochs. We then change the loss function to loss function 2 as shown in Eq. 1 and train the model with SGD optimizer with the batch size of 2, learning rate of 5 × 10 −5 with 100 more epochs, where α = 1, β = 0.5, respectively. We also apply the Learning Rate Scheduler to decrease the learning rate by half every 20 epochs. Finally, the model is implemented using Tensorflow <ref type="bibr" target="#b0">[2]</ref> with the help of Tensorflow Addons package. We  <ref type="bibr" target="#b1">[3]</ref> The bold values indicates the better results. We can notice that our network is far better than the state of the arts.</p><p>experimentally found that finetuning the network again with the loss function 1 that has more weight on SSIM , the model not only gets a high PSNR but also high SSIM value. And by achieving high SSIM score, the final predicted images are more similar to the ground-truths, make them become closer to the real sharp images. We used the training dataset provided by the NTIRE2021 Defocus Deblurring Challenge [1]. The dataset is divided into three parts: training, validation and testing . We used the training set for training and validation set, test set for validating and testing the model, respectively. Because of the memory limitation, we did not use the original images for training, but we cropped into many patches of size 560 × 560 from the training and validation sets by sliding over images with strides of 140 × 140. As a results, we end up with more than 2000 images for training and 500 images for validation. For testing, we keep the original sizes. The training took approximately three days using a computer with Intel® Core™ i7, 32GB RAM, and Nvidia V100 GPU. After training and fine-tuning, we use the original test set provided by the competition. The model takes about 0.5 second for one image, which is reasonably fast.</p><formula xml:id="formula_0">Loss = α × SSIM Loss + β × M AELoss</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative and Qualitative Evaluation</head><p>We evaluate the performance of the proposed network using the test set provided by the NTIRE2021 Defocus Deblurring Challenge <ref type="bibr">[1]</ref>. The competition provides two different test sets. One of them has ground truth images which we use to compare the qualitative metrics with state-of-theart methods recently, the other one does not include the ground truth, so we only use them for visual comparison, as shown on <ref type="figure" target="#fig_6">Figure 7</ref>. <ref type="table">Table 1</ref> compares the PSNR, MAE and SSIM calculated using the former test set mention above. We only compare with Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> method as we notice that Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> 's method is the only method that tries to solve the defocus deblurring problem using deep learning model, which is close to our work. The proposed method outper-   blurring results on the test set with ground truths provided. Abuolaim et al. <ref type="bibr" target="#b1">[3]</ref> still yields blur artifacts while ATTSF preserves sharp edges and fine details more faithfully. We also verify the effectiveness of the our proposed method us- ing the second test set provided by the competition. As this set does not contain the ground truth images, we only able to compare our results without the ground truth. The results shown on <ref type="figure" target="#fig_6">Figure 7</ref> again show that our proposed method successfully recover the blur region and out-perform the state-of-the-art algorithm. Although there was no ground truth to compare the results qualitatively, it is reported that the our PSNR on this test set was 26.4243 dB, 9 th position in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed an attention deep learning network which leverages the original encoder and decoder architecture by adding the dual-attention modules before every encoder blocks to attentionally extract the feature in each blur input image. Furthermore, at the bottleneck point, we also added the triple local and global local modules in parallel to efficiently extract the local features in different level as well as keep the global context of the input images. The features are then being concatenated with the encoded feature at every level and being decoded by the decoder </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of the proposed ATTSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Dual Attention Module. GAP, GMP are Global Average Pooling and Global Max Pooling, respectively. × denotes channel-wise multiplication, and C denotes the concatenate operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Triple Local Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Global Local Module forms the state-of-the-art algorithm in terms of qualitative metrics, as the feature of the input images are extracted attentionally, and contributed to output the sharp images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 and</head><label>5</label><figDesc>Figure 6visually compare the defocus de-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>(a) Input (b) Ground Truth (c) Abuolaim et al. [3] (d) Proposed Visual comparison of the proposed algorithm and Abuolaim et al. [3] 's algorithm. The proposed algorithm outperforms the Abuolaim et al. [3] as it faithfully recovers the wall region, makes it close to the ground truth image.(a) Input (b) Ground Truth (c) Abuolaim et al. [3] (d) Proposed Visual comparison of the proposed algorithm and Abuolaim et al. [3]'s algorithm. The proposed algorithm successfully deblur the blurry regions such as the door or the light on the ceiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visual comparison of the proposed algorithm and Abuolaim et al. [3]'s algorithm. Abuolaim et al. [3] fails to produce the non-blur output while the proposed algorithm faithfully remove the blur artifact and generate sharp images. modules, then finally restore the sharp output image. We demonstrated the effectiveness of the proposed defocus deblurring architecture through the NTIRE2021 Defocus Deblurring Challenge [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method PSNR SSIM MAE Abuolaim et al. [3] 25.13 78.59 0.0406 Ours 25.98 81.15 0.0377 Table 1: PSNR, SSIM and MAE values of our proposed algorithm, compared with Abuolaim et al.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defocus deblurring using dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-parametric blur map regression for depth of field extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>D&amp;apos;andrès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kochale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1660" to="1673" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene segmentation with dual relationaware attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="2" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edge-based defocus blur estimation with adaptive scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1137" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Machi. Learn</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines vinod nair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machi. Learn</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified approach of multi-scale deep and hand-crafted features for defocus estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative blur detection features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2965" to="2972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utilizing optical aberrations for extended-depth-of-field panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huixuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiriakos</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asean. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="365" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-domain deep convolutional neural networks for image demoireing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Vien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1934" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Defocus blur detection via multi-stream bottom-top-bottom network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1884" to="1897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
