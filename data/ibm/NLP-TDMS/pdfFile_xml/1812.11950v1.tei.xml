<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Liu</surname></persName>
							<email>liuzhou@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution via RL-CSC: When Residual Learning Meets Convolutional Sparse Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple yet effective model for Single Image Super-Resolution (SISR), by combining the merits of Residual Learning and Convolutional Sparse Coding (RL-CSC). Our model is inspired by the Learned Iterative Shrinkage-Threshold Algorithm (LISTA). We extend LISTA to its convolutional version and build the main part of our model by strictly following the convolutional form, which improves the network's interpretability. Specifically, the convolutional sparse codings of input feature maps are learned in a recursive manner, and high-frequency information can be recovered from these CSCs. More importantly, residual learning is applied to alleviate the training difficulty when the network goes deeper. Extensive experiments on benchmark datasets demonstrate the effectiveness of our method. RL-CSC (30 layers) outperforms several recent state-of-thearts, e.g., DRRN (52 layers) and MemNet (80 layers) in both accuracy and visual qualities. Codes and more results are available at https://github.com/axzml/ RL-CSC. arXiv:1812.11950v1 [cs.CV] 31 Dec 2018 1 code can be found: https://github.com/tyshiwo/DRRN_ CVPR17</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single Image Super-Resolution (SISR), which aims to restore a visually pleasing high-resolution (HR) image from its low-resolution (LR) version, is still a challenging task within computer vision research community <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>. Since multiple solutions exist for the mapping from LR to HR space, SISR is highly ill-posed and a variety of algorithms, especially the current leading learning-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref> are proposed to address this problem.</p><p>In recent years, Convolutional Neural Network (CNN) has shown remarkable performance for various computer vision task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b33">33]</ref> owing to its powerful capabilities of learning informative hierarchical representations. Dong et al. <ref type="bibr" target="#b3">[4]</ref> firstly proposed the seminal CNN model for SR termed as SRCNN, which exploits a shallow convolutional  <ref type="formula">(5)</ref> VDSR <ref type="bibr" target="#b19">(20)</ref> DRCN <ref type="bibr" target="#b19">(20)</ref> RED-Net <ref type="bibr" target="#b29">(30)</ref> DRRN <ref type="formula" target="#formula_1">(52)</ref> MemNet (80) RL-CSC <ref type="bibr" target="#b29">(30)</ref> RL-CSC (53) <ref type="figure">Figure 1</ref>: PSNRs of recent CNN models versus the number of parameters for scale factor ×3 on Set5 <ref type="bibr" target="#b0">[1]</ref>. The number of layers are marked in the parentheses. Red points represent our models. RL-CSC with 25 recursions achieves competitive performance with MemNet <ref type="bibr" target="#b23">[24]</ref>. When increasing the number of recursions without introducing any parameters, the performance of RL-CSC can be further improved.</p><p>neural network to learn a nonlinear LR-HR mapping in an end-to-end manner and dramatically overshadows conventional methods <ref type="bibr" target="#b32">[32]</ref>. Inspired by VGG-net <ref type="bibr" target="#b20">[21]</ref>, Kim et al. <ref type="bibr" target="#b12">[13]</ref> firstly constructed a very deep network up to 20 layers named VDSR, which shows significant improvements over SRCNN. Techniques like skip-connection, adjustable gradient clipping were introduced to mitigate the vanishinggradient problem when the network goes deeper. Kim et al. further proposed a deeply-recursive convolutional network (DRCN) <ref type="bibr" target="#b13">[14]</ref> with a very deep recursive layer, and performance can be even improved by increasing recursion depth without new parameters introduced. As the extraordinary success of ResNet <ref type="bibr" target="#b8">[9]</ref> in image recognition, extensive ResNet or residual units based models for SR have emerged. SRResNet <ref type="bibr" target="#b14">[15]</ref> made up of 16 residual units sets up a new state of the art for large upscaling factors (×4). EDSR <ref type="bibr" target="#b15">[16]</ref> removes Batch Normalization <ref type="bibr" target="#b11">[12]</ref> (BN) layers in residual units and produces astonishing results in both qualitative and quantitative measurements. Tai et al. <ref type="bibr" target="#b22">[23]</ref> proposed DRRN in which modified residual units are learned in a recursive manner, leading to a deeper yet concise network. They further introduced memory block to build MemNet <ref type="bibr" target="#b23">[24]</ref> based on dense connections. In <ref type="bibr" target="#b16">[17]</ref>, an encodingdecoding network named RED-Net was proposed to take full advantage of many symmetric skip connections. Despite achieving amazing success in SR, the aforementioned models usually lack convincing analyses about why they worked. Numerous questions are expected to be explored, i.e., what role each module plays in the network, whether BN is needed, etc. In the past decades, sparse representation with strong theoretical support has been widely used <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37]</ref> due to its good performance. Its still valuable even nowadays data-driven models have became more and more popular. Wang et al. <ref type="bibr" target="#b28">[29]</ref> introduced a sparse coding based network (SCN) for image super-resolution task, by combining the powerful learning ability of neural network and peoples domain expertise of sparse coding, which fully exploits the approximation of sparse coding learned from LISTA <ref type="bibr" target="#b5">[6]</ref>. With considerable improvements of SCN over traditional sparse coding methods <ref type="bibr" target="#b32">[32]</ref> and SRCNN <ref type="bibr" target="#b3">[4]</ref> are observed, the authors claim that peoples domain knowledge is still valuable and when it combines with the merits of deep learning, results can benefit a lot. However, layers in SCN are strictly corresponding to each step in the procedure of traditional sparse coding based image SR, so the network still attempts to learn the mapping from LR to HR images. It turns out to be inefficient as indicated in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, which limits the results to be further improved. Moreover, as the experimental results of SCN show no observable advancement when the number of recurrent stages k is increased, the authors finally choose k = 1 causing SCN to become a shallow network.</p><p>Convolutional Sparse Coding (CSC) has attained much attention from researchers <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref> for years. As CSC inherently takes the consistency constraint of pixels in overlapped patches into consideration, Gu et al. <ref type="bibr" target="#b6">[7]</ref> proposed CSC based SR (CSC-SR) model and revealed the potential of CSC for image super-resolution over conventional sparse coding methods. In order to build a computationally efficient CSC model, Sreter et al. <ref type="bibr" target="#b21">[22]</ref> introduced a convolutional recurrent sparse auto-encoder by extending the LISTA method to a convolutional version, and demonstrated its efficiency in image denoising and inpainting tasks.</p><p>To add more interpretability to CNN models for SR and inspire more researches to focus on this topic, we propose a novel model for SR, simple yet effective, to attempt to combine the merits of Residual Learning and Convolutional Sparse Coding (RL-CSC). In a nutshell, the contributions of this paper are three-fold:</p><p>1. Unlike many researchers referring to networks proposed in the field of image recognition for inspiration, our model, termed as RL-CSC, is deduced from LISTA. So we provide a new effective way to facilitate model construction, in which every module has welldefined interpretability.</p><p>2. Analyses about the advantages over <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref> are discussed in detail.</p><p>3. Thanks to the guidelines of sparse coding theory, RL-CSC (30 layers) has achieved competitive results with DRRN <ref type="bibr" target="#b22">[23]</ref> (52 layers) and MemNet <ref type="bibr" target="#b23">[24]</ref> (up to 80 layers) in image super-resolution task. <ref type="figure">Figure 1</ref> shows the performances of several recent CNN models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> in SR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse Coding and LISTA</head><p>Sparse Coding (SC) has been widely used in a variety of applications such as image classification, super resolution and visual tracking <ref type="bibr" target="#b37">[37]</ref>. The most popular form of sparse coding attempts to find the optimal sparse code that minimizes the objective function (1), which combines a data fitting term and an 1 -norm sparsity-inducing regularization:</p><formula xml:id="formula_0">arg min z 1 2 y − Dz 2 2 + λ z 1 ,<label>(1)</label></formula><p>where z ∈ R m is the sparse representation of a given input signal y ∈ R n w.r.t. an n × m dictionary D, and regularization coefficient λ is used to control the sparsity penalty. m &gt; n is satisfied when D is overcomplete. One popular method to optimize (1) is the so-called Iterative Shrinkage Thresholding Algorithm (ISTA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">37]</ref>. At the k th iteration, the sparse code is updated as:</p><formula xml:id="formula_1">z k+1 = h λ/L z k + 1 L D T (y − Dz k ) ,<label>(2)</label></formula><p>where L ≤ µ max , and µ max denotes the largest eigenvalue of D T D. h θ (·) is an element-wise soft shreshold operator defined as</p><formula xml:id="formula_2">h θ (α) = sign(α) max(|α| − θ, 0).<label>(3)</label></formula><p>However, ISTA suffers from slow convergence speed, which limits its application in real-time situations. To address this issue, Gregor and LeCun <ref type="bibr" target="#b5">[6]</ref> proposed a fast algorithm termed as Learned ISTA (LISTA) that produces approximate estimates of sparse code with the power of neural network. LISTA can be obtained by rewriting <ref type="formula" target="#formula_1">(2)</ref> as </p><formula xml:id="formula_3">z k+1 = h θ (W e y + Gz k ) ,<label>(4)</label></formula><formula xml:id="formula_4">where 1 L D T is replaced with W e ∈ R m×n , I − 1 L D T D with G ∈ R m×m and λ</formula><p>L with a vector θ ∈ R m + (so every entry has its own threshold value). Unlike ISTA, parameters W e , G and θ in LISTA are all learned from training samples using back-propagation procedure. After a fixed number of iterations, the best possible approximation of the sparse code will be produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convolutional Sparse Coding (CSC)</head><p>Most of conventional sparse coding based algorithms divide the whole image into overlapped patches and cope with them separately and the consistency constraint, i.e., pixels in the overlapping area of adjacent patches should be exactly the same , is not considered. The convolutional sparse coding (CSC) model <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5]</ref> is inherently suitable for this issue, as it processes the whole image directly:</p><formula xml:id="formula_5">arg min f ,Z 1 2 Y − N i=1 f i ⊗ Z i 2 2 + λ N i=1 Z i 1 , (5) where Y ∈ R m×n represents an input image, {f i } N i=1</formula><p>is a group of s×s convolution filters with their respective sparse feature maps Z i ∈ R m×n . The reconstruction image can be derived by a summation of convolution results:</p><formula xml:id="formula_6">Y = N i=1 f i ⊗ Z i .<label>(6)</label></formula><p>The current leading strategies on CSC are based on the Alternating Direction Method of Multipliers (ADMM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref>. However, when these methods are utilized to solve (5), the whole training set is optimized at once, tends to cause a heavy memory burden. Gu et al. <ref type="bibr" target="#b6">[7]</ref> proposed a CSC based SR (CSC-SR) method which takes consistency constraint of neighboring patches into consideration for better image reconstruction. SA-ADMM <ref type="bibr" target="#b38">[38]</ref> is used in their work to alleviate the memory burden issue of ADMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Residual Learning</head><p>Residual learning for SR was first introduced in VDSR [13] to tackle the vanishing/exploding gradients issue when the network goes deeper. As LR image and HR image are similar to a large extent, fitting the residual mapping seems easier for optimization. Given a training set of N LR-HR pairs {I</p><formula xml:id="formula_7">(i) y , I (i) x } N i=1 , the residual image is defined as r (i) = I (i) x − I (i)</formula><p>y , the goal is to learn a model f with parameters Θ that minimizes the following objective function:</p><formula xml:id="formula_8">L(Θ) = 1 N N i=1 f I (i) y − r (i) 2 2 .<label>(7)</label></formula><p>VDSR <ref type="bibr" target="#b13">[14]</ref> uses a single skip connection to link the input Interpolated LR (ILR) image and the final output of the network for HR image reconstruction, termed as Global Residual Learning (GRL), which benefits both the convergency speed and the reconstruction accuracy a great deal. In DRRN <ref type="bibr" target="#b22">[23]</ref>, both Global and Local Residual Learning (LRL) are adopted to help the gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction from ILR Image</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, our model takes the Interpolated Low-Resolution (ILR) image I y as input, and predicts the output HR image I x . Two convolution layers, F 0 ∈ R n×c×s×s consisting of n filters of spatial size c × s × s and F 1 ∈ R n×n×s×s containing n filters of spatial size n×s×s are utilized for hierarchical features extraction from ILR image:</p><formula xml:id="formula_9">y = ReLU F 1 ⊗ ReLU (F 0 ⊗ I y ) ,<label>(8)</label></formula><p>where ⊗ is the convolution operator, and ReLU (·) denotes the Rectified Linear Unit (ReLU) activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning CSC of ILR Features</head><p>Its worth noting that CSC model can be considered as a special case of conventional SC model, for the convolution operation can be viewed as a matrix multiplication by converting one of the inputs into a Toeplitz matrix. So the CSC model (5) has a similar structure to the traditional SC model (1) when the convolution operation is transformed to matrix multiplication. In addition, LISTA is an efficient and effective tool to learn the approximate sparse coding vector of (1). It takes the exact form of equation <ref type="formula" target="#formula_3">(4)</ref> with the weights W e and G represented as linear layers and it can be viewed as a feed-forward neural network with G shared over layers.</p><p>In order to efficiently solving (5), we extend (4) to its convolutional version by substituting W e ∈ R m×n for W 1 ∈ R m×n×s×s , G ∈ R m×m for S ∈ R m×m×s×s . The convolutional case of (4) can be reformulated as:</p><formula xml:id="formula_10">z k+1 = h θ (W 1 ⊗ y + S ⊗ z k ) .<label>(9)</label></formula><p>The sparse feature maps z ∈ R m×m×c×c are learned after K recursions. As for the activation function h θ , <ref type="bibr" target="#b18">[19]</ref> reveals two important facts: (1) the expressiveness of the sparsity inspired model is not affected even by restricting the coefficients to be nonnegative; (2) the ReLU and the soft nonnegative thresholding operator are equal, that is:</p><formula xml:id="formula_11">h + θ (α) = max(α − θ, 0) = ReLU (α − θ).<label>(10)</label></formula><p>So we choose ReLU as activation function in RL-CSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recovery of Residual Image</head><p>When the sparse feature maps z are obtained, they're then fed into a convolution layer W 2 ∈ R m×n×s×s to recover the features of high-frequency information. The last convolution layer H ∈ R c×n×s×s is used for highfrequency information reconstruction:</p><formula xml:id="formula_12">R = H ⊗ ReLU (W 2 ⊗ z).<label>(11)</label></formula><p>Note that we pad zeros before all convolution operations to keep all the feature maps have the same size, which is a common strategy used in a variety of methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. So the residual image R has the same size as the input ILR image I y , and the final HR image I x will be reconstructed by the addition of I y and R:</p><formula xml:id="formula_13">I x = I y + R.<label>(12)</label></formula><p>We build our model by strictly following these analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Structure</head><p>The entire network structure of RL-CSC is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. There are totally 6 trainable layers in our model: two convolution layers F 0 and F 1 used for feature extraction, W 1 and S for learning CSC, W 2 and H for residual image reconstruction. The weight of S is shared during every recursion. When K recursions are applied in the training process, the depth d of the network can be calculated as:</p><formula xml:id="formula_14">d = K + 5.<label>(13)</label></formula><p>The loss function of Mean Square Error (MSE) is exploited in our training process. Given N LR-HR image patch pairs {I</p><formula xml:id="formula_15">(i) y , I (i) x } N i=1</formula><p>as a training set, our goal is to minimize the following objective function with RL-CSC:</p><formula xml:id="formula_16">L(Θ) = 1 N N i=1 RL-CSC I (i) y + I (i) y − I (i) x 2 2 ,<label>(14)</label></formula><p>where Θ denotes the learnable parameters. Stochastic gradient descent (SGD) is used for optimization and we implement our model using the PyTorch <ref type="bibr" target="#b19">[20]</ref> framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussions</head><p>In this section, we discuss the advantages of RL-CSC over several recent CNN models for SR with recursive learning strategy applied. Specifically, DRRN <ref type="bibr" target="#b22">[23]</ref>, SCN <ref type="bibr" target="#b28">[29]</ref> and DRCN <ref type="bibr" target="#b13">[14]</ref> are used for comparison. The simplified structures of these models are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. "Conv is the abbreviation for Convolution layer, "BN represents Batch Normalization <ref type="bibr" target="#b11">[12]</ref>, "Linear stands for Linear layer and "Shreshold means Soft Shreshold operator. The digits on the left of the recursion line is the number of recursions.</p><p>Difference to DRRN. The main part of DRRN <ref type="bibr" target="#b22">[23]</ref> is the recursive block structure, in which several residual units are stacked. To further improve the performance, a multi-path structure (all residual units share the same input) and a pre-activation structure (activation layers come before the weight layers) are utilized. These strategies are proved to be effective. The interesting part is, RL-CSC, deduced from LISTA <ref type="bibr" target="#b5">[6]</ref>, includes a multi-path structure and uses pre-activation inherently. In addition, guided by (9), RL-CSC contains no BN layers at all. BN consumes much amount of GPU memory and increases computational complexity. Experiments on this topic are conducted in Section 5.4. Furthermore, every module in RL-CSC has a good interpretability, which helps the choice of parameter settings for better performance. Experimental results on benchmark datasets under commonly-used assessments demonstrate the superiority of RL-CSC in Section 5.3.</p><p>Difference to SCN. There are three main differences between SCN <ref type="bibr" target="#b28">[29]</ref> and RL-CSC. Firstly, RL-CSC (30 layers) is much deeper than SCN (5 layers). As indicated in <ref type="bibr" target="#b12">[13]</ref>, a deeper network will have a larger receptive filed, which means the network can utilize more contextual information in an image to infer image details. Secondly, we extend LISTA to its convolutional version in <ref type="bibr" target="#b8">(9)</ref>, instead of using linear layers, so more hierarchical information will be extracted. Last but not the least, RL-CSC adopts residual learning, which is a powerful tool for training deeper networks. With the help of residual learning, we can use more recursions, i.e., 25, even 48, to achieve better performance.</p><p>Difference to DRCN. In the recursive part, RL-CSC differs with DRCN <ref type="bibr" target="#b13">[14]</ref> in two aspects. One for Local Residual Learning <ref type="bibr" target="#b22">[23]</ref> (LRL) and the other is pre-activation. Besides, DRCN is not easy to train, so recursive-supervision and skip-connection are introduced to facilitate network to converge. Moreover, an ensemble strategy (In <ref type="figure" target="#fig_1">Figure 3(c)</ref>, the final output is the weighted average of all intermediate predictions) is used to further improve the performance. RL-CSC is relived from these strategies and can be easily trained with more recursions. Advantages of RL-CSC are further illustrated in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, performances of our method on four benchmark datasets are evaluated. We first give a brief introduction to the datasets used for training and testing. Then the implementation details are provided. Finally, comparisons with state-of-the-arts are presented and more analyses about RL-CSC are illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>By following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, our training set consists of 291 images, where 91 of these images are from Yang et al. <ref type="bibr" target="#b32">[32]</ref> with the addition of 200 images from Berkeley Segmentation Dataset <ref type="bibr" target="#b17">[18]</ref>. During testing, we choose the dataset Set5 <ref type="bibr" target="#b0">[1]</ref>, and Set14 <ref type="bibr" target="#b35">[35]</ref> which are widely used for benchmark. Moreover, the BSD100 <ref type="bibr" target="#b17">[18]</ref>, consisting of 100 natural images are used for testing. Finally, the Urban100 of 100 urban images introduced by Huang et al. <ref type="bibr" target="#b10">[11]</ref> is also employed. Both the Peak Signal-to-Noise Ratio (PSNR) and the Structural SIMilarity (SSIM) on Y channel (i.e., luminance) of transformed YCbCr space are calculated for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>To enlarge the training set, data augmentation, which includes flipping (horizontally and vertically), rotating (90, 180, and 270 degrees), scaling (0.7, 0.5 and 0.4), is performed on each image of 291-image dataset. In addition, inspired by prior works, i.e., VDSR <ref type="bibr" target="#b12">[13]</ref> and DRRN <ref type="bibr" target="#b22">[23]</ref>, we also train a single multi-scale model, which means scale augmentation is exploited by combining images of different scales (×2, ×3 and ×4) into one training set. Not only for the network scalability, but the fair comparison with other state-of-the-arts. Furthermore, all training images are partitioned into 33 × 33 patches with the stride of 33, providing a total of 1, 929, 728 LR-HR training pairs.</p><p>The dimensions of all convolution layers are determined as follows:</p><formula xml:id="formula_17">F 0 ∈ R 128×1×3×3 , F 1 ∈ R 128×128×3×3 , W 1 ∈ R 256×128×3×3 , S ∈ R 256×256×3×3 , W 2 ∈ R 256×128×3×3 , H ∈ R 1×128×3×3 .</formula><p>As for the number of recursion, we choose K = 25 in our final model so the depth of RL-CSC is 30 according to <ref type="bibr" target="#b12">(13)</ref>. Further discussions about the number of network parameters and K will be illustrated in Section 5. <ref type="bibr" target="#b3">4</ref> We follow the same strategy as He et al. <ref type="bibr" target="#b7">[8]</ref> for weight initialization where all weights are drawn from a normal distribution with zero mean and variance 2/n out , where n out is the number of output units. The network is optimized using SGD with mini-batch size of 128, momentum parameter of 0.9 and weight decay of 10 −4 . The learning   rate is initially set to 0.1 and then decreased by a factor of 10 every 10 epochs. We train a total of 35 epochs as no further improvements of the loss are observed after that. For maximal convergence speed, we utilize the adjustable gradient clipping strategy stated in <ref type="bibr" target="#b12">[13]</ref>, with gradients clipped to [−θ, θ], where θ = 0.4 is the gradient clipping parameter. A NVIDIA Titan Xp GPU is used to train our model of K = 25, which takes approximately four and a half days .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State of the Arts</head><p>We now compare the proposed RL-CSC model with other state-of-the-arts in recent years. Specifically, SRCNN <ref type="bibr" target="#b3">[4]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, DRRN <ref type="bibr" target="#b22">[23]</ref> and MemNet <ref type="bibr" target="#b23">[24]</ref> are used for benchmarks. All of these models apply bicubic interpolation to the original LR images before passing them to the networks. As the prior methods crop image pixels near borders before evaluation, for fair comparison, we crop the pixels to the same amount as well, even if this is unnecessary for our method. <ref type="table" target="#tab_2">Table 1</ref> shows the PSNR and SSIM on the four benchmark testing sets, and results of other methods are obtained from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Our model RL-CSC with 30 layers outperforms DRRN (52 layers) and MemNet (80 layers) in all datasets and scale factors (both PSNR and SSIM).</p><p>Furthermore, the metric Information Fidelity Criterion (IFC), which has the highest correlation with perceptual scores for SR evaluation <ref type="bibr" target="#b30">[31]</ref>, is also used for comparison. Experimental results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. The IFCs of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> and DRRN are obtained from <ref type="bibr" target="#b22">[23]</ref>. By following <ref type="bibr" target="#b22">[23]</ref>, BSD100 is not evaluated. Its obvious that our method achieves better performances than other methods in all datasets and scale factors.</p><p>Qualitative results are provided in Figures 4, 5 and 6. Our method tends to produce shaper edges and more correct textures, while other images may be blurred or distorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">K, Residual Learning, Number of Filters and Batch Normalization</head><p>The number of recursions K is a key parameter in our model. When K is increased, a deeper RL-CSC model will be constructed. We have trained and tested RL-CSC with 15, 20, 25, 48 recursions, and according to <ref type="bibr" target="#b12">(13)</ref>, the depths of the these models are 20, 25, 30, 53 respectively. The results are presented in <ref type="figure">Figure 7</ref>. The performance curves clearly show that increasing K can promote the final performance (K = 15 33.98dB, K = 20 34.06dB, K = 25 34.11dB, K = 48 34.16dB), which indicates deeper is better. Similar conclusions are observed in LISTA <ref type="bibr" target="#b5">[6]</ref> that more iterations help prediction error decreased. However, when we extend LISTA to its convolutional version and attempt to combine the powerful learning ability of CNN, the characteristics of CNN itself must also be considered. With  <ref type="figure">Figure 4</ref>: SR results of "8023" from BSD100 with scale factor ×4. The direction of the stripes on the feathers is correctly restored in RL-CSC, while other methods fail to recover the pattern.    <ref type="figure">Figure 6</ref>: SR results of "img076" from Urban with scale factor ×4. More details are recovered by RL-CSC, while others produce blurry visual results. more recursions used, deeper networks tend to be bothered by the gradient vanishing/exploding problems. Residual learning is such a useful tool that not only solves these difficulties, but helps network converge faster. We remove the identity branch in RL-CSC and use the same parameter settings as stated in Section 5.2 to train the new network. The results are summarized in <ref type="table" target="#tab_8">Table 3</ref>. Without residual learn-ing, the new network cannot even converge. We stop the training process at the 14 th epoch in advance.</p><p>We also evaluate our model with different number of filters. Specifically, two types of parameter settings are applied:   which is less than VDSRs 664k.</p><formula xml:id="formula_18">i) F 0 ∈ R 128×1×3×3 , F 1 ∈ R 128×128×3×3 , W 1 ∈ R 256×128×3×3 , S ∈ R 256×256×3×3 ,</formula><p>Results are shown in <ref type="figure">Figure 8</ref>. Increasing the the number of filters can benefit the performance, and our model with less parameters, e.g., 592k, still outperforms VDSR, whose PSNR is 33.66dB for scale factor ×3 on Set5. Our final model uses the parameter settings illustrated in Section 5.2.</p><p>Although RL-CSC has more parameters than DRRN <ref type="bibr" target="#b22">[23]</ref>, in our experiment we find DRRN consumes much more GPU memory resources. We test both models with different batch sizes and patch sizes of training data, and the results are summarized in <ref type="table" target="#tab_9">Table 4</ref>. The memory usage datas are derived from the nvidia-smi tool. Patch size of 31 is the default setting in DRRN. Training DRRN 1 of 52 layers is difficult with one Titan Xp GPU, using the default settings given by the authors, because of the Out-of-Memory (OOM) issue. The reason is that the recursive unit of DRRN is based on the residual unit of ResNet <ref type="bibr" target="#b8">[9]</ref>, so the BN layers are exploited, which tend to be memory intensive and increase computational burden. Guided by the analyses presented in Section 3.2, BN layers are not needed in our design. As for inference time, RL-CSC takes 0.15 second to process a 288 × 288 image on a Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we have proposed a novel network for image super-resolution task by combining the merits of Residual Learning and Convolutional Sparse Coding. Our model is derived from LISTA so it has inherently good interpretability. We extend the LISTA method to its convolutional version and build the main part of our model by strictly following the convolutional form. Furthermore, residual learning is adopted in our model, with which we are able to construct a deeper network by utilizing more recursions without introducing any new parameters. Extensive experiments show that our model achieves competitive results with state-of-the-arts and demonstrate its superiority in SR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed RL-CSC framework. Our model takes an interpolated LR image I y as input and predicts the residual component R. Two convolution layers F 0 and F 1 are used for feature extraction and output the feature maps y, which then go through a convolutional LISTA based sub-network (with K recursions surrounded by the dashed box). When the sparse feature maps z are obtained, W 2 is utilized to recover the features of high-frequency information and the convolution layer H maps the features to residual image R. The final HR image I x can be restored by the addition of ILR image I y and residual image R. The unfolded version of the recusive sub-network is shown in the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Network structures of: (a) DRRN [23]. (b) SCN [29]. (c) DRCN [14]. (d) Our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>SR results of "ppt3" from Set14 with scale factor ×3. Texts in RL-CSC are sharp while character edges are blurry in other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red color indicates the best performance and blue color indicates the second best performance.</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>Bicubic</cell><cell>SRCNN [4]</cell><cell>SelfEx [11]</cell><cell>VDSR [13]</cell><cell>DRRN B1U9</cell><cell>DRRN B1U25</cell><cell>RL-CSC</cell></row><row><cell></cell><cell>×2</cell><cell>6.083</cell><cell>8.036</cell><cell>7.811</cell><cell>8.569</cell><cell>8.583</cell><cell>8.671</cell><cell>9.095</cell></row><row><cell>Set5</cell><cell>×3</cell><cell>3.580</cell><cell>4.658</cell><cell>4.748</cell><cell>5.221</cell><cell>5.241</cell><cell>5.397</cell><cell>5.565</cell></row><row><cell></cell><cell>×4</cell><cell>2.329</cell><cell>2.991</cell><cell>3.166</cell><cell>3.547</cell><cell>3.581</cell><cell>3.703</cell><cell>3.791</cell></row><row><cell></cell><cell>×2</cell><cell>6.105</cell><cell>7.784</cell><cell>7.591</cell><cell>8.178</cell><cell>8.181</cell><cell>8.320</cell><cell>8.656</cell></row><row><cell>Set14</cell><cell>×3</cell><cell>3.473</cell><cell>4.338</cell><cell>4.371</cell><cell>4.730</cell><cell>4.732</cell><cell>4.878</cell><cell>4.992</cell></row><row><cell></cell><cell>×4</cell><cell>2.237</cell><cell>2.751</cell><cell>2.893</cell><cell>3.133</cell><cell>3.147</cell><cell>3.252</cell><cell>3.324</cell></row><row><cell></cell><cell>×2</cell><cell>6.245</cell><cell>7.989</cell><cell>7.937</cell><cell>8.645</cell><cell>8.653</cell><cell>8.917</cell><cell>9.372</cell></row><row><cell>Urban100</cell><cell>×3</cell><cell>3.620</cell><cell>4.584</cell><cell>4.843</cell><cell>5.194</cell><cell>5.259</cell><cell>5.456</cell><cell>5.662</cell></row><row><cell></cell><cell>×4</cell><cell>2.361</cell><cell>2.963</cell><cell>3.314</cell><cell>3.496</cell><cell>3.536</cell><cell>3.676</cell><cell>3.816</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Benchmark results. Average IFCs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates the best performance and blue color indicates the second best performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Figure 7: PSNR for RL-CSC with different recursions. The models are tested under Set5 with scale factor ×3.</figDesc><table><row><cell></cell><cell>34.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>34.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>33.0 33.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K = 48</cell></row><row><cell></cell><cell>32.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K = 25</cell></row><row><cell></cell><cell>32.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K = 20 K = 15</cell></row><row><cell></cell><cell>32.0</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell>1</cell><cell></cell><cell>5</cell><cell>10</cell><cell>13</cell></row><row><cell></cell><cell cols="8">With Residual 33.24 33.61 33.61 34.02</cell></row><row><cell></cell><cell></cell><cell cols="2">No Residual</cell><cell cols="2">6.54</cell><cell>6.54</cell><cell>6.54</cell><cell>6.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>W 2 ∈</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>PSNR (dB) for RL-CSC and its non-residual counterpart. Tests on Set5 with scale factor ×3.Figure 8: Results on two types of parameter settings. The tests are conducted on Set5 with scale factor ×3. R 256×128×3×3 , H ∈ R 1×128×3×3 , and K = 15. The total number of parameters is about 1, 329k;ii) F 0 ∈ R 128×1×3×3 , F 1 ∈ R 128×128×3×3 , W 1 ∈ R 128×128×3×3 , S ∈ R 128×128×3×3 , W 2 ∈ R 128×128×3×3 , H ∈ R 1×128×3×3 , and K = 15.The total number of parameters is approximately 592k,</figDesc><table><row><cell></cell><cell>34.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>34.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>33.6 33.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>setting 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>setting 2</cell></row><row><cell></cell><cell>33.2</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>GPU memory usage of different models. RL-CSC with 30 layers are evaluated, compared to DRRN<ref type="bibr" target="#b22">[23]</ref> with 20 and 52 layers. A Titan Xp with 12 GB is used.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<title level="m">Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding. BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Dictionary Learning: A Comparative Review and New Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia-Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Fast Approximations of Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Convolutional Sparse Coding for Image Super-Resolution. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers -Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast and flexible convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Deeply-Recursive Convolutional Network for Image Super-Resolution. CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks analyzed via convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2887" to="2938" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learned convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sreter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename></persName>
		</author>
		<title level="m">0002. Image Super-Resolution via Deep Recursive Residual Network. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">NTIRE 2018 Challenge on Single Image Super-Resolution -Methods and Results. CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="7173" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03344</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Survey of Sparse Representation -Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="490" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast Stochastic Alternating Direction Method of Multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
