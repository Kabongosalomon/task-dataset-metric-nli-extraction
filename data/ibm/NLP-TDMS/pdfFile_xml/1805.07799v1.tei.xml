<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Al-Sabahi</surname></persName>
							<email>k.alsabahi@csu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zuping</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Nadher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Long short-term memory</term>
					<term>hierarchical structured self-attention</term>
					<term>document summarization</term>
					<term>abstract features</term>
					<term>sentence embedding</term>
					<term>document embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advance in neural network architecture and training algorithms has shown the effectiveness of representation learning. The neural-network-based models generate better representation than the traditional ones. They have the ability to automatically learn the distributed representation for sentences and documents. To this end, we proposed a novel model that addresses several issues that are not adequately modeled by the previously proposed models, such as the memory problem and incorporating the knowledge of document structure. Our model uses a hierarchical structured self-attention mechanism to create the sentence and document embeddings. This architecture mirrors the hierarchical structure of the document and in turn enables us to obtain better feature representation. The attention mechanism provides extra source of information to guide the summary extraction. The new model treated the summarization task as a classification problem in which the model computes the respective probabilities of sentence-summary membership. The model predictions are broken up by several features such as information content, salience, novelty, and positional representation. The proposed model was evaluated on two well-known datasets, the CNN/Daily Mail and DUC 2002. The experimental results show that our model outperforms the current extractive state of the art by a considerable margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Text summarization is one of the most active research in natural language processing. It is an optimal way to tackle the problem of information overload by reducing the size of long document(s) into a few sentences or paragraphs. The popularity of handheld devices, such as smart phone, makes document summarization very urgent in the face of tiny screens and limited bandwidth <ref type="bibr" target="#b0">[1]</ref>. It can also serve as a reading comprehension test for machines. To generate a good summary, it is important for a machine learning model to be able to understand the document(s) and distill the important information from it. These tasks are highly challenging for computers, especially as the size of a document increases. Even though the most sophisticated search engines are empowered by advanced information retrieval techniques, they lack the ability to synthesize information from multiple sources and to give users a concise yet informative response. Moreover, there is a need for tools that provide timely access to, and digest of, various sources. These concerns have sparked a great interest in the development of automatic document summarization systems. Traditional Text Summarization approaches typically rely on sophisticated feature engineering that based mostly on the statistical properties of the document being summarized. In short, these systems are complex, and a lot of engineering effort goes into building them. On the top of that, those methods mostly fail to produce a good document representation and a good summary as a result. End-to-end learning models are interesting to try as they demonstrate good results in other areas, like speech recognition, language translation, image recognition, and even question-answering. Recently, neural networkbased summarization approaches draw much attention; several models have been proposed and their applications to the news corpus were demonstrated, as in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>.</p><p>There are two common types of neural text summarization, extractive and abstractive. Extractive summarization models automatically determine and subsequently concatenate relevant sentences from a document to create its summary while preserving its original information content. Such models are common and widely used for practical applications <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. A fundamental requirement in any extractive summarization model is to identify the salient sentences that represent the key information mentioned in <ref type="bibr" target="#b3">[4]</ref>. In contrast, abstractive text summarization techniques attempt to build an internal semantic representation of the original text and then create a summary closer to a human-generated one. The state-of-theart abstractive models are still quite weak, so most of the previous work has focused on the extractive summarization <ref type="bibr" target="#b4">[5]</ref>.</p><p>Despite their popularity, neural networks still have some issues while applied to document summarization task. These methods lack the latent topic structure of contents. Hence the summary lies only on vector space that can hardly capture multi-topical content <ref type="bibr" target="#b5">[6]</ref>. Another issue is that the most common architectures for Neural Text Summarization are variants of recurrent neural networks (RNN) such as Gated recurrent unit (GRU) and Long short-term memory (LSTM). These models have, in theory, the power to 'remember' past decisions inside a fixed-size state vector; however, in practice, this is often not the case. Moreover, carrying the semantics along all-time steps of a recurrent model is relatively hard and not necessary <ref type="bibr" target="#b6">[7]</ref>. In this work, we use a hierarchical structured self-attention mechanism to tackle the problem. In which, a weighted average of all the previous states will be used as an extra input to the function that computes the next state. This gives the model the ability to attend to a state produced several time steps earlier, so the latest state does not have to store all the information <ref type="bibr" target="#b7">[8]</ref>.</p><p>The contribution of this paper is proposing a general neural network-based approach for summarization that extracts sentences from a document by treating the summarization problem as a classification task. The model computes the score for each sentence towards its summary membership by modeling abstract features such as content richness, salience with respect to the document, redundancy with respect to the summary and the positional representation. The proposed model has two improvements that enhance the summarization effectiveness and accuracy: (i) it has a hierarchical structure that reflects the hierarchical structure of documents; (ii) while building the document representation, two levels of selfattention mechanism are applied at word-level and sentencelevel. This enables the model to attend differentially to more and less important content.</p><p>In this paper, two interesting questions are arising: (1) how to mirror the hierarchical structure of the document to improve the embedding representation of sentence and document that can help discovering the coherent semantic of the document; (2) how to extract the most important sentences from the document to form a desired summary <ref type="bibr" target="#b5">[6]</ref>.</p><p>The key difference between this work and the previous ones is that our model uses a hierarchical structured self-attention mechanism to create sentence and document embeddings. The attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the document representation and to the selection process of the summary sentences as well. To evaluate the performance of our model in comparison to other common state-of-the-art architectures, two well-known datasets are used, the CNN/Daily Mail, and DUC 2002. The proposed model outperforms the current state-of-the-art models by a considerable margin.</p><p>The rest of the paper is organized as follows. In section 2, the proposed approach for summarizing documents is presented in details. Section 3 describes the experiments and the results. The related work is briefly described in section 4. Finally, we discuss the results and conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED MODEL</head><p>Recurrent Neural Network variants, such as LSTM, have been used widely in text summarization problem. To prepare the text tokens to be used as an input to these networks, word embeddings, as language models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, are used to convert language tokens to vectors. Moreover, attention mechanisms <ref type="bibr" target="#b10">[11]</ref> make these models more effective and scalable, allowing them to focus on some past parts of the input sequence while making the final decision or generating the output.</p><p>Definition 1: Given a document D consisting of a sequence of sentences (s 1 , s 2 . . . , s n ) and a set of words (w 1 , w 2 , . . . , w m ). Sentence extraction aims to create a summary from D by selecting a subset of M sentences (where M &lt; n). we predict the label of the j th sentence y j as (0,1). The labels of sentences in the summary set are set as y = 1.</p><p>To set the scene of this work, we begin with a brief overview about the self-attention. Given a query vector representation qand an input sequence of tokens x = [x 1 ; x 2 ; . . . ; x n ], where x i denotes the embedded vector of the i-th token); then, the function f (x i ; q) is used to calculate an alignment score between q and each token x i as the vanilla attention of q to x i <ref type="bibr" target="#b10">[11]</ref>. Self-attention is a special case of attention where the query q stems from the input sequence itself. Therefore, self-attention mechanism can model the dependencies between tokens from the same sequence. The function f (x i ; x j ) is used to compute the dependency of x j on another token x i , where the query vector q is replaced by the token x j .</p><p>The work of Yang et al. <ref type="bibr" target="#b7">[8]</ref> demonstrates that using the hierarchical attention yields a better document representation, which they used for document classification task. In this work, we propose a new hierarchical structured self-attention architecture modeled by recurrent neural networks based on recent neural extractive summarization approaches <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. However, our summarization framework is applicable to all models of sentence extraction using the distributed representation as input.</p><p>The proposed model has a hierarchical self-attention structure which reflects the hierarchical structure of the document FIGURE 1. A Hierarchical Structured Self-attention-based summary-sentence classifier: the first layer is a word-level layer for each sentence. The second layer operates on the sentence-level. After each layer, there is attention layer. The logistic layer is the classification layer which determines the sentence-summary membership, where 1's indicate that the sentence is a summary sentence and 0's determine that the sentence is not where words form sentences and sentences form a document. In the new model, there are two level of attention, one at the word-level and the second at the sentence-level. FIGURE 1 shows the overall architecture of the proposed model that will be explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. WORD ENCODER</head><p>Suppose we have a document (D) with n sentences and m words, let D = (s 1 , s 2 , . . . , s n ) where s j (1 ≤ j ≤ n) denotes the j th sentence, and</p><formula xml:id="formula_0">V = (v 1 , v 2 , . . . , v m ) where v i (1 ≤ i ≤ m)</formula><p>is a vector standing for a d dimensional word embedding for the i th word. In this work, we use a bidirectional LSTM to encode the words in the sentences. The bidirectional LSTM summarizes information from both directions. It contains the forward LSTM which reads the sentence s i from v i1 to v im and a backward LSTM which reads</p><formula xml:id="formula_1">from v im to v i1 : − → h t = − −− → LSTM (v t , − − → h t−1 ) (1) ← − h t = ← −− − LSTM (v t , ← − − h t−1 )<label>(2)</label></formula><p>To obtain the hidden state h t that summarizes the information of the whole sentence centered around v it , we concate- <ref type="table">as in Equation 3</ref>.</p><formula xml:id="formula_2">nate each − → h t with ← − h t ,</formula><formula xml:id="formula_3">h t = [ − → h t , ← − h t ]<label>(3)</label></formula><p>Where the number of the hidden units for each unidirectional LSTM is u. Let H s ∈ R nx2u denotes the whole FIGURE 2. The Self-Attention Unit. The attention mechanism takes the whole LSTM hidden states H with R nx2u dimension as an input, and outputs a vector of attention weights, a. Here w s1 is a weight matrix with a shape of ∈R kx2u . and w s2 is a vector of parameters with a size of R k , where k is a hyperparameter can be set arbitrarily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM hidden states, as in Equation 4</head><p>:</p><formula xml:id="formula_4">H s = (h 1 , h 2 , . . . h n ) (4) B. WORD ATTENTION</formula><p>The intuition behind the attention mechanism is to pay more or less attention to words according to their contribution to the representation of the sentence meaning. Our objective is to encode a variable length sentence into a fixed size embedding using a self-attention mechanism <ref type="bibr" target="#b6">[7]</ref> that takes the whole LSTM hidden states H s as input and yields a vector of weights, a s , as output, as in Equation <ref type="bibr" target="#b4">5</ref>.</p><formula xml:id="formula_5">a s = softmax(w s 2 tanh(w s 1 H T s ))<label>(5)</label></formula><p>Where w s 2 ∈ R kx2u and w s 1 ∈ R k are learnable parameters; k is a hyperparameter can be set arbitrarily. The softmax() is used to normalize the attention weights to sum up to 1.</p><p>Given the attention vector a s , we obtain the sentence vector as a weighted sum of the LSTM hidden states weighted by a s , as shown in FIGURE 2 and Equation 6:</p><formula xml:id="formula_6">s i = a s H s<label>(6)</label></formula><p>C. SENTENCE ENCODER After getting the sentence vector s i , we can get the document representation in the same way. A bidirectional LSTM is used to encode the sentences:</p><formula xml:id="formula_7">− → h st = − −− → LSTM (s i , − − → h t−1 ) (7) ← − h st = ← −− − LSTM (s i , ← − − h t+1 ) (8) VOLUME 6, 2018</formula><p>Similar to the word encoder, the forward and backward LSTM hidden states are concatenated to get the annotation h st , which summarizes the adjacent sentences around the sentence s i but focus on the i th sentence, as in <ref type="bibr">Equation 9</ref>.</p><formula xml:id="formula_8">h st = [ − → h st , ← − h st ]<label>(9)</label></formula><p>Let u denotes the number of the hidden units for each unidirectional LSTM, N is the number of sentences in the d th document, and H d denotes the whole LSTM hidden states calculated by Equation 10, the dimension of H d is R Nx2u .</p><formula xml:id="formula_9">H d = (h s1 , h s2 , . . . h sN )<label>(10)</label></formula><p>D. SENTENCE ATTENTION Every sentence in a document contributes differently to the representation of the whole meaning of the document. The self-attention mechanism used in this work takes the whole LSTM hidden states H d as input and yields a vector of weights, a d , as output, calculated by Equation <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_10">a d = softmax(w s 2 tanh(w s 1 H T d ))<label>(11)</label></formula><p>Where w s 2 and w s 1 are learnable parameters. The softmax() is used to normalize the attention weights to sum up to 1.</p><p>Given the attention vector a d , we obtain the document vector as a weighted sum of the LSTM hidden states weighted by a d , as shown in FIGURE 2, and Equation 12:</p><formula xml:id="formula_11">d = a d H d<label>(12)</label></formula><p>E. CLASSIFICATION LAYER Inspired by an interesting work proposed by Nallapati et al. <ref type="bibr" target="#b1">[2]</ref>, we used a logistic layer that makes a binary decision to determine whether a sentence belongs to the summary or not. The classification decision at the j th sentence depends on the representation of the abstract features, such as the sentence's content richness C j , its salience with respect to the document M j , the novelty of the sentence with respect to the accumulated summary N j and the positional feature P j . The probability of the sentence belonging to the summary is given by Equation <ref type="bibr" target="#b17">18</ref>:</p><p>The information content of the j th sentence in the document is represented by Equation <ref type="bibr" target="#b12">13</ref>:</p><formula xml:id="formula_12">C j = W c s j<label>(13)</label></formula><p>Equation <ref type="bibr" target="#b13">14</ref> captures the salience of the sentence with respect to the document:</p><formula xml:id="formula_13">M j = s T j W s d<label>(14)</label></formula><p>Equation <ref type="bibr" target="#b14">15</ref> models the novelty of the sentence with respect to the current state of the summary:</p><formula xml:id="formula_14">N j = s T j W r tanh(o j ),<label>(15)</label></formula><p>where o j is the summary representation calculated by Equation <ref type="bibr" target="#b16">17</ref>.</p><p>The position of the sentence with respect to the document is modeled by Equation <ref type="bibr" target="#b15">16</ref>:</p><formula xml:id="formula_15">P j = W p p j ,<label>(16)</label></formula><p>where p j is the positional embedding of the sentence calculated by concatenating the embeddings corresponding to the forward and backward position indices of the sentence in the document. W c , W s , W p , and W r are automatically learned scalar weights to model the relative importance of various abstract features.</p><p>The summary representation, o j ,at the sentence j is calculated using Equation <ref type="bibr" target="#b16">17</ref>:</p><formula xml:id="formula_16">o j = j−1 i=1 s i P(y i = 1|s i , o i , d)<label>(17)</label></formula><p>Where y j is a binary number determines whether the sentence j is included in the summary or not.</p><p>Putting Equations 13, 14, 15 and 16 together, we get the final probability distribution for the sentence label y j , as in Equation <ref type="bibr" target="#b17">18</ref>:</p><formula xml:id="formula_17">P(y j = 1|s j , o j , d) =σ (C j + M j −N j + P j + b),<label>(18)</label></formula><p>where σ is the sigmoid activation function, and b is the bias term.</p><p>Including the summary representation, o j , in the scoring function allows the model to take into account the previously made decisions in terms of determining the summarysentence membership.</p><p>At training phase, the negative log-likelihood of the observed labels is minimized, as in Equation <ref type="bibr" target="#b18">19</ref>: <ref type="bibr" target="#b18">(19)</ref> where y d j is the binary summary label for the j th sentence in the d th document, n d is the number of the sentences in the document (d), and N is the number of documents.</p><formula xml:id="formula_18">l(W , b) = − N d=1 n d j=1 (y d j logP(y d j = 1|s j , o j , d d ) + (1−y d j ) log(1 − P(y d j = 1|s j , o j , d d )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS A. DATASETS</head><p>The proposed model was evaluated on two well-known datasets, CNN/Daily Mail and DUC 2002. The first dataset was originally built by Hermann et al. <ref type="bibr" target="#b11">[12]</ref> for question answering task and then re-used for extractive <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and abstractive text summarization tasks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. From the Daily Mail dataset, we used 196,557 documents for training, 12,147 documents for validation and 10,396 documents for testing. In the joint CNN/Daily Mail dataset, there are 286,722 for training, 13,362 for validation and 11,480 for testing. The average number of sentences per document is 28. One of the contributions of <ref type="bibr" target="#b1">[2]</ref> is preparing the joint CNN/Daily Mail dataset for the extractive summarization task. In which, they provide sentence-level binary labels for each document, representing the summary-membership of the sentences. We refer the reader to that paper for a detailed description.</p><p>The second dataset is DUC 2002 used as an out-of-domain test set. It contains 567 news articles belonging to 59 different clusters of various news topics, and the corresponding 100-word manual summaries generated for each of these documents (single-document summarization), or the 100-word summaries generated for each of the 59 document clusters formed on the same dataset (multi-document summarization). In this work, we used the single-document summarization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BASELINES</head><p>There are so many approaches to the text summarization problem; for comparison, we choose the ones that are comparable to our work on the two datasets as follows:  <ref type="bibr" target="#b14">[15]</ref> as abstractive baselines. • We also used TGRAPH <ref type="bibr" target="#b15">[16]</ref>, a graph based approaches, and URANK <ref type="bibr" target="#b16">[17]</ref> as baselines on DUC 2002 as they achieve high performance on this dataset.</p><formula xml:id="formula_19">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SETTINGS</head><p>We got the word embedding initialization by training word2vec <ref type="bibr" target="#b1">[2]</ref> on the CNN/Daily Mail dataset. The validation set was used to tune the hyperparameters. The word embedding dimension was set to 100 and the model hidden state size to 200. The concatenation of forward and backward LSTMs gives us a dimension of 400 for both word encoder and sentence encoder. The word and sentence attention context vectors also have a dimension of 400. The vocabulary size was limited to 150k. We set the maximum sentence length to 50 words and the maximum number of sentences per document to 100. At training time, the batch size was 64, and adadelta <ref type="bibr" target="#b17">[18]</ref> was used to train the model and the gradient clipping to regularize it. At test time, we sorted the output probabilities for the sentence-summary membership and then pick the sentences with the top probabilities until we exceed the compression rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EVALUATION</head><p>In this work, the ROUGE (Recall-Oriented Understudy for Gisty Evaluation) metrics <ref type="bibr" target="#b18">[19]</ref> are used for the automatic evaluation of the generated summaries. ROUGE metrics are based on the comparison of n-grams between the summary to be evaluated and one or several human written reference summaries, as in <ref type="bibr">Equation 20</ref>.</p><formula xml:id="formula_20">ROUGE N = S∈{reference Summaries} gram n ∈S Countmatch(gram n ) S∈{reference Summaries} gramn∈S (gram n )<label>(20)</label></formula><p>Remark 1: To ensure that the recall-only evaluation will be unbiased to length, we use the ''-l 75'' options in ROUGE to truncate longer summaries in DUC 2002.</p><p>Remark 2: It is noticed that all the baselines use full-length F1 as an evaluation metric on the entire CNN/DailyMail since the neural abstractive models learn when to stop generating word for the summary. To ensure a fair comparison, we apply the same metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. EXPERIMENTAL RESULTS</head><p>The ROUGE Toolkit 1 and the pyrouge package 2 are used to evaluate the performance of the proposed model. ROUGE-1, ROUGE-2, and ROUGE-L were applied with the settings that mentioned in Remark 1 and Remark 2. We compared our model with several extractive and abstractive baselines, mentioned in section 3.2. The model and the baselines are evaluated on the two datasets, CNN/Daily Mail and DUC 2002. The output of the evaluation was compared to the human-generated summaries in these datasets. The evaluation results, shown in <ref type="table" target="#tab_1">Table 1, TABLE 2,</ref>  <ref type="figure" target="#fig_0">FIGURE 3, and  FIGURE 4</ref>, assert that the proposed model achieves promising results. From the obtained results, we can make the following observations:</p><p>• As shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_0">FIGURE 3</ref>, the obtained results for ROUGE-1, ROUGE-2, and ROUGE-L indicate that our proposed method, HSSAS, performs the best for all ROUGE metrics used in this experiment on DUC 2002 dataset. This asserts that using hierarchical self-attention leads to better sentence and document representations and enhances the abstract features that can be used to yield state-of-the-art performance on the text summarization task. • In the case of CNN/ <ref type="figure" target="#fig_1">Daily Mail, TABLE 2 and  FIGURE 4</ref>, the results assert that the proposed models, HSSAS, outperforms all the baselines in the term of almost all ROUGE metrics used in this experiment.  • In news articles, it is usual for the important information to be put in the beginning of the article. This justifies the good ROUGE results of the LEAD-3 baseline in DUC 2002 which makes it hard to be beaten; however, our model has performed better. • In the context of abstractive based models, while ROUGE measures the n-gram overlap between the generated summary and a reference one, summaries with high ROUGE scores are not necessarily the more readable ones. One potential issue of generative summarization models is that optimizing for a specific discrete metric like ROUGE does not guarantee an increase in quality and readability of the generated summary <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This may justify the competitive ROUGE scores of the abstractive baselines used in this work. • Another issue related to the ROUGE metric is that the reliability of ROUGE increased by the number of the reference summaries per document. This inflexibility of ROUGE makes the Rouge scores on the datasets that has one reference summary per document much lower compared to the ones that have multiple reference summaries <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>. • Finally, our proposed model, HSSAS, obtained good results competing with the state-of-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATED WORK</head><p>Summarization systems fall into two main categories, extractive and abstractive. Extractive summarization models generate the summary by extracting some key subset of the content for the original document in a way that this subset contains the core information. By contrast, abstractive summarization models are more sophisticated and more complex since they leverage the language semantics to create representations. They use different words to describe the contents of the original documents rather than extracting the original ones <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>Since it is comparatively harder to build abstraction-based summarizers, most of the previously proposed models focus on the extraction-based models. Recently, neural network methods have been being used for extractive summarization. For example, a recursive autoencoder based model proposed by Kragebäck et al. <ref type="bibr" target="#b21">[22]</ref> to summarize documents on the Opinosis dataset <ref type="bibr" target="#b22">[23]</ref>. For multi-document extractive summarization task, Yin and Pei <ref type="bibr" target="#b24">[24]</ref> have used Convolutional Neural Networks (CNN) to project sentences to a continuous vector space and then they used the sentence 'diverseness' and 'prestige' to minimizing the cost function. A queryfocused model for multi-document summarization was proposed by Cao et al. <ref type="bibr" target="#b25">[25]</ref>. In which, they addressed the problem using query-attention-weighted CNNs. Another extractive summarization approach has been proposed by Nallapati et al. <ref type="bibr" target="#b26">[26]</ref>. They used an RNN based classifier that sequentially labels the sentences with binary labels 0/1 for their membership in the summary. The score for each sentence is computed by explicitly modeling abstract features such as content richness, salience with respect to the document.</p><p>For extractive query-oriented single-document summarization, Yousefi-Azar and Hamey <ref type="bibr" target="#b27">[27]</ref> used a deep autoencoder to compute a feature space from the term-frequency (tf) input. They developed a local word representation in which each vocabulary is designed to build the input representation for sentences in the document. Then, a random noise is added to the word representation vector, affecting both the input and output of the auto-encoder.</p><p>A recent work proposed by See et al. <ref type="bibr" target="#b14">[15]</ref> in which they augmented the standard sequence-to-sequence attentional model in two orthogonal ways. In first way, they used a hybrid pointer-generator network that can copy words from the source text via pointing. In the second one, they used the coverage to keep track of what has been summarized so far. Another study carried out by Cao et al. <ref type="bibr" target="#b25">[25]</ref> tried to learn the distributed representations for sentences by applying an attention mechanism, which used to learn query relevance ranking and sentence saliency ranking simultaneously. Another extractive summarization model was proposed by Cheng and Lapata <ref type="bibr" target="#b2">[3]</ref> in which they have treated the single document summarization as a sequence labeling task using a document encoder and attention-based extractor. They applied the attention directly to extract sentences and words for the summary.</p><p>The most similar work to ours is the one proposed by Nallapati et al. <ref type="bibr" target="#b1">[2]</ref>. They used a recurrent neural network (RNN) based model for extractive summarization applied to the CNN/Daily Mail corpus. In which, they treated extractive summarization as a sequence classification problem. They used neural networks for the sentential extractive summarization of single documents. In their model, each sentence is visited sequentially as it appears in the original document and a binary decision is taken to determine whether the sentence should be included in the summary or not. It is worth mentioning that they did not use any attention mechanism. Different from their approach, our model uses the structured self-attentive mechanism that has the capability to guide the sentence and document representations.</p><p>The recent advancement of the generative neural models for text makes the abstractive summarization techniques increasingly popular. In 2015, Rush et al. <ref type="bibr" target="#b28">[28]</ref> published an encoder-decoder model, in which the encoder is a convolutional network and the decoder is a feedforward neural network language model. They enhanced the convolutional encoder by integrate it with attention model. Then they used the trained neural network as a feature to a log-linear model. As the convolutional encoder need a fix number of features, they used a bag of n-grams model. That means they ignore the overall sequence order while generating the hidden representation. They only used the first sentence of each news article to generate its title. Another recent abstractive model was proposed by Paulus et al. <ref type="bibr" target="#b12">[13]</ref>. In which, they combined the standard supervised word prediction with reinforcement learning (RL).</p><p>Despite the popularity of abstractive techniques, extractive techniques are still attractive as they are less expensive, less complex and most of the time, they can generate grammatically and semantically correct summaries. Moreover, the performance of RNN-based encoder-decoder models for abstractive summarization is quite good for short input and output sequences, but for longer documents and summaries, these models often struggle from serious problems such as repetition, unreadability and incoherence.</p><p>As we mentioned earlier in the paragraph before the last one in section 1, our work differs from the previous ones by it is capability to capture the hierarchical structure of the documents. Moreover, it uses the hierarchical structured self-attention to deliver a better embedding representation for sentences and documents. The attention mechanism that we use in this work puts more focus on the semantics of the whole sentence that each word contributes to rather than just focusing on the relations between words like the previous attention-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>The proposed model is another way of utilizing the attention mechanism to create a sentence and document embeddings. The experimental results of the proposed model assert that those embeddings deliver a better representation which in turn enhances the document summarization task and outperforms the state-of-the-art models on the same datasets. This work is different from the previous work in the sense of three points. First, it uses the hierarchical attention that mirror the document structure. Second, it uses the structured self-attention, which creates a very good embedding. Third, the abstract features are weighted and automatically learned during the learning process taking in consideration the previously classified sentences. We believe that combining the reinforcement learning with sequenceto-sequence training objective is an interesting direction for further research. Another research effort should be directed toward proposing another evaluation metric beside ROUGE metric to optimize on summarization model especially for long sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 3 .</head><label>3</label><figDesc>The performance comparison of the proposed model with respect to the baselines on DUC 2002.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 4 .</head><label>4</label><figDesc>The performance comparison of the proposed model with respect to the baselines on CNN/Daily Mail dataset using full-length F1 variant of ROUGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc>The performance comparison of the proposed models with respect to the baselines on DUC 2002.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 .</head><label>2</label><figDesc>The performance comparison of the proposed models with respect to the baselines on CNN/Daily Mail using full-length F1 variant of ROUGE.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.berouge.com/Pages/default.aspx ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -r 1000 -f A -p 0.5 -t 0 2 https://pypi.python.org/pypi/pyrouge/0.1.3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We are grateful to the support of the National Natural </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating incremental length summary based on hierarchical topic coverage maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636" />
	</analytic>
	<monogr>
		<title level="m">the 31st AAAI Conf. Artif. Intell. (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1603.07252" />
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extractive summarization using multi-task learning with document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to improve text summarization and classification by mutual cooperation on an integrated framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="222" to="233" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aligning Gaussiantopic with embedding network for summarization ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Web (APWeb) Web-Age Inf. Manage. (WAIM) Joint Conf. Web Big Data</title>
		<meeting>Asia-Pacific Web (APWeb) Web-Age Inf. Manage. (WAIM) Joint Conf. Web Big Data</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="610" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.03130" />
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Empirical Methods Natural Lang. Process. (EMNLP)</title>
		<meeting>Empirical Methods Natural Lang. ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1506.03340" />
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.04304" />
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th SIGNLL Conf. Comput. Natural Lang. Learn. (CoNLL)</title>
		<meeting>20th SIGNLL Conf. Comput. Natural Lang. Learn. (CoNLL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.04368" />
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topical coherence for graphbased extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Ramsl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1949" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards a unified approach to simultaneous single-document and multi-document summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Comput. Linguistics</title>
		<meeting>23rd Int. Conf. Comput. Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Workshop Text Summarization Branches</title>
		<meeting>ACL Workshop Text Summarization Branches<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1230" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recent advances in document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extractive Summarization using Continuous Vector Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Workshop Continuous Vector Space Models Compositionality (CVSC)</title>
		<meeting>2nd Workshop Continuous Vector Space Models Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd</title>
		<meeting>23rd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Int</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
		<title level="m">Comput. Linguistics</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing sentence modeling and selection for document summarization,&apos;&apos; presented at the 24th Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AttSum: Joint learning of focusing and summarization with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. Comput. Linguistics (COLING)</title>
		<meeting>26th Int. Conf. Comput. Linguistics (COLING)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Classify or select: Neural architectures for extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.04244" />
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text summarization using unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousefi-Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hamey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1509.00685" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
