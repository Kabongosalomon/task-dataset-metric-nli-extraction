<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Efficient Representations for Keyword Spotting with Triplet Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vygon</surname></persName>
							<email>rvygon@ntr.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Mikhaylovskiy</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Higher IT School</orgName>
								<orgName type="institution">Tomsk State University &amp; NTR Labs Tomsk</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Higher IT School</orgName>
								<orgName type="institution">Tomsk State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">NTR Labs Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Efficient Representations for Keyword Spotting with Triplet Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>keyword spotting</term>
					<term>spoken term detection</term>
					<term>triplet loss</term>
					<term>kNN</term>
					<term>representation learning</term>
					<term>audio classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past few years, triplet loss-based metric embeddings have become a de-facto standard for several important computer vision problems, most notably, person reidentification. On the other hand, in the area of speech recognition the metric embeddings generated by the triplet loss are rarely used even for classification problems. We fill this gap showing that a combination of two representation learning techniques: a triplet loss-based embedding and a variant of kNN for classification instead of cross-entropy loss significantly (by 26% to 38%) improves the classification accuracy for convolutional networks on a LibriSpeech-derived LibriWords datasets. To do so, we propose a novel phonetic similarity based triplet mining approach. We also improve the current best published SOTA (for small-footprint models) for Google Speech Commands dataset V2 10+2-class classification by about 16%, achieving 98.37% accuracy, and the current best published SOTA for 35-class classification on Google Speech Commands dataset V2 by 47%, achieving 97.0% accuracy. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The goal of keyword spotting is to detect a relatively small set of predefined keywords in a stream of user utterances, usually in the context of small-footprint device <ref type="bibr" target="#b0">[1]</ref>. Keyword spotting (KWS for short) is a critical component for enabling speech-based user interactions for such devices <ref type="bibr" target="#b1">[2]</ref>. It is also important from an engineering perspective for a wide range of applications <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this article we show how the use of the triplet loss-based embeddings allows us to improve the classification accuracy of the existing small-footprint neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Previous work on KWS</head><p>The first work on KWS was most likely published in 1967 <ref type="bibr" target="#b3">[4]</ref>. Over years, a number of machine learning architectures for <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/roman-vygon/triplet_loss_kws small-footprint KWS have been proposed (see, for example <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>. With the renaissance of neural networks, they become the architecture class of choice for KWS systems (see, for example, <ref type="bibr" target="#b0">[1]</ref>[2][10] <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>). Probably, the onlybut notable -exception from this trend is the very recent work of Lei et al. <ref type="bibr" target="#b14">[15]</ref> that uses Tsetlin machines for keyword spotting for their extremely low power consumption.</p><p>Publication of the Google Speech Command dataset <ref type="bibr" target="#b16">[16]</ref> have provided a common ground for KWS system evaluation and allowed for accelerating research. Further, we denote V1 and V2 versions 1 and 2 of the dataset, respectively. When publishing the dataset, Warden <ref type="bibr" target="#b16">[16]</ref> have also provided a baseline model based on the convolutional architecture of Sainath and Parada <ref type="bibr" target="#b10">[11]</ref>, achieving the accuracy of 85.4% and 88.2% on V1 and V2, respectively. The respective Kaggle competition winner has achieved 91% accuracy on V1. neural network architecture, achieving a new SOTA of 98% on V2. Wei et al. <ref type="bibr" target="#b21">[21]</ref> proposed a new architecture, EdgeCRNN, which is based on depthwise separable convolution and residual structure, apparently drawing inspiration from MatchboxNet <ref type="bibr" target="#b18">[18]</ref> and Attention RNN <ref type="bibr" target="#b2">[3]</ref>, to achieve a slight improvement in accuracy and a SOTA of 98.05%. Tang et al. <ref type="bibr" target="#b22">[22]</ref> have released Howl -a productionalized, open-source wake word detection toolkit, explored a number of models and achieved nearly-SOTA accuracy with a residual convolutional network architecture.</p><p>Berg et al. have published a work <ref type="bibr" target="#b23">[23]</ref> on gigantic, by KWS measures, models. Their smallest model is about 5 times larger than ours, mid-sized one, with a comparable performance, 20 times larger, and the model achieving the state-of-the-art of 98.6% is about 50 times larger than ours. Given that we operate in a small-footprint setting, these models are no competitors to the other approaches listed above and ours as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Previous work on the use of triplet loss for the metric embedding learning</head><p>The goal of metric embedding learning is to learn a function : → , which maps semantically similar points from the data manifold in onto metrically close points in , and semantically different points in onto metrically distant points in <ref type="bibr" target="#b24">[24]</ref>.</p><p>The triplet loss for this problem was most likely first introduced in <ref type="bibr" target="#b25">[25]</ref> in the framework of image ranking:</p><formula xml:id="formula_0">( , , ) = 0, + ( ), ( ) − ( ), ( )<label>(1)</label></formula><p>where , , are the anchor image, positive image, and negative image, respectively, is a gap parameter that regularizes the gap between the distance of the two image pairs: ( , ) and ( , ), and is a distance function that can be, for example, Euclidean distance in the image embedding space:</p><formula xml:id="formula_1">( ), ( ) = ‖ ( ) − ( )‖ (2)</formula><p>A similar loss function was earlier proposed by Chechik et al. in <ref type="bibr" target="#b26">[26]</ref>, but the real traction came to the triplet loss in the area of face re-identification after the works of Schroff, Kalenichenko, and Philbin on FaceNet <ref type="bibr" target="#b27">[27]</ref> and Hermans, Beyer, and Leibe <ref type="bibr" target="#b24">[24]</ref>.</p><p>In the speech domain, the use of triplet loss is more limited, but there still are several important works we need to mention. In particular, Huang J. et al <ref type="bibr" target="#b28">[28]</ref>, Ren et al. <ref type="bibr" target="#b29">[29]</ref>, Kumar et al. <ref type="bibr" target="#b30">[30]</ref>, and Harvill et al. <ref type="bibr" target="#b31">[31]</ref> use triplet loss with varied neural network architectures for the task of the speech emotion recognition. Bredin <ref type="bibr" target="#b32">[32]</ref> and Song et al. <ref type="bibr" target="#b33">[33]</ref> use triplet-loss based learning approaches for the speaker diarization, and Zhang and Koshida <ref type="bibr" target="#b34">[34]</ref> and Li et al. <ref type="bibr" target="#b35">[35]</ref> -for the related task of speaker verification.</p><p>Turpault et al. <ref type="bibr" target="#b36">[36]</ref> propose a strategy for augmenting data with transformed samples, in line with more recent works in varied machine learning areas.</p><p>The most similar works to ours are probably <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, and <ref type="bibr" target="#b41">[41]</ref>, but there are many important differences:  Sacchi et al. <ref type="bibr" target="#b37">[37]</ref> operate in the open-vocabulary setting, which required the authors to design a system with a common embedding for text and speech, while we concentrate on improving the quality of existing lowfootprint architectures for closed-vocabulary keyword spotting  Shor et al. <ref type="bibr" target="#b38">[38]</ref> concentrate on building an unified embedding that works well for non-semantic tasks, while we concentrate on the semantic task of keyword spotting  Yuan et al. <ref type="bibr" target="#b39">[39]</ref> operate in a two-stage detection/ classification framework and use a BLSTM network with a mix of triplet, reverse triplet and hinge loss  Huh et al. <ref type="bibr" target="#b40">[40]</ref> start from the same res15 model as we do, but primarily focus on detection metrics and use SVM for classification, so our classification metrics are significantly better  Huang et. al <ref type="bibr" target="#b41">[41]</ref> concentrate on Query-by-Example KWS application and adopt the softtriple loss -a combination of triplet loss and softmax loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our contributions</head><p>Our contributions in this work are the following:</p><p> We show that combining two representation learning methods: triplet-loss based metric embeddings and a kNN classifier allows us to significantly improve the accuracy of CNN-based models that use cross-entropy to classify audio information  We propose a novel batch sampling approach based on phonetic similarity that allows to improve F1 metric when classifying highly imbalanced datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MODEL ARCHITECTURES</head><p>Most of the current state-of-the-art keyword spotting architectures are present in the work of Rybakov et al. <ref type="bibr" target="#b20">[20]</ref>, with the best model to date being the Bidirectional GRU-based Multihead attention RNN. It takes a mel-scale spectrogram and convolves it with a set of 2D convolutions. Then two bidirectional GRU layers are used to capture two-way long term dependencies in the audio data. The feature in the center of the bidirectional LSTM's output sequence is projected using a dense layer and is used as a query vector for the multi-head attention (4 heads) mechanism. Finally, the weighted (by attention score) average of the bidirectional GRU output is processed by a set of fully connected layers for classification.</p><p>We have mostly experimented with ResNet-based models res8 <ref type="bibr" target="#b22">[22]</ref> [1] and res15 <ref type="bibr" target="#b42">[42]</ref> [1]. The initial experiments have shown that RNN-based architectures show significantly worse results when trained for the triplet loss, so they were discarded in our later work We used the encoder part of each of the models above to generate triplet-loss based embeddings, that are later classified using the K-Nearest Neighbor (kNN) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input Preprocessing</head><p>Sixty-four-dimensional Mel-Spectrograms are constructed and stacked using a 25-millisecond window size and a 10-millisecond frame shiſt. Our implementation stacks all such shifted 30-millisecond windows within the one-second sample of Google Speech Commands. Libriwords samples are constrained to have a duration of 0.1-3 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Resnet architecture.</head><p>Our resnet implementation is taken directly from <ref type="bibr" target="#b42">[42]</ref> with very minor code changes and is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>When working with triplet loss, the softmax layer is removed.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>A. Datasets and tasks 1) SpeechCommands Google Speech Commands dataset Version 1 has 65K utterances from various speakers, each utterance 1 second long. Each of these utterances belongs to one of 30 classes corresponding to common words like "Go", "Stop", "Left", "Down", etc. Version 2 has 105K utterances, each 1 second long, belonging to one of 35 classes. The sampling rate of both datasets is 16kHz.</p><p>In our experiments we have considered the following tasks based on these datasets <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b16">[16]</ref>:</p><p> Recognition of all 35 words using Google Speech Dataset V2</p><p> Recognition of 10 words ("Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", and "Go") and additional labels for "Unknown" and "Silence".</p><p>For these tasks and each architecture studied we have measured top-1 classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) LibriWords Datasets</head><p>To further explore the possibilities of triplet loss models we needed a dataset that consists of a large number of different words to classify.</p><p>Thus, we have used LibriSpeech [43] -a collection of 1,000 hours of read English speech. The dataset was split on the word level by Lugosch et al. <ref type="bibr" target="#b44">[44]</ref>. Since LibriSpeech is aligned on sentence level only, the Montreal Forced Aligner <ref type="bibr" target="#b45">[45]</ref> was used to obtain intervals for individual words. The alignments are available online <ref type="bibr" target="#b44">[44]</ref>. Further we call this derived dataset LibriWords.</p><p>We have created four different versions of the dataset (LibriWords10, LibriWords100, LibriWords1000, LibriWords10000) that correspond to the first 10, 100 etc. words by popularity in the LibriSpeech 1000h corpus. For example, the LibriWords10 words are: "the", "and", "of", "to", "a", "in", "he", "I", "that", "was".</p><p>Durations of the words range from 0.03 seconds to 2.8 seconds, with mean duration of 0.28 seconds. The details on the datasets metrics are available in the Appendix 1. We have split the dataset into train\val\test in in 8:1:1 proportion, and tried to make sure this proportion holds for each word in the dataset. We release NeMo-like manifests for ease of use and reproduction. Since the motivation behind the dataset is to model real-life speech recognition scenarios, there was no further quality assurance on the data.</p><p>B. Approach to training models 1) Batch sampling When working with Speech Commands and LibriWords10 datasets, to ensure a meaningful representation of the anchorpositive distances, following <ref type="bibr" target="#b27">[27]</ref>, we sample an equal number of objects from all the classes available. For unbalanced datasets with a large number of words, we also needed an efficient classsampling method, otherwise the network will often train on irrelevant batches where embeddings of the words are already far from each other. To achieve better class selection we have used three sampling approaches:</p><p> Uniform: sample batch_size classes randomly from a uniform distribution.</p><p> Proportional: sample batch_size classes randomly from a distribution proportional to the word distribution in the dataset. Motivation behind this approach is twofold. First, the popular words are short (the, a, I)) so they are not easy to distinguish from the rest. Second, if you equally train on them, there will be the same amount of errors, and that's a lot in terms of the absolute value. (If we classify 2% of a popular word incorrectly, this would significantly spoil the metric for the entire dataset).</p><p> Phonetic: Calculate a matrix of phonetic similarity for all the words in the dataset, sample batch_size/2 classes, then, for each sampled class add a random phonetically similar word to the batch. Similarity score is calculated using SoundEx, Caverphone, Metaphone and NYSIIS algorithms <ref type="bibr" target="#b46">[47]</ref>.</p><p>These sound similarity algorithms were all developed with tasks different from ours in mind. To achieve a baseline applicable to LibriWords we used a weighted average of distances calculated using all 4 algorithms. The weights go as follows:</p><formula xml:id="formula_2">= * 0.2 + * 0.2 + * 0.5 + * 0.1</formula><p>The metaphone algorithm has a bigger weight due to its original task being the nearest to ours. The optimal use of these algorithms is a matter of future research, for example, we had to adjust manually the distances of a handful of pairs of words: e.g. the pair "know-no" had a large distance while being similar. The problem was found while analyzing the confusion matrix.</p><p>We have evaluated these three triplet mining approaches alone and in combinations, mixing them with equal probabilities. Thus, for example, Uniform+Phounetic in the TABLE II. below means that we have sampled 50% of triplets in the batch using Uniform strategy, and 50% using Phonetic strategy, and Uniform+Proportional +Phonetic means that 1/3 of the triplets were sampled using Uniform strategy, 1/3 -using Phonetic strategy, and 1/3 -using Proportional strategy.</p><p>The results in the TABLE II. show that the proportional sampling method improves the accuracy by increasing the score of more popular words while the phonetic sampling method improves the F1 metric due to better classification of difficult pairs like "at"-"ate", "an"-"anne". Uniform sampling usage is essential as one of the sampling strategies, as it provides the proper class coverage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Triplet selection</head><p>An important part of TL models is the selection of triplets used to calculate the loss, since taking all possible triplets from a batch is computationally expensive. We have used a randomized approach to the online batch triplet mining based on <ref type="bibr" target="#b24">[24]</ref>, where the negative sample to a hard pair of the anchor and a positive sample is selected randomly from the set of negative samples resulting in non-zero loss. Our initial experiments have shown that this modification of the online batch triplet mining performs better than hard or semi-hard batch loss options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Optimization and training process</head><p>Baseline models were trained until they reached a plateau on a validation set. We monitored the validation accuracy of triplet loss models each 1k batches and stopped the training process if the accuracy didn't increase for more than .1% for 3 consecutive times. The number of epochs is listed in the TABLE III. below.</p><p>Three augmentation techniques were used:</p><p>1. Shifting samples in range (-100ms; +100ms). 2. SpecAugment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adding background noise from audio files in Google</head><p>Speech Commands Dataset. The decrease in epochs for larger datasets is due to classimbalance -triplet models sample classes directly, so instead of seeing all objects in the dataset it sees the same number of objects, but distributed more evenly between classes. The baseline, cross-validation based models converge to predict the most popular words well, while ignoring the rest. One can see this from the low F1 metric on LibriWords10000 dataset. The batch size was 35*10 for TL-res8, 35*4 for TL-res15 and 128 for the baseline models.</p><p>Training was done using the Novograd <ref type="bibr" target="#b47">[48]</ref> algorithm with initial learning rate of 0.001 and cosine annealing decay to 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Influence of kNN</head><p>We have tested kNN for several values of k, and have found that for LibriWords the best performing value varies depending on the dataset size, while for Speech Commands the best performing value was k=5 (see 0).  For each dataset/task there is an optimal number of segments that reduces accuracy by 1.6% -13.6%, and reduces the memory consumption by a factor of 7 to 13 (see <ref type="table" target="#tab_0">Tables VIII-XIII,  Appendix 2)</ref> IV. RESULTS AND DISCUSSION The results below were obtained by training a model for 3 different runs in each scenario and averaging the results to avoid the "lucky seed" effect. We can see that triplet loss + kNN based models provide better accuracy than baseline ones, achieve or match state of the art results on Speech Commands dataset, while being more lightweight and faster in convergence than the mh-att-rnn <ref type="bibr" target="#b20">[20]</ref> model.</p><p>In particular, triplet loss + kNN based models improve the accuracy on the datasets studied by 14% to 38% and F1 measure by 8% to 57% compared to extremely strong crossentropy based baselines (see <ref type="table">TABLE V</ref>. ) The bigger the number of classes in the dataset, the bigger the difference between crossentropy and triplet loss based classifiers.</p><p>On Google Speech Commands dataset V2/35 task, our res15 network trained with triplet loss and kNN classifier, achieves state of the art, improving the best previously published result <ref type="bibr" target="#b2">[3]</ref> by 47%. On Google Speech Commands dataset V2/12 task it improves the state of the art <ref type="bibr" target="#b20">[20]</ref> by about 16% (see   Nikolay Shmyrev for pointing out to the works <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>res* architecture (from [1])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .Fig. 3 .</head><label>123</label><figDesc>Distribution of words in LibriWords10 Distribution of words in LibriWords100 Distribution of words in LibriWords1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>ENCODER MODEL SIZES FOR THE KEY MODELS STUDIED.</figDesc><table><row><cell></cell><cell>Embedding dimension</cell><cell>Model encoder size,</cell></row><row><cell></cell><cell></cell><cell>[K]</cell></row><row><cell>Mh-Att-RNN</cell><cell>256</cell><cell>743</cell></row><row><cell>res8</cell><cell>128</cell><cell>885</cell></row><row><cell>res15</cell><cell>45</cell><cell>109</cell></row><row><cell>Att-RNN</cell><cell>128</cell><cell>202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>above compares the model sizes for the main models studied.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc>THE EFFECTS OF THE DIFFERENT SAMPLING STRATEGIES FOR TRIPLET LOSS OF RES15 MODEL ON LIBRIWORDS10000</figDesc><table><row><cell>Method(s)</cell><cell>Accuracy</cell><cell>F1</cell></row><row><cell>Uniform</cell><cell>79.4</cell><cell>0.72</cell></row><row><cell>Proportional</cell><cell>77.1</cell><cell>0.61</cell></row><row><cell>Phonetic</cell><cell>76.9</cell><cell>0.73</cell></row><row><cell>Uniform+Phonetic</cell><cell>78.9</cell><cell>0.76</cell></row><row><cell>Uniform+Proportional</cell><cell>81.2</cell><cell>0.74</cell></row><row><cell>Proportional+Phonetic</cell><cell>80.0</cell><cell>0.72</cell></row><row><cell>Uniform+Proportional +Phonetic</cell><cell>80.8</cell><cell>0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III .</head><label>III</label><figDesc>THE NUMBER OF EPOCHS MODELS WERE TRAINED FOR</figDesc><table><row><cell></cell><cell>TL, epochs</cell><cell>Baseline, epochs</cell></row><row><cell>Speech Commands</cell><cell>30</cell><cell>30</cell></row><row><cell>Libri10</cell><cell>10</cell><cell>30</cell></row><row><cell>Libri100</cell><cell>5</cell><cell>10</cell></row><row><cell>Libri1000</cell><cell>5</cell><cell>7</cell></row><row><cell>Libri10000</cell><cell>3</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV .</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">CLASSIFICATION ACCURACY FOR RES15 MODEL TRIPLET LOSS</cell></row><row><cell cols="5">EMBEDDINGS WITH KNN CLASSIFICATION FOR VARIOUS K.</cell></row><row><cell>k</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>30</cell></row><row><cell>Speech</cell><cell>98.18</cell><cell>98.37</cell><cell>98.27</cell><cell>98.29</cell></row><row><cell>Commands</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V2 / 12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LW10</cell><cell>89.91</cell><cell>91.48</cell><cell>91.74</cell><cell>91.72</cell></row><row><cell>LW100</cell><cell>83.93</cell><cell>86.53</cell><cell>86.9</cell><cell>86.98</cell></row><row><cell>LW1000</cell><cell>80.43</cell><cell>83.82</cell><cell>84.29</cell><cell>84.37</cell></row><row><cell>LW10000</cell><cell>77.57</cell><cell>80.82</cell><cell>81.17</cell><cell>80.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell>. )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TABLE V.</cell><cell cols="6">COMPARISON OF ACCURACY AND F1 MEASURE OF TRIPLET</cell></row><row><cell cols="6">LOSS AND CROSSENTROPY LOSS BASED RES15 MODELS</cell><cell></cell></row><row><cell>Task</cell><cell cols="2">Triplet Loss</cell><cell cols="2">Crossentropy</cell><cell cols="2">Relative improvement</cell></row><row><cell></cell><cell>Accur acy, %</cell><cell>F1</cell><cell>Accur acy, %</cell><cell>F1</cell><cell>Accura cy, %</cell><cell>F1,%</cell></row><row><cell>Speech</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Commands V2</cell><cell>97.0</cell><cell>0.965</cell><cell>95.96</cell><cell>0.955</cell><cell>25.74</cell><cell>22.22</cell></row><row><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speech</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Commands V2</cell><cell>98.37</cell><cell>0.98</cell><cell>97.8</cell><cell>0.963</cell><cell>25.9</cell><cell>45.94</cell></row><row><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LibriWords10</cell><cell>91.7</cell><cell>0.90</cell><cell>88.8</cell><cell>0.88</cell><cell>26.25</cell><cell>16.67</cell></row><row><cell>LibriWords100</cell><cell>86.9</cell><cell>0.87</cell><cell>82.3</cell><cell>0.81</cell><cell>25.99</cell><cell>31.58</cell></row><row><cell>LibriWords 1000</cell><cell>84.3</cell><cell>0.86</cell><cell>78.2</cell><cell>0.78</cell><cell>27.94</cell><cell>36.36</cell></row><row><cell>LibriWords 10000</cell><cell>81.2</cell><cell>0.75</cell><cell>69.3</cell><cell>0.41</cell><cell>38.66</cell><cell>57.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="4">MODEL ACCURACY COMPARISON ON GOOGLE SPEECH</cell></row><row><cell></cell><cell cols="2">COMMANDS DATASET TASKS</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Loss</cell><cell>Model Size, KB</cell><cell>V2 35 accuracy</cell><cell>V2 12 accuracy</cell></row><row><cell></cell><cell>Triplet</cell><cell>901</cell><cell>95.33</cell><cell>97.48</cell></row><row><cell>res8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Crossentropy</cell><cell>885</cell><cell>95.25</cell><cell>97.39</cell></row><row><cell></cell><cell>Triplet</cell><cell>252</cell><cell>97.0</cell><cell>98.37</cell></row><row><cell>res15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Crossentropy</cell><cell>237</cell><cell>95.96</cell><cell>97.8</cell></row><row><cell>EdgeCRNN [21]</cell><cell>Crossentropy</cell><cell></cell><cell></cell><cell>98.05</cell></row><row><cell>Mh-Att-RNN [20]</cell><cell>Crossentropy</cell><cell>743</cell><cell></cell><cell>98.0</cell></row><row><cell>Attention RNN [3]</cell><cell>Crossentropy</cell><cell>202</cell><cell>93.9</cell><cell></cell></row><row><cell></cell><cell cols="3">V. ACKNOWLEDGMENTS</cell><cell></cell></row><row><cell cols="3">The authors are grateful to</cell><cell></cell><cell></cell></row><row><cell cols="5"> colleagues at NTR Labs Machine Learning Research</cell></row><row><cell cols="4">group for the discussions and support;</cell><cell></cell></row><row><cell cols="5"> Prof. Sergey Orlov and Prof. Oleg Zmeev for the</cell></row><row><cell cols="3">computing facilities provided;</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII .</head><label>VII</label><figDesc>STATS OF LIBRIWORDS DATASETS INFLUENCE OF KNN QUANTIZATION ON SPEED, MEMORY CONSUMPTION AND ACCURACY OF CLASSIFIERS TABLE VIII. MEMORY CONSUMPTION, TEST SET PREDICT TIME AND ACCURACY FOR DIFFERENT KNN QUANTIZATION SEGMENT NUMBER FOR LIBRIWORDS10 DATASET. TABLE XI. MEMORY CONSUMPTION, TEST SET PREDICT TIME AND ACCURACY FOR DIFFERENT KNN QUANTIZATION SEGMENT NUMBER FOR LIBRIWORDS1000 DATASET.</figDesc><table><row><cell cols="2">APPENDIX 2. Memory, MB</cell><cell>Time, s</cell><cell cols="2">Accuracy, %</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Basic</cell><cell>246</cell><cell cols="2">17.15</cell><cell>91.74</cell><cell></cell><cell></cell><cell cols="3">Memory, MB</cell><cell>Time, s</cell><cell>Accuracy, %</cell></row><row><cell>4</cell><cell>5.95</cell><cell cols="2">0.53</cell><cell>91.34</cell><cell></cell><cell>Basic</cell><cell></cell><cell cols="2">1260</cell><cell>440.66</cell><cell>79.4</cell></row><row><cell>8</cell><cell>7.87</cell><cell cols="2">0.62</cell><cell>91.48</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>29.8</cell><cell>4.27</cell><cell>68.56</cell></row><row><cell>16</cell><cell>11.7</cell><cell cols="2">0.73</cell><cell>91.56</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>39.6</cell><cell>5.69</cell><cell>73.33</cell></row><row><cell>32</cell><cell>19.4</cell><cell cols="2">1.1</cell><cell>91.61</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>59.4</cell><cell>7.23</cell><cell>75.51</cell></row><row><cell>64</cell><cell>34.8</cell><cell cols="2">2.33</cell><cell>91.58</cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell>98.8</cell><cell>12.03</cell><cell>76.35</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell>178</cell><cell>27.15</cell><cell>76.59</cell></row><row><cell>TABLE IX.</cell><cell cols="5">MEMORY CONSUMPTION, TEST SET PREDICT TIME AND</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ACCURACY FOR DIFFERENT KNN QUANTIZATION SEGMENT NUMBER FOR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">LIBRIWORDS100 DATASET.</cell><cell></cell><cell></cell><cell>TABLE XII.</cell><cell cols="5">MEMORY CONSUMPTION, TEST SET PREDICT TIME AND</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ACCURACY FOR DIFFERENT KNN QUANTIZATION SEGMENT NUMBER FOR</cell></row><row><cell>Basic</cell><cell>638</cell><cell>118.6</cell><cell></cell><cell>86.9</cell><cell></cell><cell></cell><cell cols="4">SPEECH COMMANDS V2 35 TASK.</cell></row><row><cell>4</cell><cell>15.1</cell><cell>1.72</cell><cell></cell><cell>91.34</cell><cell></cell><cell></cell><cell cols="3">Memory, MB</cell><cell>Time, s</cell><cell>Accuracy, %</cell></row><row><cell>8</cell><cell>20.1</cell><cell>2.08</cell><cell></cell><cell>85.23</cell><cell></cell><cell>Basic</cell><cell></cell><cell></cell><cell>15.7</cell><cell>0.54</cell><cell>96.19</cell></row><row><cell>16</cell><cell>30.1</cell><cell>2.7</cell><cell></cell><cell>86.11</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>1.2</cell><cell>0.1</cell><cell>96.07</cell></row><row><cell>32</cell><cell>50</cell><cell>4.18</cell><cell></cell><cell>86.24</cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell>1.55</cell><cell>0.11</cell><cell>95.98</cell></row><row><cell>64</cell><cell>90</cell><cell>8.98</cell><cell></cell><cell>86.22</cell><cell></cell><cell cols="6">TABLE XIII. PERFORMANCE OF THE BEST KNN QUANTIZATIONS FOR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DIFFERENT DATASETS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Me</cell><cell>Accu</cell><cell>Memo</cell><cell>Accur</cell><cell>Memor</cell><cell>Accur</cell></row><row><cell>TABLE X.</cell><cell cols="5">MEMORY CONSUMPTION, TEST SET PREDICT TIME AND</cell><cell></cell><cell>mor</cell><cell>racy,</cell><cell>ry,</cell><cell>acy,</cell><cell>y</cell><cell>acy</cell></row><row><cell cols="6">ACCURACY FOR DIFFERENT KNN QUANTIZATION SEGMENT NUMBER FOR LIBRIWORDS1000 DATASET.</cell><cell></cell><cell>y, basi</cell><cell>basic</cell><cell>quanti zation</cell><cell>quanti zation</cell><cell>econo my,</cell><cell>degrad ation,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>c</cell><cell></cell><cell></cell><cell></cell><cell>times</cell><cell>%</cell></row><row><cell></cell><cell cols="2">Memory, MB</cell><cell>Time, s</cell><cell cols="2">Accuracy, %</cell><cell>Commands</cell><cell>15.7</cell><cell>96.19</cell><cell>1.55</cell><cell>95.98</cell><cell>10.13</cell><cell>5.51</cell></row><row><cell>Basic</cell><cell></cell><cell>977</cell><cell>272.37</cell><cell></cell><cell>84.2</cell><cell>LW10</cell><cell>246</cell><cell>91.74</cell><cell>19.4</cell><cell>91.61</cell><cell>12.68</cell><cell>1.57</cell></row><row><cell>4</cell><cell></cell><cell>23.1</cell><cell>2.85</cell><cell></cell><cell>78.75</cell><cell>LW100</cell><cell>638</cell><cell>86.9</cell><cell>50</cell><cell>86.24</cell><cell>12.76</cell><cell>5.04</cell></row><row><cell>8</cell><cell></cell><cell>30.7</cell><cell>3.87</cell><cell></cell><cell>81</cell><cell>LW1000</cell><cell>977</cell><cell>84.2</cell><cell>138</cell><cell>82.65</cell><cell>7.08</cell><cell>9.81</cell></row><row><cell>16</cell><cell></cell><cell>46</cell><cell>4.92</cell><cell></cell><cell>82.18</cell><cell>LW10000</cell><cell cols="2">1260 79.4</cell><cell>178</cell><cell>76.59</cell><cell>7.08</cell><cell>13.64</cell></row><row><cell>32</cell><cell cols="2">Total words 76.5</cell><cell>8.13</cell><cell cols="2">Most popular word 82.5</cell><cell cols="3">Least popular word</cell><cell></cell><cell cols="2">Class imbalance</cell></row><row><cell>libri10 64</cell><cell>875 043</cell><cell>138</cell><cell cols="2">the 224 240 17.54</cell><cell>82.65</cell><cell>was 42 757</cell><cell></cell><cell></cell><cell>5.2</cell><cell></cell></row><row><cell>libri100</cell><cell cols="2">1 890 091</cell><cell cols="2">the 224 240</cell><cell></cell><cell>never 4 121</cell><cell></cell><cell></cell><cell cols="2">54.4</cell></row><row><cell>libri1000</cell><cell cols="2">2 723 023</cell><cell cols="2">the 224 240</cell><cell></cell><cell>path 329</cell><cell></cell><cell></cell><cell cols="2">681.6</cell></row><row><cell>libri10000</cell><cell cols="2">3 394 530</cell><cell cols="2">the 224 240</cell><cell></cell><cell>parade 21</cell><cell></cell><cell></cell><cell cols="2">10678</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">As the model size is of a great concern for the keyword spotting application, and for the larger datasets kNN part of the model can take a lot of memory, we have also studied the effect of kNN quantization available from<ref type="bibr" target="#b48">[49]</ref> on the size, speed and accuracy of the resulting model, varying the number of segments for the Product Quantizer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>ArXiv 1710.10361</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>arXiv 1711.07128</idno>
		<title level="m">Hello Edge: Keyword Spotting on Microcontrollers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno>arXiv 1808.08929</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experimental, limited vocabulary, speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kellett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Focht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="130" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous hidden Markov modeling for speaker-independent word spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rohlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="627" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Phoneme based acoustics keyword spotting in informal continuous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Szoke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<idno type="DOI">10.1007/11551874_39</idno>
		<editor>Matousek V., Mautner P., Pavelka T. Text, Speech and Dialogue</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="302" to="309" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved mandarin keyword spotting using confusion garbage model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page" from="3700" to="3703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech Keyword Spotting with Rule Based Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greibus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Telksnys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information and Software Technologies. ICIST 2013</title>
		<editor>Skersys T., Butleris R., Butkiene R.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An integrated system for voice command recognition and emergency detection based on audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Principi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bonfigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piazza</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2015.02.036</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5668" to="5683" />
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for smallfootprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05390</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="474" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Streaming Small-Footprint Keyword Spotting using Sequence-to-Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09617</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Low-Power Audio Keyword Spotting using Tsetlin Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="http://arxiv.org/abs/2101.11336" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech commands: A public dataset for single-word speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Single-word speech recognition with Convolutional Neural Networks on raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansson</surname></persName>
		</author>
		<imprint>
			<pubPlace>Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information technology, ARCADA University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Degree Thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ginsburg</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<title level="m">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compressing 1D Time-Channel Separable Convolutions using Sparse Random Ternary Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Keirsbilck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2103.17142" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenzo</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06720</idno>
		<title level="m">Streaming keyword spotting on mobile devices</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EdgeCRNN: an edgecomputing oriented model of acoustic feature enhancement for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Humaniz. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Howl: A Deployed, Open-Source Wake Word Detection System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bicking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09606</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Cruz</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2104.00769" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermans_A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leibe</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4661</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philbin</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="3673" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-modal Correlated Network for emotion recognition in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">End-to-end Triplet Loss based Emotion Embedding System for Speech Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06200</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retrieving speech samples with similar emotional content using a Triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotfian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8683273</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7400" to="7404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tristounet: triplet loss for speaker turn embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04301</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Willi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01535</idno>
		<title level="m">Triplet Network with Attention for Speaker Diarization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1608</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02304</idno>
		<title level="m">Deep Speaker: an End-to-End Neural Speaker Embedding System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised triplet loss based learning of ambient audio embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">2025824</biblScope>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Open-Vocabulary Keyword Spotting with Audio and Text Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerňak</surname></persName>
		</author>
		<idno type="DOI">3362-3366.10.21437/Interspeech.2019-1846</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havivy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12764</idno>
		<title level="m">Towards Learning a Universal Non-Semantic Representation of Speech</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Verifying Deep Keyword Spotting Detection with Acoustic Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU46091.2019.9003781</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Autom. Speech Recognit. Underst. Work. ASRU 2019 -Proc</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="613" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Metric learning for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.087762020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Query-by-Example Keyword Spotting system using Multi-head Attention and Softtriple Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2102.07061" />
		<imprint>
			<date type="published" when="2021-03-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Honk: A PyTorch reimplementation of convolutional neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.06554" />
		<imprint>
			<date type="published" when="2017-10-17" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv. arXiv</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="814" to="818" />
		</imprint>
	</monogr>
	<note>2019-Sept</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Montreal forced aligner: Trainable text-speech alignment using kaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Socolof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mihuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sonderegger</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1386</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="498" to="502" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Do your resources sound similar?: On the impact of using phonetic similarity in link discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C N</forename><surname>Ngomo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360901.3364426</idno>
	</analytic>
	<monogr>
		<title level="m">K-CAP 2019 -Proceedings of the 10th International Conference on Knowledge Capture</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.11286" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbdata.2019.2921572</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
