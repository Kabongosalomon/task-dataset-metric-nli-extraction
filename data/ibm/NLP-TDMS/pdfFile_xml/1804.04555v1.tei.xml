<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRAJECTORY FACTORY: TRACKLET CLEAVING AND RE-CONNECTION BY DEEP SIAMESE BI-GRU FOR MULTIPLE OBJECT TRACKING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Yang</surname></persName>
							<email>csyang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
							<email>zhuangyq@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zhangziw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
							<email>hzjia@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRAJECTORY FACTORY: TRACKLET CLEAVING AND RE-CONNECTION BY DEEP SIAMESE BI-GRU FOR MULTIPLE OBJECT TRACKING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computer Vision</term>
					<term>Siamese Bi-GRU</term>
					<term>Tracklet Association</term>
					<term>Multi-Object Tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-Object Tracking (MOT) is a challenging task in the complex scene such as surveillance and autonomous driving. In this paper, we propose a novel tracklet processing method to cleave and re-connect tracklets on crowd or longterm occlusion by Siamese Bi-Gated Recurrent Unit (GRU). The tracklet generation utilizes object features extracted by CNN and RNN to create the high-confidence tracklet candidates in sparse scenario. Due to mis-tracking in the generation process, the tracklets from different objects are split into several sub-tracklets by a bidirectional GRU. After that, a Siamese GRU based tracklet re-connection method is applied to link the sub-tracklets which belong to the same object to form a whole trajectory. In addition, we extract the tracklet images from existing MOT datasets and propose a novel dataset to train our networks. The proposed dataset contains more than 95160 pedestrian images. It has 793 different persons in it. On average, there are 120 images for each person with positions and sizes. Experimental results demonstrate the advantages of our model over the state-of-the-art methods on MOT16.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multi-object tracking (MOT) is a significant task of identifying each object and predicting their trajectories in a video sequence. It has a wide range of applications in computer vision, such as video surveillance, pedestrian flow analysis and autonomous driving. MOT based methods are aiming to address this problem by data association, which jointly optimize the matching process of bounding boxes detected by detector within the inter-frames of a sequence. One of the major applications of MOT focuses on pedestrian tracking. The same individual has regular temporal or spatial cues in video. For example, a person has slight appearance, velocity and direction changes for monocular sequence in a single  camera. Therefore, MOT usually depends on the combination of multiple cues (e.g. appearance, motion and interactions) to associate the similar bounding boxes. Although the performance is gradually improving at the MOT challenges <ref type="bibr" target="#b0">[1]</ref>, the effectiveness of MOT is still limited by object detection quality, long-term occlusion and scene complexity. To solve this sophisticated problem, previous works aim to extract the competitive feature, design effective association metric and adopt reliable detector.</p><p>Tracking-by-detection is becoming dominant solutions for MOT , which compares feature and position of bounding boxes to link similar objects into trajectories. The aim of Tracking-by-detection is to search the optimal assignment from multiple cues within a set of bounding boxes. The person's appearance is a convincing cue for data association. Conventional algorithms tend to extract hand-crafted features. Currently, deep neural networks such as convolutional neural networks (CNN) and recurrent neural networks (RNN) have achieved state-of-the-art performance in MOT <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. CNN extracts the feature of bounding box to represent the pedes-trian appearance. RNN is able to summarize the general characteristics of images from the same tracklet. These association methods are detection-to-detection or detection-totracklet. However, for a given tracklet with long-term occlusion, the detected pedestrian image may contain different degrees of occlusions. Thus, feature extraction on these occluded pedestrian images, even with subsequent complex matching techniques, is often not quite reliable.</p><p>In this paper, we propose a novel tracklet association to address the above problem by Siamese Bi-Gated Recurrent Unit(GRU), which is a tracklet-to-tracklet based method. GRU is the long-term version of RNN. Our method can be divided into three steps as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>• Tracklet Generation: We firstly utilize the nonmaximum suppression (NMS) to eliminate redundant bounding boxes and associate them with less or none occlusion by appearance and motion cues to generate the high confidence tracklet candidates.</p><p>• Tracklet Cleaving: Due to occlusion, one tracklet may belongs to multiple persons. Therefore, we utilize bidirectional GRU to split the tracklet into several subtracklets and ensure that each sub-tracklet only belong to independent tracked person.</p><p>• Tracklet Re-connection: We extract features from each tracklet candidate or split sub-tracklet by siamese GRU. The tracklets are matched by using temporal and spatial cues and re-connected according to their similarity. At last, we fill the gap among matched tracklets by polynomial curve fitting to form the whole trajectory and smooth every trajectory by smoothing function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Multi-object tracking in videos has attracted great attention. The performance of MOT improves gradually at the MOT benchmark. Tracking-by-detection has become one of the most popular tracking frameworks. Among the methods of MOT, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> focus on designing an ingenious data association or multiple hypothesis. <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> rely on network flow and graph optimization which are powerful approaches for tracking. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> are presented to improve the tracklet association and tracklet confidence to achieve the tracklet task. The inter-relation of targets have multiple cues in a sequence including appearance, motion and interaction, which summarized are by <ref type="bibr" target="#b3">[4]</ref>. In addition, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> adopt the appearance model of some early traditional algorithms such as color histogram to represent the image feature, or <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref> utilize covariance matrix or hand-crafted keypoint features. <ref type="bibr" target="#b16">[17]</ref> uses a novel multi-object tracking formulation to incorporate several detector into a tracking system. <ref type="bibr" target="#b6">[7]</ref> extends the multiple hypothesis by enhancing detection model. The motion model expresses the rule of object movement, which are divided into linear position prediction <ref type="bibr" target="#b17">[18]</ref> and non-linear position prediction <ref type="bibr" target="#b18">[19]</ref>. The interaction model describes the inter-relationship of different pedestrians in the same scene. <ref type="bibr" target="#b14">[15]</ref> designs the structural constraint by the location of people to optimize assignment. Recently, deep neural networks have been used gradually for tracking. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> train the CNN on the basis of person re-identification to extract the image features, and <ref type="bibr" target="#b17">[18]</ref> utilizes the quadruplet loss to enhance the feature expression. <ref type="bibr" target="#b2">[3]</ref> builds the CNN model to generate visibility maps to solve the occlusion problem. Following the success of RNN models for sequence prediction tasks, <ref type="bibr" target="#b19">[20]</ref> proposes social-LSTM to predict the position of each person in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTI-OBJECT TRACKING FRAMEWORK</head><p>Our solution aims at long-term occlusion and crowd which are difficult to track precisely. In this Section, the data association metric which generates tracklets from relative sparse scenario as the tracklet candidate is described in Section 3.1.</p><p>We present how to estimate the tracklet reliability and split the unreliable tracklets in Section 3.2. Section 3.3 gives the traclets re-connection and association strategy, moreover, the training method of our network is also discussed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tracklet Generation</head><p>Firstly, we execute a simple multi-object tracking algorithm to generate tracklets. We choose the target which is easy to track in order to produce the high-confidence tracklets. So we denote the set of detection bounding boxes D t (d k t ∈ D t )and the set of tracked objects</p><formula xml:id="formula_0">candidates C t (c k n ∈ C t ; n ≤ t, C t = C t−1 D t−1 ), where d k</formula><p>t and c k t are k-th detection and candidate in frame t, respectively. To connect the candidate and detection within inter-frames, we match the candidates c k t and d k t in a bipartite graph with Hungarian algorithm <ref type="bibr" target="#b20">[21]</ref>. The bi-</p><formula xml:id="formula_1">partite graph G = (V, E) whose node V are divided into left part C t ∈ V L and right part D t ∈ V R , e ij ∈ E is the edge of c i t and d j t .</formula><p>The tracked objects are defined as 7 dimensions [t, id, x, y, w, h, s] that contain the tracklet id by tracker, the object time, the center position (x, y), width and height of the bounding box, and the state of the tracklet. The state of tracklet includes "tracked", "lost" and "quitted", which are similar to Markov Decision Processes <ref type="bibr" target="#b4">[5]</ref> (as described in <ref type="figure" target="#fig_2">Fig. 2</ref>). The detail of state transition is introduced in Section 3.3. And then we obtain the set of tracklets T (τ k ∈ T ) in the whole sequence. The formulation of optimized graph is given by</p><formula xml:id="formula_2">argmin eij ∈E S(c i t ,</formula><p>which is defined in 4 dimensions [x,ŷ,ŵ,ĥ] that stand for the prediction of x, ycoordinate, weight and height, respectively. The prediction position by output of Long Short-Term Memory (LSTM) depends on historical position as the input of LSTM. Some more details of CNN and LSTM are discussed in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tracklet Cleaving</head><p>After tracklet generation, we have the coarse set of tracklet T in sequence. However, the tracker in Section 3.1 may mis-track the wrong person when two persons cross each other. To guarantee the tracklet with the single person, we design a bidirectional output Gated Recurrent Unit (GRU) to estimate the tracklet reliability and cleave the false tracklets in time. The reliable tracklets T + and unreliable tracklets T − are defined as</p><formula xml:id="formula_3">τ k ∈ T + ∀i, j, r k i , r k j ∈ τ k , r k i (id) ≡ r k j (id) τ k ∈ T − ∃i, j, r k i , r k j ∈ τ k , r k i (id) = r k j (id)<label>(4)</label></formula><p>where r k i , r k j are the i-th, j-th element on tracklet τ k . All of the tracklets τ k ∈ T are fed into the bi-GRU to distinguish whether the tracklet is reliable, or find out the split position of the unreliable tracklet. The tracklet cleaving network (bidirectional-GRU) is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. First of all, we utilize the CNN to extract the image features ϕ k c,i , i ∈ [1, L k ] from the tracklet. Secondly, all the features ϕ k c,i is inputted the forward-GRU and backward-GRU respectively. Both GRUs have the shared weights, and the out-</p><formula xml:id="formula_4">put is ϕ k g,i , i ∈ [−L k , −1] ∪ [1, L k ]</formula><p>, the positive and negative values stand for forward and backward feature from GRU. And then, we calculate the adjacent vector distance between the forward and the backward (e.g. length=10, {ϕ k g,1 , ϕ k g,−9 }, {ϕ k g,2 , ϕ k g,−8 }, ... ) as a series of feature distance to combine a 1 ×(L k − 1) vector ϕ k d :</p><formula xml:id="formula_5">ϕ k d,i = ϕ k g,i − ϕ k g,i−L k 2 2 , i ∈ [1, L k − 1] ϕ k d = ([ϕ k d,1 , ϕ k d,2 , ..., ϕ k d,L k ])<label>(5)</label></formula><p>The algorithm calculates the distance ϕ k d,i between the  features from the left and the right to current position and search the maximum disparity from these distances. The final output of the cleaving network is a single vector ϕ k d , which can find the most suitable splitting point by the position of peak value. However, if all of the vector values are less than the threshold, the tracklet includes the same person. The example is described in <ref type="figure" target="#fig_4">Fig. 4</ref>. In this figure, the input is a unreliable tracklet (the length is 10) which includes the white coat person at the front part and the blue coat person at the latter part. The network calculates every adjacent feature distance ϕ k d,i , i ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> from left and right, and find the maximum distance to define the best splitting point(ϕ k d,4 ). So our cleaving network not only distinguishes the tracklet availability, but also cleaves the unreliable tracklet.</p><p>GRU is a long-term version of RNN. The advantage of RNN is able to summarize the general characteristics with the same person and eliminate occlusion in order to obtain preferable feature expression. The pre-train model of cleaving network is the half of the re-connection network (shown in <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>). The details of training strategy is described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tracklet Re-connection</head><p>We construct the Deep Siamese Bi-GRU to perform the cleaving and re-connection tasks. To obtain the competitive feature descriptor, we combine various losses to reduce the withinclass distance and enlarge the between-class distance simultaneously. Our network is designed with the verification loss and identification loss at each GRU output. The loss is defined as:</p><formula xml:id="formula_6">L sum = L glo + L loc<label>(6)</label></formula><p>Where the L glo and L loc indicate the global loss and local loss of the network, respectively. We use the contrastive loss by Euclidean distance for the verification and the crossentropy losses in the multi-classification task for the identifi-cation. The details of the losses are shown as:</p><formula xml:id="formula_7">E(ϕ k1 f , ϕ k2 f ) = y ϕ k1 f − ϕ k2 f 2 2 + (1 − y)max{0, (η− ϕ k1 f − ϕ k2 f 2 2 )} (7) F (ϕ k f ) = K i=1 −p i log(p i ),p i = sof tmax(ϕ k f )<label>(8)</label></formula><p>where ϕ k f indicates the output feature of GRU ϕ k g after fully-connected (FC) layer and ReLU. E(ϕ i , ϕ j ) is the contrastive function, y ∈ {0, 1} is the label indicator, η is a margin constant. F (ϕ) denotes the multi-classification crossentropy function. The representation of loss can be formulated as follows:</p><formula xml:id="formula_8">L glo = λ v L v + λ id (L id1 + L id2 ) = λ v E(ϕ k1 f , ϕ k2 f ) + λ id (F (ϕ k1 f ) + F (ϕ k2 f )) (9) ϕ k f = 1 2L k ( −1 i=−L k ϕ k f,i + L k j=1 ϕ k f,j ) (10)</formula><p>where ϕ k f is the temporal pooling <ref type="bibr" target="#b21">[22]</ref> of each output of GRU.</p><formula xml:id="formula_9">L loc = λ loc v L loc v + λ loc id L loc id (11) L loc v = ϕ k1 f,1 − ϕ k1 f,L k 1 2 2 + ϕ k2 f,1 − ϕ k2 f,L k 2 2 2 − ϕ k2 f,1 − ϕ k2 f,1 2 2 − ϕ k1 f,L k 1 − ϕ k2 f,L k 2 2 2 +δ (12) L loc id = k∈k1,k2 ( −1 i=−L k F (ϕ k f,i ) + L k j=1 F (ϕ k f,j ))<label>(13)</label></formula><p>λ is the loss weight coefficient. L v , L id * , L loc v and L loc id denote the verification and identification loss of global and local respectively. L loc v is similar to triplet loss (refer to <ref type="bibr" target="#b17">[18]</ref>), including the disparity of head and tail of the tracklet, head between different tracklets and tail between different tracklets. δ is the threshold of margin. L loc id is the multiclassification task for each output.</p><p>After training the re-connection network, we can cleave or match the tracklets. For cleaving the tracklet, we calculate the peak value of the feature which is concatenated by ϕ k d,i ,</p><formula xml:id="formula_10">ϕ k d = ([ϕ k d,1 , ϕ k d,2 , ..., ϕ k d,L k ])</formula><p>. For re-connecting the tracklet, we match the temporal pooling features ϕ k f to compare the distance between the tracklets. Tracklet Association. This process is the tracklet assignment from the set of tracklet T + after cleaving network. The matching of tracklets τ k ∈ T + are restricted previously by temporal and spatial constraints. The constraint of matching is shown as: </p><formula xml:id="formula_11">IOU (r i L i +∆ti,j , r j 1 ) = area(B(r i L i +∆t i,j )∩B(r j 1 )) area(B(r i L i +∆t i,j )∪B(r j</formula><formula xml:id="formula_12">wherer i L i +∆ti,j (x, y) = (τ i (v) + µ) · ∆t i,j ∆t i,j = (r j 1 (t) − r i L i (t)) s.t. r j 1 (t) &gt; r i L i (t), τ i (s), τ j (s) = quitted (15) B(·) is the bounding box of object [x, y, w, h].τ i (v)</formula><p>is the average velocity of tracklet τ i , µ is velocity constant. r i L i and r j 1 indicate the tail of tracklet τ i and the head of tracklet τ j respectively. The states of tracklet node r k i (s) include "tracked" "lost" and "quitted". The node is unmatched more than one frame, which is transferred to "lost" until the node is matched by next frame node and transfer to "tracked".The node is quitted when the node position will be out of the boundary. In addition, tracklets also has states τ k (s), which depend on the last node state of this tracklet. For tracklet association, we give up the quitted tracklets, and re-connection network associates the tracklets which meet the temporal-spatial conditions. Lastly, we re-connect the tracklets which satisfy with the constraint by the output ϕ k f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>In our experiments, our networks consist of CNN, LSTM and GRU. For tracklet generation, we train the Siamese-CNN network of appearance model with ResNet-50, the images are resize to 224 × 224 from the Re-identification dataset Market1501 <ref type="bibr" target="#b22">[23]</ref>   <ref type="bibr" target="#b23">[24]</ref>. The outputs of the GRU ϕ k g,i , i ∈ [−L k , −1] ∪ [1, L k ] are also 128-dimensional vectors, which are fed to FC network for verification loss and for comparison of the corresponding features for classification loss. We use the AdamOptimizer <ref type="bibr" target="#b24">[25]</ref> to train our network. The training dataset is extracted from dataset PathTrack <ref type="bibr" target="#b25">[26]</ref> and video re-identification dataset MARS <ref type="bibr" target="#b26">[27]</ref>.  <ref type="bibr" target="#b28">[29]</ref> include Multiple Object Tracking Accuracy (MOTA), ID F1 Score (IDF1), Mostly tracked targets (MT), Mostly lost targets (ML), False Positives (FP), False Negatives (FN), Identity Switches (IDSw.), the total number of Fragment (Frag) and Processing Speed (Hz). MOTChallenge Benchmark. We evaluated performance of our method on MOT16 <ref type="bibr" target="#b0">[1]</ref>. The sequences of dataset are captured from surveillance, hand-held shooting and driving recorder by static camera and moving camera. Result Comparison. We compare the state-of-the-art methods on MOT16. The result of MOT benchmark is presented in <ref type="table" target="#tab_1">Table 1</ref>. GCRA G is the tracklet generation in this paper, and the GCRA is our final method. Obviously, our method achieves higher performance of MOTA which is the primary evaluation metric. The result of static camera sequence is better than others especially, but moving camera is unsatisfactory because the temporal and spatial constraints are not suitable for it(as shown in <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We propose a novel tracklet association scheme to cleave and re-connect the tracklets on crowd or long-term occlusion by Deep Siamese Bi-GRU. The method calculates each output of bidirectional GRU to search the suitable split position and match the tracklets to reconnect the same person. For training, we extracted the tracklet dataset from existing MOT datasets for training our frameworks. Our proposal has better performance for static camera such as surveillance. The algorithm achieves 48.2% in MOTA that approaches the state-of-the-art methods on MOT16 benchmark dataset. The qualitative result is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>The corresponding author, Changshui Yang is with the Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>An example of our method: 1.Generating the tracklet candidates by appearance and motion model. 2.Cleaving the mistracked tracklets of a person. 3.Re-connecting the tracklets of the same person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The structure of tracklets generation and the demonstration of the sets of C t , D t and T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of tracklet cleaving and re-connection network, (a) Cleaving the tracklets by bidirectional outputs of GRU, (b) Re-connecting the tracklets by the features of siamese GRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The explanation of the cleaving network: Top of the dots indicate feature distribution for two persons, and the arrows denote normal tendency with a single person and disrupted tendency by each other. Bottom of the figure is the unreliable tracklet corresponding to the top figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on the MOT16 benchmark. (a): MOT16-01 (b): MOT16-03</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Forwar d directio n Forw ard direc tion with out mist rack ing Ba ck wa rd di re ct io n wi th ou t m ist ra ck in g Blue coat human feature distribution The maximum distance White coat human feature distribution Backw ard direct ion</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Only single person feature distribution</cell><cell cols="3">Including multiple person feature distribution</cell><cell>Only single person feature range</cell><cell cols="2">Calculating the feature distance</cell></row><row><cell>= 2.43</cell><cell>= 9.52</cell><cell cols="2">= 14.67</cell><cell>= 18.34</cell><cell>= 12.01</cell><cell>= 7.54</cell><cell>= 4.67</cell><cell>= 1.58</cell><cell>= 0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results on the MOT16 test dataset (G: Generation C: Cleaving R: Re-connection A: Association)</figDesc><table><row><cell>Tracker</cell><cell cols="2">MOTA↑ IDF1↑</cell><cell>MT↑</cell><cell>ML↓</cell><cell>FP↓</cell><cell>FN↓</cell><cell cols="2">IDSw.↓ Frag↓</cell><cell>Hz↑</cell></row><row><cell>QuadMOT16 [18]</cell><cell>44.1</cell><cell>38.3</cell><cell>14.6%</cell><cell>44.9%</cell><cell>6388</cell><cell>94775</cell><cell>745</cell><cell>1096</cell><cell>1.8</cell></row><row><cell>EDMT [7]</cell><cell>45.3</cell><cell>47.9</cell><cell>17.0%</cell><cell>39.9%</cell><cell>11122</cell><cell>87890</cell><cell>639</cell><cell>946</cell><cell>1.8</cell></row><row><cell>MHT DAM [17]</cell><cell>45.8</cell><cell>46.1</cell><cell>16.2%</cell><cell>43.2%</cell><cell>6412</cell><cell>91758</cell><cell>590</cell><cell>781</cell><cell>0.8</cell></row><row><cell>STAM16 [3]</cell><cell>46.0</cell><cell>50.0</cell><cell>14.6%</cell><cell>43.6%</cell><cell>6895</cell><cell>91117</cell><cell>473</cell><cell>1422</cell><cell>0.2</cell></row><row><cell>NOMT [17]</cell><cell>46.4</cell><cell>53.3</cell><cell>18.3%</cell><cell>41.4%</cell><cell>9753</cell><cell>87565</cell><cell>359</cell><cell>504</cell><cell>2.6</cell></row><row><cell>AMIR [4]</cell><cell>47.2</cell><cell>46.3</cell><cell>14.0%</cell><cell>41.6%</cell><cell>2681</cell><cell>92856</cell><cell>774</cell><cell>1675</cell><cell>1.0</cell></row><row><cell>NLLMPa [12]</cell><cell>47.6</cell><cell>50.9</cell><cell>15.2%</cell><cell>38.3%</cell><cell>9253</cell><cell>85431</cell><cell>792</cell><cell>1858</cell><cell>18.5</cell></row><row><cell>FWT [17]</cell><cell>47.8</cell><cell>44.3</cell><cell cols="2">19.1% 38.2%</cell><cell>8886</cell><cell>85487</cell><cell>852</cell><cell>1534</cell><cell>0.6</cell></row><row><cell>LMP [2]</cell><cell>48.8</cell><cell>51.3</cell><cell>18.2%</cell><cell>40.1%</cell><cell>6654</cell><cell>86245</cell><cell>481</cell><cell>595</cell><cell>0.5</cell></row><row><cell>GCRA G(Ours)</cell><cell>47.4</cell><cell>41.4</cell><cell>14.4%</cell><cell>39.8%</cell><cell>7516</cell><cell>87219</cell><cell>1147</cell><cell>1156</cell><cell>7.5</cell></row><row><cell>GCRA G+C+R+A(Ours)</cell><cell>48.2</cell><cell>48.6</cell><cell>12.9%</cell><cell>41.1%</cell><cell>5104</cell><cell>88586</cell><cell>821</cell><cell>1117</cell><cell>2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and the output of CNN produced a 1024dimensional vector to describe the image. In addition , the inputs of the LSTM network for motion model is a series of 4-dimensional vector [x, y, w, h] with a tracklet (the length of input vectors L ∈ [3, 10]), and the LSTM output is the prediction of the position and size [x,ŷ,ŵ,ĥ]. For tracklet cleaving and re-connection, the model is a deep siamese bi-GRU, which includes four hidden-layers and the maximum length of GRU is 120 frames. The inputs of the GRU is a series of 128-dimensional features by CNN which is a wide residual networks (WRN)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>MOTA of each MOT16 sequences</figDesc><table><row><cell>Sequence</cell><cell>01</cell><cell>03</cell><cell>06</cell><cell>07</cell><cell>08</cell><cell>12</cell><cell>14</cell></row><row><cell>Static(s)&amp;Moving(m)</cell><cell>s</cell><cell>s</cell><cell>m</cell><cell>m</cell><cell>s</cell><cell>m</cell><cell>m</cell></row><row><cell>QuadMOT16 [18]</cell><cell>30.8</cell><cell>51.0</cell><cell>49.2</cell><cell>41.9</cell><cell>29.9</cell><cell>38.0</cell><cell>24.0</cell></row><row><cell>EDMT [7]</cell><cell>35.3</cell><cell>51.2</cell><cell>49.8</cell><cell>46.1</cell><cell>32.3</cell><cell>43.1</cell><cell>24.9</cell></row><row><cell>MHT DAM [17]</cell><cell>35.8</cell><cell>52.7</cell><cell>49.1</cell><cell>39.3</cell><cell>33.2</cell><cell>44.3</cell><cell>26.1</cell></row><row><cell>STAM16 [3]</cell><cell>35.7</cell><cell>53.8</cell><cell>48.4</cell><cell>38.0</cell><cell>32.3</cell><cell>42.3</cell><cell>24.6</cell></row><row><cell>NOMT [17]</cell><cell>34.2</cell><cell>53.0</cell><cell>51.3</cell><cell>44.9</cell><cell>36.7</cell><cell>39.3</cell><cell>23.5</cell></row><row><cell>AMIR [4]</cell><cell>37.8</cell><cell>53.8</cell><cell>49.2</cell><cell>45.5</cell><cell>32.5</cell><cell>40.4</cell><cell>29.4</cell></row><row><cell>NLLMPa [12]</cell><cell>30.7</cell><cell>56.4</cell><cell>49.8</cell><cell>40.7</cell><cell>33.3</cell><cell>43.3</cell><cell>23.3</cell></row><row><cell>FWT [17]</cell><cell>33.6</cell><cell>55.7</cell><cell>51.8</cell><cell>40.3</cell><cell>35.1</cell><cell>44.67</cell><cell>24.7</cell></row><row><cell>LMP [2]</cell><cell>39.9</cell><cell>56.1</cell><cell>52.3</cell><cell>43.1</cell><cell>33.8</cell><cell>43.7</cell><cell>28.8</cell></row><row><cell>Rank</cell><cell>1</cell><cell>1</cell><cell>10</cell><cell>3</cell><cell>2</cell><cell>8</cell><cell>3</cell></row><row><cell>GCRA(Ours)</cell><cell>42.5</cell><cell>56.7</cell><cell>35.9</cell><cell>44.1</cell><cell>35.4</cell><cell>39.6</cell><cell>28.3</cell></row><row><cell cols="5">4.2. Results of Multi-Object Tracking</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Evaluation Metrics. The MOTChallenge Benchmark de-</cell></row><row><cell cols="8">pends on multiple evaluation index of trackers. These met-</cell></row><row><cell>rics [28]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , d i t )e ij<ref type="bibr" target="#b0">(1)</ref> where S(c i t , d i t ) indicates the cost function with c i t and d j t . In addition, e ij is the indicator parameter e ij ∈ {0, 1}.The cost function is defined asS(c i t , d i t ) = αF a (c i t , d i t ) + βF m (c i t , d i t ) (2) F a (c i t , d i t ) = f c i t − f d i t 2 2 , F m (c i t , d i t ) = p c i t − p d i t 2 2 (3) where F a (c i t , d i t )denotes the appearance cue which calculates the Euclidean distance and L 2 normalized to measure the similarity of c i t and d j t . Furthermore, the appearance features f c i t , f d i t of a person are created by Convolutional Neural Network (CNN). α, β are the weight coefficients of the function. F m (c i t , d i t ) indicates the motion cue, and the function compares the distance between the detection position p d i t and candidate prediction positionp c i</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">)) &gt; 0<ref type="bibr" target="#b13">(14)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is partially supported by the National Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MOT16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>abs/1603.00831</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4836" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to track: Online multiobject tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4696" to="4704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing detection model for multiple hypothesis tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Joint graph decomposition and node labeling: Problem, algorithms, applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Globally consistent multi-people tracking using motion patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maksai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint learning of convolutional neural networks and temporally constrained metrics for tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Luk</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1218" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term time-sensitive costs for crf-based tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online multiobject tracking via structural constraint event aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1392" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal dynamic appearance modeling for online multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A novel multi-detector fusion framework for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5620" to="5629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The way they move: Tracking multiple targets with similar appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dicle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2304" to="2311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kalman filter and iterative-hungarian algorithm implementation for low complexity point tracking as part of fast multiple object tracking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adiprawita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pathtrack: Fast trajectory annotation with path supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">246309</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
