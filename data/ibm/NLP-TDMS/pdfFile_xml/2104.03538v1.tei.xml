<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-An</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Plantinga</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Mila-Quebec AI Institute</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
							<email>yu.tsao@citi.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech enhancement</term>
					<term>speech quality optimization</term>
					<term>black-box score optimization</term>
					<term>MetricGAN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The discrepancy between the cost function used for training a speech enhancement model and human auditory perception usually makes the quality of enhanced speech unsatisfactory. Objective evaluation metrics which consider human perception can hence serve as a bridge to reduce the gap. Our previously proposed MetricGAN was designed to optimize objective metrics by connecting the metric with a discriminator. Because only the scores of the target evaluation functions are needed during training, the metrics can even be non-differentiable. In this study, we propose a MetricGAN+ in which three training techniques incorporating domainknowledge of speech processing are proposed. With these techniques, experimental results on the VoiceBank-DEMAND dataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the previous MetricGAN and achieve stateof-the-art results (PESQ score = 3.15).</p><p>Index Terms: speech enhancement, speech quality optimization, black-box score optimization, MetricGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are many different applications and goals for a speech enhancement (SE) model. For example, in human-to-human communication, we care about speech quality or intelligibility (e.g., during a phone call with severe background noise, the intelligibility may be more important than quality). On the other hand, in human-to-machine communication, the goal of SE is to improve the speech recognition performance (e.g., reducing the word error rate (WER) under noisy conditions for an automated speech recognition (ASR) system). Therefore, training a task-specific SE model may obtain better performance for its targeted applications.</p><p>To deploy a task-specific SE model, the most intuitive way is to adopt a loss function that is relevant to the final goal. Although directly applying a measure based on the difference in signal level (e.g., 1 or 2 loss) is straightforward, several studies have shown that it is not highly correlated to speech quality <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, intelligibility <ref type="bibr" target="#b3">[4]</ref>, and WER <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>An alternative would be to directly optimize speech quality or intelligibility. This is often very challenging and normally objective evaluation metrics are used as surrogates. Among the human perception-related objective metrics, the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b6">[7]</ref> and shorttime objective intelligibility (STOI) <ref type="bibr" target="#b7">[8]</ref> are two popular functions used to evaluate speech quality and intelligibility, respectively. The design of these two metrics considers human auditory perception and has shown higher correlation to subjective listening tests than simple 1 or 2 distance between clean and degraded speech signals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The current techniques to optimize these objective scores can be categorized into two types depending on whether the details of evaluation metrics have to be known: 1) white-box: these methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> approximate the complex evaluation metrics with a hand-crafted, differentiable one. However, the details of the metrics have to be known and it can only be used for the targeted metric. (2) black-box: these methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> mainly treat the metric as a reward and apply reinforcement learning based techniques to increase the scores. However, the training is usually inefficient with limited performance improvement.</p><p>MetricGAN <ref type="bibr" target="#b14">[15]</ref> falls into the black-box category, and it can achieve better training efficiency and moderate performance improvement (the average PESQ score increases more than 0.1) compared to conventional 1 loss. Although MetricGAN can be easily applied to optimize different evaluation metrics (e.g., PESQ, STOI, or WER), we mainly considered PESQ score optimization as an example. Other extensions can be found at <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>.</p><p>In this study, to further boost the performance of the MetricGAN framework and reveal the important factors that affect the performance, we propose MetricGAN+. The basic idea behind MetricGAN+ does not change and the improvement comes from including three training techniques that incorporate domain-knowledge of speech processing. Two improvements are proposed for the discriminator (D) and one for the generator (G): For the discriminator: 1) Include noisy speech for discriminator training: In addition to enhanced and clean speech, noisy speech is used to minimize the distance between the discriminator and target objective metrics. 2) Increase sample size from replay buffer: Speech generated from the previous epochs is reused for training D. This can prevent D from catastrophic forgetting <ref type="bibr" target="#b18">[19]</ref>. For the generator: 1) Learnable sigmoid function for mask estimation: A conventional sigmoid is not optimal for mask estimation because it is the same for all frequency bands and has a maximum value of 1. A per-frequency learnable sigmoid function is more flexible and improves the performance of SE. To foster reproducibility, the MetricGAN+ is available within the SpeechBrain toolkit 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction to MetricGAN</head><p>The main idea of MetricGAN is to mimic the behavior of a target evaluation function (e.g., PESQ function) with a neural network (e.g., Quality-Net <ref type="bibr" target="#b19">[20]</ref>). The surrogate evaluation function is learned from raw scores, treating the target evaluation function as a black box. Once the surrogate evaluation is trained, it can be used as a loss function for the speech enhancement model. Unfortunately, a static surrogate is easily fooled by adversarial examples <ref type="bibr" target="#b21">[22]</ref> (estimated quality scores increase but true scores decrease <ref type="bibr" target="#b20">[21]</ref>). To mitigate this issue, we recently proposed a learning framework where the surrogate loss and enhancement model are alternately updated <ref type="bibr" target="#b14">[15]</ref>. This method is called MetricGAN because its goal is to optimize black-box metric scores, with a training flow that is similar to the one of generative adversarial networks (GANs). Below, we briefly introduce the training of MetricGAN.</p><p>Let ′ (I) be a function that represents the target evaluation metric normalized between 0 and 1, where I denotes the input of the metric. For example, for PESQ and STOI, I denotes a pair of enhanced speech, ( ) (or noisy speech, ) that we want to evaluate, and its corresponding clean speech, y. To ensure that the discriminator network (D) behaves similar to ′ , the objective function of D is</p><formula xml:id="formula_0">D(MetricGAN) = , [( ( , ) − ′ ( , )) 2 + ( ( ( ), ) − ′ ( ( ), )) 2 ] (1)</formula><p>The two terms are used to minimize the difference between (.) and ′ (.) for clean and enhanced speech, respectively. Note that, ′ ( , ) = 1 and 0 ≤ ′ ( ( ), ) ≤ 1.</p><p>The training of the generator network (G) can completely rely on the adversarial loss</p><formula xml:id="formula_1">G(MetricGAN) = [( ( ( ), ) − ) 2 ]<label>(2)</label></formula><p>where s denotes the desired assigned score. For example, to generate clean speech, we can simply assign s to be 1. The overall training flow is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From MetricGAN to MetricGAN+</head><p>To improve the performance of the MetricGAN framework, some advanced learning techniques are proposed. During the investigation, we also study the factors that significantly influence the performance or training efficiency. The improvement of MetricGAN+ mainly comes from the following three modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning the Metrics Scores for Noisy Speech</head><p>Kawanaka et al. <ref type="bibr" target="#b15">[16]</ref> proposed to include noisy speech when training the discriminator. This turned out to stabilize the learning process. We adopt the same strategy for MetricGAN+ as well. The loss function of the discriminator network is thus modified as follows:</p><formula xml:id="formula_2">D(MetricGAN+) = , [( ( , ) − ′ ( , )) 2 + ( ( ( ), ) − ′ ( ( ), )) 2 + ( ( , ) − ′ ( , )) 2 ]</formula><p>(3) where the third term is used to minimize the difference between (.) and ′ (.) for noisy speech.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Samples from Experience Replay Buffer</head><p>As in the deep Q-network <ref type="bibr" target="#b22">[23]</ref>, we found that reusing the data generated from the previous epochs to train the discriminator brings a huge improvement in performance. Intuitively, without experience replay, the discriminator may forget the behavior of target ′ function at the previous generated speech, thus making (.) less similar to ′ (.). To illustrate how the replay buffer works, we present the training process in Algorithm 1. In MetricGAN+, we increase history_portion from 0.1 (used in MetricGAN) to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learnable Sigmoid Function for Mask Estimation</head><p>Most mask-based speech enhancement methods <ref type="bibr" target="#b23">[24]</ref> apply a sigmoid activation to the output layer to constrain the mask to be between 0 and 1. However, due to the phase difference, the sum of clean (‖ ( , )‖) and noise (‖ ( , )‖) magnitude spectrograms do not exactly match the noisy ( ‖ ( , )‖ ) magnitude spectrogram <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_3">‖ ( , )‖ ≠ ‖ ( , )‖ + ‖ ( , )‖<label>(4)</label></formula><p>The ideal magnitude mask ‖ ( , )‖ ‖ ( , )‖ ⁄ is hence not guaranteed to be smaller than 1. Therefore, we set the scale variable β equals to 1.2 in (5). In addition, the standard sigmoid function to compress the input value may not be optimal for speech processing. For example, because the patterns of both noise and speech in high and low frequency bands are distinct, different frequency bands could have their own compression function for mask estimation. To give this flexibility to our model, we design a learnable sigmoid function as follows:</p><formula xml:id="formula_4">= β (1 + − ) ⁄<label>(5)</label></formula><p>where is learned from training data. Different frequency bands have their own .</p><p>controls the shape of the compression function. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, large (red) behaves like a hard threshold and most output values are either 0 or 1 (more non-smooth and more saturated <ref type="bibr" target="#b25">[26]</ref>, like a binary mask). On the other hand, small (green) behaves more like a linear function, which can be observed that it has larger overlap with the green dotted line (the overlap between the dotted lines and sigmoid functions can roughly show the range of linearity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>To compare the proposed MetricGAN+ with other existing methods, we use the publicly available VoiceBank-DEMAND dataset <ref type="bibr" target="#b26">[27]</ref>. This dataset contains a large amount of premixed noisy-clean paired data and is already used by several SE models. Details about the data can be found in the original paper. In addition to the PESQ score, we evaluate the performance with other three metrics: CSIG predicts the mean opinion score (MOS) of the signal distortion, CBAK predicts the MOS of the background noise interferences, and COVL predicts the MOS of the overall speech quality. All these three metrics range from 1 to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Structure</head><p>The generator used in this experiment is a BLSTM <ref type="bibr" target="#b27">[28]</ref> with two bidirectional LSTM layers, with 200 neurons each. The LSTM is followed by two fully connected layers, each with 300 LeakyReLU nodes and 257 (learnable) sigmoid nodes for mask estimation, respectively. When this mask is multiplied with the input noisy magnitude spectrogram, the noise components should be removed. In addition, as reported in <ref type="bibr" target="#b2">[3]</ref>, to prevent musical noise, flooring was applied to the estimated mask before T-F mask processing. Here, we set the lower threshold of the T-F mask to 0.05. The discriminator herein   is a CNN with four two dimensional (2-D) convolutional layers with 15 filters and a kernel size of <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>. To handle the variable length input (different speech utterances have different length), a 2-D global average pooling layer was added such that the features can be fixed at 15 dimensions (15 is the number of feature maps in the previous layer). Three fully connected layers were added subsequently, each with 50 and 10 LeakyReLU neurons, and 1 linear node. In addition, to make D a smooth function (ie., no small modification in the input spectrogram can make a significant change to the estimated score), the discriminator is constrained to be 1-Lipschitz continuous by using spectral normalization <ref type="bibr" target="#b28">[29]</ref>. number_of_samples is set to100 in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>We first show the effects of the different training techniques introduced in section 3. In <ref type="table" target="#tab_0">Table 1</ref>, the results in each row are achieved with the setting from the previous row plus the current technique. From the table, it emerges that removing the input spectrogram normalization and including noisy speech for D training leads to a larger improvement than further increasing the sample size from the buffer. Use these samples x to train G using <ref type="bibr" target="#b1">(2)</ref>. # Store into the buffer:</p><p>Store these samples and the corresponding true scores ( ( ), ′ ( ( ), )) into the replay buffer. # Train Discriminator: 1) Update D with current sampled data x using (3).</p><p>2) Randomly sample history_portion of the buffer and update D with the historical enhanced data. However, <ref type="table" target="#tab_1">Table 2</ref> shows that if we do not keep the replay buffer, the PESQ score only reaches 2.82. When we randomly sample 10% and 20% of historical enhanced data from the buffer, the score increases by 0.2 and 0.23, respectively. We do not observe further improvement with history_portion=0.3. This implies that without the buffer, the discriminator may just focus on the evaluation results on the current samples and ignore its correctness on the previously generated speech (catastrophic forgetting <ref type="bibr" target="#b18">[19]</ref>). Due to the discrepancy between and ′ , the gradient of may thus not a good approximation for that of ′ . <ref type="table" target="#tab_0">Table 1</ref> also reveals that applying learnable sigmoid can further increase the scores. We tried to make β learnable as well, but the performance did not further improve. The learned values of are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Most of are smaller than 1 (original value in the conventional sigmoid function) and close to 0.5. As pointed out in section 3.3, this implies that for most frequency bins, the learnable sigmoid is behaving more like a linear function. This naturally leads to a gradient backpropagation that is more efficient than the one happening in saturated regions. On the other hand, for high frequency bins, the learned are much larger than 1, and hence the mask is behaving more like a binary mask. We conjecture this is because the noise types in the training data do not occupy the high frequency regions, or this is due to the characteristics of PESQ function. However, more experiments are needed to verify the possible reasons.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we show the training curves of MetricGAN with different training techniques and the same BLSTM model structure as the generator in MetricGAN, but trained with MSE loss. From this figure, we can first observe that all MetricGAN-based methods outperform the MSE loss by a large margin. In addition, both including noisy speech for discriminator training and applying a learnable sigmoid not only improve the final performance but also lead to a higher training efficiency. <ref type="table" target="#tab_3">Table 3</ref> compares the proposed MetricGAN+ with other popular methods. Although our generator is just a conventional BLSTM with magnitude spectrogram as inputs, with appropriate loss function, it outperforms recent models (e.g., attention mechanism <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>) or phase-aware inputs (e.g., waveform <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b32">[33]</ref> or phase <ref type="bibr" target="#b33">[34]</ref>). We also noticed that the scores reported in <ref type="bibr" target="#b34">[35]</ref> are higher than ours, however, additional datasets are needed for the perceptual loss training. Compared to BLSTM (MSE), our MetricGAN+ increases the PESQ score from 2.71 to 3.15. We also find that our model's CBAK score is lower than other state-of-the-arts; this may be because the lower threshold of the T-F mask is set to 0.05 as pointed out in section 4.2 and hence some noise remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future work</head><p>The code of MetricGAN+ is available within the SpeechBrain toolkit and we encourage the community to continuously improve its performance and training efficiency. In the following, we list some directions that are worth exploring: 1) Since MetricGAN is a black-box framework, it can be used to optimize different metrics. In <ref type="bibr" target="#b16">[17]</ref>, it was applied to optimize speech intelligibility. To the best of our knowledge, it has not been used for WER minimization under noisy conditions for a black-box ASR model (e.g., Google ASR).</p><p>2) The structure of the discriminator can be further investigated. Right now it is just a simple CNN with global average pooling. More advanced mechanisms such as attention <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> may be able to replace the global pooling. In addition, if the target ′ function is much more complicated (e.g., WER of an ASR model), the complexity of discriminator may also need to be increased. 3) It is time consuming to train with a replay buffer, especially when there are already lots of historical data in the buffer. Incremental learning <ref type="bibr" target="#b39">[40]</ref> may be a good solution for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this study, we proposed several techniques to improve the performance of the MetricGAN framework. We find that including noisy speech for discriminator training and applying learnable sigmoid are the most useful techniques. Our MetricGAN+ achieves state-of-the-arts results on the VoiceBank-DEMAND dataset, and the PESQ scores can increase 0.3 and 0.45 compared to MetricGAN and BLSTM (MSE), respectively. From the experimental results, we believe that the proposed framework can be further improved and applied on different tasks; therefore we made the code publicly available. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training flow of MetricGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sigmoid function with different .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 Algorithm 1</head><label>51</label><figDesc>Training with replay buffer end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves of different settings (structure of G is fixed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Learned values of in learnable sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of MetricGAN+.</figDesc><table><row><cell></cell><cell>PESQ CSIG CBAK COVL</cell></row><row><cell>MetricGAN [15]</cell><cell>2.86 3.99 3.18 3.42</cell></row><row><cell>-Input normalization</cell><cell>2.95 4.03 3.11 3.49</cell></row><row><cell>+ Include noisy (Sec. 3.1)</cell><cell>3.02 4.13 3.23 3.57</cell></row><row><cell>+ Increase history_portion from</cell><cell>3.05 4.11 3.15 3.57</cell></row><row><cell>replay buffer (Sec. 3.2)</cell><cell></cell></row><row><cell>+ Learnable sigmoid (Sec. 3.3)</cell><cell>3.15 4.14 3.16 3.64</cell></row><row><cell>= MetricGAN+</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of different history_portions.</figDesc><table><row><cell>history_portion</cell><cell>PESQ</cell><cell>CSIG</cell><cell cols="2">CBAK COVL</cell></row><row><cell>0</cell><cell>2.82</cell><cell>3.99</cell><cell>3.38</cell><cell>3.41</cell></row><row><cell>0.1</cell><cell>3.02</cell><cell>4.13</cell><cell>3.23</cell><cell>3.57</cell></row><row><cell>0.2</cell><cell>3.05</cell><cell>4.11</cell><cell>3.15</cell><cell>3.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Compared MetricGAN+ with other methods on the VoiceBank-DEMAND dataset.</figDesc><table><row><cell></cell><cell cols="4">PESQ CSIG CBAK COVL</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell></row><row><cell>SEGAN [36]</cell><cell>2.16</cell><cell>3.48</cell><cell>2.94</cell><cell>2.80</cell></row><row><cell>MMSE-GAN [37]</cell><cell>2.53</cell><cell>3.80</cell><cell>3.12</cell><cell>3.14</cell></row><row><cell>SERGAN [38]</cell><cell>2.62</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BLSTM (MSE)</cell><cell>2.71</cell><cell>3.94</cell><cell>3.28</cell><cell>3.32</cell></row><row><cell>MetricGAN [15]</cell><cell>2.86</cell><cell>3.99</cell><cell>3.18</cell><cell>3.42</cell></row><row><cell>HiFi-GAN [32]</cell><cell>2.94</cell><cell>4.07</cell><cell>3.07</cell><cell>3.49</cell></row><row><cell>DeepMMSE [39]</cell><cell>2.95</cell><cell>4.28</cell><cell>3.46</cell><cell>3.64</cell></row><row><cell>MHSA+SPK [30]</cell><cell>2.99</cell><cell>4.15</cell><cell>3.42</cell><cell>3.57</cell></row><row><cell>PHASEN [34]</cell><cell>2.99</cell><cell>4.21</cell><cell>3.55</cell><cell>3.62</cell></row><row><cell>SDR-PESQ [11]</cell><cell>3.01</cell><cell>4.09</cell><cell>3.54</cell><cell>3.55</cell></row><row><cell>T-GSA [31]</cell><cell>3.06</cell><cell>4.18</cell><cell>3.59</cell><cell>3.62</cell></row><row><cell>DEMUCS [33]</cell><cell>3.07</cell><cell>4.31</cell><cell>3.40</cell><cell>3.63</cell></row><row><cell>MetricGAN+</cell><cell>3.15</cell><cell>4.14</cell><cell>3.16</cell><cell>3.64</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://speechbrain.github.io/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DNSMOS: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15258</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A differentiable perceptual audio metric learned from just noticeable differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dnn-based source enhancement to increase objective sound quality assessment score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1780" to="1792" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral feature mapping with mimic loss for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5609" to="5613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Acoustics-guided evaluation (AGE): a new measure for estimating performance of speech enhancement algorithms for robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11517</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITU-T Recommendation</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">862</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptually guided speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5074" to="5078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martin-Donas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">End-to-end multi-task denoising for joint sdr and pesq optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Kharmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monaural speech enhancement using deep neural networks by maximizing a shorttime objective intelligibility measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbae K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DNN-based source enhancement self-optimized by reinforcement learning using sound quality measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training supervised speech separation system to improve STOI and PESQ directly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5374" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stable training of DNN for speech enhancement based on perceptuallymotivated black-box cost function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yatabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">iMetricGAN: Intelligibility enhancement for speech-in-noise using generative adversarial network-based metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boosting objective scores of speech enhancement model through metricgan post-processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. APSIPA</title>
		<meeting>APSIPA</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quality-Net: An end-to-end non-intrusive speech quality assessment model based on blstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with learned loss function: Speech enhancement with quality-net to improve perceptual evaluation of speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hashnet: Deep learning to hash by continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5608" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speech enhancement using self-adaptation and multihead self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yaiabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Maxuxama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2020</title>
		<meeting>ICASSP 2020</meeting>
		<imprint>
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformer with Gaussian weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HiFi-GAN: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phasen: A phase-andharmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI 2020</title>
		<meeting>AAAI 2020</meeting>
		<imprint>
			<biblScope unit="page" from="9458" to="9465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptual loss based speech denoising with an ensemble of audio pattern recognition and self-supervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Time-frequency maskingbased speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5039" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sergan: Speech enhancement using relativistic generative adversarial networks with gradient penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verhulst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepmmse: A deep learning approach to mmse-based noise power spectral density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1404" to="1415" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SERIL: Noise adaptive speech enhancement using regularization-based incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
