<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Recognition Using Deep Multi-Pose Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Abdalmageed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Rawls</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Harel</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Open University Raanana</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Open University Raanana</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatuporn</forename><surname>Toy Leksut</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungyeon</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems University of Southern</orgName>
								<orgName type="institution">California University of Southern California Marina Del Rey</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face Recognition Using Deep Multi-Pose Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce our method and system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several posespecific deep convolutional neural network (CNN) models to generate multiple pose-specific features. 3D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, CNN layer selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPA's CS2 and NIST's IJB-A in both verification and identification (i.e. search) tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has been one of the most challenging and attractive areas of computer vision. The goal of face recognition algorithms is to answer the question, who is this person in a given image or video frame? Face recognition algorithms generally try to address two problems -identify verification and subject identification. Face verification, as known as the 1 : 1 matching problem <ref type="bibr" target="#b10">[11]</ref>, answers the question, are these two people actually the same?, while face identification, also known as the 1 : N problem <ref type="bibr" target="#b10">[11]</ref>, answers the question, who is this person, given a database of faces?</p><p>Labeled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b7">[8]</ref> is considered one of the most important benchmarks for face recognition research. Recent advances, especially in applying convolutional neural networks (CNN) to face recognition, enabled researchers of achieving close to 100% recognition rates <ref type="bibr" target="#b5">[6]</ref>. However, face recognition problem is far from solved, especially in an uncontrolled environment with extreme pose, illumination, expression and age variations. Indeed, as discussed in <ref type="bibr" target="#b10">[11]</ref>, state-of-the-art commercial and open-source face recognition systems performed far less than satisfactory on the recently released National Institute of Standards and Technology's (NIST) IARPA Janus Benchmark-A (IJB-A), which is considered much more challenging, than LFW, in terms of the variability in pose, illumination, expression, aging, resolution, etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>IJB-A dataset has been quickly adopted by the research community. In <ref type="bibr" target="#b18">[19]</ref>, the authors represent a face image as a feature vector using a trained convolutional neural network and hash this feature vector to achieve fast face search. Chowdhury et al. in <ref type="bibr" target="#b14">[15]</ref> fine-tunes a trained base-model of a symmetric bilinear convolutional neural network (BCNN) to extract face features, and trains subject-based SVM classifiers to identify individuals. In <ref type="bibr" target="#b4">[5]</ref>, Patel et al. use a trained CNN model to represent a face, and additional joint-Bayesian metric learning to assess the similarity between two face representations.</p><p>In this paper we present our face recognition pipeline using a novel multi-pose deep face representation. Unlike previous efforts <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref> that consider pose variations implicitly, we explicitly leverage the variations of face poses by <ref type="bibr" target="#b0">(1)</ref> representing a face with different (aligned and rendered) poses using different pose-specific CNNs and (2) perform face similarity comparisons only using same pose CNNs. To our best knowledge, this is the first attempt to use multi-pose in face recognition. This novel approach is applied to IJB-A dataset and is shown to surpasses the stateof-the-art algorithms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>, without additional domain adaptation or metric learning.</p><p>The remainder of this paper is organized as follows. Section 2 discusses the training and testing datasets. In Section 3 we discuss our conceptual single representation along with instances of different pose experts, and the proposed multi-pose representation for face recognition. Evaluation protocols and results of the proposed recognition pipeline are presented in Section 4. We conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Facial Datasets Employed</head><p>We use CASIA-WebFace <ref type="bibr" target="#b21">[22]</ref> for training and both IJB-A <ref type="bibr" target="#b10">[11]</ref> and IARPA's Janus CS2 for evaluation. Following is a brief description of these datasets. <ref type="bibr" target="#b21">[22]</ref> dataset is the largest known public dataset for face recognition. Specifically, CASIA-WebFace contains 10, 575 subjects with a total of 494, 414 images. We perform the following data clean up steps before using WebFace for training our pose-specific CNN models -(1) exclude images of all subjects in WebFace that overlap with the testing and evaluation data sets, (2) remove all images of subject with fewer than five images in the dataset and (3) remove images with undetectable faces. Approximately 400, 000 images for 10, 500 subjects remain of which we use 90% and 10% for training and validation, respectively, of the CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CASIA-WebFace</head><p>The objective of IARPA's Janus program is push the frontiers of unconstrained face recognition. Janus datasets contains images that have full-pose variations, as illustrated by the pose distribution histogram in <ref type="figure" target="#fig_0">Figure 2</ref>, and provide manual annotations, including facial bounding box, seed landmarks of two-eye and nose base, lighting conditions and some subject attributes, such as ethnicity, gender, etc. These datasets introduce the novel concept of a template, which is a collection of images and videos of the same subject. IJB-A and CS2 share 90% of image data. However, they differ in evaluation protocols. In particular, IJB-A includes protocols for both open-set face identification (i.e. 1 : N search) and face verification (i.e. 1 : 1 comparison), while CS2 focuses on closed-set identification. The datasets are divided into gallery and probe subsets. The gallery subset is a collection of templates for subjects to be enrolled in the system, while the probe subset is a collection of templates for unknown subjects for testing and evaluation purposes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Face Recognition Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From Images to Representations</head><p>Given an input face image X, a typical recognition pipeline applies a sequence of processing steps in order to transform the 2D color image into a fixed-dimensional feature vector representation, as shown in <ref type="figure" target="#fig_2">Figure 1</ref>. To simplify future discussions, we refer to this transformation process as a function rep(·), and the resulting feature representation of an image X as F X = rep(X). In the following we discuss the processing steps of rep(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Facial Landmark Detection and Face Alignment</head><p>A facial landmark detection algorithm, lmd, takes a face image X and estimates n predefined key points, such as eye corners, mouth corners, nose tip, etc., as shown in Equation </p><p>where P x and P y denote a key point's coordinate along x and y axes, respectively. Depending on the number of defined facial key points, landmarks can be roughly classified into two types -(1) sparse landmark, such as in the 5point multitask facial landmark dataset <ref type="bibr" target="#b22">[23]</ref> and (2) dense landmark, such as in the 68-point 300w dataset <ref type="bibr" target="#b15">[16]</ref>. Regardless of the facial landmark detector used, landmark detection produces anchor points of a face as a preprocessing step to face recognition and other face analysis tasks. Detected landmarks are then used to estimate the roll angel of the face pose. The image is then pose-corrected by rotating the image. Since landmark detection algorithms are often trained with imbalanced datasets, which creates some detection errors, the landmark detection is re-applied to the pose-corrected image, as shown in <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>In order for downstream processing steps, such as feature extraction and recognition, to work on consistent feature spaces, all face images must be brought to a common coordinate system through face alignment, which reduces pose variations. This is attained by aligning detected landmarks with a set of landmarks from a reference model using distance minimization. If the reference model is in 2D, the process is called in-plane alignment and if it is a 3D model the process is called out-of-plane alignment. In our recognition pipeline, we use both non-reflective similarity transformation for in-plane alignment, and a perspective transformation for out-of-plane alignment.</p><p>Specifically, given a set of reference facial landmarks lmd(R), we seek similarity transformation T to align a face image to the reference model, such that</p><formula xml:id="formula_1">T * = argmin T T [ lmd(X) | 1 ] − [ lmd(R) | 1] 2 2 (2)</formula><p>where [lmd(X) | 1] is simply an expansion of lmd(X) by adding an all-one vector 1, and T is a homogeneous matrix defined by rotation angle θ, scaling factor s, and translation vector [t x , t y ], as shown in Equation <ref type="formula" target="#formula_2">(3)</ref>.</p><formula xml:id="formula_2">T =   s cos θ −s sin θ t x s sin θ s cos θ t y 0 0 1  <label>(3)</label></formula><p>Unlike 2D alignment, our 3D alignment relies on a 3D generic face shape model as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, although we still need the detected facial landmark to estimate initial face shape. Once we successfully fit our generic 3D face shape model to a given face image, we can render face images with arbitrary yaw-pitch-roll parameters (details can be found in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b6">[7]</ref>). For example, <ref type="figure" target="#fig_3">Figure 3</ref>(c) shows rendered face at different yaw and pitch values. <ref type="figure" target="#fig_3">Figure 3</ref>(d) shows different face images aligned to the same yaw-pitchroll configuration. Finally, aligned images are cropped to a fixed size of 160×128 pixels, which is used in subsequent recognition steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Feature Extraction and Domain Adaptation</head><p>Feature extraction is an essential module in our representation pipeline, for its responsibility to produce features that provide the power for discriminating between different subjects. Classically, face recognition systems used hand-crafted features. For example, local binary pattern (LBP) <ref type="bibr" target="#b0">[1]</ref> and its variants <ref type="bibr" target="#b19">[20]</ref> have been effectively used for face recognition, and Gabor wavelet features have also been widely used <ref type="bibr" target="#b20">[21]</ref>. Since many of such hand-crafted features involve hyper-parameters, multiscale, multi-resolution, and/or multi-orientation features have also been useful for face recognition, although they require more computations and occupy more space. Often, hand-crafted features do not require learning from data. However, with the availability of training data, higher-level feature representations could be learned <ref type="bibr" target="#b16">[17]</ref>. This learning process still depends on hand-crafted features, rather than raw data.</p><p>In contrast, recent advances in deep learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> demonstrate that feature representations from raw data can be learned along with the recognition task, where all layers, except the last one, in a deep neural network (DNN) are considered to be feature learning layers. As a result, one may use DNNs for feature extraction. Details of how we learn feature extraction using DNNs are explained in Section 3.2. For now, we deal with a any feature extractor as a black box that takes an aligned and cropped image as as input and produces a high-dimensional feature vector.</p><p>The objective of domain adaptation is to close the gap between training and testing data. We use principal component analysis (PCA) to learn the orthogonal transformation from a testing dataset, such that 95% of its original feature variance is kept, because it does not require any labeling and is bounded by a low computational complexity. Although feature dimension reduction is not the main purpose here, possible noise is dropped along with the discarded feature dimensions. Although one may directly use adapted features for matching, additional normalization can be very helpful, such as L 2 and power normalization <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Face Representation in Practice</head><p>Although one may implement the discussed conceptual pipeline in many different ways, we believe that the two most important components of a face recognition pipeline are alignment and feature extraction. Therefore, we mainly focus on the combination of face alignment and face feature extraction, and study the configurations shown in <ref type="table" target="#tab_0">Table  1</ref>. In this table, HDLBP stands for high-dimensional local binary pattern <ref type="bibr" target="#b0">[1]</ref>, and AlexNet, VGG16 and VGG19 refer to CNN architectures of <ref type="bibr" target="#b11">[12]</ref>, and config-D and E in <ref type="bibr" target="#b17">[18]</ref>, respectively; "Ref.</p><p>Model" denotes the reference model used in face alignment, and "avg-all-face-lmd" means that we use the averaged landmark vector of all training data, while "gene-face-yaw@45" means that we use the generic 3D face model at 45 • yaw and 0 • pitch (see <ref type="figure" target="#fig_3">Figure 3</ref>(b)). For HDLBP feature extraction, we compute a patch-based LBP histogram of 6, 098 predefined facial key points, and then concatenate these histograms into a feature vector. out-of-plane gene-face-yaw@0 AlexNet 4,000 ALEX-FY45 out-of-plane gene-face-yaw@45 AlexNet 4,000 VGG16-AF in-plane avg-face-lmd VGG16 4,000 VGG19-AF in-plane avg-all-face-lmd VGG19 4,000 VGG19-FF in-plane avg-frontal-face-lmd VGG19 4,000 VGG19-PF in-plane avg-profile-face-lmd VGG19 4,000 VGG19-FY0 out-of-plane gene-face-yaw@0 VGG19 4,000 VGG19-FY45 out-of-plane gene-face-yaw@45 VGG19 4,000 VGG19-FY75 out-of-plane gene-face-yaw@75 VGG19 4,000</p><p>From this point, we concentrate on the discussion of obtaining deep learning features. CASIA-WebFace is used as our standard data set for training and validating different CNN architectures. We preprocessed the dataset in order to obtain aligned and cropped version of the data prior to training the CNNs.</p><p>Regardless of the CNN architecture used, we always use CASIA-WebFace as our training and validation dataset. Depending on the representation pipeline used, we preprocess all CASIA-WebFace data until face alignment. This produces approximately 400,000 samples from 10,500 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Transfer Learning</head><p>Since the amount of face data in CASIA-WebFace is relatively limited (400, 000 images for 10, 500 subjects), we initialize our CNNs with pre-trained CNNs using the publicly available models from the ILSVRC2014 image classification task, whose ImageNet dataset contains more than 100 million images for 1000 classes. In order to use a pretrained model as an initial model for the CASIA-WebFace recognition task, we keep all weights of all CNN layers except those from the last dense layer, since the number of output nodes of the last layer must correspond to the number of subject in WebFace (i.e. 10, 500) and reinitialize this layer with random weights. After we construct this base model, we begin transfer learning process <ref type="bibr" target="#b12">[13]</ref> for face recognition in using Caffe library, and obtain new CNN models of ALEX-AF, VGG16-AF, and VGG19-AF, modified to match WebFace subjects and adapted to face feature extraction. Caffe provides pretrained models of AlexNet, VGG16 and VGG19 and automatically performs the required preprocessing, such as resizing an input image to 224×224 pixels, subtracting an average image, etc. With respect to model training, we use the stochastic gradient descent optimizer with the learning rate starting at 1e − 3 and reducing it to 1e-4 as the convergence plateaus.</p><p>To verify the performance of transfer learning, we cluster the output of fc7 layer. <ref type="figure" target="#fig_5">Figure 4</ref> shows cluster-wise average faces from the CASIA-WebFace <ref type="bibr" target="#b21">[22]</ref> data using raw RGB values and the fc7 layer features of VGG19-AF. It can be seen that VGG19-AF learns many important facial attributes, such as gender, face shape, ethnicity, hair color, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pose-wise Fine Tuning</head><p>Once we have our face recognition baseline CNN, we then apply fine tuning to learn pose-specific features. For example, we use the ALEX-AF model as our base model, and further train ALEX-FF and ALEX-PF, which focus on near-frontal and near-profile faces, respectively. Based on the ALEX-PF CNN, we can further fine-tune it using ren-  dered faces aligned at 0 • yaw, and obtain ALEX-FY0 CNN. The essence behind this CNN training is that we always only move one step forward. For example, we do not use AlexNet as the base model for ALEX-FF because ALEX-Net's training data contains objects from different poses.</p><p>Similarly, we do not use the near-profile model VGG19-PF as the base model for VGG19-FY0, because the near-frontal CNN model VGG-FF is more appropriate in the sense that rendered faces at 0 • yaw are also frontal. <ref type="table" target="#tab_1">Table 2</ref> gives the full set of descriptions of how we learn these CNNs for face recognition. At the end of each fine tuning process, the last (i.e. classification) layer of the CNN is discarded and the CNN is used as a feature extractor by concatenating the outputs of one or more layers of the CNN. Note that we use different versions of preprocessed CASIA-WebFace data to train different CNNs. Specifically, all real refers to all CASIA-WebFace images, real frontal and real profile only refer to those whose yaw angles are close to 0 • and 75 • , respectively. <ref type="figure" target="#fig_6">Figure 5</ref> shows the averaged images of different training partitions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Modal Representation for Recognition</head><p>Without loss of generality, assume that we have k distinctive representations of a single input facial image X, namely,</p><formula xml:id="formula_3">R(X) = {rep 1 (X), rep 2 (X), · · · , rep k (X)} (4)</formula><p>where each representation is obtained by applying the conceptual pipeline of <ref type="figure" target="#fig_2">Figure 1</ref> using a different feature extractor. When this general multi-modal representation only involves pose-specific models, i.e. "fine tuned" models in <ref type="table" target="#tab_1">Table 2</ref>, we call this representation a multi-pose representation.</p><p>Once we define the face representation of Equation <ref type="formula">(4)</ref>, we compute the similarity between two face images in two steps -(1) compare a similarity score between features from the same representation pipeline, and (2) fuse similarities scores across different representation pipelines, as shown in <ref type="figure" target="#fig_7">Figure 6</ref>. Although one may use various ways to compute feature similarities and to fuse a set of scores, we simply use direct representation-to-representation (i.e. feature-to-feature) comparison in pair-wise fashion to avoid having to construct new classifiers based on the target (i.e. testing) data set. Specifically, we use Equation <ref type="formula" target="#formula_4">(5)</ref> to compute the pair-wise image similarity score</p><formula xml:id="formula_4">sim(X, Y )=fuse {rsim(rep j (X), rep j (Y )}| k j=1 )<label>(5)</label></formula><p>where we use a cosine similarity metric defined in Equation <ref type="bibr" target="#b5">(6)</ref> to quantify the similarity between two features, and use softmax weights to fuse different scores, as shown in Equation <ref type="formula" target="#formula_5">(7)</ref>, with the bandwidth parameter β = 10.</p><formula xml:id="formula_5">rsim(rep(X), rep(Y )) = &lt; rep(X), rep(Y ) &gt; rep(X) · rep(Y ) (6) fuse({s 1 , s 2 , · · · , s k }) = k i=1 s i · exp(β · s k ) k i=1 exp(β · s k )<label>(7)</label></formula><p>As mentioned before, since the IJB-A dataset uses the notion of templates, we need to compare the similarity between two templates X and Y instead of two images. Therefore, we use one more step of softmax fusion over all pairwise scores from images in two templates, as shown in Equation <ref type="formula">(8)</ref>.</p><formula xml:id="formula_6">tsim(X, Y) = fuse ({sim(X, Y )|X ∈ X, Y ∈ Y}) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Time Complexity</head><p>We use a single NVIDIA Tesla K40 GPU for training and testing. Fully training and fine tuning a VGG19-AFlike CNN model takes approximately one week with fully preprocessed images. Testing a single IJB-A/CS2 data split with one CNN model takes roughly one hour using the proposed facial representation pipeline (see <ref type="figure" target="#fig_2">Fig. 1</ref>). The most time consuming step in testing is PCA adaptation and it costs about 20 minutes for a single IJB-A/CS data split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics and Protocols</head><p>IJB-A dataset contains two types of protocols, namely search and compare. The search protocol measures the accuracy of open-set and closed-set search among N gallery templates in terms of the true acceptance rate (TAR) at various false acceptance rates (FAR) as well as receiver operating characteristic (ROC) plots. The compare protocol measures the verification accuracy between two templates. However, IJB-A carefully designed challenging template pairs by ensuring that subjects of templates have the same gender, and that their skin colors do not differ more than one level. Metrics of rank-1, rank-5, and the missing rate correspond to false alarm rates of 1 10 and 1 100 . Detailed descriptions of evaluation protocols and metrics can be found in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Overview</head><p>In the rest of this section, we report the results of several experiments based on our proposed face representation and matching scheme. Since each experiment may have its own baseline, comparing performances across different experiments may be inacurate. Although CS2 and IJBA both contains 10 splits of data, we only report averaged scores across these 10 splits to save space. Because CS2 and IJBA provide many associated attributes for a given face image, we do use provided subject face bounding boxes and seed landmarks in most of the following experiments, except those that explicitly state not to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Selecting CNN Layers for Feature Extraction</head><p>As in many recognition problems, feature plays a core role in face recognition. Given a trained face recognition CNN, we may treat each layer as a feature, and also a collection of features from different layers. Therefore, it is interesting to investigate which combination of features is the best for face recognition. In this experiment, we exhaustively try all possible layer combinations of the last six layers of each CNN architecture, and report face recognition performance on selected combinations with reasonable performance in <ref type="table" target="#tab_2">Table 3</ref>, where 'x' indicates that this layer of feature is used.</p><p>As one can see, the same collection of features means something different for ALEX-AF and VGG19-AF. Especially, we notice that when we only use the fc7 layer, the second-to-last dense layer, it is the best among all possible layer combinations for VGG19-AF, but not for ALEX-AF. This result clearly confirms that an optimal feature can be a set of features from different layers. From now on, all of our future experiments on ALEX-* representations will be based on the feature combination of (pool5, fc7, fc8, prob), while VGG* representations are based on the feature combination of (fc8). Furthermore, we compare four face recognition pipelines based on different features; specifically HDBLP, ALEX-AF, VGG16-AF, and VGG19-AF as shown in <ref type="table" target="#tab_3">Table 4</ref>. It is clear that deep learning features outperform classic HDLBP features by a large margin, even though we spend an almost comparable amount of time for grid searching over the HDLBP hyper-parameter space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of Landmark Detection Algorithm</head><p>We mainly focus on evaluating facial landmark performance for face recognition. Our baseline of face recognition uses the ALEX-AF representation with cosine similarity and softmax score fusion, and the dataset used is CS2. Four state-of-the-art facial landmarks are used -(1) DLIB <ref type="bibr" target="#b8">[9]</ref> with 68 points, (2) FPS3K <ref type="bibr" target="#b3">[4]</ref> with 68 points, (3) TD-CNN <ref type="bibr" target="#b22">[23]</ref> with 5 points, and (4) CLNF with 68 points <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and its variant CLNFs that use seed landmarks provided in CS2 <ref type="bibr" target="#b10">[11]</ref> for the estimation of an initial face shape. Note that we use DLIB, FPS3K and TDCNN out-of-shelf, and because they do not provide an interface to set the initial facial landmarks, we cannot report their performance with seed landmarks. Detailed results are listed in <ref type="table" target="#tab_4">Table 5</ref>. All of these landmark detectors share the same set of face bounding boxes. In general, different landmark detectors do not make a huge performance difference compared to different features, and this implies that CNN features attain a certain level of spatial invariance. On the other hand, initial seed landmarks do help to largely improve face recognition performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Single versus Multi-Pose Representations</head><p>We investigate the influence of using three different numbers of representations, namely single, quadruple, and quintuple. Specifically, single uses only one of *-AF, *-FF, *-PF, *-FY0, *-FY45, *-FY75 representations, quadruple uses the representation tuple of ( *-FF, *-PF, *-FY0, *-FY45 ) and quintuple uses the representation tuple of ( *-FF, *-PF, *-FY0, *-FY45, *-FY75 ), where * denotes either ALEX or VGG19 CNN architecture. As shown in <ref type="table" target="#tab_7">Table 8</ref>, face recognition performance significantly improves as the number of pose representations increases, regardless of whether we use ALEX architecture or VGG19 architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparing to State-of-the-Art Methods</head><p>We now compare our VGG19-quintuple representation, namely (VGG19-FF, -PF, -FY0, -FY45, -FY75), to state-ofthe-art methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref>, along with the baseline GOTS and COTS from <ref type="bibr" target="#b10">[11]</ref> on the IJB-A dataset. It is clear that our multi-pose representation-based recognition pipeline outperforms other state-of-the-art methods. It is worthy to mention that the algorithm presented in <ref type="bibr" target="#b4">[5]</ref> involves both fine tuning on IJB-A data and uses metric learning using labeled IJB-A training data, while our algorithm is dataagnostic and is used out of the box on both CS2 and IJB-A without target domain specific tuning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a multi-pose representation for face recognition, which is a collection of face representations learned from specific face poses. We show that this novel representation significantly improves face recognition performance on IJB-A benchmark compared not only to the single best CNN representations but also those state-of-theart methods that heavily rely on supervised learning, such gallery fine-tuning and metric learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Face pose distribution in LFW and JANUS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Facial representation pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Out-of-plane face alignment via rendering: (a) reference generic 3D face shape; (b) face image with estimated 3D face shape; (c) rendered face at different yaw-and-pitch grids; (d) aligned faces at yaw 45 • and pitch-0 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>GMM 64 Clustering CASIA-WebFace data according to (a) raw RGB-values, and (b) VGG19-AF fc7 feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Averaged faces of different training partitions. (a) all real, (b) real frontal, (c) real profile, (d) rendered yaw0, (e) rendered yaw45, and (f) rendered yaw75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Facial matching overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Face representation pipelines.</figDesc><table><row><cell cols="2">Pip. Acronym Alignment</cell><cell cols="2">Ref. Model Feature Rep. Dim.</cell></row><row><cell>HLBP</cell><cell>in-plane</cell><cell>avg-face HDLBP</cell><cell>100,000</cell></row><row><cell>ALEX-AF</cell><cell>in-plane</cell><cell>avg-all-face-lmd AlexNet</cell><cell>4,000</cell></row><row><cell>ALEX-FF</cell><cell cols="2">in-plane avg-frontal-face-lmd AlexNet</cell><cell>4,000</cell></row><row><cell>ALEX-PF</cell><cell cols="2">in-plane avg-profile-face-lmd AlexNet</cell><cell>4,000</cell></row><row><cell>ALEX-FY0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Deep learning features for face recognition</figDesc><table><row><cell cols="6">Pip. Acronym Learning Type Base CNN Model Training Partition</cell></row><row><cell>ALEX-AF</cell><cell>transfer</cell><cell></cell><cell>AlexNet</cell><cell></cell><cell>all real</cell></row><row><cell>ALEX-FF</cell><cell>finetune</cell><cell></cell><cell>ALEX-AF</cell><cell></cell><cell>real frontal</cell></row><row><cell>ALEX-PF</cell><cell>finetune</cell><cell></cell><cell>ALEX-AF</cell><cell></cell><cell>real profile</cell></row><row><cell>ALEX-FY0</cell><cell>finetune</cell><cell></cell><cell>ALEX-FF</cell><cell cols="2">rendered yaw0</cell></row><row><cell>ALEX-FY45</cell><cell>finetune</cell><cell></cell><cell>ALEX-PF</cell><cell cols="2">rendered yaw45</cell></row><row><cell>VGG16-AF</cell><cell>transfer</cell><cell></cell><cell>VGG16</cell><cell></cell><cell>all real</cell></row><row><cell>VGG19-AF</cell><cell>transfer</cell><cell></cell><cell>VGG19</cell><cell></cell><cell>all real</cell></row><row><cell>VGG19-FF</cell><cell>finetune</cell><cell></cell><cell>VGG19-AF</cell><cell></cell><cell>real frontal</cell></row><row><cell>VGG19-PF</cell><cell>finetune</cell><cell></cell><cell>VGG19-AF</cell><cell></cell><cell>real profile</cell></row><row><cell>VGG19-FY0</cell><cell>finetune</cell><cell></cell><cell>VGG19-FF</cell><cell cols="2">rendered yaw0</cell></row><row><cell>VGG19-FY45</cell><cell>finetune</cell><cell></cell><cell>VGG19-PF</cell><cell cols="2">rendered yaw45</cell></row><row><cell>VGG19-FY75</cell><cell>finetune</cell><cell></cell><cell>VGG19-FY45</cell><cell cols="2">rendered yaw75</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>DNN Features for Face Recognition in CS2 .374 .457 .572 .687 .688 .662 .703 .549 FAR@TAR=0.85 .039 .058 .060 .102 .042 .452 .044 .041 .046 .038 RANK@10 .875 .844 .820 .741 .867 .885 .878 .868 .883 .872 Metric VGG19-AF TAR@FAR=0.01 .816 .724 .548 .749 .815 .788 .789 .753 .748 .816 FAR@TAR=0.85 .018 .018 .034 .030 .017 .022 .023 .028 .027 .017 RANK@10 .909 .892 .879 .913 .909 .921 .919 .916 .913 .909</figDesc><table><row><cell>Layer Name</cell><cell></cell><cell cols="7">CNN Feature of Layer Combinations</cell></row><row><cell>prob</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>fc8</cell><cell>x</cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>fc7</cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell><cell></cell><cell>x</cell></row><row><cell>fc6</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell></row><row><cell>pool5</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell></row><row><cell>Metric</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ALEX-AF</cell><cell></cell><cell></cell></row><row><cell>TAR@FAR=0.01</cell><cell cols="2">.572 .403</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Feature Influences for Face Recognition in CS2</figDesc><table><row><cell cols="5">Metric HDLBP ALEX-AF VGG16-AF VGG19-AF</cell></row><row><cell>TAR@FAR=0.01</cell><cell>.274</cell><cell>.703</cell><cell>.779</cell><cell>.816</cell></row><row><cell>TAR@FAR=0.10</cell><cell>.511</cell><cell>.906</cell><cell>.918</cell><cell>.929</cell></row><row><cell>FAR@TAR=0.85</cell><cell>.680</cell><cell>.046</cell><cell>.025</cell><cell>.017</cell></row><row><cell>FAR@TAR=0.95</cell><cell>.930</cell><cell>.275</cell><cell>.228</cell><cell>.210</cell></row><row><cell>RANK@1</cell><cell>.398</cell><cell>.665</cell><cell>.739</cell><cell>.773</cell></row><row><cell>RANK@5</cell><cell>.596</cell><cell>.834</cell><cell>.862</cell><cell>.880</cell></row><row><cell>RANK@10</cell><cell>.689</cell><cell>.883</cell><cell>.895</cell><cell>.909</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Landmark Influence for Face Recognition in CS2</figDesc><table><row><cell cols="4">Metric DLIB FPS3K TDCNN CLNF CLNF-s</cell></row><row><cell>TAR@FAR=0.01 .689 .708</cell><cell>.692</cell><cell>.682</cell><cell>.703</cell></row><row><cell>TAR@FAR=0.10 .894 .903</cell><cell>.906</cell><cell>.878</cell><cell>.906</cell></row><row><cell>FAR@TAR=0.85 .056 .049</cell><cell>.049</cell><cell>.062</cell><cell>.046</cell></row><row><cell>FAR@TAR=0.95 .249 .237</cell><cell>.228</cell><cell>.504</cell><cell>.275</cell></row><row><cell>RANK@1 .658 .636</cell><cell>.608</cell><cell>.661</cell><cell>.665</cell></row><row><cell>RANK@5 .821 .804</cell><cell>.803</cell><cell>.807</cell><cell>.834</cell></row><row><cell>RANK@10 .871 .861</cell><cell>.861</cell><cell>.862</cell><cell>.883</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Representation Influences for Face Recognition on CS2</figDesc><table><row><cell cols="3">Metric AF -FF -PF -FY0 -FY45 -FY75 quadruple quintuple</cell></row><row><cell>ALEX-Arch.</cell><cell></cell><cell></cell></row><row><cell>TAR@FAR=0.01 .703 .635 .332 .754 .797 .802</cell><cell>.814</cell><cell>.814</cell></row><row><cell>TAR@FAR=0.10 .906 .729 .443 .913 .935 .937</cell><cell>.939</cell><cell>.941</cell></row><row><cell>FAR@TAR=0.85 .046 .376 .558 .036 .020 .019</cell><cell>.017</cell><cell>.017</cell></row><row><cell>FAR@TAR=0.95 .275 .485 .661 .249 .151 .149</cell><cell>.143</cell><cell>.126</cell></row><row><cell>RANK@1 .665 .576 .263 .706 .751 .753</cell><cell>.781</cell><cell>.799</cell></row><row><cell>RANK@5 .834 .694 .367 .844 .883 .882</cell><cell>.898</cell><cell>.906</cell></row><row><cell>RANK@10 .883 .723 .419 .889 .918 .920</cell><cell>.928</cell><cell>.936</cell></row><row><cell>VGG19-Arch.</cell><cell></cell><cell></cell></row><row><cell>TAR@FAR=0.01 .816 .688 .364 .806 .858 .860</cell><cell>.873</cell><cell>.897</cell></row><row><cell>TAR@FAR=0.10 .929 .738 .453 .930 .948 .948</cell><cell>.950</cell><cell>.959</cell></row><row><cell>FAR@TAR=0.85 .017 .489 .659 .020 .009 .009</cell><cell>.006</cell><cell>.003</cell></row><row><cell>FAR@TAR=0.95 .210 .616 .763 .171 .112 .111</cell><cell>.101</cell><cell>.065</cell></row><row><cell>RANK@1 .733 .655 .305 .769 .817 .802</cell><cell>.854</cell><cell>.865</cell></row><row><cell>RANK@5 .880 .723 .397 .881 .914 .913</cell><cell>.927</cell><cell>.934</cell></row><row><cell>RANK@10 .909 .743 .439 .912 .936 .936</cell><cell>.946</cell><cell>.949</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on CS2 Metric COTS GOTS FV[5] DCNN-all[5] [19] Ours TAR@FAR=0.01 .581 .467 .411 .876 .733 .897 TAR@FAR=0.10 .761 .675 .704</figDesc><table><row><cell></cell><cell>.973</cell><cell>.895</cell><cell>.959</cell></row><row><cell>RANK@1 .551 .413 .381</cell><cell>.838</cell><cell>.820</cell><cell>.865</cell></row><row><cell>RANK@5 .694 .517 .559</cell><cell>.924</cell><cell>.929</cell><cell>.934</cell></row><row><cell>RANK@10 .741 .624 .637</cell><cell>.949</cell><cell>-</cell><cell>.949</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: 1:N Results on IJB-A</cell><cell></cell><cell></cell></row><row><cell cols="7">Metric COTS GOTS [15] DCNN-all[5] [19] Ours</cell></row><row><cell></cell><cell></cell><cell cols="3">1:N (Search Protocol)</cell><cell></cell><cell></cell></row><row><cell cols="3">TAR@FAR=0.01 .406 .236</cell><cell>-</cell><cell>-</cell><cell>.733</cell><cell>.876</cell></row><row><cell cols="3">TAR@FAR=0.10 .627 .433</cell><cell>-</cell><cell>-</cell><cell>.895</cell><cell>.954</cell></row><row><cell cols="4">RANK@1 .443 .246 .588</cell><cell>.860</cell><cell>.820</cell><cell>.846</cell></row><row><cell cols="4">RANK@5 .595 .375 .797</cell><cell>.943</cell><cell>.929</cell><cell>.927</cell></row><row><cell>RANK@10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.947</cell></row><row><cell></cell><cell></cell><cell cols="3">1:1 (Verification Protocol)</cell><cell></cell><cell></cell></row><row><cell>TAR@FAR=0.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.787</cell></row><row><cell>TAR@FAR=0.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.911</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">Equal contributors and corresponding authors at wamageed, yue wu, srawls@isi.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research is mainly based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA's 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. We thank the NVIDIA Corporation for the donation of the Tesla K40 GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d constrained local model for rigid and non-rigid facial tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2610" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01722</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face recognition via multimodal deep face representation. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d face reconstruction from a single image using a single reference face shape. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01342</idno>
		<title level="m">Face identification with bilinear cnns</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (IC-CVW), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring bag of words architectures in the facial expression domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012. Workshops and Demonstrations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective unconstrained face recognition by combining multiple descriptors and learned background statistics. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1978" to="1990" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fusing local patterns of gabor magnitude and phase for face recognition. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
