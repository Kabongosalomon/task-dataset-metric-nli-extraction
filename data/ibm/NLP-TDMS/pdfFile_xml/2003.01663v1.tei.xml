<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holistically-Attracted Wireframe Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
							<email>xuenan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NC State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">NC State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
							<email>fudong-wang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<email>guisong.xia@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Holistically-Attracted Wireframe Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a fast and parsimonious parsing method to accurately and robustly detect a vectorized wireframe in an input image with a single forward pass. The proposed method is end-to-end trainable, consisting of three components: (i) line segment and junction proposal generation, (ii) line segment and junction matching, and (iii) line segment and junction verification. For computing line segment proposals, a novel exact dual representation is proposed which exploits a parsimonious geometric reparameterization for line segments and forms a holistic 4-dimensional attraction field map for an input image. Junctions can be treated as the "basins" in the attraction field. The proposed method is thus called Holistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed method is tested on two benchmarks, the Wireframe dataset [14] and the YorkUrban dataset [8]. On both benchmarks, it obtains state-of-the-art performance in terms of accuracy and efficiency. For example, on the Wireframe dataset, compared to the previous state-of-the-art method L-CNN [40], it improves the challenging mean structural average precision (msAP) by a large margin (2.8% absolute improvements), and achieves 29.5 FPS on single GPU (89% relative improvement). A systematic ablation study is performed to further justify the proposed method. * Corresponding author (a) Image (b) Learned Lines (c) HAWP (score&gt;0.9) (d) Junction Proposals (e) Enumerated Lines (f) L-CNN (score&gt;0.9)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivations and Objectives</head><p>Line segments and junctions are prominent visual patterns in the low-level vision, and thus often used as important cues/features to facilitate many downstream vision tasks such as camera pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>, image matching <ref type="bibr" target="#b34">[35]</ref>, image rectification <ref type="bibr" target="#b35">[36]</ref>, structure from motion (SfM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>, visual SLAM <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, and surface reconstruction <ref type="bibr" target="#b15">[16]</ref>. Both line segment detection and junction detection remain <ref type="bibr">Figure 1</ref>. Illustration of the proposed HAWP in comparison with L-CNN <ref type="bibr" target="#b39">[40]</ref> in wireframe parsing. The two methods adopt the same two-stage parsing pipeline: proposal (line segments and junctions) generation and proposal verification. They use the same junction prediction in (d) and verification modules. The key difference lies in the line segment proposal generation. L-CNN bypasses directly learning line segment prediction module and resorts to a sophisticated sampling based approach for generation line segment proposals in (e). Our HAWP proposes a novel line segment prediction method in (b) for more accurate and efficient parsing, e.g., the parsing results of the window in (c) and (f). challenging problems in computer vision <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Line segments and junctions are often statistically coupled in images. So, a new research task, wireframe parsing, is recently emerged to tackle the problem of jointly detecting meaningful and salient line segments and junctions with large-scale benchmarks available <ref type="bibr" target="#b13">[14]</ref>. And, end-to-end trainable approaches based on deep neural networks (DNNs) are one of the most interesting frameworks, which have shown remarkable performance.</p><p>In wireframe parsing, it can be addressed relatively better to learn a junction detector with state-of-the-art deep learning approaches and the heatmap representation (inspired by its widespread use in human pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>). This motivated a conceptually simple yet powerful wireframe parsing algorithm called L-CNN <ref type="bibr" target="#b39">[40]</ref>, which achieved stateof-the-art performance on the Wireframe benchmark <ref type="bibr" target="#b13">[14]</ref>. L-CNN bypasses learning a line segment detector. It develops a sophisticated and carefully-crafted sampling schema to generate line segment proposals from all possible candidates based on the predicted junctions, and then utilizes a line segment verification module to classify the proposals. A large number of proposals are entailed for achieving good results at the expense of computational costs. And, ignoring line segment information in the proposal stage may not take full advantage of the deep learning pipeline for further improving performance.</p><p>On the other hand, without leveraging junction information in learning, the recently proposed attraction field map (AFM) based approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> are the state-of-the-art methods for line segment detection. AFM is not strictly end-to-end trainable. The reparameterization of pixels in the lifting process is for lines, instead of line segments (i.e., we can only infer a line with a given displacement vector, and that is why the squeezing module is needed).</p><p>In this paper, we are interested in learning an end-to-end trainable and fast wireframe parser. First, we aim to develop an exact dual and parsimonious reparameterization scheme for line segments, in a similar spirit to the AFM <ref type="bibr" target="#b32">[33]</ref>, but without resorting to the heuristic squeezing process in inference. Then, we aim to tackle wireframe parsing by leveraging both line segment and junction proposals to improve both accuracy and efficiency and to eliminate the carefully-crafted sampling schema as done in L-CNN <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Method Overview</head><p>In general, a parsing algorithm adopts two phases as proposed in the generic image parsing framework <ref type="bibr" target="#b26">[27]</ref>: proposal generation and proposal verification, which are also realized in the state-of-the-art object detection and instance segmentation framework <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref>. The current state-of-the-art wireframe parser, L-CNN <ref type="bibr" target="#b39">[40]</ref> follows the two-phase parsing paradigm. The proposed method in this paper also adopts the same setup. As illustrated in <ref type="figure" target="#fig_0">Fig. 1 and Fig. 2</ref>, the proposed method consists of three components: i) Proposal initialization: line segment detection and junction detection. Given an input image, it first passes through a shared feature backbone (e.g., the stacked Hourglass network <ref type="bibr" target="#b21">[22]</ref>) to extract deep features. Then, for junction detection, we adopt the same head regressor based on the heatmap representation as done in L-CNN <ref type="bibr" target="#b39">[40]</ref> (Section 4.2), from which the top-K junctions are selected as initial junction proposals. For computing line segment proposals, a novel method is proposed (Section 4.1).</p><p>ii) Proposal refinement: line segment and junction match- ing. The matching is to calculate meaningful alignment between line segment initial proposals and junction initial proposals. In the refinement (Section 4.3), a line segment proposal is kept if its two end-points are supported by two junction proposals. If a junction proposal does not find any support line segment proposal, it will be removed.</p><p>iii) Proposal verification: line segment and junction classification. The verification process is to classify (doublecheck) the line segments and junctions from the proposal refinement stage. We utilize the same verification head classifier (Section 4.4) as done in L-CNN <ref type="bibr" target="#b39">[40]</ref>, which exploits a Line-of-Interest (LOI) pooling operation to compute features for a line segment, motivated by the Region-of-Interest (ROI) pooling operation used in the popular two-stage R-CNN frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Geometrically speaking, the proposed wireframe parser is enabled by the holistic 4-D attraction field map and the "basins" of the attraction field revealed by junctions. We thus call the proposed method a Holistically-Attracted Wireframe Parser (HAWP). The proposed HAWP is end-to-end trainable and computes a vectorized wireframe for an input image in single forward pass. The key difference between our HAWP and the current state-of-the-art L-CNN <ref type="bibr" target="#b39">[40]</ref> approach is the novel line segment reparameterization and its end-to-end integration in the parsing pipeline. Our HAWP outperforms L-CNN by a large margin in terms of both accuracy and efficiency (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work and Our Contributions</head><p>The fundamental problem in wireframe parsing is to learn to understand the basic physical and geometric constraints of our world. The problem can date back to the pioneering work of understanding Blocks World by Larry Roberts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b11">12]</ref> at the very beginning of computer vision. We briefly review two core aspects as follows.</p><p>Representation of Line Segments. There is a big gap between the mathematically simple geometric representation of line segments (at the symbol level) and the raw image data (at the signal level). A vast amount of efforts have been devoted to closing the gap with remarkable progress achieved. Roughly speaking, there are three-level representations of line segments developed in the literature: (i) Edge-pixel based representations, which are the classic approaches and have facilitated a tremendous number of line segment detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. Many of these line segment detectors suffer from two fundamental issues inherited from the underlying representations: the intrinsic uncertainty and the fundamental limit of edge detection, and the lack of structural information guidance from pixels to line segments. The first issue has been eliminated to some extent by state-of-theart deep edge detection methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref>. (ii) Local support region based representations, e.g., the level-line based support region used in the popular LSD method <ref type="bibr" target="#b28">[29]</ref> and its many variants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. The local support region is still defined on top of local edge information (gradient magnitude and orientation), thus inheriting the fundamental limit. (iii) Global region partition based representation, which is recently proposed in the AFM method <ref type="bibr" target="#b32">[33]</ref>. AFM does not depend on edge information, but entails powerful and computationally efficient DNNs in learning and inference. AFM is not strictly an exact line segment representation, but a global region partition based line representation. The issue is addressed in this paper by proposing a novel holistic AFM representation that is parsimonious and exact for line segments.</p><p>Wireframe Parsing Algorithm Design. The recent resurgence of wireframe parsing, especially in an end-to-end way, is driven by the remarkable progress of DNNs which enables holistic map-to-map prediction (e.g., from raw images to heatmaps directly encoding edges <ref type="bibr" target="#b31">[32]</ref> or human keypoints <ref type="bibr" target="#b29">[30]</ref>, etc.). As aforementioned, the general framework of parsing is similar between different parsers. Depending on whether line segment representations are explicitly exploited or not, the recent work on wireframe parsing can be divided into two categories: (i) Holistic wireframe parsing, which include data-driven proposal generation for both line segments and junctions, e.g., the deep wireframe parser (DWP) <ref type="bibr" target="#b13">[14]</ref> presented along with the wireframe benchmark. DWP is not end-to-end trainable and relatively slow. (ii) Deductive wireframe parsing, which utilizes data-driven proposals only for junctions and resorts to sophisticated top-down sampling methods to deduce line segments based on detected junctions, e.g., PPG-Net <ref type="bibr" target="#b36">[37]</ref> and L-CNN <ref type="bibr" target="#b39">[40]</ref>. The main drawbacks of deductive wireframe parsing are in two-fold: high computational expense for line segment verification, and over-dependence on junction prediction. The proposed HAWP is in the first category, but enjoys end-to-end training and real-time speed.</p><p>Our Contributions. This paper makes the following main contributions to the field of wireframe parsing: -It presents a novel holistic attraction field to exactly characterize the geometry of line segments. To our knowledge, this is the first work that facilitates an exact dual representation for a line segment from any distant point in the image domain and that is end-to-end trainable. -It presents a holistically-attracted wireframe parser (HAWP) that extracts vectorized wireframes in input images in a single forward pass. -The proposed HAWP achieves state-of-the-art performance (accuracy and efficiency) on the Wireframe dataset <ref type="bibr" target="#b13">[14]</ref> and the YorkUrban dataset <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Holistic Attraction Field Representation</head><p>In this section, we present the details of our proposed holistic attraction field representation of line segments. The goal is to develop an exact dual representation using geometric reparameterization of line segments, and the dual representation accounts for non-local information and enables leveraging state-of-the-art DNNs in learning. By an exact dual representation, it means that in the ideal case it can recover the line segments in closed form. Our proposed holistic attraction field representation is motivated by, and generalizes the recent work called attraction field map (AFM) <ref type="bibr" target="#b32">[33]</ref>.</p><p>We adopt the vectorized representation of wireframes in images <ref type="bibr" target="#b13">[14]</ref>, that is we use real coordinates for line segments and junctions, rather than discrete ones in the image lattice. Denote by Λ and D ⊂ R 2 the image lattice (discrete) and the image domain (continuous) respectively. A line segment is denoted by its two end-points,l = (x 1 , x 2 ), where x 1 , x 2 ∈ D (2-D column vector). The corresponding line equation associated withl is defined by, l : a T l · x + bl = 0 where al ∈ R 2 and bl ∈ R, and they can be solved in closed form given the two end-points.</p><p>Background on the AFM method <ref type="bibr" target="#b32">[33]</ref>. To be selfcontained, we briefly overview the AFM method. The basic idea is to "lift" a line segment to a region, which facilitates leveraging state-of-the-art DNNs in learning. To compute the AFM for a line segment map, each (pixel) point p ∈ Λ is assigned to a line segmentl if it has the minimum distance tol among all line segments in a given image. The distance is calculated as follows. Let p be the point projected onto the line l of a line segmentl. If p is not on the line segmenẗ l itself, it will be re-assigned to one of the two end-points that has the smaller Euclidean distance. Then, the distance between p andl is the Euclidean distance between p and p . If p is assigned tol, it is reparameterized as p − p , i.e., the displacement vector in the image domain. The AFM of a line segment map is a 2-D vector field, which is created by reparameterizing all the (pixel) points in the image lattice Λ and often forms a region partition of the image lattice. A heuristic squeezing module is also proposed in the AFM work to recover a line segment from a 2-D vector field region (a.k.a., attraction).</p><p>The proposed holistic attraction field map. Strictly speaking, the displacement vector based geometric reparameterization scheme in the AFM method can only provide Rotation Scaling Translation <ref type="figure">Figure 3</ref>. An illustration for representing line segments in images with the related distant points. (a) shows one of the line segments (marked black with two blue endpoints), the corresponding support region (marked gray) calculated by AFM <ref type="bibr" target="#b32">[33]</ref> and one of the distant points in the support region. (b) shows the process of extending the attraction field representation and transforming the line segment into a standard local coordinate originated at p with a horizontal unit attraction vector.</p><p>complete information for the underlying line l of a line segmentl (when the projection is not outside the line segment). One straightforward extension of the AFM method is as follows. As illustrated in the first column in <ref type="figure">Fig. 3 (b)</ref>, consider a distant (pixel) point p outside a line segmentl with the projection point being on the line segment, if we not only use the displacement vector between p and its projection point, but also include the two displacement vectors between p and the two end-points of the line segment, we can reparameterize p by its 6-D displacement vector which can completely determine the line segment (i.e., an exact dual representation). There are some points (pixels) (e.g., points on any line segment) that should not be reparameterized to avoid degradation and are treated as the "background". Thus, we can create a 6-D attraction field and each line segment is supported by a region in the field map (shown by the gray region in <ref type="figure">Fig. 3 (a)</ref>). This was our first attempt in our study, and it turns out surprisingly that the 6-D attraction field can not be accurately and reliably learned in training with deep convolutional neural networks. We hypothesis that although the 6-D attraction field captures the sufficient and necessary information for recovering line segments in closed form, it is not parsimoniously and complementarily encoded using 3 displacement vectors for each point, which may increase the difficulty of learning even with powerful DNNs.</p><p>We derive an equivalent geometric encoding that is parsimonious and complementary as shown in the right two columns in <ref type="figure">Fig. 3</ref>. For a line segmentl, our derivation undergoes a simple affine transformation for each distant pixel point p in its support region. Let d be the distance between p andl, i.e., d = |a T l · p + bl| &gt; 0. We have, i) Translation: The point p is then used as the new coordinate origin. ii) Rotation: The line segment is then aligned with the vertical y-axis with the end-point x 1 on the top and the point p (the new origin) to the left. The rotation angle is denoted by θ ∈ [−π, π). iii) Scaling: The distance d is used as the unit length to normalize the x-/ y-axis in the new coordinate system.</p><p>In the new coordinate system after the affine transformation, let θ 1 and θ 2 be the two angles as illustrated in <ref type="figure">Fig. 3</ref> (θ 1 ∈ (0, π 2 ) and θ 2 ∈ (− π 2 , 0]). So, a point p in the support region of a line segmentl is reparameterized as, p(l) = (d, θ, θ 1 , θ 2 ), (1) which is completely equivalent to the 6-D displacement vector based representation and thus capable of recovering the line segment in closed form in the ideal case. For the "background" points which are not attracted by any line segment based on our specification, we encode them by a dummy 4-D vector (−1, 0, 0, 0).</p><p>The derived 4-D vector field map for a line segment map is called a holistic attraction field map highlighting its completeness and parsimoniousness for line segments, compared to the vanilla AFM <ref type="bibr" target="#b32">[33]</ref>.</p><p>High-level explanations of why the proposed 4-D holistic AFM is better than the 6-D vanilla AFM. Intuitively, for a line segment and a distant point p, we can view the support region (the grey one in <ref type="figure">Fig. 3 (a)</ref>) as "a face" with the point p being the left "eye" center and the line segment being the vertical "head bone". So, the affine transformation stated above is to "align" all the "faces" w.r.t. the left "eye" in a canonical frontal viewpoint. It is well-known that this type of "representation normalization" can eliminate many nuisance factors in data to facilitate more effective learning. Furthermore, the joint encoding that exploits displacement distance and angle effectively decouples the attraction field w.r.t. complementary spanning dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Holistically-Attracted Wireframe Parser</head><p>In this section, we present details of our Holistically-Attracted Wireframe Parser (HAWP).</p><p>Data Preparation. Let D train = {(I i , L i ); i = 1, · · · , N } be the set of training data where all the images I i 's are resized to the same size of Λ = H × W pixels, and L i is the set of n i annotated line segments in the image I i , L i = {l i,1 , · · · ,l i,ni } and each line segmenẗ l i,j = (x i,j,1 , x i,j,2 ) is represented by its two annotated end-points (the vectorized wireframe representation).</p><p>The groundtruth junction heatmap representations. We adopt the same settings used in L-CNN <ref type="bibr" target="#b39">[40]</ref>. For an image I ∈ D train (the index subscript is omitted for simplicity), the set of unique end-points from all line segments are the junctions, denoted by J. Then, we create two maps: the junction mask map, denoted by J , and the junction 2-D offset map, denoted by O. A coarser resolution is used in computing the two maps by dividing the image lattice into H × W bins (assuming all bins have the same size, B×B, i.e., </p><formula xml:id="formula_0">− 1 2 , 1 2 ) × [− 1 2 , 1 2 )</formula><p>. The groundtruth holistic attraction field map. It is straightforward to follow the definitions in Section 3 to compute the map for an image I ∈ D train . Denote by A be the map of the size H × W (the same as that of the two junction maps), which is initialized using the method in Section 3. Then, we normalize each entry of the 4-D attraction field vector (Eqn. (1)) to be in the range [0, 1). We select a distance threshold d max . We filter out the points in A if their d's are greater than d max by changing them to the "background" with the dummy vector (−1, 0, 0, 0). Then, we divide the distances (the first entry) of the remaining non-background points by d max . Here, d max is chosen such that all line segments still have sufficient support distant points (d max = 5 in our experiments). It also helps remove points that are far away from all line segments and thus may not provide meaningful information for LSD. For the remaining three entries, it is straightforward to normalize based on their bounded ranges. For example, an affine transformation is used to normalize θ to θ 2π + 1 2 . Feature Backbone. We chose the stacked Hourglass network <ref type="bibr" target="#b21">[22]</ref> which is widely used in human keypoint estimation and corner-point based object detection <ref type="bibr" target="#b16">[17]</ref>, and also adopted by L-CNN <ref type="bibr" target="#b39">[40]</ref>. The size of the output feature map is also H × W . Denote by F the output feature map for an input image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computing Line Segment Proposals</head><p>Line segment proposals are computed by predicting the 4-D AFM A from F. LetÂ be the predicted 4-D map.Â is computed by an 1 × 1 convolutional layers followed by a sigmoid layer. WithÂ, it is straightforward to generate line segment proposals by reversing the simple normalization step and the geometric affine transformation (Section 3). However, we observe that the distance (the first entry) is more difficult to predict in a sufficiently accurate way. We leverage an auxiliary supervised signal in learning, which exploits the distance residual, in a similar spirit to the method proposed for depth prediction in <ref type="bibr" target="#b5">[6]</ref>. In addition to predictÂ from F, we also compute a distance residual map, denoted by∆d, using one 1 × 1 convolutional layers followed by a sigmoid layer. The groundtruth for∆d, denoted by ∆d, is computed by the residual (the absolute difference) between the two distances in A andÂ respectively.</p><p>In training, channel-wise 1 norm is used as the loss function for both L(A,Â) and L(∆d,∆d). The total loss for computing line segments is the sum of the two losses, L LS = L(A,Â) + L(∆d,∆d). In inference, with the predictedd ∈Â and∆d ∈∆d (both are non-negative due to the sigmoid transformation), since we do not know the underlying sign of the distance residual, we enumerate three possibilities in updating the distance prediction, d (κ) =d + κ ·∆d, (2) where κ = −1, 0, 1. So, each distant point may generate up to three line segment proposals depending on whether the condition 0 &lt;d (κ) ≤ d max is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Junction Detection</head><p>Junction detection is addressed by predicting the two maps, the junction mask map and the junction offset map, from the feature map F. They are computed by one 1 × 1 convolutional layers followed by a sigmoid layer. Denote bŷ J andÔ the predicted mask map and offset map respectively. The sigmoid function for computing the offset map has an intercept −0.5. In training, the binary cross-entropy loss is used for L(J ,Ĵ ), and the 1 loss is used for L(O,Ô), following the typical setting in heatmap based regression for keypoint estimation tasks and consistent with the use in L-CNN <ref type="bibr" target="#b39">[40]</ref>. The total loss is the weighted sum of the two losses, L Junc = λ msk · L(J ,Ĵ ) + λ of f · J L(O,Ô), where represents element-wise product, and λ msk and λ of f are two trade-off parameters (we set λ msk and λ of f to 8.0 and 0.25 respectively in our experiments). In inference, we also apply the standard non-max suppression (NMS) w.r.t. a 3 × 3 neighborhood, which can be efficiently implemented by a modified max-pooling layer. After NMS, we keep the top-K junctions fromĴ . And, for a bin b, if J (b) &gt; 0, a junction proposal is generated with its position computed by x b +Ô(b) · w, where x b is the position of the junction pixel,Ô(b) is the learned offset vector, and w is a rescaling factor of the offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Line Segment and Junction Matching</head><p>Line segment proposals and junction proposals are computed individually by leveraging different information, and their matching will provide more accurate meaningful alignment in wireframe parsing. We adopt a simple matching strategy to refining the initial proposals. A line segment proposal from the initial set is kept if and only if its two endpoints can be matched with two junction proposals based on Euclidean distance with a predefined threshold τ (τ = 10 in all our experiments). A junction proposal will be removed if it does not match to any survived line segment proposal after refinement. After matching, line segments and junctions are coupled together, which will be further verified using a light-weight classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Line Segment and Junction Verification</head><p>Without loss of generality, letl be a line segment proposal after refinement. A simple 2-f c layer is used as the validation head. To extract the same-sized feature vectors in F (the output of the feature backbone) for different line segments of different length for the head classifier, the widely used RoIPool/RoIAlign operation in the R-CNN based object detection system <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> is adapted to line segments, and a simple LoIPool operation is used as done in L-CNN <ref type="bibr" target="#b39">[40]</ref>. The LoIPool operation first uniformly samples s points for a line segmentl. The feature for each sampled point is computed from F using bi-linear interpolation as done in the RoIAlign operation and the 1D max-pooling operator is used to reduce the feature dimension. Then, all the features from the s sampled points are concatenated as the feature vector for a line segment to be fed into the head classifier (s = 32 in all our experiments).</p><p>In training the verification head classifier, we assign positive and negative labels to line segment proposals (after refinement) based on their distances to the groundtruth line segments. A line segment proposal is assigned to be a positive sample if there is a groundtruth line segment and their distance is less than a predefined threshold η (η = 1.5 in all our experiments). The distance between two line segments is computed as follows. We first match the two pairs of end-points based on the minimum Euclidean distance. Then, the distance between the two line segments is the maximum distance of the two endpoint-to-endpoint distances. So, the set of line segment proposals will be divided into the positive subset and the negative subset.</p><p>As illustrated in <ref type="figure">Fig. 1(b)</ref>, the negative subset usually contains many hard negative samples since the proposed holistic AFM usually generates line segment proposals of "good quality", which is helpful to learn a better verification classifier. Apart from the learned positive and negative samples, we use a simple proposal augmentation method in a similar spirit to the static sampler used in L-CNN <ref type="bibr" target="#b39">[40]</ref>: We add all the groundtruth line segments into the positive set. We also introduce a set of negative samples that are generated based on the groundtruth junction annotations (i.e., line segments using the two end-points that do not correspond to any annotated line segment). During training, to avoid the class imbalance issue, we sample the same number, n, of positives and negatives (i.e., LoIs) from the two augmented subsets (n = 300 in all our experiments). We use binary cross entropy loss in the verification module. Denote by L V er the loss computed on the sampled LoIs.</p><p>The proposed HAWP is trained end-to-end with the following loss function,</p><formula xml:id="formula_1">L = L LS + L Junc + L V er .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present detailed experimental results and analyses of the proposed HAWP. Our reproducible Py-Torch source code will be released.</p><p>Benchmarks. The wireframe benchmark <ref type="bibr" target="#b13">[14]</ref> and the YorkUrban benchmark are used. The former consists of 5, 000 training samples and 462 testing samples. The latter includes 102 samples in total. The model is only trained on the former and tested on both.</p><p>Baselines. Four methods are used: LSD [29] 1 , AFM <ref type="bibr" target="#b32">[33]</ref>, DWP <ref type="bibr" target="#b13">[14]</ref>, and L-CNN <ref type="bibr" target="#b39">[40]</ref> (the previous state-of-the-art approach). The last three are DNN based approaches and the first one does not need training. The last two leverage junction information in training, and thus are directly comparable to the proposed HAWP.</p><p>Implementation Details. To be fair in comparison with L-CNN, we adopt the same hyper-parameter settings (including those defined in Section 4) when applicable in our HAWP. Input images are resized to 512 × 512 in both training and testing. For the stacked Hourglass feature backbone, the number of stacks, the depth of each Hourglass module and the number of blocks are 2, 4, 1 respectively. Our HAWP is trained using the ADAM optimizer <ref type="bibr" target="#b14">[15]</ref> with a total of 30 epochs on a single Tesla V100 GPU device. The learning rate, weight decay rate and batch size are set to 4 × 10 −4 , 1 × 10 −4 and 6 respectively. The learning rate is divided by 10 at the 25-th epoch. To further ensure apple-to-apple comparisons with L-CNN, we also re-train it using the same learning settings with slightly better performance obtained than those reported in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metric</head><p>We follow the accuracy evaluation settings used in L-CNN summarized as follows to be self-contained.</p><p>Structural Average Precision (sAP) of Line Segments <ref type="bibr" target="#b39">[40]</ref>. This is motivated by the typical AP metric used in evaluating object detection systems. A counterpart of the Intersection-over-Union (IoU) overlap is used. For each ground-truth line segmentl = (x 1 , x 2 ), we first find the set of parsed line segments each of which,l = (x 1 ,x 2 ), satisfies the "overlap", min (i,j)</p><formula xml:id="formula_2">x 1 −x i 2 + x 2 −x j 2 ≤ ϑ L ,<label>(4)</label></formula><p>where (i, j) = (1, 2) or (2, 1), and ϑ L is a predefined threshold. If the set of parsed line segments "overlapping" witḧ l is empty, the line segmentl is counted as a False Negative (FN). If there are multiple candidates in the set, the one with the highest verification classification score is counted as a True Positive (TP), and the rest ones will be counted as False Positives (FPs). A parsed line segment that does not belong to the candidate set of any groundtruth line segment is also counted as a FP. Then, sAP can be computed. To eliminate the influence of image resolution, the wireframe parsing results and the groundtruth wireframes are rescaled to the resolution of 128 × 128 in evaluation. We set the threshold ϑ to 5, 10, 15 and report the corresponding results, denoted by sAP 5 , sAP 10 , sAP <ref type="bibr" target="#b14">15</ref> . The overall performance of a wireframe parser is represented by the mean of the sAP values with different thresholds, denoted by msAP.</p><p>Heatmap based F-score, F H and AP H of Line Segments. These are traditional metrics used in LSD and wire-  <ref type="table">Table 1</ref>. Quantitative results and comparisons. Our propsed HAWP achieves state-of-the-art results consistently except for the FPS. The FPS of our HAWP is still significantly better than that of the three deep learning based methods. Note that for fair and apple-to-apple comparisons, we also retrained a L-CNN model using their latest released code and the same learning hyper-parameters used in our HAWP. Our retrained L-CNN obtained slightly better performance than the original one. † means that the post-processing scheme proposed in L-CNN <ref type="bibr" target="#b39">[40]</ref> is used. The FPS of L-CNN is computed without the post-processing. See text for details.  <ref type="figure">Figure 4</ref>. Precision-Recall (PR) curves of sAP <ref type="bibr" target="#b9">10</ref> and AP H for DWP <ref type="bibr" target="#b13">[14]</ref>, AFM <ref type="bibr" target="#b32">[33]</ref>, L-CNN <ref type="bibr" target="#b39">[40]</ref> and HAWP (ours) on the wireframe benchmark (the left two plots) and the YorkUrban benchmark (the right two plots). Best viewed in color and magnification.</p><p>frame parsing <ref type="bibr" target="#b13">[14]</ref>. Instead of directly using the vectorized representation of line segments, heatmaps are used, which are generated by rasterizing line segments for both parsing results and the groundtruth. The pixel-level evaluation is used in calculating the precision and recall curves with which F H and AP H are computed. Vectorized Junction Mean AP (mAP J ) <ref type="bibr" target="#b39">[40]</ref>. It is computed in a similar spirit to msAP of line segments. Let ϑ J be the thresold for the distance between a predicted junction and a groundtruth one. The mAP J is computed w.r.t. ϑ J = 0.5, 1.0, 2.0.</p><p>Speed. Besides accuracy, speed is also important in practice. We use the frames per second (FPS) in evaluation. For fair comparisons, we compute the FPS for different methods under the same setting: the batch-size is 1, and single CPU thread and single GPU (Tesla V100) are used. Note that the LSD <ref type="bibr" target="#b28">[29]</ref> method does not take advantage of GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Comparisons</head><p>Quantitative Results. <ref type="table">Table 1</ref> summarizes the results and comparisons in terms of the evaluation metric stated in Section 5.1. Our HAWP obtains state-of-the-art performance consistently. In terms of the challenging msAP metric, it outperforms L-CNN by 2.8% and 1.3% (absolute improvement) on the wireframe benchmark and the YorkUrban benchmark respectively. It also runs much faster than L-CNN with 89% relative improvement in FPS. AFM and DWP are relatively slow due to their non-GPU friendly postprocessing modules entailed for performance. In terms of the # Junctions # Proposals sAP 10 FPS # GT Lines L-CNN <ref type="bibr" target="#b39">[40]</ref> 159.  heatmap based evaluation metric, our HAWP is also significantly better than L-CNN regardless of the post-processing module proposed in L-CNN. <ref type="figure">Fig. 4</ref> shows comparisons of PR curves. Since our proposed HAWP and L-CNN use very similar wireframe parsing pipelines and adopt the same design choices when applicable. The consistent accuracy gain of our HAWP must be contributed by the novel 4-D holistic attraction field representation and its integration in the parsing pipeline. In terms of efficiency, our HAWP runs much faster since a significantly fewer number of line segment proposals are used in the verification module. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our HAWP uses 5.5 times fewer number of line segment proposals.</p><p>Qualitative Results. <ref type="figure">Fig. 5</ref> shows wireframe parsing results by the five methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We compare the effects of three aspects: our proposed H-AFM vs. the vanilla AFM <ref type="bibr" target="#b32">[33]</ref>, the distance residual module (Section 4.1), and the composition of negative samples in training verification module (Section 4.4).  We observe that our HAWP is less sensitive to those samplers due to the informative line segment proposal generation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Discussions</head><p>This paper presents a holistically-attracted wireframe parser (HAWP) with state-of-the-art performance obtained on two benchmarks, the wireframe dataset and the YorkUrban dataset. The proposed HAWP consists of three components: proposal (line segments and junctions) initialization, proposal refinement and proposal verification, which are end-to-end trainable. Compared to the previous state-of-theart wireframe parser L-CNN <ref type="bibr" target="#b39">[40]</ref>, our HAWP is enabled</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the architecture of our proposed HAWP. It consists of three components, proposal initialization, proposal refinement and proposal verification. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the down-sampling rate is B = H H = W W ). Then, for each bin b, let Λ b ⊂ Λ and x b ∈ Λ b be its corresponding patch and the center of the patch respectively in the original image lattice and we have, J (b) = 1 and O(b) = (x b − p) if ∃p ∈ J, and p ∈ Λ b and both are set to 0 otherwise, where the offset vector in O(b) is normalized by the bin size, so the range of O(b) is bounded by [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>both H-AFM and the distance residual module are important for improving performance. The natural negative sampler N * randomly chooses negative line segments based on the matching results (with respect to the annotations). The rest of three negative example samplers (D * , D − , S − ) are also investigated in L-CNN and their full combination is needed for training L-CNN. D * randomly selects a part of examples from the online generated line segment proposals, regardless of the matching results. D − tries to match the proposals with pre-computed hard negative examples and the matched proposals are used as negative samples. S − directly obtains the negative examples from the pre-computed hard negative examples set. In our experiment, the number of samples for N * , D * , D − and S − are set to 300, 300, 300, 40 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>sAP 10 sAP 15 msAP mAP J AP H F H sAP 5 sAP 10 sAP 15 msAP mAP J AP H F H</figDesc><table><row><cell cols="2">Method sAP 5 LSD [29] /</cell><cell>/</cell><cell cols="3">Wireframe Dataset / / /</cell><cell>55.2</cell><cell>62.5</cell><cell>/</cell><cell>/</cell><cell cols="3">YorkUrban Dataset / / /</cell><cell>50.9</cell><cell>60.1</cell><cell>FPS 49.6</cell></row><row><cell>AFM [33]</cell><cell>18.5</cell><cell>24.4</cell><cell>27.5</cell><cell>23.5</cell><cell>23.3</cell><cell>69.2</cell><cell>77.2</cell><cell>7.3</cell><cell>9.4</cell><cell>11.1</cell><cell>9.3</cell><cell>12.4</cell><cell>48.2</cell><cell>63.3</cell><cell>13.5</cell></row><row><cell>DWP [14]</cell><cell>3.7</cell><cell>5.1</cell><cell>5.9</cell><cell>4.9</cell><cell>40.9</cell><cell>67.8</cell><cell>72.2</cell><cell>1.5</cell><cell>2.1</cell><cell>2.6</cell><cell>2.1</cell><cell>13.4</cell><cell>51.0</cell><cell>61.6</cell><cell>2.24</cell></row><row><cell>L-CNN [40]</cell><cell>58.9</cell><cell>62.9</cell><cell>64.9</cell><cell>62.2</cell><cell>59.3</cell><cell cols="2">80.3 82.8  † 81.3  † 76.9</cell><cell>24.3</cell><cell>26.4</cell><cell>27.5</cell><cell>26.1</cell><cell>30.4</cell><cell cols="2">58.5 59.6  † 65.3  † 61.8</cell><cell>15.6</cell></row><row><cell cols="2">L-CNN (re-trained) 59.7</cell><cell>63.6</cell><cell>65.3</cell><cell>62.9</cell><cell>60.2</cell><cell cols="2">81.6 83.7  † 81.7  † 77.9</cell><cell>25.0</cell><cell>27.1</cell><cell>28.3</cell><cell>26.8</cell><cell>31.5</cell><cell cols="2">58.3 59.3  † 65.2  † 62.2</cell><cell>15.6</cell></row><row><cell>HAWP (ours)</cell><cell>62.5</cell><cell>66.5</cell><cell>68.2</cell><cell>65.7</cell><cell>60.2</cell><cell cols="2">84.5 86.1  † 83.1  † 80.3</cell><cell>26.1</cell><cell>28.5</cell><cell>29.7</cell><cell>28.1</cell><cell>31.6</cell><cell cols="2">60.6 61.2  † 66.3  † 64.8</cell><cell>29.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance profiling on the Wireframe dataset. #Proposals represents the number of line segments in verification. The average number of groundtruth is listed in the last row.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 Table 3 .</head><label>33</label><figDesc>Wireframe parsing examples on the Wireframe dataset<ref type="bibr" target="#b13">[14]</ref>. D − S − sAP 5 sAP 10 sAP<ref type="bibr" target="#b14">15</ref> The ablation study of three design and learning aspects in the proposed HAWP. See text for details.</figDesc><table><row><cell>summarizes the comparisons. We observe that</cell></row></table><note>* D*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The built-in LSD in OpenCV v3.2.0 is used in evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>by a novel 4-D holistic attraction field map representation (H-AFM) for line segments in proposal generation stages. Our HAWP also achieves real-time speed with a single GPU, and thus is useful for many downstream tasks such as SLAM and Structure from Motion (SfM). The proposed H-AFM is also potentially useful for generic LSD problems in other domains such as medical image analysis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Edlines: A real-time line segment detector with a false detection control. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuneyt</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihan</forename><surname>Topal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MCMLSD: A Dynamic Programming Approach to Line Segment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Emilio J Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elder</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure-from-motion using lines: Representation, triangulation, and bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="441" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Novel Linelet-Based Representation for Line Segment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1195" to="1208" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Edge-Based Methods for Estimating Manhattan Frames in Urban Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meaningful alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnès</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plmp -point-line minimal problems in complete multi-view visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathlen</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blocks world revisited: Image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="482" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Surface reconstruction from 3d line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alain</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="553" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular-vision based SLAM using line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Kevis-Kokitsi Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Robust detection of lines using the progressive probabilistic hough transform. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="119" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure from motion with line segments under relaxed endpoint constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branislav</forename><surname>Micusík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Wildenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Camera pose estimation from lines using plücker coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bronislav</forename><surname>Pribyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemcík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cadík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2015, BMVC</title>
		<meeting>the British Machine Vision Conference 2015, BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="45" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Absolute pose estimation from line correspondences using direct linear transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bronislav</forename><surname>Pribyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemcík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cadík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="144" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine perception of threedimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><forename type="middle">Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LSD: A Fast Line Segment Detector with a False Detection Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>R G Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J M</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate junction detection and characterization in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning regional attraction for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anisotropic-scale junction detection and matching for indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to calibrate straight lines for fisheye image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhucun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ppgnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structslam: Visual SLAM with building structure lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danping</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rendong</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a CNN coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="914" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust visual SLAM with point and line features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoquan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
