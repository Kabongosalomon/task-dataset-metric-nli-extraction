<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fully Attention-Based Information Retriever</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><forename type="middle">H C</forename><surname>Correia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Escola Politecnica -Universidade de Sao Paulo</orgName>
								<address>
									<settlement>Sao Paulo</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">L M</forename><surname>Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Escola Politecnica -Universidade de Sao Paulo</orgName>
								<address>
									<settlement>Sao Paulo</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">De C</forename><surname>Martins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Escola Politecnica -Universidade de Sao Paulo</orgName>
								<address>
									<settlement>Sao Paulo</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Escola Politecnica -Universidade de Sao Paulo</orgName>
								<address>
									<settlement>Sao Paulo</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fully Attention-Based Information Retriever</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/IJCNN.2018.8489656</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks are now the state-ofthe-art in natural language processing because they can build rich contextual representations and process texts of arbitrary length. However, recent developments on attention mechanisms have equipped feedforward networks with similar capabilities, hence enabling faster computations due to the increase in the number of operations that can be parallelized. We explore this new type of architecture in the domain of question-answering and propose a novel approach that we call Fully Attention Based Information Retriever (FABIR). We show that FABIR achieves competitive results in the Stanford Question Answering Dataset (SQuAD) while having fewer parameters and being faster at both learning and inference than rival methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Question-answering (QA) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community. An interesting strategy in the design of such systems is information extraction, where the answer is sought in a set of support documents. However, extracting information from large texts is still a challenging task, and most state-of-the-art models restrict themselves to single paragraphs. That is, in fact, the proposed focus of recent open-domain QA datasets, such as SQuAD <ref type="bibr" target="#b0">[1]</ref>.</p><p>In SQuAD, each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P. That format reduces the complexity of the task and also facilitates training, as one can learn a probability distribution over the words that compose the passage. Since its publication in 2016, SQuAD has been targeted by many research groups, and the proposed models are gradually approaching (even overcoming) human-level performances. All but a few of these models rely on Recurrent Neural Networks (RNNs), which currently dominate the stateof-the-art in most Natural Language Processing (NLP) tasks. However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on "attention" mechanisms <ref type="bibr" target="#b1">[2]</ref>. We later clarify the meaning of attention in this context.</p><p>Inspired by the positive results of Vaswani et al. in machine translation, we have applied a similar architecture to the domain of question-answering, a model that we have named Fully Attention-Based Information Retriever (FABIR). Our goal then was to verify how much performance we can get exclusively from the attention mechanism, without combining it with several other techniques. We validated our model in the SQuAD dataset, which proved that FABIR not only achieves competitive results (F1:77.6%, EM:67.7%) but also has fewer parameters and is faster at both training and testing times than competing methods. Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible:</p><p>• Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. • Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <ref type="bibr" target="#b1">[2]</ref> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). • Column-wise cross-attention: we modify the crossattention operation by <ref type="bibr" target="#b1">[2]</ref> and propose a new technique that is better suited to question-answering. This article is organized as follows. We first introduce some of the related work in question-answering and then present FABIR's architecture and its basic design choices. Subsequently, we report and comment our results in the SQuAD dataset. Finally, we compare the performance of FABIR with RNN-based models and draw some conclusions, suggesting directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The vast majority of papers that address the SQuAD dataset have adopted RNN-based models <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b25">[26]</ref>. They all follow a similar pipeline, with pre-trained word-embeddings that are processed by bidirectional RNNs. Question and passage are processed independently, and their interaction is modeled by attention mechanisms <ref type="bibr" target="#b26">[27]</ref> to produce an answer. There are slight differences in how each model employs attention, but they all calculate it over the hidden states of an RNN. Vaswani et al. were the first to apply attention directly over the word-embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state-ofthe-art results in machine translation <ref type="bibr" target="#b1">[2]</ref>. In this section, we briefly discuss both types of attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traditional Attention Mechanisms</head><p>In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b26">[27]</ref> and natural language inference <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Indeed, most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage.</p><p>Attention can be defined as a mechanism that gives a score α i to a vector p i from a set P = [p 1 , ..., p m ] with respect to a vector q j from Q = [q 1 , ..., q n ]. This score is a function of both P and Q and is shown in its most general form in <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">s i,j = f (p i , q j ),<label>(1a)</label></formula><formula xml:id="formula_1">α i,j = exp(s i,j ) n k=1 exp(s i,k ) ,<label>(1b)</label></formula><p>where s i and α i are scalars and f is a score function that measures the importance of p i relative to q j . Intuitively, a large weight α i means that the vector p i is somehow strongly related to Q. In the literature, two alternatives for f have been proposed, additive <ref type="bibr" target="#b26">[27]</ref> and multiplicative <ref type="bibr" target="#b29">[30]</ref> attentions:</p><formula xml:id="formula_2">f (p i , q j ) = W 3 g(W 1 p i + W 2 q j ) (additive) p T i W 1 q j (multiplicative),<label>(2)</label></formula><p>where W 1 , W 2 and W 3 are learnable parameters and g is a elementwise nonlinear function. For small vectors, additive and multiplicative attention mechanisms have been shown to produce similar results <ref type="bibr" target="#b30">[31]</ref>.</p><p>In most models, the attention scores α are used to create a context vector c given by a weighted sum of P , which is processed by an RNN:</p><formula xml:id="formula_3">c t = i α i,t p i , (3a) v t = RNN(v t−1 , c t ),<label>(3b)</label></formula><p>where v t is the hidden state of the RNN at time t. Notably, in the SQuAD dataset, P and Q are the vectorial representations of passage and question, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Google's Transformer</head><p>The Transformer is a machine translation model introduced in [2] that achieved state-of-the-art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position-encoded embedding vectors. It defines three different matrices U, K and V that are associated with queries, keys, and values, respectively. Every attention operation in the Transformer is performed by multiplying these matrices as shown in <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_4">att(U, K, V ) = softmax U W U K K T V W V ,<label>(4)</label></formula><p>where W U K , W V ∈ R d model ×d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <ref type="bibr" target="#b1">[2]</ref> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as</p><formula xml:id="formula_5">logits i = U W U,i (KW K,i ) T ,<label>(5a)</label></formula><formula xml:id="formula_6">att i (U, K, V ) = softmax (logits i ) V W V,i ,<label>(5b)</label></formula><p>where W U,i , W K,i , W V,i ∈ R d model ×d head are again learnable weight matrices and d head is the embedding dimension of each head. Finally, attention is computed by the concatenation of every head attention att i , followed by an affine transformation:</p><formula xml:id="formula_7">att(U, K, V ) = [att 1 ; ...; att n heads ] W O ,<label>(6)</label></formula><p>where W O ∈ R n heads * d head ×d model . If one wants to model the interdependence of words within a single piece of text, U, K and V are all equal and consist of the text of interest embedded in some vectorial space. This type of attention is often called "self-attention" or "self-alignment" <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Conversely, if one seeks the relationship between words from two different passages, then U represents one, while K and V represent the other. In that case, we talk about "cross-attention".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other RNN-free Models</head><p>We also identified another QA model <ref type="bibr" target="#b31">[32]</ref> that is inspired by the architecture introduced by Vaswani et al. <ref type="bibr" target="#b1">[2]</ref>. Their model differs from ours in that it heavily relies on convolutions (46 layers against 2 in FABIR), which approximates it to other CNN NLP models <ref type="bibr" target="#b32">[33]</ref>, rather than purely attention based models. Although they report high F1 and EM scores (82.7% and 73.3%), our model is almost twice as fast in inference (259 samples/s against 440 in FABIR). Also, their model probably has a higher number of learned parameters due to the increased number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FABIR</head><p>In this section, we present FABIR's architecture and the main design decisions we have made to develop a lighter and faster question-answering model. In particular, we introduce the convolutional attention, the column-wise cross-attention, and the reduction layer, which build on the Transformer model <ref type="bibr" target="#b1">[2]</ref> to enable its application to question-answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Embeddings</head><p>We model each piece of text at the level of a word, i.e., sentences are defined by a sequence of vectors ω, each one representing a word in a vectorial space R dinput . Thus, we build a new representation of question and passage to which we will refer as Ω Q and Ω P , respectively.</p><formula xml:id="formula_8">P, Q embed − −− → Ω P ∈ R P len ×dinput , Ω Q ∈ R Q len ×dinput ,<label>(7)</label></formula><p>where Q len and P len denote the number of tokens in the question and the passage, respectively. These embeddings are composed by word-level and character-level representations. The former is denoted by ω w ∈ R 100 and was imported from the pre-trained embeddings of GloVe "6B" <ref type="bibr" target="#b33">[34]</ref>. The latter is denoted by ω c ∈ R 100 and is computed for each word as a result of the composition of its characters. Given a word with length l, C = [c 1 , c 2 , ..., c l ], in which c i ∈ R 8 are learned character embeddings, we compute ω c by convolving C with kernel H ∈ R 1×5×8×100 and applying max-over time pooling <ref type="bibr" target="#b34">[35]</ref>. Finally, we squeeze ω c values to [−1, 1] using a hyperbolic tangent activation function and pass the concatenation of ω w and tanh(ω c ) through a two-layer Highway-Network <ref type="bibr" target="#b35">[36]</ref> to obtain the final representation of a word. ω = Highway([ω w ; tanh (ω c )]).</p><p>(8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder</head><p>In contrast to an RNN, FABIR does not process words in sequence, and hence needs to model the position of each word in a sentence differently. We add positional information to each word embedding using a trigonometric encoder as proposed in <ref type="bibr" target="#b1">[2]</ref>. Therefore, given a sequence of embedding vectors of even size d model , the position of the i th word is encoded in a vector e i as follows:</p><formula xml:id="formula_9">e i =       sin(i * f 1 ) cos(i * f 1 ) ... sin(i * f d model /2 ) cos(i * f d model /2 )       ,<label>(9)</label></formula><p>where f k are scalars, which were chosen according to <ref type="bibr" target="#b1">[2]</ref>. The encoding of an embedding matrix Ω is represented by E and the whole operation can be summarized as</p><formula xml:id="formula_10">P, Q encode −−−→ E P ∈ R P len ×d model , E Q ∈ R Q len ×d model , (10)</formula><p>where d model is the size of each position encoding, which is not necessarily equal to d input .</p><p>The encoding E can be summed to Ω to include the information of the position of each word in the text. Indeed, in <ref type="bibr" target="#b1">[2]</ref>, the final vectorial representation of a piece of text is defined by the sum of the embeddings Ω with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it "reduction layer". The architecture of this type of layer is addressed further on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convolutional Attention</head><p>In FABIR the attention mechanism is inspired by the Transformer model introduced in <ref type="bibr" target="#b1">[2]</ref>. However, we hypothesize the word-to-word relationship in (1a) fails to capture the complexity of expressions involving groups of words. To facilitate the modeling of the interdependence of surrounding words, we redefine s i,j as</p><formula xml:id="formula_11">s i,j = f (p i− h−1 2 , ..., p i+ h−1 2 , q j− w−1 2 , ..., q j+ w−1 2 ),<label>(11)</label></formula><p>where h and w are the height and width of a convolution kernel. This new type of attention, which we named "convolutional-attention", is entirely defined by the following sequence of steps:</p><formula xml:id="formula_12">logits i = U W U,i (KW K,i ) T ,<label>(12a)</label></formula><formula xml:id="formula_13">logits i,padded = pad(logits i ),<label>(12b)</label></formula><formula xml:id="formula_14">logits conv = Conv(logits padded , H),<label>(12c)</label></formula><formula xml:id="formula_15">att head,i = softmax(logits conv,i )V W V,i , (12d) att conv (U, K, V ) = [att 1 ; ...; att n heads ] W O ,<label>(12e)</label></formula><p>where Conv represents a single convolutional layer with a trainable kernel H ∈ R h×w×n heads ×n heads that has height h, width w, and number of filters and channels both equal to n heads . Note that in (12b) zero-padding is applied so that logits conv maintains the same dimension of logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sublayers</head><p>After converting question and passage to their vectorial representation Q and P , we apply a series of operations that we call sublayers. In this section, we introduce each of these operations.</p><p>1) Self-attention: Self-attention (att self ) is the mechanism that models the interdependence between words in the same piece of text. It has been proven to help relating distant words, which is crucial to understand the long sentences that appear in context paragraphs in SQuAD. In FABIR, self-attention is a sublayer that applies such operation via convolutional attention and is defined as att self (P ) = att conv (P, P, P ).</p><p>2) Column-wise Cross-attention: Cross-attention (att cross ) differs from other types of attention by relating two different pieces of text. Given P and Q, cross-attention of Q over P is defined as</p><formula xml:id="formula_17">att cross (P, Q) = att conv (P, Q, Q).<label>(14)</label></formula><p>In contrast to Vaswani et al. <ref type="bibr" target="#b1">[2]</ref>, where the softmax in (12d) is applied in a row-wise manner, we suggest column-wise cross-attention. More precisely, we sum over i instead of j in (1b). Row-wise softmax is inadequate in QA because, in practice, it computes a weighted average of the question words for every passage word, and thus cannot model the likely scenario in which not every word in the passage is related to the question. In contrast, the column-wise softmax attributes greater weights to passage words that are more closely related to the respective question word, which seems appropriate for the SQuAD task.</p><p>Many question-answering models employ cross-attention in both directions: att cross (P, Q) and att cross (Q, P ) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, in FABIR we have observed better results when only the former is used.</p><p>3) Feedforward: The feedforward sublayer is solely composed of a neural network with a single hidden layer, which is applied vector-wise. Following the architecture suggested by Vaswani et al. <ref type="bibr" target="#b1">[2]</ref>, the feedforward sublayer is implemented in (15) with a two-layer neural network:</p><formula xml:id="formula_18">x i,out = ReLU (x i W 1 + b 1 ) W 2 + b 2 ,<label>(15)</label></formula><p>where </p><formula xml:id="formula_19">W 1 ∈ R d model ×d hidden , W 2 ∈ R d hidden ×d model , b 1 ∈ R<label>1×d</label></formula><formula xml:id="formula_20">Ω P , Ω Q d input P 0 , Q 0 d model P 1 , Q 1 d model P 2 , Q 2 d model P 3 , Q 3 d model y 1 ,ŷ 2 FABIR</formula><p>Reduction Layer Processing Layer Q t P t <ref type="figure">Fig. 1</ref>. On the left, a block diagram representation of FABIR, which receives as input the raw passage and question texts, P and Q. The passage and question embedding matrices are Ω P and Ω Q , respectively, and they both have an embedding dimension of d input , which is a result of the tokenization, followed by the word/char embedding process. After the layer reduction, subsequent representations of P and Q have embedding size d model and already include the encoding of word positions. Finally,ŷ 1 andŷ 2 are the indices, which define the answer to the passage-question pair P and Q. On the center, a block representation of the reduction layer, which is the third layer in FABIR's pipeline. Finally, on the right side, it is the processing layer, which is similar to the reduction layer except by the absence of the "matrix reduction" sublayer and the substitution of the decoupled attention by a self-attention block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ Position Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Normalization:</head><p>This operation is also applied vectorwise and it normalizes the embedding of each word so that its variance and mean are reduced to 1 and 0, respectively. The primary goal of layer normalization is to accelerate training as shown in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Layers</head><p>A layer L is a combination of sublayers that produce a transformation in the representation of question and passage:</p><formula xml:id="formula_21">P l+1 , Q l+1 = L(P l , Q l ).<label>(16)</label></formula><p>Typically, a layer is composed of self-attention with shared weights applied to P and Q individually, followed by crossattention and feedforward sublayers. This standard layer is called "processing layer" and is illustrated in <ref type="figure">Figure 1</ref>. Note that, to facilitate training, every sublayer is followed by normalization.</p><p>FABIR is formed by stacking layers on top of each other as shown in <ref type="figure">Figure 1</ref>. Not counting the pre-processing and answer selector layers, our best performing model was composed of four layers, of which the last three are processing layers and the first is a reduction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Reduction Layer</head><p>The SQuAD dataset is relatively small for the training of word embeddings, and pre-trained word vectors have been favored in the literature <ref type="bibr" target="#b20">[21]</ref>. Nonetheless, we observed that the new architecture introduced by Vaswani et al. <ref type="bibr" target="#b1">[2]</ref> is more susceptible to overfitting than RNNs when presented with large embedding sizes. Hence, we needed a method to compress the word representations, and thus facilitate and speed up training by reducing the number of parameters. A straightforward method to reduce the input embedding size is to multiply it by a matrix with the required dimensions:</p><formula xml:id="formula_22">ω model = W Reduction ω input ,<label>(17)</label></formula><p>where ω model ∈ R 1×d model , ω input ∈ R 1×dinput are the embedding vectors, and W Reduction ∈ R d model ×dinput is a weight matrix, to which we refer as "reduction matrix". Although matrix reduction is quite simple, it discards information before any processing and hence might hamper performance by preventing the network from using some relevant data. To incorporate that information before discarding it, we could add a large processing layer followed by a matrix reduction, but our experiments have shown that this approach does not yield positive results in FABIR. Our interpretation of that behavior is that the position encoding is somehow dissolved in the matrix reduction process.</p><p>A possible solution is to process embeddings of size d input and encodings of size d model independently and thus limit the reduction operation to the embeddings. We then suggest decoupled attention, a mechanism that allows us to apply selfattention to embeddings and encodings separately to preserve their different sizes, as described in <ref type="figure">Figure 2</ref>. Both operations use the full embedding Ω ∈ R Ω len ×dinput , but the decoupled attention outputs embeddings with size d input and encodings with final size d model .</p><p>After applying decoupled attention with shared weights in P and Q, we add a full processing layer for the embeddings Ω with size d input . That layer is equivalent to a regular processing layer L, but it processes only Ω , leaving the encoding E untouched. Finally, we use a reduction matrix to scale Ω down to d model and add the encoder matrix E of same size, which is the output from the decoupled attention sublayer. <ref type="figure">Figure 1</ref> describes this whole process, which we named "reduction layer".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Answer Selection</head><p>Given that in SQuAD the answer is always contained in the supporting paragraph P, the output of the model is merely the indicesŷ 1 andŷ 2 that represent the first and the last word of the answer, respectively. Therefore, we can model the answer as two probability distributions π 1 and π 2 over the passage P, and train the model to minimize the negative log-likelihood. Given the true indices y 1 and y 2 , the cost function is then defined as follows: J = −(y 1 log(π 1 ) + y 2 log(π 2 )).</p><p>To compute the cost function, we apply a two layered convolutional neural network with hidden layer size 32 and output size 2, as we need one dimension for each probability distribution. Both convolutions have kernel size 9 and the activation function (ReLU) is applied only after the first layer. Subsequently, each output dimension passes through a softmax operation to compute the probability distributionsπ y1 and π y2 . Finally, selecting the indicesŷ 1 andŷ 2 becomes an optimization problem:</p><formula xml:id="formula_24">maximize i,jπ y1 i * π y2 j subject to i ≤ j &lt; i + 15,<label>(19)</label></formula><p>where 15 represents the maximum allowed answer length. This superior limit is imposed to avoid long answers, since short answers are more frequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM. We developed our model in Tensorflow <ref type="bibr" target="#b38">[39]</ref> and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297/ for replicability.</p><p>We pre-processed the texts with the NLTK Tokenizer <ref type="bibr" target="#b39">[40]</ref>. As suggested in <ref type="bibr" target="#b1">[2]</ref>, we have chosen the Adam optimizer <ref type="bibr" target="#b40">[41]</ref> with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <ref type="bibr" target="#b1">[2]</ref> of 0.9 in processing layers and of 0.8 in the reduction layer. In the character-level embedding process, a dropout of 0.75 was added before the convolution. Additionally, a dropout of 0.8 was added before each convolutional layer in the answer selector. We set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed-forward hidden size to 200 in processing layers and 400 in the reduction layer. Convolution kernels in attention sublayers had spatial dimensions 1 × 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Evaluation</head><p>To better evaluate FABIR's architecture, we ran controlled tests on each of its key elements. <ref type="table" target="#tab_1">Table I</ref> show the results of these experiments regarding the F1 and EM scores, and the Training Time (TT) over 18-epoch runs. This analysis confirms the effectiveness of char-embeddings, as its addition increased the F1 and EM scores, by 2.7% and 3.1%, respectively. Most importantly, when the convolutional attention was replaced by the standard attention mechanism proposed in <ref type="bibr" target="#b1">[2]</ref>, the performance dropped by 2.4% in F1 and 2.5% in EM. That validates the contribution of this new attention method in building elaborate contextual representations. Moreover, the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings. Indeed, when we replaced that layer by a standard feedforward layer with the same reduction ratio, there was a drop of 2.1% and 2.5% in the F1 and EM scores, respectively. Finally, we observed that the column-wise cross-attention outperforms its row-wise counterpart by 2.0% and 1.9% in F1 and EM, respectively. It confirms the intuition that applying the softmax over the passage words is more adequate in QA.</p><p>The training times indicate that each one of the new mechanisms introduced in FABIR incurred an increase in the processing cost. Nonetheless, that was outweighed by the improvement in performance, especially because FABIR is still significantly faster than competing RNN models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FABIR vs BiDAF</head><p>This section compares our model to traditional RNNbased question-answering models. To have a comprehensive comparison, we took a state-of-the-art model <ref type="bibr" target="#b20">[21]</ref> developed in Tensorflow <ref type="bibr" target="#b38">[39]</ref> that had its code openly available at https://github.com/allenai/bi-att-flow. That way, we could run our experiments with both models in the same piece of hardware to have a fair comparison between them. In <ref type="table" target="#tab_1">Table  II</ref>, the BiDAF scores without parentheses were achieved after training their model for 18,000 iterations of batch size 60 in our hardware. Conversely, values in parentheses are BiDAF's official scores in the SQuAD ranking <ref type="bibr" target="#b41">[42]</ref>. Note that for both models, we batch the examples by paragraph length to improve computational efficiency. Regarding EM and F1 scores, FABIR and BiDAF showed similar performances. Their similar scores render further comparisons even more telling, because their differences cannot be explained by their overall performances, but exclusively by their architectures. Although both models required similar training times to reach these scores, the time for training one epoch in FABIR was more than four times shorter, which could be useful for tackling larger data sets.</p><p>Concerning inference time, FABIR was more than five times faster in processing the 10,570 question-passage pairs in the development data set. FABIR's faster inference is a substantial advantage in large-scale applications, such as information extraction in large corpora. Indeed, when running applications such as search tools or user interfaces, the inference time is critical to tackle real-world problems. Concerning the number of training variables, FABIR has almost 50% fewer parameters than BiDAF, which incurs two major advantages. First, its training time is expected to be shorter, because the number of variables to be updated in every iteration is smaller. Secondly, it has lower memory requirements, which is attractive to applications that dispose of low computational power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FABIR and BiDAF Statistics</head><p>In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD. <ref type="figure">Figure 3</ref> shows that shorter answers are easier for both models: while they reach more than 75% F1 for answers that are shorter than four words, for answers longer than ten words these scores drop to 60.4% and 67.3% for FABIR and BiDAF, respectively. Long answers are not only more challenging but are also underrepresented in the dataset, which introduces a bias towards short responses. More than 79% of the answers in SQuAD have five words or fewer.</p><p>In contrast to what has been observed for answers, longer questions seem not to increase the complexity of the task. In <ref type="figure">Figure 4</ref>, the F1 scores for both models varied by less than 2.5% in the considered question length intervals.</p><p>Question type is a strong predictor of performance. <ref type="figure">Figure  5</ref> shows that both models had their best performance with "when" questions. Answers to these types of questions are easier to infer because they are usually composed of timerelated words, such as months, years, seasons or weekdays, which are easier to distinguish from the rest of the text. Together with "when" questions, "how long" and "how many" also proved easier to respond, as they possess the same property of having a smaller universe of possible answers. In contrast to these, "how" and "why" questions resulted in considerably lower F1 and EM scores, as they can be answered by any sentence, and hence require a deeper understanding of the text. Note that "how" questions do not include "how many", "how much" or "how long" questions, whose answers are more predictable. "Other" questions include alternatives, such as "Name a type of..." or "Does it..." or even questions with typos, such as "Hoe was ...", which should be "How was ...". These questions are more challenging because they might have multiple correct answers and require higher levels of abstraction. For instance, to respond to a question such as "Name an ingredient...", the model would need a deep understanding of the semantics of the word "ingredients" to identify "tomatoes" or "cheese" as possible answers. Questions which expect a "yes" or a "no" as an answer are also difficult because it is not always possible to find those words in a snippet from the passage. <ref type="figure">Figure 6</ref> shows the performance of FABIR and BiDAF against the passage length. It is curious that shorter passages showed the worst performance for both models. It is hard to interpret that result as, intuitively, one would expect brief passages to be easier to interpret. One possible explanation is that short passages give fewer options of simple questions, such as "when", "who", or "how many", and the annotators of the dataset had to resort to more elaborate alternatives.  <ref type="bibr" target="#b3">[4]</ref> 78.234 85.344 FRC <ref type="bibr" target="#b31">[32]</ref> 76.240 84.599 RaSoR + TR + LM <ref type="bibr" target="#b4">[5]</ref> 77.583 84.163 Stochastic Answer Networks <ref type="bibr" target="#b5">[6]</ref> 76.828 84.396 r-net <ref type="bibr" target="#b6">[7]</ref> 76.461 84.265 FusionNet <ref type="bibr" target="#b7">[8]</ref> 75.968 83.900 DCN+ <ref type="bibr" target="#b8">[9]</ref> 75.087 83.081 Conductor-net <ref type="bibr" target="#b9">[10]</ref> 74.405 82.742 BiDAF + Self Attention <ref type="bibr" target="#b10">[11]</ref> 72.139 81.048 smartnet <ref type="bibr" target="#b11">[12]</ref> 71.415 80.160 Ruminating Reader <ref type="bibr" target="#b12">[13]</ref> 70.639 79.456 jNet <ref type="bibr" target="#b13">[14]</ref> 70.607 79.821 ReasoNet <ref type="bibr" target="#b14">[15]</ref> 70.555 79.364 Document Reader <ref type="bibr" target="#b15">[16]</ref> 70.733 79.353 RaSoR <ref type="bibr" target="#b16">[17]</ref> 70.849 78.741 FastQAExt <ref type="bibr" target="#b17">[18]</ref> 70.849 78.857 Multi-Perspective Matching <ref type="bibr" target="#b18">[19]</ref> 70.387 78.784 SEDT <ref type="bibr" target="#b19">[20]</ref> 68.163 77.527 FABIR (Ours) 67.744 77.605 BiDAF <ref type="bibr" target="#b20">[21]</ref> 67.974 77.323 Dynamic Coattention Networks <ref type="bibr" target="#b21">[22]</ref> 66.233 77.896 Match-LSTM with Bi-Ans-Ptr <ref type="bibr" target="#b22">[23]</ref> 64.744 73.743 Fine-Grained Gating <ref type="bibr" target="#b23">[24]</ref> 62.446 73.327 OTF dict+spelling <ref type="bibr" target="#b24">[25]</ref> 64.083 73.056 Dynamic Chunk Reader <ref type="bibr" target="#b25">[26]</ref> 62.499 70.956</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>The experiments validate that attention mechanisms alone are enough to power an effective question-answering model. Above all, FABIR proved roughly five times faster at both training and inference than BiDAF, a competing RNN-based model with similar performance <ref type="bibr" target="#b20">[21]</ref>. These results strengthen some of FABIR's compelling advantages, notably, an architecture that is both more parallelizable and lighter, with half of the number of parameters in comparison to BiDAF <ref type="bibr" target="#b20">[21]</ref>.</p><p>FABIR also brings three significant contributions to this new class of neural network architectures. The convolutional attention, the reduction layer, and the column-wise crossattention individually increased the model's F1 and EM scores by more than 2%. Moreover, being thoroughly compatible with the Transformer <ref type="bibr" target="#b1">[2]</ref>, these new mechanisms are valuable assets to further developments in attention models. In fact, an intriguing line for future research is to evaluate their impact on other NLP tasks, such as machine translation or parsing.</p><p>Although FABIR is still far from surpassing the models at the top of the SQuAD leaderboard <ref type="table" target="#tab_1">(Table III)</ref>, we believe that its faster and lighter architecture already make it an attractive alternative to RNN-based models, especially for applications with limited processing power or that require low-latency. Also, being a distinct technique, FABIR might have low correlation with existing RNN-based models, increasing the potential of ensemble methods. How to combine FABIR with other systems is then an interesting topic for future research in diverse NLP applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>hidden and b 2 ∈ R 1×d model are all trainable parameters, d hidden is the dimension of the hidden layer in<ref type="bibr" target="#b14">(15)</ref> and ReLU(x) = max(0, x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ARCHITECTURE</head><label>I</label><figDesc>VARIATIONS. F1 AND EM SCORES IN THE DEV SET.</figDesc><table><row><cell>Architecture</cell><cell>F1(%)</cell><cell cols="2">EM(%) TT</cell></row><row><cell>FABIR</cell><cell>75.6</cell><cell>65.1</cell><cell>2h14m</cell></row><row><cell>FABIR without char embedding</cell><cell>72.9</cell><cell>62.0</cell><cell>1h48m</cell></row><row><cell>FABIR without convolutional attention</cell><cell>73.2</cell><cell>62.6</cell><cell>1h49m</cell></row><row><cell>FABIR without the reduction layer</cell><cell>73.5</cell><cell>62.6</cell><cell>1h59m</cell></row><row><cell>FABIR with row-wise cross attention</cell><cell>73.6</cell><cell>63.2</cell><cell>2h08m</cell></row><row><cell>FABIR with 2 attention heads</cell><cell>73.8</cell><cell>63.2</cell><cell>2h12m</cell></row><row><cell>FABIR with linear answer selector</cell><cell>74.0</cell><cell>62.8</cell><cell>2h10m</cell></row><row><cell>FABIR with 4 layers L Q→X</cell><cell>75.5</cell><cell>64.7</cell><cell>2h47m</cell></row><row><cell>FABIR with 2 layers L Q→X</cell><cell>75.0</cell><cell>64.1</cell><cell>1h55m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>BETWEEN FABIR AND BIDAF<ref type="bibr" target="#b20">[21]</ref> MODELS.</figDesc><table><row><cell></cell><cell>FABIR</cell><cell>BiDAF</cell></row><row><cell># of Training Variables</cell><cell>1,385,198</cell><cell>2,695,851</cell></row><row><cell>Inference Sample/Second</cell><cell>440</cell><cell>78</cell></row><row><cell>Training Sample/Second</cell><cell>202</cell><cell>45</cell></row><row><cell>Training Time/Epoch</cell><cell>7m13s</cell><cell>32m30s</cell></row><row><cell>Training Epochs</cell><cell>54</cell><cell>12</cell></row><row><cell>Training Time</cell><cell>6h30m</cell><cell>6h30m</cell></row><row><cell>F1 in the dev set (%)</cell><cell>77.6</cell><cell>77.0 (77.3)</cell></row><row><cell>EM in the dev set (%)</cell><cell>67.6</cell><cell>67.3 (68.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III EM</head><label>III</label><figDesc>AND F1 SCORES IN THE TEST SET FOR BEST PUBLISHED SINGLE MODELS IN THE SQUAD LEADERBOARD<ref type="bibr" target="#b41">[42]</ref> </figDesc><table><row><cell>Model</cell><cell>EM (%)</cell><cell>F1 (%)</cell></row><row><cell>Reinforced Mnemonic Reader [3]</cell><cell>79.545</cell><cell>86.654</cell></row><row><cell>MEMEN</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforced Mnemonic Reader for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MEMEN: Multilayer Embedding with Memory Networks for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Contextualized Word Representations for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03609</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic Answer Networks for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03556</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-Net: Machine Reading Comprehension with Self-Matching Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Natural Language Computing Group</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Phase Conductor on Multi-layered Attentions for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chikina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10504</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simple and Effective Multi-Paragraph Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02772</idno>
		<title level="m">Smarnet: Teaching Machines to Read and Comprehend Like Human</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ruminating Reader: Reasoning with Gated Multi-Hop Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ReasoNet: Learning to Stop Reading in Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05284</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning Recurrent Span Representations for Extractive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FastQA: A Simple and Efficient Neural Architecture for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seiffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-Perspective Context Matching for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structural Embedding of Syntactic Trees for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic Coattention Networks For Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine Comprehension Using Match-LSTM and Answer Pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Words or Characters? Fine-grained Gating for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00286</idno>
		<title level="m">Learning to Compute Word Embeddings On the Fly</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation By Jointly Learning To Align and Translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26. Lake Tahoe</title>
		<meeting><address><addrLine>Nevada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Natural Language Inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Massive Exploration of Neural Machine Translation Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional Sequence to Sequence Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-Aware Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Phoenix, Arizona</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Highway Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>The International Machine Learning Society</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<title level="m">Natural Language Processing with Python</title>
		<meeting><address><addrLine>Sebastopol, California: O&apos;Reilly</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Stanford Question Answering Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<ptr target="https://rajpurkar.github.io/SQuAD-explorer/" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
