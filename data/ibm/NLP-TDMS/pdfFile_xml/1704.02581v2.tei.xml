<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Wang</surname></persName>
							<email>hongsong.wang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, skeleton based action recognition gains more popularity due to cost-effective depth sensors coupled with real-time skeleton estimation algorithms. Traditional approaches based on handcrafted features are limited to represent the complexity of motion patterns. Recent methods that use Recurrent Neural Networks (RNN) to handle raw skeletons only focus on the contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons. In this paper, we propose a novel two-stream RNN architecture to model both temporal dynamics and spatial configurations for skeleton based action recognition. We explore two different structures for the temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed according to human body kinematics. We also propose two effective methods to model the spatial structure by converting the spatial graph into a sequence of joints. To improve generalization of our model, we further exploit 3D transformation based data augmentation techniques including rotation and scaling transformation to transform the 3D coordinates of skeletons during training. Experiments on 3D action recognition benchmark datasets show that our method brings a considerable improvement for a variety of actions, i.e., generic actions, interaction activities and gestures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition <ref type="bibr" target="#b1">[2]</ref> has become an active area in computer vision and there are many important research problems, such as event recognition <ref type="bibr" target="#b22">[23]</ref>, group based activities recognition <ref type="bibr" target="#b26">[27]</ref>, human object interactions <ref type="bibr" target="#b14">[15]</ref> and activities in egocentric videos <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11]</ref>. Most approaches have been proposed to recognize actions in RGB videos  recorded by 2D cameras. However, it still remains a challenging problem for three reasons. First, it is hard to well extract useful information from the high dimensional and low quality input data. Second, the RGB video is highly sensitive to some factors like illumination changes, occlusion and background clutter. Third, the identification of actions is related to high-level visual clues such as human poses and objects, which are very difficult to obtain from RGB videos directly.</p><p>Humans can recognize actions with a few spots describing motions of the main joints of skeletons <ref type="bibr" target="#b23">[24]</ref>, and experiments show that a large set of actions can be recognized solely from skeletons <ref type="bibr" target="#b24">[25]</ref>. In contrast to RGB based action recognition, skeleton based action recognition can avoid the awful task of feature extraction from videos and explicitly model the dynamics of actions. There are three ways to obtain skeletons: motion capture systems, RGB images and depth maps. Sophisticated motion capture systems are very expensive and require the user to wear a motion capture suit with markers. Extracting reliable skeletons from monocular RGB images or videos, i.e., pose estimation, is still an unsolved problem. Fortunately, with the recent advent of affordable depth sensors, it is much easier and cheaper to obtain 3D skeletons from depth maps. For example, Shot-ton et al. <ref type="bibr" target="#b37">[38]</ref> propose a method to quickly and accurately predict 3D positions of body joints from a single depth image. These advances excite considerable interest for skeleton based action recognition and various algorithms have been proposed recently.</p><p>Traditional skeleton based action recognition approaches are mainly divided into two categories: joint based approaches and body part based approaches. Joint based approaches consider the human skeleton as a set of points and use various positions based features such as joint positions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> and pairwise relative joint positions <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51]</ref> to characterize actions. While body part based approaches regard the human skeleton as a connected set of segments, and then focus on individual or connected pairs of body parts <ref type="bibr" target="#b49">[50]</ref> and joint angles <ref type="bibr" target="#b32">[33]</ref>. Based on handcrafted low-level features, both approaches employ relatively simple time series models, e.g., hidden Markov model <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, to recognize actions. However, human-engineered features are limited to represent the complexity of the intrinsic characteristics of actions and the subsequent time series models do not unleash the full potential of the sequential data. Inspired by the great success of deep learning for RGB based action recognition <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref>, there is a growing trend of using deep neural networks for skeleton based action recognition. Different structures of Recurrent Neural Networks (RNN), e.g., hierarchical RNN <ref type="bibr" target="#b6">[7]</ref>, RNN with regularizations <ref type="bibr" target="#b54">[55]</ref>, differential RNN <ref type="bibr" target="#b42">[43]</ref> and part-aware Long Short-Term Memory (LSTM) <ref type="bibr" target="#b36">[37]</ref>, have been used to learn motion representations from raw skeletons. However, considering an action is a continuous evolution of articulated rigid segments connected by joints <ref type="bibr" target="#b53">[54]</ref>, these RNNbased methods only model the contextual information in the temporal domain by concatenating skeletons for each frame. In fact, different actions are performed with different spatial configurations of joints of skeletons. The dependency in the spatial domain also reflects the characteristics of actions and should not be neglected for skeleton based action recognition.</p><p>To this end, we introduce a novel two-stream RNN architecture which incorporates both spatial and temporal networks for skeleton based action recognition. <ref type="figure" target="#fig_1">Figure 1</ref> shows the pipeline of our method. The temporal stream uses a RNN based model to learn the temporal dynamics from the coordinates of joints at different time steps. We employ two different RNN models, stacked RNN and hierarchical RNN. Compared with stacked RNN, hierarchical RNN is designed according to human body kinematics and has fewer parameters. At the same time, the spatial stream learns the spatial dependency of joints. We propose a simple and effective method to model the spatial structure that first casts the spatial graph of articulated skeletons into a sequence of joints, then feeds this resulting sequence into a RNN structure. Different methods are explored to turn the graph structure into a sequence for the purpose of better maintaining the spatial relationships. The two channels are then combined by late fusion and the whole network is end-to-end trainable. Finally, to avoid overfitting and improve generalization, we exploit data augmentation techniques by using 3D transformation, i.e., rotation transformation, scaling transformation and shear transformation to transform the 3D coordinates of skeletons during training.</p><p>In summary, the main contributions of this paper are listed as follows. First, we propose a two-stream RNN architecture to utilize both spatial and temporal relations of joints of skeletons. Second, we exploit and compare different architectures of both streams. Third, we propose data augmentation techniques based on 3D transformation and demonstrate the effectiveness for skeleton based action recognition. Finally, our method obtains the state-of-theart results on three important benchmarks for a variety of actions, i.e., generic actions (NTU RGB+D), interaction activities (SBU Interaction) and gestures (ChaLearn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we briefly review action recognition approaches related to ours. The two aspects are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action recognition with deep networks</head><p>Deep neural networks have made great progress in the area of action recognition. 3D Convolutional Neural Networks (CNN) is proposed and different architectures are studied to take advantage of local spatio-temporal information <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>. To capture complementary information between appearance and motion, a two-stream CNN architecture is developed for RGB based action recognition <ref type="bibr" target="#b38">[39]</ref>.</p><p>Recently, Recurrent Neural Networks (RNN) have been widely used for action recognition. Srivastava et al. <ref type="bibr" target="#b39">[40]</ref> use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Donahue et al. <ref type="bibr" target="#b3">[4]</ref> develop an end-to-end trainable Long-term Recurrent Convolutional Networks (LRCN) architecture which can simultaneously learn temporal dynamics and convolutional perceptual representations from RGB videos. Deep Convolutional and Recurrent Neural Networks has also been proposed and applied for activity recognition <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Prior to our work, several models have been proposed based on RNN for skeleton based action recognition. Du et al. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> first design an end-to-end hierarchical RNN architecture for skeleton based action recognition. Zhu et al. <ref type="bibr" target="#b54">[55]</ref> propose a fully connected deep LSTM network with regularization terms to learn co-occurrence features of joints. Veeriah et al. <ref type="bibr" target="#b42">[43]</ref> present differential RNN that extends LSTM structure by modeling the dynamics of states evolving over time. Shahroudy et al. <ref type="bibr" target="#b36">[37]</ref> propose a part-aware extension of LSTM to utilize the physical structure of the human body. These methods only model the motion dynamics in the temporal doamin and neglect the spatial configurations of articulated skeletons. Recently, Liu et al. <ref type="bibr" target="#b29">[30]</ref> extend LSTM to spatial-temporal domain for the purpose of modeling the dependencies between joints. As temporal dynamics and spatial configurations are separate visual pathways <ref type="bibr" target="#b13">[14]</ref>, we employ a two-stream architecture to model them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Features based on skeletons</head><p>Previous skeleton based action recognition methods mainly focus on handcrafted features <ref type="bibr" target="#b0">[1]</ref>. To get representations of postures, one straightforward feature is the pairwise joint location difference, which can be simply concatenated <ref type="bibr" target="#b31">[32]</ref>, or casted into 3D cone bins to build a histogram of 3D joints locations <ref type="bibr" target="#b48">[49]</ref> for action recognition.</p><p>Joint orientation is another good feature as it is invariant to the human body size. For example, Sempena et al. <ref type="bibr" target="#b35">[36]</ref> apply dynamic time warping based on the feature vector built from joint orientation along time series. Bloom et al. <ref type="bibr" target="#b2">[3]</ref> use AdaBoost to combine five types of features, i.e., pairwise joint position difference, joint velocity, velocity magnitude, joint angle velocity and 3D joint angle to recognize gaming actions, for real-time action recognition.</p><p>There are some work that groups the joints of skeletons to construct planes from joints and then measures the joint-to-plane distance and motion. Yun et al. <ref type="bibr" target="#b52">[53]</ref> capture the geometric relationship between the joint and the plane spanned by three joints. Sung et al. <ref type="bibr" target="#b40">[41]</ref> compute the joint's rotation matrix w.r.t. the person's torso, hand position w.r.t. the torso and joint rotation motion as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of RNN</head><p>Different from feedforward neural networks that map from one input vector/matrix to one output vector/matrix, recurrent neural networks (RNN) map an input sequence X to another output sequence Y .</p><p>RNN architectures are naturally suitable for the sequence classification, where each input sequence is assigned with a single class. Layers of RNN can be stacked to build a deep RNN by considering the output sequence of the previous layer as the input sequence of the current layer. A typical structure of RNN for sequence classification is shown in <ref type="figure" target="#fig_3">Figure 2</ref>(a), which contains a stack of RNN layers with a softmax classification layer on top of the last hidden layer.</p><p>Due to the vanishing gradient and error blowing up problems <ref type="bibr" target="#b15">[16]</ref>, the standard RNN cannot store information for long periods of time or access the long range of context. Long short-term memory (LSTM) <ref type="bibr" target="#b16">[17]</ref> addresses this problem by using additional gates to determine when the input is significant enough to remember, when it should continue to remember or forget the value, and when it should output the value. The LSTM unit has been shown to be capable of  storing and accessing information over very long timespans <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_3">Figure 2</ref>(b) depicts a LSTM unit:</p><formula xml:id="formula_0">i t = σ(W xi x t + W hi h t−1 + W ci c t−1 + b i ) f t = σ(W xf x t + W hf h t−1 + W cf c t−1 + b f ) c t = f t c t−1 + i t tanh(W xc x t + W hc h t−1 + b c ) o t = σ(W xo x t + W ho h t−1 + W co c t + b o ) h t = o t tanh(c t )<label>(1)</label></formula><p>where i, f, o correspond to the input gate, forget gate and output gate, respectively. All the matrices W are the connection weights and all the variables b are biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Two-stream RNN</head><p>The sequence of skeletons determines the evolution of actions, which has both spatial and temporal structures. The spatial structure displays a spot of the pictorial form of joints while the temporal structure tracks and represents the movement of joints. Accordingly, we devise an end-to-end two-stream architecture based on RNN, which is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Here the fusion is performed by combining the softmax class posteriors from the two nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Temporal RNN</head><p>We begin with the description of the temporal channel of RNN, which models the temporal dynamics of skeletons. Similar to the previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37]</ref>, it concatenates the 3D coordinates of different joints at each time step and handles the generated sequence with a RNN architecture. We focus on the following two model structures. Stacked RNN. This structure feeds the RNN network with the concatenated coordinates of all joints at each time step. Here we stack two layers of RNN and find that adding more layers would not considerably improve the performance. As the length of skeleton sequences is relatively long (e.g., 50∼200), we adopt LSTM neurons for all layers. Although simple, stacked RNN has been widely used to process and recognize sequences of variable lengths. Hierarchical RNN. The human skeleton can be divided into five parts, i.e., two arms, two legs and one trunk. We observe that an action is performed by either an independent part or a combination of several parts. For example, kicking depends on legs and running involves both legs and arms. Thus, a hierarchical structure of RNN is used to model the motions of different parts as well as the whole body. <ref type="figure" target="#fig_4">Figure  3</ref> shows the proposed structure. To be consistent with the stacked RNN structure, our hierarchical RNN also has two layers vertically.</p><p>In the first layer, we use a corresponding RNN to model the temporal movement of each body part based on its concatenated coordinates of joints at each time step. In the second layer, we concatenate the outputs of the RNN of different parts and adopt another RNN to model the movement of the whole body. Compared with the pioneered hierarchical structure in <ref type="bibr" target="#b6">[7]</ref>, our structure is more succinct and straightforward, and does not use additional fully connected layers before the logistic regression classifier with softmax activation. Compared with the stacked structure, the hierarchical structure has relatively fewer parameters and is less likely to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial RNN</head><p>Human body can be considered as an articulated system of rigid segments connected by joints. Take the MSR Ac-tion3D dataset <ref type="bibr" target="#b27">[28]</ref> as an example, the physical structure of the 20 joints is represented by an undirected graph in <ref type="figure" target="#fig_8">Figure  4</ref>(a). Nodes denote the joints and edges denote the physical connections. When an action takes place, this undirected graph displays some varied patterns of spatial structures. For example, clapping is performed with the joints of the two palms striking together, and bending is acted when the joints of the trunk shape into a curve.</p><p>To model the spatial dependency of joints, we cast the graph structure into a sequence of joints and exactly develop a relevant RNN architecture. The input of the RNN architecture at each step corresponds to the vector of coordinates of a certain joint. As a joint has only three coordinates, we select a temporal window centered at the time step and concatenate the coordinates inside this window to represent this joint. This RNN architecture models the spatial relationships of joints in a graph structure and is called spatial RNN. The central problem is how to convert a graph into a sequence. We provide two alternative methods below. Chain sequence. We assume the joints are arranged in a chain-like sequence with the order of arms, trunk and legs. The trunk is placed in the middle as it connects both arms and legs. For example, the 20 joints graph of the MSR Ac-tion3D dataset is arranged in a chain sequence in <ref type="figure" target="#fig_8">Figure  4(b)</ref>. The chain sequence maintains the physical connections of joints of each body part (arms, trunk and legs), and the joints are placed in a sequence without duplication. One of the drawbacks is that there is no physical connections at the boundary of joints between hands, trunk and legs. For instance, the joint whose index is 13 is not connected with the joint whose index is 20. But the two joints are adjacent in the generated chain-like sequence. Traversal sequence. To address the limitation of the chain sequence, we propose a graph traversal method to visit the joints in a sequence in the light of the adjacency relations, partly inspired by the tree-structure based traversal method <ref type="bibr" target="#b29">[30]</ref>.As illustrated in <ref type="figure" target="#fig_8">Figure 4</ref>(c), we first select the central spine joint as the starting point, and visit the joints of the left arm. While reaching an end point, it goes back. Then we visit the right arm, the upper trunk, etc. After visiting all joints, it finally returns to the starting point. We arrange the graph into a sequence of joints according to the visiting order. The traversal sequence guarantees the spatial relationships in a graph by accessing most joints twice in both forward and reverse directions.</p><p>Different from the temporal RNN, spatial RNN could recognize actions by a glimpse of one frame (when the size of temporal window equals 1). Here, we do not use a hierarchical structure based on body parts, as the number of joints is limited (e.g., <ref type="bibr" target="#b24">25</ref> for the NTU RGB+D dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D transformation of skeletons</head><p>For skeleton based action recognition, the input data is a sequence 3D coordinates of joints. As neural networks often require a lot of data to improve generalization and prevent overfitting, we exploit several data augmentation techniques based on 3D transformation to make the best use of limited supply of training data. Note that the 3D transformation techniques are only used during training. Rotation. Based on Euler's rotation theorem, any 3D rotation can be given as a composition of rotations about three axes. The three basic rotation matrices in terms of rotate angles α, β, γ about the x, y, z axis in a counterclockwise direction are represented as below:</p><formula xml:id="formula_1">R x (α) =   1 0 0 0 cos α − sin α 0 sin α cos α   (2) R y (β) =   cos β 0 sin β 0 1 0 − sin β 0 cos β   (3) R z (γ) =   cos γ − sin γ 0 sin γ cos γ 0 0 0 1  <label>(4)</label></formula><p>General rotations can be obtained from these three basic rotation matrices using matrix multiplication:</p><formula xml:id="formula_2">R = R z (γ)R y (β)R x (α)<label>(5)</label></formula><p>where R is the general rotation matrix in the 3D coordinate system. For the 3D coordinates of joints, we randomly rotate the input sequence of skeletons within a certain range for the x, y axis, as the rotation plane of the camera is perpendicular to the z axis. The rotation transformation simulates the viewpoint changes of the camera and improves the robustness of our model for cross view experimental settings. We find the recent work <ref type="bibr" target="#b5">[6]</ref> also uses the rotation transformation for cross view recognition of actions. Scaling. Scaling transformation is used to change the size of skeletons. The transformation matrix can be formulated as:</p><formula xml:id="formula_3">S =   s x 0 0 0 s y 0 0 0 s z  <label>(6)</label></formula><p>where s x , s y , s z are scaling factors along with the three axes, respectively. The scaling transformation can either expand or compress the dimensions of skeletons by using random scaling factors. As different action performers have varied heights and body sizes, the dimensions of their skeletons may be different. Thus the scaling transformation is beneficial for cross subject experimental settings. Shear. Shear transformation is a linear map that displaces each point in a fixed direction. It slants the shape of the coordinates of joints and changes the angles between them. The transformation matrix can be represented as below: </p><p>where sh y x , sh z x , sh x y , sh z y , sh x z , sh y z are shear factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The proposed model is evaluated on three datasets: NTU RGB+D dataset <ref type="bibr" target="#b36">[37]</ref>, SBU Interaction dataset <ref type="bibr" target="#b52">[53]</ref>, and ChaLearn Gesture Recognition dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>NTU RGB+D dataset. Currently, this is the largest depthbased action recognition dataset, providing 3D coordinates of 25 joints collected by Kincet v2. It contains more than 56 thousand sequences and 4 million frames, captured in various background conditions. The dataset has 60 different action classes including daily, mutual, and health-related actions. The actions are performed by 40 different human subjects, whose age range is from 10 to 35. Numerous variations in subjects and views, and large amount of samples make it highly suitable for deep learning methods. We follow the cross subject and cross view evaluations <ref type="bibr" target="#b36">[37]</ref> and report the classification accuracy in percentage. SBU Interaction dataset. This is a complex human activity dataset depicting two person interactions captured with Kinect. Each skeleton has 15 joints. It includes 282 skeleton sequences in 6822 frames. All videos are recorded in the same laboratory environment with 8 activities performed by 7 participants. The dataset is very challenging because the interactions are non-periodic, and have very similar body movements. Following the 5-fold cross validation <ref type="bibr" target="#b52">[53]</ref>, we split the 21 sets of this dataset into 5 folds and give the average recognition accuracy. ChaLearn Gesture Recognition dataset. This dataset contains 20 Italian gestures performed by 27 different persons. There are 23 hours of Kinect data, consisting of RGB, depth, foreground segmentation and skeletons. The dataset has 955 videos in total. Each video lasts 1 to 2 minutes and contains 8 to 20 noncontinuous gestures. Here, we only use skeletons for gesture recognition. As done in the literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, we report the precision, recall and F1-score measures on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We normalize skeletons by subtracting the central joint, which is the average of 3D coordinates of the hip center, hip left and hip right. The sequences are converted to a fixed length T by sampling and zero padding to allow for batch learning. T should be larger than the length of most sequences to reduce loss of information caused by sampling.</p><p>The NTU RGB+D dataset has a variable (one or two) number of persons performing actions. For samples with two persons, we only process one sequence each time, and average the predicted scores of the two. We set T = 100 for this dataset, as most sequences are less than 100 in length. For the SBU Interaction dataset with a pair of skeletons representing interactions of two persons, we concatenate the two 3D coordinates for each joint at each time step and regard it as one sequence of 6D coordinates. We set the normalized sequence length T = 35 for this dataset. For the ChaLearn Gesture Recognition dataset, we set T = 50.</p><p>For the NTU RGB+D dataset, the number of neurons of each layer of stacked RNN is 512. For hierarchical RNN, the number of neurons of the body part and the whole body are 128 and 512, respectively. For the ChaLearn Gesture Recognition dataset, the networks structures are the same as those of the NTU RGB+D dataset. Compared with the above two datasets, the SBU Interaction dataset has less number of training samples and the sequence length is shorter. So we reduce the number of neurons of stacked RNN of the temporal RNN to 256, and set the number of neurons of the body part and the whole body to 64 and 256, respectively. For all the datasets, the structure of the spatial RNN is the same as that of stacked RNN of the temporal RNN. We adopt LSTM neurons for all layers due to its excellent performance for sequence recognition.</p><p>To demonstrate the effectiveness of the two-stream RNN, we simply adopt stacked RNN for the temporal channel and chain sequence for the spatial channel. The weight of predicted scores of the temporal RNN is 0.9, and the temporal window size of the spatial RNN is one fourth of the fixed length T , both are determined by cross-validation. The networks are trained using stochastic gradient descent. The learning rate, initiated with 0.02, is reduced by multiplying it by 0.7 every 60 epochs during training. The implementation is based on Theano <ref type="bibr" target="#b41">[42]</ref> and Lasagne 1 . One NVIDIA TITAN X GPU is used to run all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental results</head><p>Comparison between models. The comprehensive results of our two-stream RNN on three datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. We can see that the two-stream RNN consistently outperforms the individual temporal RNN and spatial RNN, which confirms that the spatial and temporal channels are both effective and complementary. In addition, for two activity recognition datasets, the 3D transformation techniques bring significant performance improvement for skeleton based recognition, especially for cross view evaluation. For example, on the NTU RGB+D dataset, the twostream RNN with 3D transformation outperforms that with-1 https://github.com/Lasagne/Lasagne out 3D transformation by 7.8% for cross view evaluation, much higher than the outperformed value of 2.7% for cross subject evaluation. The explanation is straightforward that rotation transformation randomly generates new skeletons from different views, thus making our two-stream RNN robust to the viewpoint changes.</p><p>Generally, the results of the temporal RNN are much better than those of the spatial RNN. This observation is consistent with the fact that most previous RNN based methods adopt the temporal RNN to recognize actions. For the temporal RNN, the hierarchical structure generally performs better than the stacked structure. For example, on the NTU RGB+D dataset, hierarchical RNN outperforms stacked RNN by an average of 1.6%. For the spatial RNN, the results of the traversal sequence are better than those of the chain sequence as the traversal method maintains better spatial relationships of the graph structure by visiting most joints twice in both forward and reverse directions. Comparison between structures. In Section 5.2 we manually define the structures of both stacked RNN and hierarchical RNN. Here we empirically study the effects of the number of stacked layers and the number of neurons for each layer on the performance. Due to the limited space, we only give results on the NTU RGB+D dataset by cross view protocol in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>For stacked RNN, we observe that two stacked layers (R512-512) performs better than one layer (R512), and three stacked layers (R512-512-512) performs even better than two stacked layers. For the number of neurons of RNN layers, decreasing it to 256 (R256-256) reduces the accuracy and increasing it to 1024 (R1024-1024) does not necessarily improve the result. As adding more layers and increasing hidden neurons result more parameters and increase the computational complexity of our model, we adopt R512-512 as the default structure for stacked RNN.</p><p>For hierarchical RNN, using two stacked RNN layers for the part (P128-128, B512) and increasing the number of neurons of the part from 128 to 256 (P256, B512) improve the performances. The accuracy can be further improved by increasing the number of neurons of both the part and the whole body (P256, B1024). To make a fair comparison with the stacked structure (R512-512) and reduce the computational cost, we keep the structure with two layers and choose 128 as the number of neurons for the part, which is one fourth of the number of neurons for the whole body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Two-stream RNN versus temporal RNN</head><p>As previous RNN based methods merely use the temporal RNN, here we aim to show the superiority of our twostream RNN over the temporal RNN.</p><p>We plot and compare the confusion matrices of our twostream RNN and the temporal RNN on the SBU Interaction dataset in <ref type="figure" target="#fig_9">Figure 6</ref>. We can observe that there are three pairs  <ref type="figure">Figure 5</ref>. Accuracy for each action on the NTU RGB+D dataset. We also depict the accuracy of each action. <ref type="figure">Figure 5</ref> shows the results of cross subject evaluation on the NTU RGB+D dataset. For most actions, the accuracy of our twostream RNN is higher than that of the temporal RNN. For example, for brushing teeth, shaking head, and walking towards, the accuracy of the two-stream RNN is more than 8% higher than that of the temporal RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Parameter sensitivity</head><p>In this section, we evaluate the impact of parameters on the performance. Our two-stream RNN has two parameters, i.e., the size of temporal window of the spatial channel, and the weight of the temporal channel, denoted by λ and τ , respectively. <ref type="figure" target="#fig_10">Figure 7</ref> shows the evaluation results on the SBU Interaction dataset. It should be noted that similar results are observed for other datasets. <ref type="figure" target="#fig_10">Figure 7</ref>(a) shows the accuracy of the two-stream RNN w.r.t. the parameter λ, λ ∈ {0, 0.1, · · · , 0.9, 1}. We can see the best performance is reached when λ=0.8 or λ=0.9. When λ &lt; 0.8, the accuracy decreases with a smaller value of λ. The best result is much higher than the two extreme points where λ ∈ {0, 1}, which correspond to the spatial and temporal RNN, respectively.</p><p>We choose τ ∈ {1, 3, 5, · · · , T } and plot the accuracy of the spatial RNN in <ref type="figure" target="#fig_10">Figure 7</ref>(b). We find that when 5 ≤ τ ≤ 17, i.e., T /7 ≤ τ ≤ T /2, the temporal RNN obtains the best result. The performance drops when τ is not in this range. We conclude that our result is not sensitive to τ for a wide range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison with the state-of-the-art</head><p>We compare our two-stream RNN method with the recent methods in the literature. <ref type="table">Table 3</ref> shows the results on the NTU RGB+D dataset. We first compare our method with three traditional methods, i.e., 3D skeletons representation in a Lie group <ref type="bibr" target="#b43">[44]</ref>, Fisher vector encoding of skeletal quads <ref type="bibr" target="#b9">[10]</ref> and FTP dynamic <ref type="bibr" target="#b17">[18]</ref>. We observe that our performances are significantly higher, which shows the superiority of deep learning methods over the methods based on handcrafted features. Then our method is compared with other deep learning methods based on RNN. Our results are much better than the reported results of HBRNN <ref type="bibr" target="#b6">[7]</ref> and Part-aware LSTM <ref type="bibr" target="#b36">[37]</ref>, both of which only model temporal dynamics of actions. Moreover, our method outperforms the newest spatio-temporal LSTM with trust gates <ref type="bibr" target="#b29">[30]</ref> by 2.1% and 1.8% for both cross subject evaluation and cross view evaluation, respectively.</p><p>The results on the SBU Interaction dataset are shown in <ref type="table">Table 4</ref>. Our result is 7.9% higher than the best result based on handcrafted features (Joint Feature <ref type="bibr" target="#b21">[22]</ref>). In addition, our approach is superior than recent RNN based approaches by outperforming the existing best result by 1.5%. This experiment demonstrates our two-stream RNN model can recognize interactions performed by two persons very well.</p><p>The results on the Chalearn Gesture Recognition dataset are summarized in <ref type="table" target="#tab_2">Table 5</ref>. Here our two-stream RNN is only compared with the methods solely based on skeletons. For precision, recall and F1-score, our approach yields state-of-the-art performance, outperforming the recently proposed VideoDarwin <ref type="bibr" target="#b11">[12]</ref> by more than 16%. <ref type="table">Table 3</ref>. Comparison of the proposed approach with the state-ofthe-art methods on the NTU RGB+D dataset.</p><p>Method Cross subject Cross view Lie Group <ref type="bibr" target="#b43">[44]</ref> 50.1 52.8 Skeletal Quads <ref type="bibr" target="#b9">[10]</ref> 38.6 41.4 FTP Dynamic <ref type="bibr" target="#b17">[18]</ref> 60.2 65.2 HBRNN <ref type="bibr" target="#b6">[7]</ref> 59.1 64.0 Part-aware LSTM <ref type="bibr" target="#b36">[37]</ref> 62.9 70.3 Trust Gate ST-LSTM <ref type="bibr" target="#b29">[30]</ref> 69.2 77.7 Two-stream RNN 71.3 79.5 <ref type="table">Table 4</ref>. Comparison of the proposed approach with the state-ofthe-art methods on the SBU Interaction dataset. Method Accuracy Joint Feature <ref type="bibr" target="#b52">[53]</ref> 80.3 Joint Feature <ref type="bibr" target="#b21">[22]</ref> 86.9 HBRNN <ref type="bibr" target="#b6">[7]</ref> 80.4 Deep LSTM <ref type="bibr" target="#b54">[55]</ref> 86.0 Co-occurrence LSTM <ref type="bibr" target="#b54">[55]</ref> 90.4 Trust Gate ST-LSTM <ref type="bibr" target="#b29">[30]</ref> 93.3 Two-stream RNN 94.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed an end-to-end twostream RNN architecture for skeleton based action recognition, with the temporal stream modeling temporal dynamics and the spatial stream processing spatial configurations. We explore two structures to model the sequence of joints of skeletons for the temporal stream. For the spatial stream, we also devise two methods to convert the structure of skeleton into a sequence before using a RNN to handle the spatial dependency. Moreover, to improve generalization and prevent overfitting for deep learning based methods, we employ rotation transformation, scaling transformation and shear transformation as data augmentation techniques based on 3D transformation of skeletons. Our experiments have shown that two-stream RNN outperforms existing state-ofthe-art skeleton based approaches on datasets for generic actions (NTU RGB+D), interaction activities (SBU Interaction) and gestures (ChaLearn). In the future, we will consider to learn the structure patterns for the spatial channel and further improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>A two-stream RNN architecture for skeleton based action recognition. Here Softmax denotes a fully connected layer with a softmax activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a) A two-layer stacked RNN for sequence classification. (b) A LSTM block with input, output, and forget gates<ref type="bibr" target="#b16">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Hierarchical RNN for skeleton based action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>(a) The physical structure of 20 joints. (b) Convert the joints graph into a sequence. The joints of arms come first, then that of body, finally is that of legs. (c) Use a traversal method to transform the joints graph into a sequence. The order of the sequence is the same as the visiting order of the arrow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of confusion matrices on the SBU Interaction dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Parameter sensitivity analysis on the SBU Interaction dataset. Here 0 ≤ λ ≤ 1 and 1 ≤ τ ≤ T , where T = 35 is the sequence length after preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comprehensive evaluation results of two-stream RNN on three datasets.</figDesc><table><row><cell cols="5">Channel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(%)</cell><cell></cell><cell></cell><cell></cell><cell cols="16">NTU RGB+D Cross subject Cross view</cell><cell></cell><cell cols="9">SBU Interaction</cell><cell></cell><cell cols="16">ChaLearn Gesture Precision Recall F1-score</cell></row><row><cell cols="7">Temporal RNN</cell><cell></cell><cell></cell><cell cols="7">Stacked Hierarchical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">66.1 67.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">68.9 70.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">89.0 90.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">89.5 89.8</cell><cell></cell><cell></cell><cell></cell><cell cols="3">89.6 89.9</cell><cell></cell><cell></cell><cell></cell><cell cols="3">89.5 89.7</cell></row><row><cell cols="6">Spatial RNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Chain Traversal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">53.7 55.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">58.9 60.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">82.2 86.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">81.9 84.0</cell><cell></cell><cell></cell><cell></cell><cell cols="3">82.1 84.2</cell><cell></cell><cell></cell><cell></cell><cell cols="3">81.9 84.0</cell></row><row><cell cols="8">Two-stream RNN</cell><cell></cell><cell cols="7">No transform 3D transform</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">68.6 71.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">71.7 79.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">91.9 94.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">91.3 91.7</cell><cell></cell><cell></cell><cell></cell><cell cols="3">91.3 91.8</cell><cell></cell><cell></cell><cell></cell><cell cols="3">91.3 91.7</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Temporal RNN</cell><cell></cell><cell></cell><cell cols="6">Two−stream RNN</cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>drink water</cell><cell>eat meal</cell><cell>brushing teeth</cell><cell>brushing hair</cell><cell>drop</cell><cell>pickup</cell><cell>throw</cell><cell>sitting down</cell><cell>standing up</cell><cell>clapping</cell><cell>reading</cell><cell>writing</cell><cell>tear up paper</cell><cell>wear jacket</cell><cell>take off jacket</cell><cell>wear a shoe</cell><cell>take off a shoe</cell><cell>wear on glasses</cell><cell>take off glasses</cell><cell>put on a hat</cell><cell>take off hat</cell><cell>cheer up</cell><cell>hand waving</cell><cell>kicking something</cell><cell>put inside pocket</cell><cell>hopping</cell><cell>jump up</cell><cell>answer phone</cell><cell>playing with phone</cell><cell>type keyboard</cell><cell>point with finger</cell><cell>take selfie</cell><cell>check time</cell><cell>rub hands</cell><cell>nod head</cell><cell>shake head</cell><cell>wipe face</cell><cell>salute</cell><cell>put palms together</cell><cell>cross hands</cell><cell>sneeze</cell><cell>staggering</cell><cell>falling</cell><cell>touch head</cell><cell>touch chest</cell><cell>touch back</cell><cell>touch neck</cell><cell>nausea</cell><cell>use a fan</cell><cell>punching</cell><cell>kicking person</cell><cell>pushing person</cell><cell>pat on person</cell><cell>point finger</cell><cell>hugging</cell><cell>giving something</cell><cell>touch his pocket</cell><cell>handshaking</cell><cell>walking towards</cell><cell>walking apart</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Empirical study of networks structures. For stacked RNN, R512-512 denotes two stacked layers of RNN with 512 hidden neurons. Similarly, R1024 denotes one RNN layer with 1024 hidden neurons. For hierarchical RNN, P128-128, B512 denotes two stacked RNN layers with 128 hidden neurons for the body part and one RNN layer with 512 hidden neurons for the whole body. And so on for the other symbols. The default structures of stacked RNN and hierarchical RNN are R512-512 and P128, B512, respectively.</figDesc><table><row><cell cols="2">Stacked RNN</cell><cell cols="2">Hierarchical RNN</cell></row><row><cell>R512-512</cell><cell>68.9</cell><cell>P128, B512</cell><cell>70.5</cell></row><row><cell cols="4">R512-512-512 69.2 P128-128, B512 71.4</cell></row><row><cell>R512</cell><cell>68.6</cell><cell>P256, B512</cell><cell>71.4</cell></row><row><cell>R1024-1024</cell><cell>68.9</cell><cell>P128, B1024</cell><cell>70.6</cell></row><row><cell>R256-256</cell><cell>68.2</cell><cell>P256, B1024</cell><cell>72.2</cell></row><row><cell cols="4">of misclassified actions for the temporal RNN, but only one</cell></row><row><cell cols="4">pair for our two-stream RNN. Moreover, for pushing, the</cell></row><row><cell cols="4">samples are 22% misclassified as punching by the temporal</cell></row><row><cell cols="4">RNN, while our two-stream RNN can correctly recognize</cell></row><row><cell>all the samples.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Comparison of the proposed approach with the state-ofthe-art methods on the ChaLearn Gesture Recognition dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F1-score</cell></row><row><cell>Skeleton Feature [48]</cell><cell>59.9</cell><cell>59.3</cell><cell>59.6</cell></row><row><cell>Portfolios [52]</cell><cell>-</cell><cell>-</cell><cell>56.0</cell></row><row><cell>Gesture Spotting [35]</cell><cell>61.2</cell><cell>62.3</cell><cell>61.7</cell></row><row><cell>HiVideoDarwin [45]</cell><cell>74.9</cell><cell>75.6</cell><cell>74.6</cell></row><row><cell>CNN for Skeleton [5]</cell><cell>91.2</cell><cell>91.3</cell><cell>91.2</cell></row><row><cell>VideoDarwin [12]</cell><cell>75.3</cell><cell>75.1</cell><cell>75.2</cell></row><row><cell>Two-stream RNN</cell><cell>91.7</cell><cell>91.8</cell><cell>91.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is jointly supported by National Key Research and Development Program of China (2016YFB1001000), National Natural Science Foundation of China (61525306, 61633021, 61420106015) and Beijing Natural Science Foundation (4162058). This work is also supported by grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">G3d: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACPR</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chalearn looking at people challenge 2014: Dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ponce-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-modal gesture recognition challenge 2013: dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<editor>ICMI. ACM</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A field guide to dynamical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive body part contrast mining for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">High-level event recognition in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJMIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual motion perception. Scientific American</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond actions: Discriminative models for contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognition and segmentation of 3-d human action using hmm and multi-class adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Measuring and reducing observational latency when recognizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Laviola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordóñez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Domain-adaptive discriminative one-shot learning of gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human action recognition using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sempena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">U</forename><surname>Maulidevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Aryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICEEI. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Human activity detection from rgbd images. Plan, Activity, and Intent Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<idno>abs/1605.02688</idno>
		<title level="m">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv eprints</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical motion evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACPR. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fusing multi-modal features for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Parameterized modeling and recognition of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Eigenjoints-based action recognition using naive-bayes-nearest-neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gesture recognition portfolios for personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using bodypose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Kinematics of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zatsiorski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Kinetics</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
