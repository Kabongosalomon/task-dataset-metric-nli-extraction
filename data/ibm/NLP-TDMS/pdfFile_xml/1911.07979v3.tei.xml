<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
							<email>ekagra.ranjan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Guwahati</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNN) have been shown to work effectively for modeling graph structured data to solve tasks such as node classification, link prediction and graph classification. There has been some recent progress in defining the notion of pooling in graphs whereby the model tries to generate a graph level representation by downsampling and summarizing the information present in the nodes. Existing pooling methods either fail to effectively capture the graph substructure or do not easily scale to large graphs. In this work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and differentiable pooling method that addresses the limitations of previous graph pooling architectures. ASAP utilizes a novel self-attention network along with a modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs to form the pooled graph. Through extensive experiments on multiple datasets and theoretical analysis, we motivate our choice of the components used in ASAP. Our experimental results show that combining existing GNN architectures with ASAP leads to state-of-the-art results on multiple graph classification benchmarks. ASAP has an average improvement of 4%, compared to current sparse hierarchical state-of-the-art method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an increasing interest in developing Graph Neural Networks (GNNs) for graph structured data. CNNs have shown to be successful in tasks involving images <ref type="bibr" target="#b12">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b9">He et al. 2016</ref>) and text <ref type="bibr" target="#b10">(Kim 2014)</ref>. Unlike these regular grid data, arbitrary shaped graphs have rich information present in their graph structure. By inherently capturing such information through message propagation along the edges of the graph, GNNs have proved to be more effective for graphs <ref type="bibr" target="#b8">(Gilmer et al. 2017;</ref><ref type="bibr" target="#b9">Hamilton, Ying, and Leskovec 2017)</ref>. GNNs have been successfully applied in tasks such as semantic role labeling (Marcheggiani and Titov 2017), relation extraction <ref type="bibr" target="#b19">(Vashishth et al. 2018b)</ref>, neural machine translation <ref type="bibr" target="#b0">(Bastings et al. 2017)</ref>, document dating <ref type="bibr" target="#b18">(Vashishth et al. 2018a)</ref>, and molecular feature extraction <ref type="bibr" target="#b9">(Kearnes et al. 2016)</ref>. While some of the works focus on learning node-level representations to perform tasks such as node classification ) and link prediction <ref type="bibr" target="#b17">(Schlichtkrull et al. 2017;</ref><ref type="bibr" target="#b19">Vashishth et al. 2019)</ref>, others focus on learning graph-level representations for tasks like graph classification <ref type="bibr" target="#b1">(Bruna et al. 2013;</ref><ref type="bibr">Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b25">Ying et al. 2018;</ref><ref type="bibr" target="#b7">Gao and Ji 2019;</ref><ref type="bibr" target="#b13">Lee, Lee, and Kang 2019)</ref> and graph regression <ref type="bibr" target="#b24">(Xie and Grossman 2018;</ref><ref type="bibr" target="#b16">Sanyal et al. 2018)</ref>. In this paper, we focus on graph-level representation learning for the task of graph classification.</p><p>Briefly, the task of graph classification involves predicting the label of an input graph by utilizing the given graph structure and initial node-level representations. For example, given a molecule, the task could be to predict if it is toxic. Current GNNs are inherently flat and lack the capability of aggregating node information in a hierarchical manner. Such architectures rely on learning node representations through some GNN followed by aggregation of the node information to generate the graph representation <ref type="bibr" target="#b21">(Vinyals, Bengio, and Kudlur 2016;</ref><ref type="bibr" target="#b14">Li et al. 2016;</ref>. But learning graph representations in a hierarchical manner is important to capture local substructures that are present in graphs. For example, in an organic molecule, a set of atoms together can act as a functional group and play a vital role in determining the class of the graph.</p><p>To address this limitation, new pooling architectures have been proposed where sets of nodes are recursively aggregated to form a cluster that represents a node in the pooled graph, thus enabling hierarchical learning. DiffPool <ref type="bibr" target="#b25">(Ying et al. 2018</ref>) is a differentiable pooling operator that learns a soft assignment matrix mapping each node to a set of clusters. Since this assignment matrix is dense, it is not easily scalable to large graphs <ref type="bibr" target="#b1">(Cangea et al. 2018)</ref>. Following that, TopK <ref type="bibr" target="#b7">(Gao and Ji 2019)</ref> is proposed which learns a scalar projection score for each node and selects the top k nodes. They address the sparsity concerns of DiffPool but are unable to capture the rich graph structure effectively. Recently, SAG- Pool <ref type="bibr" target="#b13">(Lee, Lee, and Kang 2019)</ref>, a TopK based architecture, has been proposed which leverages self-attention network to learn the node scores. Although local graph structure is used for scoring nodes, it is still not used effectively in determining the connectivity of the pooled graph. Pooling methods that leverage the graph structure effectively while maintaining sparsity currently don't exist. We address the gap in this paper.</p><p>In this work, we propose a new sparse pooling operator called Adaptive Structure Aware Pooling (ASAP) which overcomes the limitations in current pooling methods. Our contributions can be summarized as follows:</p><p>• We introduce ASAP, a sparse pooling operator capable of capturing local subgraph information hierarchically to learn global features with better edge connectivity in the pooled graph. • We propose Master2Token (M2T), a new self-attention framework which is better suited for global tasks like pooling. • We introduce a new convolution operator LEConv, that can adaptively learn functions of local extremas in a graph substructure.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>Various formulation of GNNs have been proposed which use both spectral and non-spectral approaches. Spectral methods <ref type="bibr" target="#b1">(Bruna et al. 2013;</ref><ref type="bibr">Henaff, Bruna, and LeCun 2015)</ref> aim at defining convolution operation using Fourier transformation and graph Laplacian. These methods do not directly generalize to graphs with different structure . Non-spectral methods <ref type="bibr" target="#b2">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b25">Xu et al. 2018;</ref><ref type="bibr" target="#b15">Monti et al. 2017;</ref>) define convolution through a local neighborhood around nodes in the graph. They are faster than spectral methods and easily generalize to other graphs. GNNs can also be viewed as message passing algorithm where nodes iteratively aggregate messages from neighboring nodes through edges <ref type="bibr" target="#b8">(Gilmer et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pooling</head><p>Pooling layers overcome GNN's inability to aggregate nodes hierarchically. Earlier pooling methods focused on deterministic graph clustering algorithms <ref type="bibr" target="#b2">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b18">Simonovsky and Komodakis 2017)</ref>. Ying et al. introduced the first differentiable pooling operator which out-performed the previous deterministic methods. Since then, new data-driven pooling methods have been proposed; both spectral <ref type="bibr" target="#b15">(Ma et al. 2019;</ref><ref type="bibr" target="#b3">Dhillon, Guan, and Kulis 2007)</ref> and non-spectral <ref type="bibr" target="#b25">(Ying et al. 2018;</ref><ref type="bibr" target="#b7">Gao and Ji 2019)</ref>. Spectral methods aim at capturing the graph topology using eigen-decomposition algorithms. However, due to higher computational requirement for spectral graph techniques, they are not easily scalable to large graphs. Hence, we focus on non-spectral methods.</p><p>Pooling methods can further be divided into global and hierarchical pooling layers. Global pooling summarize the entire graph in just one step. Set2Set <ref type="bibr" target="#b21">(Vinyals, Bengio, and Kudlur 2016)</ref> finds the importance of each node in the graph through iterative content-based attention. Global-Attention <ref type="bibr" target="#b14">(Li et al. 2016)</ref> uses an attention mechanism to aggregate nodes in the graph. SortPool  summarizes the graph by concatenating few nodes after sorting them based on their features. Hierarchical pooling is used to capture the topological information of graphs. DiffPool forms a fixed number of clusters by aggregating nodes. It uses GNN to compute a dense soft assignment matrix, making it infea-  To address these limitations, we propose ASAP, which has all the desirable properties of hierarchical pooling without compromising on sparsity in graph operations. Please see <ref type="table">Table.</ref> 1 for an overall comparison of hierarchical pooling methods. Further comparison discussions between hierarchical architectures are presented in Sec. 8.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Consider a graph G(V, E, X) with N = |V| nodes and |E| edges. Each node v i ∈ V has d-dimensional feature representation denoted by x i . X ∈ R N ×d denotes the node feature matrix and A ∈ R N ×N represents the weighted adjacency matrix. The graph G also has a label y associated with it. Given a dataset D = {(G 1 , y 1 ), (G 2 , y 2 ), ...}, the task of graph classification is to learn a mapping f : G → Y, where G is the set of input graphs and Y is the set of labels associated with each graph. A pooled graph is denoted by G p (V p , E p , X p ) with node embedding matrix X p and its adjacency matrix as A p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolution Networks</head><p>We use Graph Convolution Network (GCN)  for extracting discriminative features for graph classification. GCN is defined as:</p><formula xml:id="formula_0">X (l+1) = σ(D − 1 2ÂD 1 2 X (l) W (l) ),<label>(1)</label></formula><p>whereÂ = A + I for self-loops,D = jÂ i,j and W (l) ∈ R d×f is a learnable matrix for any layer l. We use the initial node feature matrix wherever provided, i.e., X (0) = X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Attention</head><p>Self-attention is used to find the dependency of an input on itself <ref type="bibr" target="#b2">(Cheng, Dong, and Lapata 2016;</ref><ref type="bibr" target="#b19">Vaswani et al. 2017</ref>). An alignment score α i,j is computed to map the importance of candidates c j on target query q i . In self-attention, target query q i and candidates c j are obtained from input entities h = {h 1 , ..., h n }. Self-attention can be categorized as To-ken2Token and Source2Token based on the choice of target query q <ref type="bibr" target="#b18">(Shen et al. 2018)</ref>.</p><p>Token2Token (T2T) selects both the target and candidates from the input set h. In the context of additive attention <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>, α i,j is computed as:</p><formula xml:id="formula_1">α i,j = sof tmax( v T σ(W h i W h j )).<label>(2)</label></formula><p>where is the concatenation operator.</p><p>Source2Token (S2T) finds the importance of each candidate to a specific global task which cannot be represented by any single entity. α i,j is computed by dropping the target query term. Eq.</p><p>(2) changes to the following:</p><formula xml:id="formula_2">α i,j = sof tmax( v T σ(W h j )).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Receptive Field</head><p>We extend the concept of receptive field RF from pooling operations in CNN to GNN 2 . We define RF node of a pooling operator as the number of hops needed to cover all the nodes in the neighborhood that influence the representation of a particular output node. Similarly, RF edge of a pooling operator is defined as the number of hops needed to cover all the edges in the neighborhood that affect the representation of an edge in the pooled graph G p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ASAP: Proposed Method</head><p>In this section we describe the components of our proposed method ASAP. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, ASAP initially considers all possible local clusters with a fixed receptive field for a given input graph. It then computes the cluster membership of the nodes using an attention mechanism. These clusters are then scored using a GNN as depicted in <ref type="figure" target="#fig_0">Fig 1(c)</ref>. Further, a fraction of the top scoring clusters are selected as nodes in the pooled graph and new edge weights are computed between neighboring clusters as shown in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>.</p><p>Below, we discuss the working of ASAP in details. Please refer to Appendix Sec. I for a pseudo code of the working of ASAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cluster Assignment</head><p>Initially, we consider each node v i in the graph as a medoid of a cluster c h (v i ) such that each cluster can represent only the local neighbors N within a fixed radius of h hops i.e.,</p><formula xml:id="formula_3">c h (v i ) = N h (v i )</formula><p>. This effectively means that RF node = h for ASAP. This helps the clusters to effectively capture the information present in the graph sub-structure. Let x c i be the feature representation of a cluster c h (v i ) centered at v i . We define G c (V, E, X c ) as the graph with node feature matrix X c ∈ R N ×d and adjacency matrix A c = A. We denote the cluster assignment matrix by S ∈ R N ×N , where S i,j represents the membership of node v i ∈ V in cluster c h (v j ). By employing such local clustering (Schaeffer 2007), we can maintain sparsity of the cluster assignment matrix S similar to the original graph adjacency matrix A i.e., space complexity of both S and A is O(|E|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cluster Formation using Master2Token</head><p>Given a cluster c h (v i ), we learn the cluster assignment matrix S through a self-attention mechanism. The task here is to learn the overall representation of the cluster c h (v i ) by attending to the relevant nodes in it. We observe that both T2T and S2T attention mechanisms described in Sec. 3.3 do not utilize any intra-cluster information. Hence, we propose a new variant of self-attention called Master2Token (M2T). We further motivate the need for M2T framework later in Sec. 8.2. In M2T framework, we first create a master query m i ∈ R d which is representative of all the nodes within a cluster:</p><formula xml:id="formula_4">m i = f m (x j |v j ∈ c h (v i )}),<label>(4)</label></formula><p>where x j is obtained after passing x j through a separate GCN to capture structural information in the cluster c h (v i ) 3 . f m is a master function which combines and transforms feature representation of v j ∈ c h (v i ) to find m i . In this work we experiment with max master function defined as:</p><formula xml:id="formula_5">m i = max vj ∈c h (vi) (x j ).<label>(5)</label></formula><p>This master query m i attends to all the constituent nodes v j ∈ c h (v i ) using additive attention:</p><formula xml:id="formula_6">α i,j = sof tmax( w T σ(W m i x j )).<label>(6)</label></formula><p>where w T and W are learnable vector and matrix respectively. The calculated attention scores α i,j signifies the membership strength of node v j in cluster c h (v i ). Hence, we use this score to define the cluster assignment matrix discussed above, i.e., S i,j = α i,j . The cluster representation x c i for c h (v i ) is computed as follows:</p><formula xml:id="formula_7">x c i = |c h (vi)| j=1 α i,j x j .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cluster Selection using LEConv</head><p>Similar to TopK (Gao and Ji 2019), we sample clusters based on a cluster fitness score φ i calculated for each cluster in the graph G c using a fitness function f φ . For a given pooling ratio k ∈ (0, 1], the top kN clusters are selected and included in the pooled graph G p . To compute the fitness scores, we introduce Local Extrema Convolution (LEConv), a graph convolution method which can capture local extremum information. In Sec. 5.1 we motivate the choice of LEConv's formulation and contrast it with the 3 If xj is used as it is then interchanging any two nodes in a cluster will have not affect the final output, which is undesirable. standard GCN formulation. LEConv is used to compute φ as follows:</p><formula xml:id="formula_8">φ i = σ(x c i W 1 + j∈N (i) A c i,j (x c i W 2 − x c j W 3 ))<label>(8)</label></formula><p>where N (i) denotes the neighborhood of the i th node in G c . W 1 , W 2 , W 3 are learnable parameters and σ(.) is some activation function.</p><formula xml:id="formula_9">Fitness vector Φ = [φ 1 , φ 2 , ..., φ N ] T is mul- tiplied to the cluster feature matrix X c to make f φ learnable i.e.,:X c = Φ X c ,</formula><p>where is broadcasted hadamard product. The function TOP k (.) ranks the fitness scores and gives the indicesî of top kN selected clusters in G c as follows:</p><formula xml:id="formula_10">i = TOP k (X c , kN ).</formula><p>The pooled graph G p is formed by selecting these top kN clusters. The pruned cluster assignment matrixŜ ∈ R N × kN and the node feature matrix X p ∈ R kN ×d are given by:</p><formula xml:id="formula_11">Ŝ = S(:,î), X p =X c (î, :)<label>(9)</label></formula><p>whereî is used for index slicing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Maintaining Graph Connectivity</head><p>Following (Ying et al. 2018), once the clusters have been sampled, we find the new adjacency matrix A p for the pooled graph G p usingÂ c andŜ in the following manner:</p><formula xml:id="formula_12">A p =Ŝ TÂcŜ<label>(10)</label></formula><p>whereÂ c = A c + I. Equivalently, we can see that A p i,j = k,lŜ k,iÂ c k,lŜ l,j . This formulation ensures that any two clusters i and j in G p are connected if there is any common node in the clusters c h (v i ) and c h (v j ) or if any of the constituent nodes in the clusters are neighbors in the original graph G ( <ref type="figure" target="#fig_0">Fig. 1(d)</ref>). Hence, the strength of the connection between clusters is determined by both the membership of the constituent nodes throughŜ and the edge weights A c . Note thatŜ is a sparse matrix by formulation and hence the above operation can be implemented efficiently.</p><p>5 Theoretical Analysis 5.1 Limitations of using GCN for scoring clusters GCN from Eq. (1) can be viewed as an operator which first computes a pre-scoreφ for each node i.e.,φ = XW followed by a weighted average over neighbors and a nonlinearity. If for some node the pre-score is very high, it can increase the scores of its neighbors which inherently biases the pooling operator to select clusters in the local neighborhood instead of sampling clusters which represent the whole graph. Thus, selecting the clusters which correspond to local extremas of pre-score function would potentially allow us to sample representative clusters from all parts of the graph.  <ref type="table">Table 2</ref>: Comparison of ASAP with previous global and hierarchical pooling. Average accuracy and standard deviation is reported for 20 random seeds. We observe that ASAP consistently outperforms all the baselines on all the datasets. Please refer to Sec. 7.1 for more details.</p><p>Theorem 1. Let G be a graph with positive adjacency ma-</p><formula xml:id="formula_13">trix A i.e., A i,j ≥ 0. Consider any function f (X, A) : R N ×d × R N ×N → R N ×1</formula><p>which depends on difference between a node and its neighbors after a linear transformation W ∈ R d×1 . For e.g,:</p><formula xml:id="formula_14">f i = σ(α i x i W + j∈N (i) β i,j (x i W − x j W )) where f i , α i , β i,j ∈ R and x i ∈ R d . a) If fitness value Φ = GCN (X, A) then Φ cannot learn f. b) If fitness value Φ = LEConv(X, A) then Φ can learn f.</formula><p>Proof. See Appendix Sec. F for proof.</p><p>Motivated by the above analysis, we propose to use LEConv (Eq. 8) for scoring clusters. LEConv can learn to score clusters by considering both its global and local importance through the use of self-loops and ability to learn functions of local extremas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Connectivity</head><p>Here, we analyze ASAP from the aspect of edge connectivity in the pooled graph. When considering h-hop neighborhood for clustering, both ASAP and DiffPool have RF edge = 2h + 1 because they use Eq. (10) to define the edge connectivity. On the other hand, both TopK and SAG-Pool have RF edge = h. A larger edge receptive field implies that the pooled graph has better connectivity which is important for the flow of information in the subsequent GCN layers. Theorem 2. Let the input graph G be a tree of any possible structure with N nodes. Let k * be the lower bound on sampling ratio k to ensure the existence of atleast one edge in the pooled graph irrespective of the structure of G and the location of the selected nodes. For TopK or SAGPool, k * → 1 whereas for ASAP, k * → 0.5 as N → ∞.</p><p>Proof. See Appendix Sec. G for proof.</p><p>Theorem 2 suggests that ASAP can achieve a similar degree of connectivity as SAGPool or TopK for a much smaller sampling ratio k. For a tree with no prior information about its structure, ASAP would need to sample only half of the clusters whereas TopK and SAGPool would need to sample almost all the nodes, making TopK and SAGPool inefficient for such graphs. In general, independent of any combination of nodes selected, ASAP will have better connectivity due to its larger receptive field. Please refer to Appendix Sec. G for a similar analysis on path graph and more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Permutation Equivariance</head><p>Proposition 1. ASAP is a graph permutation equivariant pooling operator.</p><p>Proof. See Appendix Sec. H for proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>In our experiments, we use 5 graph classification benchmarks and compare ASAP with multiple pooling methods. Below, we describe the statistics of the dataset, the baselines used for comparisons and our evaluation setup in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We demonstrate the effectiveness of our approach on 5 graph classification datasets. D&amp;D <ref type="bibr" target="#b18">(Shervashidze et al. 2011;</ref><ref type="bibr" target="#b4">Dobson and Doig 2003)</ref> and PROTEINS <ref type="bibr" target="#b4">(Dobson and Doig 2003;</ref><ref type="bibr" target="#b1">Borgwardt et al. 2005</ref>) are datasets containing proteins as graphs. NCI1 <ref type="bibr" target="#b22">(Wale, Watson, and Karypis 2008)</ref> and NCI109 are datasets for anticancer activity classification. FRANKENSTEIN (Orsini, Frasconi, and De Raedt 2015) contains molecules as graph for mutagen classification. Please refer to <ref type="table" target="#tab_4">Table 3</ref> for the dataset statistics.</p><p>Dataset  </p><formula xml:id="formula_15">G avg C avg V avg E avg<label>D&amp;D</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>We compare ASAP with previous state-of-the-art hierarchical pooling operators DiffPool (Ying et al. 2018), TopK <ref type="bibr" target="#b7">(Gao and Ji 2019)</ref> and SAGPool <ref type="bibr" target="#b13">(Lee, Lee, and Kang 2019)</ref>. For comparison with global pooling, we choose Set2Set <ref type="bibr" target="#b21">(Vinyals, Bengio, and Kudlur 2016)</ref>, Global-Attention <ref type="bibr" target="#b14">(Li et al. 2016)</ref> and SortPool ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training &amp; Evaluation Setup</head><p>We use a similar architecture as defined in <ref type="bibr" target="#b1">(Cangea et al. 2018;</ref><ref type="bibr" target="#b13">Lee, Lee, and Kang 2019)</ref> which is depicted in <ref type="figure" target="#fig_0">Fig.  1(f)</ref>. For ASAP, we choose k = 0.5 and h = 1 to be consistent with baselines. 4 Following SAGPool(Lee, Lee, and Kang 2019), we conduct our experiments using 10-fold cross-validation and report the average accuracy on 20 random seeds. 5</p><p>Aggregation type FITNESS CLUSTER None --Only cluster -Both </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In this section, we attempt to answer the following questions: Q1 How does ASAP perform compared to other pooling methods at the task of graph classification? (Sec. 7.1) Q2 Is cluster formation by M2T attention based node aggregation beneficial during pooling? (Sec. 7.3) Q3 Is LEConv better suited as cluster fitness scoring function compared to vanilla GCN? (Sec. 7.4) Q4 How helpful is the computation of inter-cluster soft edge weights instead of sampling edges from the input graph? (Sec. 7.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Performance Comparison</head><p>We compare the performace of ASAP with baseline methods on 5 graph classification tasks. The results are shown in <ref type="table">Table 2</ref>. All the numbers for hierarchical pooling (Diff-Pool, TopK and SAGPool) are taken from <ref type="bibr" target="#b13">(Lee, Lee, and Kang 2019)</ref>. For global pooling (Set2Set, Global-Attention and SortPool), we modify the architectural setup to make them comparable with the hierarchical variants. 6 . We observe that ASAP consistently outperforms all the baselines on all 5 datasets. We note that ASAP has an average improvement of 4% and 3.5% over previous state-of-the-art hierarchical (SAGPool) and global (SortPool) pooling methods respectively. We also observe that compared to other hierarchical methods, ASAP has a smaller variance in performance which suggests that the training of ASAP is more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Effect of Node Aggregation</head><p>Here, we evaluate the improvement in performance due to our proposed technique of aggregating nodes to form a cluster. There are two aspects involved during the creation of clusters for a pooled graph:</p><p>• FITNESS: calculating fitness scores for individual nodes. Scores can be calculated either by using only the medoid or by aggregating neighborhood information.</p><p>• CLUSTER: generating a representation for the new cluster node. Cluster representation can either be the medoid's representation or some feature aggregation of the neighborhood around the medoid.</p><p>We test three types of aggregation methods: 'None', 'Only cluster' and 'Both' as described in <ref type="table" target="#tab_5">Table 4</ref>. As shown in Table 5, we observe that our proposed node aggregation helps improve the performance of ASAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation FRANKENSTEIN NCI1</head><p>None 67.4 ±0.6 69.9 ± 2.5 Only cluster 67.5 ±0.5 70.6 ± 1.8 Both 67.8 ± 0.6 70.7 ± 2.3  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Effect of M2T Attention</head><p>We compare our M2T attention framework with previously proposed S2T and T2T attention techniques. The results are shown in <ref type="table" target="#tab_7">Table 6</ref>. We find that M2T attention is indeed better than the rest in NCI1 and comparable in FRANKENSTEIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Effect of LEConv as a fitness scoring function</head><p>In this section, we analyze the impact of LEConv as a fitness scoring function in ASAP. We use two baselines -GCN (Eq. 1) and Basic-LEConv which computes φ i = σ(x i W + j∈N (xi) A i,j (x i W − x j W )). In <ref type="table" target="#tab_9">Table 7</ref> we can see that Basic-LEConv and LEConv perform significantly  better than GCN because of their ability to model functions of local extremas. Further, we observe that LEConv performs better than Basic-LEConv as it has three different linear transformation compared to only one in the latter. This allows LEConv to potentially learn complicated scoring functions which is better suited for the final task. Hence, our analysis in Theorem 1 is emperically validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Effect of computing Soft edge weights</head><p>We evaluate the importance of calculating edge weights for the pooled graph as defined in Eq. 10. We use the best model configuration as found from above ablation analysis and then add the feature of computing soft edge weights for clusters. We observe a significant drop in performace when the edge weights are not computed. This proves the necessity of capturing the edge information while pooling graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft edge weights FRANKENSTEIN NCI1</head><p>Absent 67.8 ± 0.6 70.7 ± 2.3 Present 68.3 ± 0.5 73.4 ± 0.4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Comparison of Self-Attention variants</head><p>Source2Token &amp; Token2Token T2T models the membership of a node by generating a query based only on the medoid of the cluster. Graph Attention Network (GAT) ) is an example of T2T attention in graphs. S2T finds the importance of each node for a global task. As shown in Eq. 3, since a query vector is not used for calculating the attention scores, S2T inherently assigns the same membership score to a node for all the possible clusters that node can belong to. Hence, both S2T and T2T mechanisms fail to effectively utilize the intra-cluster information while calculating a node's cluster membership. On the other hand, M2T uses a master function f m to generate a query vector which depends on all the entities within the cluster and hence is a more representative formulation.</p><p>To understand this, consider the following scenario. If in a given cluster, a non-medoid node is removed, then the unnormalized membership scores for the rest of the nodes will remain unaffected in S2T and T2T framework whereas the change will reflect in the scores calculated using M2T mechanism. Also, from <ref type="table" target="#tab_7">Table 6</ref>, we find that M2T performs better than S2T and T2T attention showing that M2T is better suited for global tasks like pooling.</p><p>In this paper, we introduce ASAP, a sparse and differentiable pooling method for graph structured data. ASAP clusters local subgraphs hierarchically which helps it to effectively learn the rich information present in the graph structure. We propose Master2Token self-attention framework which enables our model to better capture the membership of each node in a cluster. We also propose LEConv, a novel GNN formulation that scores the clusters based on its local and global importance. ASAP leverages LEConv to compute cluster fitness scores and samples the clusters based on it. This ensures the selection of representative clusters throughout the graph. ASAP also calculates sparse edge weights for the selected clusters and is able to capture the edge connectivity information efficiently while being scalable to large graphs. We validate the effectiveness of the components of ASAP both theoretically and empirically. Through extensive experiments, we demonstrate that ASAP achieves state-ofthe-art performace on multiple graph classification datasets.</p><p>For all our experiments, Adam (Kingma and Ba 2014) optimizer is used. 10-fold cross-validation is used with 80% for training and 10% for validation and test each. Models were trained for 100 epochs with lr decay of 0.5 after every 50 epochs. The range of hyperparameter search are provided in <ref type="table" target="#tab_12">Table 9</ref>. The model with best validation accuracy was selected for testing. Our code is based on Pytorch Geometric library <ref type="bibr" target="#b5">(Fey and Lenssen 2019)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Hierarchical Pooling Setup</head><p>For hierarchical pooling, we follow SAGPool (Lee, Lee, and Kang 2019) and use three layers of GCN, each followed by a pooling layer. After each pooling step, the graph is summarized using a readout function which is a concatenation of the mean and max of the node representations (similar to SAGPool). The summaries are then added and passed through a network of fully-connected layers separated by dropout layers to predict the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Global Pooling Setup</head><p>Global Pooling architecture is same as the hierarchical architecture with the only difference that pooling is done only after all GCN layers. We do not use readout function for global pooling as they do not require them. To be comparable with other models, we restrict the feature dimension of the pooling output to be no more than 256. For global pooling layers, range for hidden dimension and lr search was same as ASAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Range</head><p>Set2Set processing-step ∈ {5, 10} Global-Attention transform ∈ {T rue, F alse} SortPool K is chosen such that output of pooling ≤ 256 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Similarities between pooling in CNN and ASAP</head><p>In CNN, pooling methods (e.g mean pool and max pool) have two hyperparameter: kernel size and stride. Kernel size decides the number of pixels being considered for computing each new pixel value in the next layer. Stride decides the fraction of new pixels being sampled thereby controlling the size of the image in next layer. In ASAP, RF node determines the neighborhood radius of clusters and k decides the sampling ratio. This makes RF node and k are analogous to kernel size and stride of CNN pooling respectively. There are however some key differences. In CNN, a given kernel size corresponds to a fixed number of pixels around a central pixel whereas in ASAP, the number of nodes being considered is variable, although the neighborhood RF node is constant. In CNN, stride uniformly samples from new pixels whereas in ASAP, the model has the flexibility to attend to different parts of the graph and sample accordingly. Intuitively, higher k will lead to more information retention. Hence, we expect an increase in performance with increasing k. This is empirically observed in <ref type="figure" target="#fig_1">Fig. 2</ref>. However, as k increases, the computational resources required by the model also increase because a relatively larger pooled graph gets propagated to the later layers. Hence, there is a trade-off between performance and computational requirement while deciding on the pooling ratio k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof of Theorem 1</head><p>Theorem 1. Let G be a graph with positive adjacency matrix A i.e., A i,j ≥ 0. Consider any function f (X, A) : R N ×d × R N ×N → R N ×1 which depends on difference between a node and its neighbors after a linear transformation W ∈ R d×d . For e.g:</p><formula xml:id="formula_16">f i = σ(α i x i W + j∈N (i) β i,j (x i W − x j W )) where f i , α i , β i,j ∈ R and x i ∈ R d . a) If fitness value φ = GCN (X, A) then φ cannot learn f. b) If fitness value φ = LEConv(X, A) then φ can learn f. Proof. For GCN, φ i = σ( j∈N (xi)∪{i} A i,j x j W ) where W is a learnable matrix. Since A i,j ≥ 0, φ i cannot have a term of the form β i,j (x i W − x j W )</formula><p>which proves the first part of the theorem. We prove the second part by showing that LEConv can learn the following function f :</p><formula xml:id="formula_17">f i = σ(α i x i W + j∈N (i) β i,j (x i W − x j W ))<label>(11)</label></formula><p>LEConv formulation is defined as:</p><formula xml:id="formula_18">φ i = σ(x i W 1 + j∈N (i) A i,j (x i W 2 − x j W 3 ))<label>(12)</label></formula><p>where W 1 , W 2 and W 3 are learnable matrices. For W 3 = W 2 = W 1 , α 1 = 1 and β i,j = A i,j we find Eq. (12) is equal to Eq. (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Graph Connectivity Proof of Theorem 2</head><p>Definition 1. For a graph G, we define optimum-nodes n * h (G) as the maximum number of nodes that can be selected which are atleast h hops away from each other. Definition 2. For a given number of nodes N , we define optimum-tree T * N as the tree which has maximum optimumnodes n * h (T N ) among all possible trees T N with N nodes. Lemma 1. Let T * N be an optimum-tree of N vertices and T * N −1 be an optimum tree with N −1 vertices. The optimumnodes of T * N and T * N −1 differ by atmost one, i.e., 0 ≤ n * h (T * N ) − n * h (T * N −1 ) ≤ 1. Proof. Consider T * N which has N nodes. We can remove any one of the leaf nodes in T * N to obtain a tree T N −1 with N − 1 nodes. If any one of the nodes in n * h (T * N ) was removed, then n * h (T N −1 ) would become n * h (T * N ) − 1. If any other node was removed , then being a leaf it does not constitute the shortest path between any of the n * h (T N −1 ) nodes. This implies that the optimum-nodes for T N −1 is atleast Since T * N −1 is the optimal-tree, we know that:</p><formula xml:id="formula_19">n * h (T * N ) − 1, i.e., n * h (T * N ) − 1 ≤ n * h (T N −1 ) ≤ n * h (T * N )<label>(13)</label></formula><formula xml:id="formula_20">n * h (T N −1 ) ≤ n * h (T * N −1 )<label>(14)</label></formula><p>Using Eq. <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref> we can write:</p><formula xml:id="formula_21">n * h (T * N ) − n * h (T * N −1 ) ≤ 1 which proves our lemma.</formula><p>Lemma 2. Let T * N be an optimum-tree of N vertices and T * N −1 be an optimum-tree of N − 1 vertices. T * N −1 is an induced subgraph of T * N . Proof. Let us choose a node to be removed from T * N and join its neighboring nodes to obtain a tree T N −1 with N − 1 nodes with an objective of ensuring a maximum n * h (T N −1 ). To do so, we can only remove a leaf node from T * N . This is because removing non-leaf nodes can reduce the shortest path between multiple pairs of nodes whereas removing leaf-nodes will reduce only the shortest path to nodes from the new leaf at that position. This ensures least reduction in optimum-nodes for T N −1 . Removing a leaf node implies that n * h (T N −1 ) cannot be lesser than n * h (T * N ) − 1 as it affects only the paths involving that particular leaf node. Using Lemma 1, we see that T N −1 is equivalent to T * N −1 , i.e., T N −1 is one of the possible optimal-trees with N − 1 nodes. Since T N −1 was formed by removing a leaf node from T * N , we find that T * N −1 is indeed an induced subgraph of T * N .</p><p>Definition 3. A starlike tree is a tree having atmost one node (root) with degree greater than two (Wikipedia contributors 2017). We consider starlike tree with height h/2 to be balanced, if there is atmost one leaf which is at a height less than h/2 while the rest are all at a height h/2 from the root. <ref type="figure" target="#fig_2">Figure 3(a)</ref> depicts an example of a balanced starlike tree with h = 2. Definition 4. A path graph is a graph such that its nodes can be placed on a straight line. There are not more than only two nodes in a path graph which have degree one while the rest have a degree of two. <ref type="figure" target="#fig_2">Figure 3(b)</ref> shows an example of a path graph (Wikipedia contributors 2019). , which is obtained if the tree is a balanced starlike tree with height h/2 if h is even.</p><p>Proof. To prove the lemma, we use induction. Here, the base case corresponds to a path graph T h+1 with h + 1 nodes, a trivial case of starlike graph, as it has only 2 nodes which are h hops away. From the formula N −1 h 2 , we get n * h (T h+1 ) = 2 which verifies the base case.</p><p>For any N − 1, let us assume that the lemma is true, i.e., a balanced starlike tree with height h/2 achieves the maximum n * h (T N −1 ) for any tree T N −1 with N −1 vertices. Consider T * N to be the optimal-tree for N nodes. From Lemma (2), we know that T * N −1 is an induced subgraph of T * N . This means that T * N can be obtained by adding a node to T * N −1 . Since we are constructing T * N , we need to add a node to T * N −1 such that maximum nodes can be selected which are atleast h hops away. There are three possible structures for the tree T * N −1 depending on the minimum height among all its branches: (a) minimum height among all the branches is less than h/2 − 1, (b) minimum height among all the branches is equal to h/2 − 1 and (c) minimum height among all the branches is equal to h/2. Although case (a) is not possible as we assumed T * N −1 to be a balanced starlike tree, we consider it for the sake of completeness. For case (a), no matter where we add the node, n * h (T * N ) will not increase. However, we should add the node to the leaf of the branch with least height as it will allow the new leaf of that branch to be chosen in case the number of nodes in tree is increased to some N &gt; N such that height of that branch becomes h/2. For case (b), we should add the node to the leaf of the branch with least height so that its height becomes h/2 and the new leaf of that branch gets selected. For case (c), no matter where we add the node, n * h (T * N ) will not increase. Unlike case (a), we should add the new node to the root so as to start a new leaf which could be selected if that branch grows to a height h/2 for some N &gt; N . For all the three cases, T * N is a balanced starlike tree as the new node is either added to the leaf of a branch if minimum height of a leaf is less than h/2 or to the root if the minimum height of the branches is h/2. Hence, by induction, the lemma is proved.</p><p>Theorem 2. Let the input graph T be a tree of any possible structure with N nodes. Let k * be the lower bound on sampling ratio k to ensure the existence of atleast one edge in the pooled graph irrespective of the structure of T and the location of the selected nodes. For TopK or SAGPool, k * → 1 whereas for ASAP, k * → 0.5 as N → ∞.</p><p>Proof. From Lemma <ref type="formula" target="#formula_4">(4)</ref>  we can show that for a pooling method with RF edge if the number of sampled clusters is greater than n * RF edge (T N ) then there will always be an edge in the pooled graph irrespective of the position of the selected clusters:</p><formula xml:id="formula_22">k * N &gt; N − 1 RF edge +1 2 k * N &gt; N − 1 RF edge +1 2 k * N + 1 &gt; N − 1 RF edge +1 2<label>(15)</label></formula><p>Let us consider 1-hop neighborhood for pooling, i.e., h = 1.</p><p>Substituting RF edge = h in Eq. (15) for TopK and SAGPool we get:</p><p>k * &gt; 1 − 2 N and as N → ∞ we obtain k * → 1. Substituting RF edge = 2h + 1 in Eq. (15) for ASAP we get:</p><formula xml:id="formula_23">k * &gt; 1 2 − 3 2N</formula><p>and as N → ∞ we obtain k * → 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similar Analysis for Path Graph</head><p>Lemma 5. For a path graph G path with N nodes, n * h (G path ) = N h . Lemma 6. Consider the input graph to be a path graph G path with N nodes. To ensure that a pooling operator with RF edge and sampling ratio k has at least one edge in the pooled graph, irrespective of the location of selected clusters, we have the following inequality on k: k ≥ ( 1 RF edge +1 + 1 N ).</p><p>Proof. From Lemma (5), we know that n * RF edge (G path ) = N RF edge . Using pigeon-hole principle we can show that for a pooling method with RF edge , if the number of sampled clusters is greater than n * RF edge (G path ), then there will always be an edge in the pooled graph irrespective of the position of the selected clusters:  Theorem 3. Consider the input graph to be a path graph with N nodes. Let k * be the lower bound on sampling ratio k to ensure the existence of atleast one edge in the pooled graph. For TopK or SAGPool, k * → 0.5 as N → ∞ whereas for ASAP, k * → 0.25 as N → ∞.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of ASAP: (a) Input graph to ASAP. (b) ASAP initially clusters 1-hop neighborhood considering all nodes as medoid 1 . For brevity, we only show cluster formations of nodes 2 &amp; 6 as medoids. Cluster membership is computed using M2T attention (refer Sec. 4.2). (c) Clusters are scored using LEConv (refer Sec. 4.3). Darker shade denotes higher score. (d) A fraction of top scoring clusters are selected in the pooled graph. Adjacency matrix is recomputed using edge weights between the member nodes of selected clusters. (e) Output of ASAP (f) Overview of hierarchical graph classification architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Validation Accuracy vs sampling ratio k on NCI1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Balanced Starlike tree with height 2. (b) Path Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>and(3), we know that among all the possible trees T N which have N vertices, the maximum n * h (T N ) achievable is N −1 Minimum sampling ratio k * vs N for Path-Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Minimum sampling ratio k * vs N for Path-Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Properties desired in hierarchical pooling methods.</figDesc><table><row><cell>sible for large graphs. TopK scores nodes based on a learn-</cell></row><row><cell>able projection vector and samples a fraction of high scor-</cell></row><row><cell>ing nodes. It avoids node aggregation and computing soft</cell></row><row><cell>assignment matrix to maintain the sparsity in graph opera-</cell></row><row><cell>tions. SAGPool improve upon TopK by using a GNN to con-</cell></row><row><cell>sider the graph structure while scoring nodes. Since TopK</cell></row><row><cell>and SAGPool do not aggregate nodes nor compute soft edge</cell></row><row><cell>weights, they are unable to preserve node and edge informa-</cell></row><row><cell>tion effectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b21">Vinyals, Bengio, and Kudlur 2016)</ref> 71.60 ± 0.87 72.16 ± 0.43 66.97 ± 0.74 61.04 ± 2.69 61.46 ± 0.47 GLOBAL-ATTENTION (Li et al. 2016) 71.38 ± 0.78 71.87 ± 0.60 69.00 ± 0.49 67.87 ± 0.40 61.31 ± 0.41 SORTPOOL (Zhang et al. 2018) 71.87 ± 0.96 73.91 ± 0.72 68.74 ± 1.07 68.59 ± 0.67 63.44 ± 0.65 DIFFPOOL (Ying et al. 2018) 66.95 ± 2.41 68.20 ± 2.02 62.32 ± 1.90 61.98 ± 1.98 60.60 ± 1.62 TOPK (Gao and Ji 2019) 75.01 ± 0.86 71.10 ± 0.90 67.02 ± 2.25 66.12 ± 1.60 61.46 ± 0.84 SAGPOOL (Lee, Lee, and Kang 2019) 76.45 ± 0.97 71.86 ± 0.97 67.45 ± 1.11 67.86 ± 1.41 61.73 ± 0.76 ASAP (Ours) 76.87 ± 0.7 74.19 ± 0.79 71.48 ± 0.42 70.07 ± 0.55 66.26 ± 0.47</figDesc><table><row><cell>Method</cell><cell>D&amp;D</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>FRANKENSTEIN</cell></row><row><cell>SET2SET (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the graph datasets. G avg , C avg , V</figDesc><table /><note>avg and E avg denotes the average number of graphs, classes, nodes and edges respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Different aggregation types as mentioned in Sec 7.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performace comparison of different aggregation methods on validation data of FRANKENSTEIN and NCI1.</figDesc><table><row><cell cols="2">Attention FRANKENSTEIN</cell><cell>NCI1</cell></row><row><cell>T2T</cell><cell>67.6 ± 0.5</cell><cell>70.3 ± 2.0</cell></row><row><cell>S2T</cell><cell>67.7 ± 0.5</cell><cell>69.9 ± 2.0</cell></row><row><cell>M2T</cell><cell>67.8 ± 0.6</cell><cell>70.7 ± 2.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of different attention framework on pooling evaluated on validation data of FRANKENSTEIN and NCI1. Please refer to Sec. 7.3 for more details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of different fitness scoring functions on validation data of FRANKENSTEIN and NCI1. Refer to Sec. 7.4 for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Effect of calculating soft edge weights on pooling for validation data of FRANKENSTEIN and NCI1. Please refer to Sec. 7.5 for more details. DiffPool considers the entire graph. As a result, in DiffPool, two nodes that are disconnected or far away in the graph can be assigned similar clusters if the nodes and their neighbors have similar features. Since this type of cluster formation is undesirable for a pooling operator<ref type="bibr" target="#b25">(Ying et al. 2018)</ref>, DiffPool utilizes an auxiliary link prediction objective during training to specifically prevent far away nodes from being clustered together. ASAP needs no such additional regularization because it ensures the localness while clustering. DiffPool's soft cluster assignment matrix S is calculated for all the nodes to all the clusters making S a dense matrix. Calculating and storing this does not scale easily for large graphs. ASAP, due to the local clustering over h-hop neighborhood, generates a sparse assignment matrix while retaining the hierarchical clustering properties of Diffpool. Further, for each pooling layer, DiffPool has to predetermine the number of clusters it needs to pick which is fixed irrespective of the input graph size. Since ASAP selects the top k fraction of nodes in current graph, it inherently takes the size of the input graph into consideration. The pooled graph in ASAP has a better edge connectivity compared to TopK and SAGPool because soft edge weights are computed between clusters using upto three hop connections in the original graph. Also, the use of LEConv instead of GCN for finding fitness values φ further allows ASAP to sample representative clusters from local neighborhoods over the entire graph.</figDesc><table><row><cell>8 Discussion</cell></row><row><cell>8.1 Comparison with other pooling methods</cell></row><row><cell>DiffPool DiffPool and ASAP both aggregate nodes to</cell></row><row><cell>form a cluster. While ASAP only considers nodes which are</cell></row><row><cell>within h-hop neighborhood from a node x i (medoid) as a</cell></row><row><cell>cluster,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter tuning Summary.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Global Pooling Hyperparameter Tuning Summary.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Lemma 3. For a balanced starlike tree with height h/2, where h is even, n * h (T N ) = N −1 h 2 , i.e., when the leaves are selected.Lemma 4. Among all the possible trees T N which have N vertices, the maximum n * h (T N ) achievable is N −1</figDesc><table><row><cell>h</cell></row><row><cell>2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">medoids are representatives of a cluster. They are similar to centroids but are strictly a member of the cluster.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please refer to Appendix Sec. D for more details on similarity between pooling methods in CNN and ASAP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Please refer to Appendix Sec. A for further details on hyperparameter tuning and Appendix Sec. E for ablation on k. 5 Source code for ASAP can be found at: https://github.com/ malllabiisc/ASAP 6 Please refer to Appendix Sec. B for more details</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgements</head><p>We would like to thank the developers of Pytorch Geometric <ref type="bibr" target="#b5">(Fey and Lenssen 2019)</ref> which allows quick implementation of geometric deep learning models. We would like to thank Matthias Fey again for actively maintaining the library and quickly responding to our queries on github.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Hyperparameter Tuning</head><p>Proof. From Lemma (6), we get k * = 1 RF edge +1 + 1 N . Using h = 1 for TopK and SAGPool when N tends to infinity i.e. k * = lim N →∞ 1 2 + 1 N , we get k * → 0.25. Using h = 3 for ASAP when N tends to infinity i.e. k * = lim N →∞ 1 4 + 1 N , we get k * → 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Connectivity via k th Graph Power.</head><p>To minimize the possibility of nodes getting isolated in pooled graph, TopK employs k th graph power i.e.Â k instead ofÂ. This helps in increasing the density of the graph before pooling. While using k th graph power, TopK can connect two nodes which are atmost k hops away whereas ASAP in this setting can connect upto k + 2h hops in the original graph. As h ≥ 1, ASAP will always have better connectivity given k th graph power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Graph Permutation Equivariance</head><p>Given a permutation matrix P ∈ {0, 1} n×n and a function f (X, A) depending on graph with node feature matrix X and adjacency matrix A, graph permutation is defined as f (P X, P AP T ), node permutation is defined as f (P X, A) and edge permutation is defined as f (X, P AP T ). Graph pooling operations should produce pooled graphs which are isomorphic after graph permutation i.e., they need to be graph permutation equivariant or invariant. We show that ASAP has the property of being graph permutation equivariant.</p><p>Proposition 1. ASAP is a graph permutation equivariant pooling operator.</p><p>Proof. Since S is computed by an attention mechanism which attends to all edges in the graph, we have:</p><p>Selecting top kN clusters denoted by indices i, changesŜ as:</p><p>Using Eq. (18) andŜ = S(:,î), X p =X c (î), we can write:</p><p>Since A p =Ŝ TÂcŜ andŜ = S(:,î), X p =X c (î), we get:</p><p>From Eq. <ref type="formula">(19)</ref> and Eq. <ref type="formula">(20)</ref>, we see that graph permutation does not change the output features. It only changes the order of the computed feature and hence is isomorphic to the pooled graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Pseudo Code</head><p>Algorithm 1 is a pseudo code of ASAP. The Master2Token working is explained in Algorithm 2.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simaan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural machine translation by jointly learning to align and translate</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<idno>arXiv:1811.01287</idno>
	</analytic>
	<monogr>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards sparse hierarchical graph classifiers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kulis ; Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>telligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Lenssen</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Graph u-nets. arXiv preprint</note>
	<note>and Ji 2019</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">595608</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Inductive representation learning on large graphs</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<editor>Pereira, F.</editor>
		<editor>Burges, C. J. C.</editor>
		<editor>Bottou, L.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems 25</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfattention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang ; Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<idno>CoRR abs/1511.05493</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph convolutional networks with eigenpooling</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05660</idno>
	</analytic>
	<monogr>
		<title level="m">MT-CGCNN: Integrating crystal graph convolutional neural network with multitask learning for material property prediction</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Twenty-Fourth International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer science review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
	</analytic>
	<monogr>
		<title level="m">Modeling relational data with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph clustering</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reside: Improving distantly-supervised neural relation extraction using side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vashishth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04361</idno>
		<idno>arXiv:1911.03082</idno>
	</analytic>
	<monogr>
		<title level="m">Composition-based multi-relational graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Watson</forename><surname>Karypis ; Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Starliketree&amp;oldid=791882487" />
	</analytic>
	<monogr>
		<title level="m">Wikipedia contributors 2017] Wikipedia contributors. 2017. Starlike tree -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 17</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Path graph -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note>Wikipedia contributors 2019] Wikipedia contributors</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32Nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>How powerful are graph neural networks?</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
