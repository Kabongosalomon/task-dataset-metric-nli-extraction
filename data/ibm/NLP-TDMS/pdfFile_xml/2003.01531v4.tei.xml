<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Voice Separation with an Unknown Number of Multiple Speakers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
						</author>
						<title level="a" type="main">Voice Separation with an Unknown Number of Multiple Speakers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new method for separating a mixed audio sequence, in which multiple voices speak simultaneously. The new method employs gated neural networks that are trained to separate the voices at multiple processing steps, while maintaining the speaker in each output channel fixed. A different model is trained for every number of possible speakers, and the model with the largest number of speakers is employed to select the actual number of speakers in a given sample. Our method greatly outperforms the current state of the art, which, as we show, is not competitive for more than two speakers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to separate a single voice from the multiple conversations occurring concurrently forms a challenging perceptual task <ref type="bibr" target="#b0">(Capon, 1969;</ref><ref type="bibr" target="#b8">Frost, 1972)</ref>. The ability of humans to do so has inspired many computational attempts, with much of the earlier work focusing on multiple microphones and unsupervised learning, e.g., the independent component analysis approach <ref type="bibr" target="#b12">(Hyvärinen &amp; Oja, 2000)</ref>.</p><p>In this work, we focus on the problem of supervised voice separation from a single microphone, which has seen a great leap in performance following the advent of deep neural networks . In this "single-channel source separation" problem, given a dataset containing both the mixed audio and the individual voices, one trains to separate a novel mixed audio that contains multiple unseen speakers.</p><p>The current leading methodology is based on an overcomplete set of linear filters, and on separating the filter outputs at every time step using a mask for two speakers, or a multiplexer for more speakers <ref type="bibr" target="#b51">Zhang et al., 2020)</ref>. The audio is then reconstructed from 1 Facebook AI Research 2 Tel-Aviv University. Correspondence to: Eliya Nachmani &lt;enk100@gmail.com&gt;, Yossi Adi &lt;yossia-didrum@gmail.com&gt;, Lior Wolf &lt;liorwolf@gmail.com&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). this partial representations. Since the order of the speakers is considered arbitrary (it is hard to sort voices), one uses a permutation invariant loss during training, such that the permutation that minimizes the loss is considered.</p><p>The need to work the aforementioned partial representations, which becomes more severe as the number of voices to be separated increases, is a limitation of this masking-based method, since the mask needs to extract and suppress more from the representation as the number speakers increases. In this work, we, therefore, set out to build a mask-free method. The method employs a sequence of RNNs that are applied to the audio. As we show, it is beneficial to evaluate the error after each RNN, obtaining a compound loss that reflects the reconstruction quality after each layer.</p><p>The RNNs are bi-directional. Each RNN block is built with a specific type of residual connection, where two RNNs run in parallel. The output of each layer is the concatenation of the element-wise multiplication of the two RNNs together with the layer input that undergoes a bypass (skip) connection.</p><p>Unlike separating known sources <ref type="bibr" target="#b3">(Défossez et al., 2019)</ref> in this case, the outputs are given in a permutation invariant fashion, hence, voices can switch between output-channels, especially during transient silence episodes. In order to tackle this, we propose a new loss that is based on a voice representation network that is trained on the same training set. The embedding obtained by this network is then used to compare the output voice to the voice of the output channel. We demonstrate that the loss is effective, even when adding it to the baseline method. An additional improvement, that is effective also for the baseline methods, is obtained by starting the separation from multiple locations along the audio file and averaging the results.</p><p>Similar to the state of the art methods, we train a single model for each number of speakers. The gap in performance of the obtained model in comparison to published methods increases as the number of speaker increases, and one can notice that the performance of our method degrades gradually, while the baseline methods show a sharp degradation as the number of speakers increases.</p><p>To support the possibility of working with an unknown number of speakers, we opt for a learning-free solution and select the number of speakers by running an activity detector arXiv:2003.01531v4 [eess.AS] 1 Sep 2020</p><p>Voice Separation with an Unknown Number of Multiple Speakers on its output. This simple method is able to select the correct number of speakers in the vast majority of the cases and leads to our method being able to handle an unknown number of speakers.</p><p>Our contributions are: (i) a novel audio separation model that employs a specific RNN architecture, (ii) a set of losses for effective training of voice separation networks, (iii) performing effective model selection in the context of voice separation with an unknown number of speakers, and (iv) state of the art results that show a sizable improvement over the current state of the art in an active and competitive domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>In the problem of single-channel source separation, the goal is to estimate C different input sources s j ∈ R T , where j ∈ [1, · · · , C], given a mixture x = C i=1 c i · s i , where c i is a scaling factor. The input length, T, is not a fixed value, since the input utterances can have different durations. In this work, we focus on the supervised setting, in which we are provided with a training set S = {x i , (s i,1 , · · · , s i,C )} n i=1 , and our goal is learn the model that given an unseen mixture x, outputs C estimated channelsŝ = (ŝ 1 , · · · ,ŝ C ) that maximize the scale-invariant source-to-noise ratio (SI-SNR) (also known as the scale-invariant signal-to-distortion ratio, SI-SDR for short), between the predicted and the target utterances. More precisely, since the order of the input sources is arbitrary and since the summation of the sources is order invariant, the goal is to find C separate channels s that maximize the SI-SNR to the ground truth signals, when considering the reorder channels (ŝ π(1) , · · · ,ŝ π(C) ) for the optimal permutation π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Description</head><p>The proposed model, depicted in <ref type="figure">Figure 1</ref>, is inspired by the recent advances in speaker separation models . The first steps of processing, including the encoding, the chunking, and the two bi-directional RNNs on the tensor that is obtained from chunking are similar. However, our RNNs contain dual heads, we do not use masking, and our losses are different.</p><p>First, an encoder network, E, gets as input the mixture waveform x ∈ R T and outputs a N -dimensional latent representation z of size T ' = (2T /L) − 1, where L is the encoding compression factor. This results in z ∈ R N ×T ' ,</p><formula xml:id="formula_0">z = E(x)<label>(1)</label></formula><p>Specifically, E is a 1-D convolutional layer with a kernel size L and a stride of L/2, followed by a ReLU non-linear activation function.</p><p>The latent representation z is then divided into R = 2T ' /K + 1 overlapping chunks of length K and hop size P , denoted as u r ∈ R N ×K , where r ∈ [1, · · · , R]. All chunks are then being concatenated along the singleton dimensions and we obtain a 3-D tensor v = [u 1 , . . . , u R ] ∈ R N ×K×R .</p><p>Next, similar to <ref type="bibr" target="#b28">(Luo et al., 2019c)</ref>, v is fed into the separation network Q, which consists of b RNN blocks. The odd blocks B 2i−1 for i = 1, . . . , b/2 apply the RNN along the time-dependent dimension of size R. The even B 2i blocks are applied along the chunking dimension of size K. Intuitively, processing the second dimension yields a shortterm representation, while processing the third dimension produce long-term representation.</p><p>At this point, our method diverges from <ref type="bibr" target="#b28">(Luo et al., 2019c)</ref>, since our RNN blocks contain the MULCAT block with two sub-networks and a skip connection. Consider, for example, the odd blocks B i , i = 1, 3, . . . , b − 1. We employ two separate bidirectional LSTM, denoted as M 1 i and M 2 i , element wise multiply their outputs, and finally concatenate the input to produce the module output.</p><formula xml:id="formula_1">B i (v) = P i ([M 1 i (v) M 2 i (v), v])<label>(2)</label></formula><p>where is the element wise product operation, and P i is a learned linear project that brings the dimension of the result of concatenating the product of the two LSTMs with the input v back to the dimension of v. A visual description of a pair of blocks is given in <ref type="figure">Figure 2</ref>.</p><p>In our method, we employ a multi-scale loss, which requires us to reconstruct the original audio after each pair of blocks. The 3D tensor undergoes the PReLU non-linearity <ref type="bibr" target="#b10">(He et al., 2015)</ref> with parameters initialized at 0.25. Then, a 1 × 1 convolution with CR output channels, denoted as D. The resulting tensor of size N × K × CR is divided into C tensors of of size N × K × R that would lead to the C output channels. Note that the same PReLU parameters and the same decoder D are used to decode the output of every pair of MULCAT blocks.</p><p>In order to transform the 3D tensor back to audio, we employ the overlap-and-add operator to the R chunks <ref type="bibr" target="#b33">(Rabiner &amp; Gold, 1975)</ref>. The operator, which inverts the chunking process, adds overlapping frames of the signal after offsetting them appropriately by a step size of L/2 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training Objective</head><p>Recall that since the identity of the speakers is unknown, our goal is to find C separate channelsŝ that maximize the SI-SNR between the predicted and target signals. Formally, the SI-SNR is defined as SI-SNR(s,ŝ) = 10 log 10 s i  <ref type="figure">Figure 1</ref>. The architecture of our network. The audio is being convolved with a stack of 1D convolutions and reordered by cutting overlapping segments of length K in time, to obtain a 3D tensor. In our method, the RNN blocks are of the type of multiply and add. After each pair of blocks, we apply a convolution D to the copy of the activations, and obtain output channels by reordering the chunks and then using the overlap and add operator.</p><formula xml:id="formula_2">2 ẽ i 2<label>(3)</label></formula><formula xml:id="formula_3">LSTM LSTM P 2i-1 Permute LSTM LSTM P 2i Permute B 2i-1 B 2i</formula><p>Concat Concat <ref type="figure">Figure 2</ref>. The multiply and concat (MULCAT ) block. In the odd blocks, the 3D tensor obtained from chunking is fed into two different bi-directional LSTMs that operate along the second dimension. The results are multiplied element-wise, followed by a concatenation of the original signal along the third dimension. A learned linear projection along this dimension is then applied to obtain a tensor of the same size of the input. In the even blocks, the same set of operations occur along the chunking axis.</p><p>where,s i = si,ŝi si si 2 , andẽ i =ŝ i −s i . Since the channels are unordered, the loss is computed for the optimal permutation π of the C different output channels and is given as:</p><formula xml:id="formula_4">(s,ŝ) = − max π∈Π C 1 C C i=1 SI-SNR(s iŝπ(i) )<label>(4)</label></formula><p>where Π C is the set of all possible permutations of 1 . . . C. The loss (s,ŝ) is often denoted as the utterance level permutation invariant training (uPIT) .</p><p>As stated above, the convolution D is used to decode after every even MULCAT block, allowing us to apply the uPIT loss multiple times along the decomposition process. Formally, our model outputs b/2 groups of output channels {ŝ j } b/2 j=1 and we consider the loss</p><formula xml:id="formula_5">(s, {ŝ j } b/2 j=1 ) = 1 b b/2 j=1 (s,ŝ j )<label>(5)</label></formula><p>Notice that the permutation of π the output channels may be different between the components of this loss.</p><p>Speaker Classification Loss. A common problem in source separation is forcing the separated signal frames belonging to the same speaker to be aligned with the same output stream. Unlike the Permutation Invariant Loss (PIT)  which is applied to each input frame independently, the uPIT is applied to the whole sequence at once. This modification greatly improves the amount of occurrences in which the output is flipped between the different sources. However, according to our experiments (See Section 3) this is still a far from being optimal.</p><p>To mitigate that, we propose to add an additional loss function which imposes a long term dependency on the output streams. For this purpose, we use a speaker recognition model that we train to identify the persons in the training set. Once this neural network is trained, we minimize the L2 distance between the network embeddings of the predicted audio channel and the corresponding source.</p><p>As the speaker recognition model, we use the VGG11 network <ref type="bibr" target="#b35">(Simonyan &amp; Zisserman, 2014</ref>) trained on the power spectrograms (STFT) obtained from 500 ms of audio. Denote the embedding obtained from the penultimate layer of the trained VGG network by G. We used it in order to  <ref type="figure">Figure 3</ref>. The training losses used in our method, shown for the case of C = 2 speakers. The mixed signal x combines the two input voices s1 and s2. Our model then separates to create two output channelsŝ1 andŝ2. The permutation invariant SI-SNR loss computes the SI-SNR between the ground truth channels and the output channels, obtained at the channel permutation π that minimizes the loss. The identity loss is then applied to the matching channels, after they have been ordered by π.</p><p>compare segments of length 500 ms of the ground truth audio s i with the output audioŝ π(i) , where π is the optimal permutation obtained from the uPIT loss, see <ref type="figure">Fig. 3</ref>.</p><p>Let s j i be the j-th segments of length 500 ms obtained by cropping audio sequence s i , and similarlyŝ j i for s i . The identity loss is give by</p><formula xml:id="formula_6">ID (s,ŝ) = 1 C|J(s)| C i=1 J(s) j=1 M SE(G(F (s j i )), G(F (ŝ j i ))) (6) where J(s)</formula><p>is the number of segments extracted from s and F is a differential STFT implementation, i.e., a network implementation of STFT that allows us to back-propagate the gradient though it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Selection</head><p>We train a different model for each number C of audio components in the mix. This allows us to directly compare with the baseline methods. However, in order to apply the method in practice, it is important to be able to select the number of speakers. While it is possible to train a classifier to determine C given a mixed audio, we opt for a nonlearned solution in order to avoid biases that arise from the distribution of data and to promote solutions in which the separation models are not detached from the selection process.</p><p>The procedure we employ is based on activity detection algorithm, in which we compute the average power of each output channel and verify that it is above a predefined thresh-old 1 . Starting from the model that was trained on the dataset with the largest number of speakers C, we apply the speech detector to each output channel. If we detect silence (noactivity) in one of the channels, we move to the model with C − 1 output channels and repeat the process until all output channels contain speech.</p><p>As can be seen in our experiments, this selection procedure is relatively accurate and leads to results with an unknown number of speakers that are only moderately worse than the results when this parameter is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>For research purposes only, we use the WSJ0-2mix and WSJ0-3mix datasets  and we further expand the WSJ-mix dataset to four and five speakers and introduce WSJ0-4mix and WSJ0-5mix datasets. All of the aforementioned datasets are based on the WSJ0 corpus <ref type="bibr" target="#b9">(Garofolo et al., 1993)</ref>. We use the same procedure as in , i.e. we use 30 hours of speech from the training set si tr s to create the training and validation sets. The four and five speakers were randomly chosen and combined with random SNR values between 0 − 5 dB. The test set is created from si et s and si dt s with 16 speakers, that differ from the speakers of the training set. For research purposes only, WSJ0-4mix and WSJ0-5mix datasets creation scripts are available as supplementary. A separate model is trained for each dataset, with the corresponding number of output channels. Sample results and db creation scripts can be found under the following link: https://enk100.github.io/speaker separation Implementation details We choose hyper parameters based on the validation set. The input kernel size L was 8 (except for the experiment where we vary it) and the number of the filter in the preliminary convolutional layer was 128. We use an audio segment of four seconds long sampled at 8kHz. The architecture uses b = 6 blocks of MULCAT , where each LSTM layer contains 128 neurons. We multiply the IDloss with 0.001 when combined the uPIT loss. The learning rate was set to 5e − 4, which was multiplied by 0.98 every two epoches. The ADAM optimizer <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2014)</ref> was used with batch size of 2. For the speaker model, we extract the STFT using a window size of 20ms with stride of 10ms and Hamming window.</p><p>In order to evaluate the proposed model, we report the scaleinvariant signal-to-noise ratio improvement (SI-SNRi) score on the test set, computed as follows,</p><formula xml:id="formula_7">SI-SNRi(s,ŝ, x) = 1 C C i=1 SI-SNR(s i ,ŝ i )−SI-SNR(s i , x)<label>(7)</label></formula><p>1 we calibrated the threshold on the validation set. We compare with the following baseline methods: ADANet , DPCL++ <ref type="bibr" target="#b13">(Isik et al., 2016)</ref>, CBLDNN-GAT <ref type="bibr" target="#b19">(Li et al., 2018)</ref>, TasNet , the Ideal Ratio Mask (IRM), the Ideal Binary Mask (IBM), ConvTasNet <ref type="bibr">(Luo &amp; Mesgarani, 2019)</ref>, FurcaNeXt <ref type="bibr" target="#b51">(Zhang et al., 2020)</ref>, and DPRNN <ref type="bibr" target="#b28">(Luo et al., 2019c)</ref>. Similarly to <ref type="bibr">Luo &amp; Mesgarani (2019)</ref>, for the IRM and IBM we use a window size of 32ms, hop length of 8ms, and 2048 FFT bins. Prior work often reports the signal-todistortion ratio (SDR). However, recent studies have argued that the above mentioned metric has been improperly used due to its filter dependence and may result in misleading findings <ref type="bibr" target="#b18">(Le Roux et al., 2019)</ref>.</p><p>The results are reported in <ref type="table" target="#tab_0">Table 1</ref>. Each column depicts a different dataset, where the number of speakers C in the mixed signal x is different. The model used for evaluating each dataset is the model that was trained to separate the same number of speakers. As can be seen, the proposed model is superior to previous methods by a sizable margin, in all four datasets.</p><p>In order to understand the contribution of each of the various components in the proposed method, we conducted an ablation study. (i) We replace the MULCAT block with a conventional LSTM ("-gating"); (ii) we train with a permutation invariant loss that is applied only at the final output ("-multiloss") of the model; and (iii) we train with and without the identity loss ("-IDloss").</p><p>First, we analyzed the importance of each loss term to the final model performance. <ref type="table">Table 2</ref> summarizes the results.</p><p>As can be seen, each of aforementioned components contributes to the performance gain of the proposed method, with the multi-layer loss being more dominant than the others. Adding the identity loss to the DPRNN model also yields a performance improvement. We would like to stress that not only being different in the multiply and concat block, the identity loss and the multiscale loss, our method <ref type="table">Table 2</ref>. Ablation analysis where we take out the two LSTM structures and replace them with a single one (-gating), remove the multiloss (-multiloss), or remove the speaker identification loss (-IDloss). We also present the results of adding the identification loss to the baseline DPRNN method. Recent studies pointed out the importance of choosing small kernel size for the encoder <ref type="bibr" target="#b28">(Luo et al., 2019c)</ref>. In ConvTas-Net the authors suggest that kernel size L of 16 <ref type="bibr">(Luo &amp; Mesgarani, 2019)</ref> performs better than larger ones, while the authors of DPRNN <ref type="bibr" target="#b28">(Luo et al., 2019c)</ref> advocate for an even smaller size of L = 2. <ref type="table">Table 3</ref> shows that unlike DPRNN, the performance of our model is not harmed by larger kernel sizes. <ref type="figure" target="#fig_2">Figure 4</ref> depicts the convergence rates of our model for various L values for the first 60 hours of training. Being able to train with kernels with L &gt; 2 leads to faster convergence to results at the range of recently published methods.</p><p>Lastly, we explored the effect of the identity loss. Recall that the identity loss is meant to reduce the frequency in which an output channel switches between the different speaker identities. In order to measure the frequency of this event, we have separated the audio into sub-clips of length 0.25sec and tested the best match, using SI-SNR, between each segment and the target speakers. If the matching switched from one voice to another, we marked the entire sample as a switching sample.</p><p>The results suggest that both DPRNN and the proposed   model benefit from the incorporation of the identity loss. However, this loss does not eliminate the problem completely. The results are depicted in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Noisy and Reverberated Conditions</head><p>Next, we compared the performance of the proposed approach under noisy and reverberated conditions using WHAM! <ref type="bibr" target="#b47">(Wichern et al., 2019)</ref> and WHAMR! (Maciejewski et al., 2020) benchmarks. We compared the proposed method to Conv-TasNet, Chimera++ <ref type="bibr" target="#b44">(Wang et al., 2018b)</ref>, Learnable filter bank <ref type="bibr" target="#b32">(Pariente et al., 2020)</ref>, and DPRNN. SI-SNRi results are presented in <ref type="table" target="#tab_3">Table 4</ref>. It can be seen that the proposed approach is superior to the baseline methods also under noisy and reverberated conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Test-time augmentation</head><p>We found out that starting the separation at different points in time yields slightly different results. For this purpose, we cut the mixed audio at a certain time point and then concatenate the first part at the end of the second. Performing this multiple times, at random starting points and then averaging the results tends to improve results.</p><p>The averaging process is as follows: first, the original starting point is restored by inverting the shifting process. Then, the channels are then matched (using MSE) to a reference set of channels, finding the optimal permutation. In our experiments, we use the separation results of the original mixed signal as the reference signal. The results from all starting points are then averaged. <ref type="table" target="#tab_4">Table 5</ref> depicts the results for both our method and DPRNN. Evidently, as the number of random shifts increases, the performance improves. To clarify: in order to allow a direct comparison with the literature, the results reported elsewhere in this paper are obtained without this augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dealing with an Unknown Number of Speakers</head><p>When there are C speakers in a given mixed audio x, one may employ a model that was trained on C &gt; C speakers. In this case, the superfluous channels seem to produce relatively silent signals for both our method and DPRNN. One can then match the C output channels to the C channels in the optimal way, discarding C − C channels, and compute the SI-SNRi score. Tab. 6 depicts the results for DPRNN and our method. As can be seen, the level of results obtained is the same level obtained by the C model when applied to C speakers, or slightly better (the mixture audio is less confusing if there are less speakers).</p><p>We next apply our model selection method (Section 2.3), which automatically selects the most appropriate model, based on an activity detector algorithm. We consider a silence channel if the output of the activity detection algo- rithm is above a predefined threshold. For a fair comparison we calibrated the threshold for silence detection to each method separately. We evaluate the proposed method, using a confusion matrix, whether this unlearned method is effective in accurately estimating the number of speakers. Additionally, we measure the obtained SI-SNRi when using the selected model and compare it to the oracle (known number of speaker in the recording).</p><p>As can be seen in <ref type="table" target="#tab_6">Table 7</ref>, simply by looking for silent output channels, we are able to identify the number of speakers in a large portion of the cases, while maintaining the SI-SNRi values close to the oracle performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Music Source Separation</head><p>Lastly, in order to demonstrate the applicability of the proposed model for other separation tasks, we evaluated our model on the task of music source separation 2 . In music source separation we are provided with an input mixture, and our goal is to learn a function which outputs C estimated channels, each for a different instrument. Both mixture and the separated channels can be either mono or stereo recordings. In the following experiments we consider the stereo setup since it is more common in modern music recordings.</p><p>We evaluate the proposed method on the MusDB dataset <ref type="bibr" target="#b34">(Rafii et al., 2017)</ref>. It is comprised of 150 songs sampled at 44100Hz. For each song, we are provided with the separated channels as supervision, where the mixture, is the sum of those four parts. We use the first 84 songs for the training set, the next 16 songs for validation set (we follow the same split as defined in the musdb python package) while the remaining 50 songs are used for test set.</p><p>We follow the SiSec Mus evaluation campaign for music separation <ref type="bibr" target="#b37">(Stöter et al., 2018)</ref>, where we separate the mixtures into the following four categories: (1) drums, (2) bass, (3) other, and (4) vocals. Unlike the blind-source-separation task, since we know the identity of the target channels (i.e., drums, bass, vocals, other), we do not need to use the permutation invariant training and can directly optimize the L1 distance between the target and output channels.</p><p>We compared the proposed model to several highly competitive baselines; namely Demucs <ref type="bibr" target="#b3">(Défossez et al., 2019)</ref>, Wave-U-Net <ref type="bibr" target="#b36">(Stoller et al., 2018)</ref>, and Open-Unmix <ref type="bibr" target="#b38">(St "oter et al., 2019)</ref>. We additionally provide an oracle results of the Ideal Ratio Mask (IRM). We report the median over all tracks of the median Signal-to-Distortion-Ration (SDR) over each track, as done in the SiSec Mus evaluation campaign <ref type="bibr" target="#b37">(Stöter et al., 2018)</ref>. For easier comparison, the All column is obtained by concatenating the metrics from all sources and then taking the median. Implementation details. We tuned all hyper parameters using the validation set. The input kernel size L was set to 14 and the number of the filter in the preliminary convolutional layer was set to 128. Similarly to the voice separation task with use 6 blocks of MULCAT for the separation module, where each LSTM layer contains 128 neurons. We optimize the model using Adam optimizer <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2014)</ref> using a learning rate of 2.5e − 4 and a batch size of 4.  Results. Results are summarized in <ref type="table" target="#tab_7">Table 8</ref>. Notice, the proposed model reach the best overall SDR and provide a new state-of-the-art performance 3 . Interestingly, while the proposed method improve performance over all categories, the biggest improvement is on the Vocals category. This implies that the proposed model is more suitable for extracting human-speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Single channel speech separation is one of the fundamental problems in speech and audio processing. It was extensively explored over the years <ref type="bibr" target="#b21">(Logeshwari &amp; Mala, 2012;</ref><ref type="bibr" target="#b31">Martin &amp; Cohen, 2018;</ref><ref type="bibr" target="#b7">Ernst et al., 2018)</ref>, where traditionally signal processing techniques were applied to the task <ref type="bibr" target="#b2">(Choi et al., 2005)</ref>. Additionally, most previous research was conducted at the spectrum level of the input signal <ref type="bibr" target="#b42">(Wang &amp; Chen, 2018;</ref><ref type="bibr" target="#b7">Ernst et al., 2018)</ref>.</p><p>Recently, deep learning models were suggested for the speech separation task, and have managed to vastly improve the performance over previous methods. <ref type="bibr" target="#b11">Hershey et al. (2016)</ref> suggests a clustering methods in which trained speech embeddings are being used for separation. Yu et al. <ref type="bibr">3</ref> We are considering the case of no additional training data. We trained all models using the Demucs package <ref type="bibr" target="#b3">(Défossez et al., 2019).</ref> (2017) proposed the Permutation Invariant Training (PIT) over the frame level for source separation, while <ref type="bibr" target="#b16">Kolbaek et al. (2017)</ref> continue this line of work by suggesting the utterance level Permutation Invariant Training (uPIT). <ref type="bibr" target="#b20">Liu &amp; Wang (2019)</ref> suggest to use both PIT and clustering over the frame levels for speaker tracking. An excellent survey of deep learning based source separation methods was provided by <ref type="bibr" target="#b42">Wang &amp; Chen (2018)</ref>. A phase-sensitive objective function with an LSTM neural network was presented in <ref type="bibr" target="#b6">Erdogan et al. (2015)</ref>, showing an improvement of SDR over the CHiME-2 <ref type="bibr" target="#b41">(Vincent et al., 2013)</ref> dataset. <ref type="bibr" target="#b46">Wang et al. (2019)</ref> introduced a method to reconstruct the phase in the Fourier transform domain, by determined uniquely the absolute phase between two speakers based on their mixture signal. <ref type="bibr" target="#b45">Wang et al. (2018c)</ref> propose a novel method to separate speech signals in the time-frequency domain. The method use the magnitude and phase simultaneously in order to separate one speaker from the other.</p><p>An influential method was introduced by , where a deep learning method for speech separation over the time domain was presented. It employs thee components: an encoder, a separator and a decoder. Specifically they used a convolutional layer as the encoder, bidirectional LSTMs as the separator network, and a fully connected layer as the decoder that constructs the separated speech signals. Then, <ref type="bibr">Luo &amp; Mesgarani (2019)</ref> suggested to replace the separator network from LSTM to a fully convolutional model using block of time depth separable dilated convolution (Conv-Tasnet). <ref type="bibr" target="#b50">Zeghidour &amp; Grangier (2020)</ref> propose to infer a set of speaker representations via clustering to better separate the speakers.</p><p>Recently, Conv-Tasnet was scaled by <ref type="bibr" target="#b51">Zhang et al. (2020)</ref>, who proposed to train several separator networks in parallel to perform an ensemble. Dual Path RNN blocks were introduced by <ref type="bibr" target="#b28">Luo et al. (2019c)</ref>. Such blocks, which we also use, first reorder the encoded representation and then process it across different dimensions.</p><p>Related to our goal of improving the performance on multiple speakers, <ref type="bibr" target="#b39">Takahashi et al. (2019)</ref> introduce a recursive method for speaker separation, based on the Conv-Tasnet model. The authors suggested to separate out a single speaker at a time in a recursive manner. It is shown that a model that was trained to separate two and three speakers can separate four speakers.</p><p>Another line of work to note is studies which leverages speaker information during separation. In <ref type="bibr" target="#b52">(Zmolikova et al., 2017;</ref><ref type="bibr" target="#b5">Delcroix et al., 2018)</ref> a neural network employed estimated i-vectors <ref type="bibr" target="#b4">(Dehak et al., 2009</ref>) in order to estimate masks, which extract the target speaker by generating beamformer filters, while <ref type="bibr" target="#b43">Wang et al. (2018a)</ref> proposed to use d-vectors <ref type="bibr" target="#b40">(Variani et al., 2014)</ref> as speaker embeddings and directly output the target separated sources.</p><p>A clustering method for speech separation was introduced by <ref type="bibr" target="#b13">Isik et al. (2016)</ref>; <ref type="bibr" target="#b14">Keshet &amp; Bengio (2009)</ref>. <ref type="bibr" target="#b13">Isik et al. (2016)</ref> introduced the deep unsupervised separation model. This method estimate the mask by extracting embedding to each segment of the spectrogram and clustering these. A deep attractor network that extract centroids in the high dimensional embedding spaces, in order to obtain the timefrequency bins for each speaker was presented in <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>.</p><p>In <ref type="bibr" target="#b44">(Wang et al., 2018b)</ref> multiple clustering network approaches were evaluated and a novel chimera network, which combines mask-inference networks with deep clustering networks, obtains an improvement of 0.7 dB on the WSJ0-2mix dataset over the alternative methods.</p><p>Further works on speech separation include input with multiple channels (i.e. multiple microphones). In <ref type="bibr" target="#b30">(Markovich et al., 2009)</ref>, the authors present an extension to the minimum variance distortionless response (MVDR) beamformer <ref type="bibr" target="#b0">(Capon, 1969;</ref><ref type="bibr" target="#b8">Frost, 1972)</ref>. The method includes the linearly constrained minimum variance (LCMV) beamformer <ref type="bibr" target="#b17">(Laufer-Goldshtein et al., 2020)</ref> which is designed to obtain the desired signals and to mitigate the interferences. The MVDR neural beamformer introduce in <ref type="bibr" target="#b48">(Xiao et al., 2016)</ref>, predict a mask in time-frequency space, which is then used to estimate the MVDR beam, showing an improvement over both the traditional MVDR method and delay-and-sum beamforming.</p><p>The FaSNet (filter-and-sum network) method <ref type="bibr" target="#b26">(Luo et al., 2019a)</ref> includes two-stage processing units. The first learns frame-level time-domain adaptive beamforming filters for a selected reference channel, and second stage calculates the filters for the remaining channels. FaSNet improves the MVDR baseline results with 14.3% relative word error rate reduction (RWERR). FaSNet was extended to form the transform-average-concatenate (TAC) model <ref type="bibr" target="#b27">(Luo et al., 2019b)</ref>. This method employs a transformation module that extract feature to each channel, a copy of these features undergoes global pooling. A concatenation module is then applied to the output of the first and the second modules. This method shows improvement in the separation for noisy environment, and experiments are conducted with both a varying numbers of microphones and a fix geometry array configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>From a broad perceptual perspective, the cocktail party problem is a difficult instance segmentation problem with many occluding instances. The instances cannot be separated due to continuity alone, since speech signals contain silent parts, calling for the use of an identification-based constancy loss. In this work, we add this component and also use it in order to detect the number of instances in the mixed signal, which is a capability that is missing in the current literature.</p><p>Unlike previous work, in which the performance degrades rapidly as the number of speakers increases, even for a known number of speakers, our work provides a practical solution. This is achieved by introducing a new recurrent block, which combines two bi-directional RNNs and a skip connection, the use of multiple losses, and a voice constancy term mentioned above. The obtained results are better than all existing method, in a rapidly evolving research domain, by a sizable gap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Training curves of our model for various kernel sizes L = 2, 4, 8, 16. Our model train faster with larger kernel size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The fraction of samples in which the model produces output channels with an identity switch, using the dataset of two speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of various models as a function of the number of speakers. Starred results (*) mark our training, using published code by the method's authors. The other baselines are obtained from the respective papers.</figDesc><table><row><cell>Model</cell><cell cols="3">#params 2spk 3spk</cell><cell cols="2">4spk 5spk</cell></row><row><cell>ADANet</cell><cell>9.1M</cell><cell>10.5</cell><cell>9.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DPCL++</cell><cell>13.6M</cell><cell>10.8</cell><cell>7.1</cell><cell>-</cell><cell>-</cell></row><row><cell>CBLDNN-GAT</cell><cell>39.5M</cell><cell>11</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TasNet</cell><cell>32.0M</cell><cell>11.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IBM</cell><cell>-</cell><cell>13.0</cell><cell>12.8</cell><cell>10.6</cell><cell>10.3</cell></row><row><cell>IRM</cell><cell>-</cell><cell>12.7</cell><cell>12.5</cell><cell>9.8</cell><cell>9.6</cell></row><row><cell>ConvTasNet</cell><cell>5.1M</cell><cell>15.3</cell><cell>12.7</cell><cell>8.5*</cell><cell>6.8*</cell></row><row><cell>FurcaNeXt</cell><cell>51.4M</cell><cell>18.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPRNN</cell><cell>3.6M</cell><cell cols="4">18.8 14.7* 10.4* 8.7*</cell></row><row><cell>Ours</cell><cell>7.5M</cell><cell>20.1</cell><cell>16.9</cell><cell>12.9</cell><cell>10.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The DPRNN results are based on our training, using the authors' published code.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">2spk 3spk 4spk 5spk</cell></row><row><cell>DPRNN</cell><cell></cell><cell cols="3">18.08 14.72 10.37 8.65</cell></row><row><cell>DPRNN + IDloss</cell><cell></cell><cell cols="3">18.42 14.91 11.29 9.01</cell></row><row><cell cols="5">Ours-gating-multiloss-IDloss 19.02 14.88 10.76 8.42</cell></row><row><cell>Ours-gating-IDloss</cell><cell></cell><cell cols="3">19.30 15.60 11.06 8.84</cell></row><row><cell>Ours-gating</cell><cell></cell><cell cols="3">19.42 15.73 11.22 8.95</cell></row><row><cell cols="2">Ours-multiloss-IDloss</cell><cell cols="3">18.84 13.73 10.40 8.65</cell></row><row><cell>Ours-multiloss</cell><cell></cell><cell cols="3">18.93 13.86 10.54 8.75</cell></row><row><cell>Ours-IDloss</cell><cell></cell><cell cols="3">19.76 16.63 12.60 10.20</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">20.12 16.85 12.88 10.56</cell></row><row><cell>Model</cell><cell>L=2</cell><cell>L=4</cell><cell>L=8</cell><cell>L=16</cell></row><row><cell>ConvTasNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.3</cell></row><row><cell>DPRNN</cell><cell>18.8</cell><cell>17.9</cell><cell>17.0</cell><cell>15.9</cell></row><row><cell>Ours</cell><cell cols="4">18.94 19.91 19.76 18.16</cell></row><row><cell cols="5">does not employ a mask when performing separation and</cell></row><row><cell cols="5">instead directly generates the separated signals.</cell></row></table><note>Table 3. Performance of three types of models as a function of the kernel size. Our model does not suffer from changing the kernel size. (Only the last row is based on our runs).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the proposed approach against several baselines using WHAM! and WHAMR! datasets.</figDesc><table><row><cell>Model</cell><cell cols="2">WHAM! WHAMR!</cell></row><row><cell>Chimera++</cell><cell>9.9</cell><cell>-</cell></row><row><cell>Learnable filter bank</cell><cell>12.9</cell><cell>-</cell></row><row><cell>Conv-TasNet</cell><cell>12.7</cell><cell>8.3</cell></row><row><cell>DPRNN</cell><cell>13.9</cell><cell>10.3</cell></row><row><cell>Ours</cell><cell>15.2</cell><cell>12.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The results of performing test-time augmentation. The x-axis is the number of shifted versions that were averaged, at inference time, to obtain the final output. The y-axis is the SI-SNRi obtained by this process. DPRNN results are obtained by running the published training code.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Number of augmentations</cell></row><row><cell>Model</cell><cell>0</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>10 15 20</cell></row><row><cell cols="6">DPRNN(2spk) 18.08 18.11 18.15 18.18 18.19 18.19 18.21</cell></row><row><cell>Ours(2spk)</cell><cell cols="5">20.12 20.16 20.24 20.26 20.29 20.3 20.31</cell></row><row><cell cols="6">DPRNN(3spk) 14.72 15.06 15.14 15.18 15.21 15.24 15.25</cell></row><row><cell>Ours(3spk)</cell><cell cols="5">16.71 16.86 16.93 16.96 16.99 17.01 17.01</cell></row><row><cell cols="6">DPRNN(4spk) 10.37 10.49 10.53 10.54 10.56 10.57 10.58</cell></row><row><cell>Ours(4spk)</cell><cell cols="5">12.88 12.91 13 13.04 13.05 13.11 13.11</cell></row><row><cell cols="6">DPRNN(5spk) 8.35 8.85 8.87 8.89 8.9 8.91 8.91</cell></row><row><cell>Ours(5spk)</cell><cell cols="5">10.56 10.72 10.8 10.84 10.88 10.92 10.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The results of evaluating models with at least the number of required output channels on the datasets where the mixes contain 2,3,4, and 5 speakers. (a) DPRNN (our training using the authors' published code), (b) our method.</figDesc><table><row><cell></cell><cell cols="4">Num. speakers in mixed sample</cell></row><row><cell>DPRNN model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">2-speaker model 18.08</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">3-speaker model 13.47 14.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">4-speaker model 10.77 11.96 10.88</cell><cell>-</cell></row><row><cell cols="2">5-speaker model 7.62</cell><cell>9.76</cell><cell>9.48</cell><cell>8.65</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Num. speakers in mixed sample</cell></row><row><cell>Our model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">2-speaker model 20.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">3-speaker model 15.63 16.85</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">4-speaker model 13.25 13.46 12.88</cell><cell>-</cell></row><row><cell cols="5">5-speaker model 11.02 11.81 11.21 10.56</cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results of automatically selecting the number of speakers C for a mixed sample x. Shown are both the confusion matrix and the SI-SNRi results obtained using automatic model selection, in comparison to the results obtained when the number of speakers in the mixture is given. (a) DPRNN, (b) Our model.</figDesc><table><row><cell></cell><cell cols="4">Num. speakers in mixed sample</cell><cell></cell><cell cols="4">Num. speakers in mixed sample</cell></row><row><cell>DPRNN model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Our model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>2spk</cell><cell>81.3%</cell><cell>7.9%</cell><cell>3.2%</cell><cell>0.7%</cell><cell>2spk</cell><cell>84.6%</cell><cell>3.6%</cell><cell>1.2%</cell><cell>0.3%</cell></row><row><cell>3spk</cell><cell cols="2">15.9% 64.4%</cell><cell>9.9%</cell><cell>2.4%</cell><cell>3spk</cell><cell cols="2">13.7% 69.0%</cell><cell>7.4%</cell><cell>1.6%</cell></row><row><cell>4spk</cell><cell>0.7%</cell><cell cols="3">14.5% 46.2% 11.3%</cell><cell>4spk</cell><cell>0.5%</cell><cell cols="2">18.2% 47.5%</cell><cell>5.8%</cell></row><row><cell>5spk</cell><cell>2.1%</cell><cell cols="3">13.2% 40.7% 85.6%</cell><cell>5spk</cell><cell>1.2%</cell><cell>9.2%</cell><cell cols="2">43.9% 92.3%</cell></row><row><cell cols="2">Ours auto-select 15.88</cell><cell>12.28</cell><cell>9.79</cell><cell>8.53</cell><cell cols="2">Ours auto-select 18.63</cell><cell>14.62</cell><cell>11.48</cell><cell>10.37</cell></row><row><cell>Ours known C</cell><cell>18.21</cell><cell>14.71</cell><cell>10.37</cell><cell>8.65</cell><cell>Ours known C</cell><cell>20.12</cell><cell>16.85</cell><cell>12.88</cell><cell>10.56</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results for the music source separation task. We report SDR results (the higher the better) for the proposed model and several baseline models. All results are reported on the MusDB benchmarks.</figDesc><table><row><cell>Model</cell><cell cols="6">Wav All Drums Bass Other Vocals</cell></row><row><cell>Open-Unmix</cell><cell>X</cell><cell>5.33</cell><cell>5.73</cell><cell>5.23</cell><cell>4.02</cell><cell>6.32</cell></row><row><cell>Wave-U-Net</cell><cell>V</cell><cell>3.23</cell><cell>4.22</cell><cell>3.21</cell><cell>2.25</cell><cell>3.25</cell></row><row><cell>Demucs</cell><cell>V</cell><cell>5.58</cell><cell>6.08</cell><cell>5.83</cell><cell>4.12</cell><cell>6.29</cell></row><row><cell>Ours</cell><cell>V</cell><cell>5.82</cell><cell>6.15</cell><cell>5.88</cell><cell>4.32</cell><cell>6.92</cell></row><row><cell>IRM (Oracle)</cell><cell>X</cell><cell>8.22</cell><cell>8.45</cell><cell>4.12</cell><cell>7.85</cell><cell>9.43</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">we omitted ID due to its un-relevance to the music source separation task</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The contribution of Eliya Nachmani is part of a Ph.D. thesis research conducted at Tel Aviv University. We would like to thank anonymous reviewer 4 for suggesting an improvement of the model selection method. The method presented in Sec. 3.3 of this version is the improved one. We also would like to thanks Alexandre Défossez for the help with the music source separation experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-resolution frequency-wavenumber spectrum analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Capon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1408" to="1418" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind source separation and independent component analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing-Letters and Reviews</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single channel target speaker extraction and recognition with speaker beam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="5554" to="5558" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech dereverberation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for linearly constrained adaptive array processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">L</forename><surname>Frost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="926" to="935" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Csr-i (wsj0) complete ldc93s6a. Web Download. Philadelphia: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02173</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spectral Clustering for Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="221" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global and local simplex representations for multichannel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laufer-Goldshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="914" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cbldnn-based speaker-independent speech separation via generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on single channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Logeshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Mala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Communication, Network, and Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="387" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speakerindependent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13387</idno>
		<title level="m">Low-latency adaptive beamforming for multi-microphone audio processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end microphone permutation and number invariant multi-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14104</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whamr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multichannel eigenspace beamforming in a reverberant noisy environment with multiple interfering speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1071" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single-channel speech presence probability estimation and noise tracking. Audio Source Separation and Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6364" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Theory and application of digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Musdb18-a corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multiscale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St &amp;quot;oter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01667</idno>
		<ptr target="https://doi.org/10.21105/joss.01667" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recursive speech separation for unknown number of speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasaarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4052" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The second chimespeech separation and recognition challenge: Datasets, tasks and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nesta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voicefilter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04826</idno>
		<title level="m">Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="686" to="690" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Endto-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01160</idno>
		<title level="m">Extending speech separation to noisy environments</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A study of learning based beamforming methods for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2016 workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavesplit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furcanext</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speaker-aware neural network based beamformer for speaker extraction in speech mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2655" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
