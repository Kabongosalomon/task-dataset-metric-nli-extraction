<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Classical Japanese Literature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Yamamoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Mikel Bober-Irizar</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Open Data in the Humanities MILA</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">National Institute of Japanese Literature Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Classical Japanese Literature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Center for Open Data in the Humanities Royal Grammar School, Guildford</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recorded historical documents give us a peek into the past. We are able to glimpse the world before our time; and see its culture, norms, and values to reflect on our own. Japan has very unique historical pathway. Historically, Japan and its culture was relatively isolated from the West, until the Meiji restoration in 1868 where Japanese leaders reformed its education system to modernize its culture. This caused drastic changes in the Japanese language, writing and printing systems. Due to the modernization of Japanese language in this era, cursive Kuzushiji (くずし字) script is no longer taught in the official school curriculum. Even though Kuzushiji had been used for over 1000 years, most Japanese natives today cannot read books written or published over 150 years ago. <ref type="bibr" target="#b13">[10,</ref><ref type="bibr" target="#b23">20]</ref>    <ref type="bibr" target="#b16">[13]</ref> is a book for women in the Edo period (left). Shinpen Shūshinkyouten Vol.3『新編修身 教典三巻』 <ref type="bibr" target="#b9">[6]</ref> is a textbook right after the standardization of Japanese in 1900 (right).</p><p>According to the General Catalog of National Books <ref type="bibr" target="#b22">[19]</ref> there have been over 1.7 million books written or published in Japan prior to 1867. In addition to the number of registered books in the national catalog, we estimate that in total there are over 3 million unregistered books and a billion historical documents preserved nationwide. Despite ongoing efforts to create digital copies of these documents-a safeguard against fires, earthquakes, and tsunamis-most of the knowledge, history, and culture contained within these texts remains inaccessible to the general public. While we have many digitized copies of manuscripts and books, only a small number of people with Kuzushiji education are able to read them and work on them, leading to a huge dataset of Japanese cultural works which cannot be read by non-experts.  In this paper we introduce a dataset specifically made for machine learning research to engage the community to the field of Japanese literature. In this work, we release three easy-to-use preprocessed datasets: Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Kuzushiji-MNIST is designed as a drop-in replacement for the MNIST <ref type="bibr" target="#b19">[16]</ref> dataset. In addition, we present baseline classification results on Kuzushiji-MNIST and Kuzushiji-49 using recent models, and also apply generative modelling to a domain transfer task between unseen Kuzushiji Kanji and Modern Kanji (See <ref type="figure" target="#fig_3">Figure 3</ref>). Through these datasets and experiments, we wish to intoducethe machine learning community into the world of classical Japanese literature. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kuzushiji Dataset</head><p>The Kuzushiji dataset is created by the National Institute of Japanese Literature (NIJL), and is curated by the Center for Open Data in the Humanities (CODH). In 2014, NIJL and other institutes begun a national project to digitize about 300,000 old Japanese books, transcribing some of them, and sharing them as open data for promoting international collaboration. During the transcription process, a bounding box was created for each character, but literature scholars did not think they were worth sharing. From a machine learning perspective, CODH suggested to make a separate dataset for bounding boxes on a page, because that can be used as the basis for many machine learning challenges and working towards automated transcription. As a result, the full Kuzushiji dataset was released in November 2016, and now the dataset contains 3,999 character types and 403,242 characters <ref type="bibr" target="#b8">[5]</ref>. Our hope is that through releasing datasets in familiar formats, we can encourage dialog between the ML and Japanese literature communities. We pre-processed characters scanned from 35 classical books printed in the 18 th century and organized the dataset into 3 parts:   The hentaigana for Ka (か) can be written using 12 different root characters (jibo, in red) <ref type="bibr" target="#b18">[15]</ref>, with some of these root characters themselves having multiple ways of being written. Many of the characters in our datasets have multiple ways of being written, so successful models need to be able to model the multi-modal distribution of each class, making the problem more challenging.</p><formula xml:id="formula_0">(1) Kuzushiji-MNIST,</formula><p>Since MNIST restricts us to 10 classes, much fewer than the 49 needed to fully represent Kuzushiji Hiragana, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST. One characteristic of classical Japanese which is very different from modern one is that Classical Japanese contains Hentaigana (変体仮名). Hentaigana or variant kana, are Hiragana characters that have more than one form of writing, as they were derived from different Kanji. Therefore, one Hiragana class of Kuzushiji-MNIST or Kuzushiji-49 may have many characters mapped to it. For instance, as seen in <ref type="figure" target="#fig_5">Figure 5</ref>, there are 3 different ways to write「つ」because this character was derived from different Kanji (川 and 津).</p><p>Another example of this many-to-one mapping is shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Even though Kuzushiji-MNIST was created as drop-in replacement for the MNIST dataset, the characteristics of Hentaigana and Arabic numbers are completely different, and is one reason why we believe the Kuzushiji-MNIST dataset is more challenging than MNIST.   Training/Test split is 6 7 and 1 7 of each class respectively. In all three datasets, the characters in the train and test sets are sampled from the same 35 books, meaning the data distributions of each class are consistent between the two sets. While Kuzushiji-MNIST is balanced across classes, Kuzushiji-49 has several rare characters with a small number of samples (such as「ゑ」which has only ∼ 400 samples).</p><p>On the other hand, Kuzushiji-Kanji is a highly imbalanced dataset due to the natural frequency of Kanji appearing in the Kuzushiji literature. In Kuzushiji-Kanji, the number of samples range from over a thousand to only one sample. This dataset is created for more creative experimental tasks rather than merely for classification and character recognition benchmarks.</p><p>Our design of a drop-in replacement for MNIST was inspired by the popular Fashion-MNIST <ref type="bibr" target="#b28">[25]</ref>, a dataset of fashion items that is considerably more difficult than the original MNIST dataset, while maintaining ease of use. One aspect of Fashion-MNIST that we believe decreases model performance compared to MNIST is that many fashion items, such as shirts, T-shirts, or coats look very similar at 28x28 pixel resolution in grayscale, making many samples ambiguous even for humans (Human performance on Fashion-MNIST is only 83.5% <ref type="bibr" target="#b27">[24]</ref>). A characteristic of Kuzushiji-MNIST that makes it more difficult compared to MNIST is that there are in fact multiple very different ways to write certain characters, while each way of writing is still unambiguous at 28x28 pixel resolution for human readers, meaning we believe there is less of a performance 'cap'. Another difference is that while fashion trends come and go, and what constitute a shirt may be different a hundred years from now, Kuzushiji will always remain Kuzushiji. We believe both Fashion-MNIST and Kuzushiji-MNIST will be useful companions to the original MNIST dataset for the research community.  <ref type="bibr" target="#b7">[4]</ref> 99.06% 95.12% 89.25% PreActResNet-18 <ref type="bibr" target="#b14">[11]</ref> 99.56% 97.82% 96.64% PreActResNet-18 + Input Mixup <ref type="bibr" target="#b29">[26]</ref> 99.54% 98.41% 97.04% PreActResNet-18 + Manifold Mixup <ref type="bibr" target="#b25">[22]</ref> 99.54% 98.83% 97.33% <ref type="table">Table 1</ref>: Test set accuracy, computed as mean of per-class accuracies to address class imbalance. We present baseline classification results on Kuzushiji-MNIST and Kuzushiji-49 in <ref type="table">Table 1</ref>. We consider 4 different baselines: A simple 4-nearest neighbours algorithm, a small 2-layer convolutional network, an 18-layer ResNet <ref type="bibr" target="#b14">[11]</ref>, and a ResNet that incorporates a manifold mixup regularizer <ref type="bibr" target="#b25">[22]</ref>. For the training setup details, please refer to the GitHub repository that contains the dataset. By comparing the performance numbers to the original MNIST dataset using various different approaches, we hope these results will provide a sense of the relative difficulty of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Baselines for Kuzushiji-MNIST and Kuzushiji-49</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain Transfer from Kuzushiji-Kanji to Modern Kanji</head><p>In addition to classification, we are interested in more creative uses of our dataset. While existing work <ref type="bibr">[3,</ref><ref type="bibr" target="#b15">12,</ref><ref type="bibr" target="#b20">17,</ref><ref type="bibr" target="#b26">23]</ref> on domain transfer focuses on pixel images, we explore instead the transfer from pixel images to vector images, across two different domains. Our proposed model aims to generate Modern Kanji versions of a given Kuzushiji-Kanji input, in both pixel and stroke-based formats.   In <ref type="figure" target="#fig_0">Figure 11</ref>, we present an overall diagram of our approach. We first train two separate Convolutional Variational Autoencoders, one on the Kuzushiji-Kanji dataset, and also a second on a pixel version of KanjiVG dataset rendered to 64x64 pixel resolution for consistency. The architecture for the VAE is identical to <ref type="bibr" target="#b12">[9]</ref> and both datasets are compressed into their own respective 64-dimensional latent space, z old and z new . As in previous work <ref type="bibr" target="#b11">[8]</ref>, we do not optimize the KL loss term below a certain threshold, ensuring some information capacity while enforcing the Gaussian prior on z.   Previous work <ref type="bibr" target="#b10">[7,</ref><ref type="bibr" target="#b30">27]</ref> utilized MDN-RNN to generate stroke-based Chinese characters. In our last step, we train a Sketch-RNN <ref type="bibr" target="#b11">[8]</ref> decoder model to generate Modern Kanji conditioned on z new . There are around 3,600 overlapping Kanji characters between the two datasets. For characters that are not in Kuzushiji-Kanji, we condition the model on the z new encoded from KanjiVG data to generate the stroke data also from KanjiVG, see <ref type="bibr">(1)</ref> in <ref type="figure" target="#fig_0">Figure 11</ref>. For characters that are in the overlapping 3,600 set, we use the z new sampled from the MDN conditioned on z old , to generate the stroke data also from KanjiVG, as per (2) in <ref type="figure" target="#fig_0">Figure 11</ref>. By doing this, the Sketch-RNN training procedure can fine tune aspects of the VAE's latent space that may not capture well parts of the data distribution of Modern Kanji when trained only on pixels, by training it again on the stroke version of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Directions</head><p>We believe the Kuzushiji datasets will not only serve as a benchmark to advance classification algorithms, but also contribute to more creative areas such as generative modelling, adversarial examples, few-shot learning, transfer learning and domain adaptation. To foster community building, we plan to organize machine learning competitions using Kuzushiji datasets to encourage further development of these research areas. We are also working on expanding the size of the dataset, and by next year, the size of the full Kuzushiji dataset will expand to over a million character images. We hope these efforts will encourage further collaboration between different research fields and at the same time, help preserve the cultural knowledge and heritage of Japanese history.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Most Japanese cannot read books over 150 years old, written in cursive Kuzushiji style. The 10 classes of Kuzushiji-MNIST, first column showing the modern Hiragana counterpart (left). Example of a Kuzushiji literature scroll, Genjimonogatari Uta Awase『源氏歌合絵巻』[21] (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The difference between a text printed in 1772 and one printed in 1900. Onna Daigaku『女 大学』</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Our domain transfer experiment, generating Modern Kanji from the Kuzushiji Kanji for unseen characters. (Section 3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>In addition to archives, classical books are circulated in manuscript bookstores, online auctions and the annual manuscript auction event held in Jimbocho, Tokyo. Ohya Shobo bookstore in Jimbocho (left). Edo books sold at the Sunday flea market at the Hanazono Shrine, Tokyo (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The 10 classes of Kuzushiji-MNIST. Train and test set sizes are 6,000 and 1,000 per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The hentaigana for Ka (か) can be written using 12 different root characters (jibo, in red) [15], with some of these root characters themselves having multiple ways of being written. Many of the characters in our datasets have multiple ways of being written, so successful models need to be able to model the multi-modal distribution of each class, making the problem more challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Examples of some of the 3832 classes in Kuzushiji-Kanji.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Kuzushiji-49 description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Kuzushiji-Kanji 64x64px samples (Top) and stroke-based Modern Kanji versions (Bottom).We employ KanjiVG[1], a font for Modern Kanji in a stroke-ordered format. Variational Autoencoders<ref type="bibr" target="#b17">[14,</ref><ref type="bibr" target="#b21">18]</ref> provide a latent space for both Kuzushiji-Kanji and a pixel version of KanjiVG. A Sketch-RNN [8] model is then trained to generate Modern Kanji strokes, conditioned on the VAE's latent space. Predicting pixel versions of Modern Kanji using a VAE also aids human transcribers as the blurry regions of the output can be interpreted as uncertain regions to focus on. In addition to the earlier Figure 3, see Figure 10 below for a demonstration of our model on test set examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>More domain transfer examples including the VAE pixel reconstructions for both domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( 2 )</head><label>2</label><figDesc>Schematic of Sketch-RNN conditioned on predicted Latent Space of Modern Kanji, given latent space of Kuzushiji Kanji.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :Algorithm 1</head><label>111</label><figDesc>Overview of our approach. (1) We first train a VAE on pixel version of KanjiVG (Modern Kanji), and a Sketch-RNN model to generate stroke versions of KanjiVG conditioned on the latent space, z new . (2) We train a VAE on Kuzushiji-Kanji, and train a Mixture Density Network [2] to predict P (z new |z old ). We generate stroke versions of Modern Kanji based on the predicted z new . Summary of training procedure in domain transfer experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The high class imbalance in Kuzushiji-49 and Kuzushiji-Kanji is due to the appearance frequency in the real source books, and kept that way to represent the real data distribution. Kuzushiji-49, as the</figDesc><table><row><cell>Hiragana Unicode Samples</cell><cell>Sample Images</cell><cell cols="2">Hiragana Unicode Samples</cell><cell>Samples Images</cell></row><row><cell>U+3042 7000</cell><cell></cell><cell cols="2">U+306F 7000</cell></row><row><cell>(a)</cell><cell></cell><cell>(ha)</cell><cell></cell></row><row><cell>U+3044 7000</cell><cell></cell><cell cols="2">U+3072 5968</cell></row><row><cell>(i)</cell><cell></cell><cell>(hi)</cell><cell></cell></row><row><cell>U+3046 7000</cell><cell></cell><cell cols="2">U+3075 7000</cell></row><row><cell>(u)</cell><cell></cell><cell>(fu)</cell><cell></cell></row><row><cell>U+3048 903</cell><cell></cell><cell cols="2">U+3078 7000</cell></row><row><cell>(e)</cell><cell></cell><cell>(he)</cell><cell></cell></row><row><cell>U+304A 7000</cell><cell></cell><cell cols="2">U+307B 2317</cell></row><row><cell>(o)</cell><cell></cell><cell>(ho)</cell><cell></cell></row><row><cell>U+304B 7000</cell><cell></cell><cell cols="2">U+307E 7000</cell></row><row><cell>(ka)</cell><cell></cell><cell>(ma)</cell><cell></cell></row><row><cell>U+304D 7000</cell><cell></cell><cell cols="2">U+307F 3558</cell></row><row><cell>(ki)</cell><cell></cell><cell>(mi)</cell><cell></cell></row><row><cell>U+304F 7000</cell><cell></cell><cell cols="2">U+3080 1998</cell></row><row><cell>(ku)</cell><cell></cell><cell>(mu)</cell><cell></cell></row><row><cell>U+3051 5481</cell><cell></cell><cell cols="2">U+3081 3946</cell></row><row><cell>(ke)</cell><cell></cell><cell>(me)</cell><cell></cell></row><row><cell>U+3053 7000</cell><cell></cell><cell>U+ 3082</cell><cell>7000</cell></row><row><cell>(ko)</cell><cell></cell><cell>(mo)</cell><cell></cell></row><row><cell>U+3055 7000</cell><cell></cell><cell cols="2">U+3084 7000</cell></row><row><cell>(sa)</cell><cell></cell><cell>(ya)</cell><cell></cell></row><row><cell>U+3057 7000</cell><cell></cell><cell cols="2">U+3086 1858</cell></row><row><cell>(shi)</cell><cell></cell><cell>(yu)</cell><cell></cell></row><row><cell>U+3059 7000</cell><cell></cell><cell cols="2">U+3088 7000</cell></row><row><cell>(su)</cell><cell></cell><cell>(yo)</cell><cell></cell></row><row><cell>U+305B 4843</cell><cell></cell><cell cols="2">U+3089 7000</cell></row><row><cell>(se)</cell><cell></cell><cell>(ra)</cell><cell></cell></row><row><cell>U+305D 4496</cell><cell></cell><cell cols="2">U+308A 7000</cell></row><row><cell>(so)</cell><cell></cell><cell>(ri)</cell><cell></cell></row><row><cell>U+305F 7000</cell><cell></cell><cell cols="2">U+308B 7000</cell></row><row><cell>(ta)</cell><cell></cell><cell>(ru)</cell><cell></cell></row><row><cell>U+3061 2983</cell><cell></cell><cell cols="2">U+308C 7000</cell></row><row><cell>(chi)</cell><cell></cell><cell>(re)</cell><cell></cell></row><row><cell>U+3064 7000</cell><cell></cell><cell cols="2">U+308D 2487</cell></row><row><cell>(tsu)</cell><cell></cell><cell>(ro)</cell><cell></cell></row><row><cell>U+3066 7000</cell><cell></cell><cell cols="2">U+308F 2787</cell></row><row><cell>(te)</cell><cell></cell><cell>(wa)</cell><cell></cell></row><row><cell>U+3068 7000</cell><cell></cell><cell cols="2">U+3090 485</cell></row><row><cell>(to)</cell><cell></cell><cell>(i)</cell><cell></cell></row><row><cell>U+306A 7000</cell><cell></cell><cell cols="2">U+3091 456</cell></row><row><cell>(na)</cell><cell></cell><cell>(e)</cell><cell></cell></row><row><cell>U+306B 7000</cell><cell></cell><cell cols="2">U+3092 7000</cell></row><row><cell>(ni)</cell><cell></cell><cell>(wo)</cell><cell></cell></row><row><cell>U+306C 2399</cell><cell></cell><cell cols="2">U+3093 7000</cell></row><row><cell>(nu)</cell><cell></cell><cell>(n)</cell><cell></cell></row><row><cell>U+306D 2850</cell><cell></cell><cell cols="2">U+309D 4097</cell></row><row><cell>(ne)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(iteration</cell><cell></cell></row><row><cell>U+306E 7000</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(no)</cell><cell></cell><cell>mark)</cell><cell></cell></row></table><note>name suggests, has 49 classes (266,407 images) and Kuzushiji-Kanji has a total of 3832 classes (140,426 images), ranging from 1,766 examples to only a single example per class. Kuzushiji-MNIST and Kuzushiji-49 consist of grayscale images of 28x28 pixel resolution, consistent with the MNIST dataset, while the Kuzushiji-Kanji images are of a larger 64x64 pixel resolution.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Location of dataset with instructions: https://github.com/rois-codh/kmnist</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Train two separate Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuzushiji-Kanji</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Train Mixture Density Network [2] to model P (znew|zold) as mixture of Gaussians</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Train Sketch-RNN [8] to generate KanjiVG strokes conditioned on either znew or znew ∼ P (znew|zold)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">2] with 2 hidden layers to model the density function of P (z new |z old ) approximated as a mixture of Gaussians. We can then sample a latent vector z new in the domain of Modern Kanji, given a latent vector z old encoded from Kuzushiji-Kanji. We note that training two separate VAE models on each dataset is much more efficient and achieves better results compared to training a single model end-to-end, which in our experience does not work well, and might explain why previous works</title>
		<imprint/>
	</monogr>
	<note>We then train a Mixture Density Network (MDN. 3, 12, 17, 23] require the use of an adversarial loss</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<ptr target="http://kanjivg.tagaini.net" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixture Density Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Open Data in the Humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>For</surname></persName>
		</author>
		<ptr target="http://codh.rois.ac.jp/char-shape/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Kuzushiji dataset</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Fukyūsha. Shinpen Shūshinkyouten</title>
		<editor>『新編修身教典 巻三』). Fukyūsha</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="http://otoro.net/kanji-rnn" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Neural Representation of Sketch Drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy6GHpkCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recurrent World Models Facilitate Policy Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01999</idno>
		<ptr target="https://worldmodels.github.io/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iikura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hisada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kobayashi-Better</surname></persName>
		</author>
		<ptr target="http://dh2016.adho.org/static/data/254.html" />
		<title level="m">The Kuzushiji Project: Developing a Mobile Learning Application for Reading Early Modern Japanese Texts. DHQ: Digital Humanities Quarterly</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kaibara</surname></persName>
		</author>
		<ptr target="http://www.wul.waseda.ac.jp/kotenseki/html/bunko30/bunko30_g0371/index.html" />
		<title level="m">Onna Daigaku (『女大学』). Shinsaibashijunkeichō, 1772</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kodama</surname></persName>
		</author>
		<title level="m">Kuzushiji Yōrei Jiten. Kondō Shuppansha</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">General Catalog of National Books (『国書総目録』). Iwanami Shoten</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shoten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Notation of the Japanese Syllabary seen in the Textbook of the Meiji first Year. The bulletin of Jissen Women&apos;s Junior College</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takashiro</surname></persName>
		</author>
		<ptr target="https://ci.nii.ac.jp/els/contents110009587135.pdf?id=ART0010042265" />
		<imprint>
			<date type="published" when="2013-03" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scroll Genjimonogatari Uta Awase (『源氏歌合絵巻』)</title>
		<idno>c. 1500</idno>
		<ptr target="http://codh.rois.ac.jp/pmjt/book/200014735/" />
		<imprint/>
	</monogr>
	<note>National Institute of Japanese Literature</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold Mixup: Learning Better Representations by Interpolating Hidden States</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Creation of Parameterized Avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1539" to="1547" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: A MNIST-like fashion product database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://github.com/zalandoresearch/fashion-mnist" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Drawing and Recognizing Chinese Characters with Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1606.06539</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
