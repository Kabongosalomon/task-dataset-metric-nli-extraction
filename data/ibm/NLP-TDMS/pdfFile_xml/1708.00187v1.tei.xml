<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Deep Video Deinterlacing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong XUETING LIU</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong XIANGYU MAO</orgName>
								<orgName type="institution" key="instit3">The Chinese University of Hong Kong TIEN-TSIN WONG</orgName>
								<orgName type="institution" key="instit4">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Deep Video Deinterlacing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Reconstruction</term>
					<term>Neu- ral networks</term>
					<term>Additional Key Words and Phrases: Video deinterlace, image interpolation, convolutional neural network, deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Input frames (d) Ours (c) Blown-ups (b) SRCNN (trained with our dataset) Leaves Soccer Fig. 1. (a) Input interlaced frames. (b) Deinterlaced results generated by SRCNN [4] re-trained with our dataset. (c) Blown-ups from (b) and (d) respectively.</p><p>(d) Deinterlaced results generated by our method. The classical super-resolution method SRCNN reconstruct each frame based on a single field and has large information loss. It also follows the conventional translation-invariant assumption which does not hold for the deinterlacing problem. Therefore, it inevitably generates blurry edges and artifacts, especially around sharp boundaries. In contrast, our method can circumvent this issue and reconstruct frames with higher visual quality and reconstruction accuracy.</p><p>Interlacing is a widely used technique, for television broadcast and video recording, to double the perceived frame rate without increasing the bandwidth. But it presents annoying visual artifacts, such as flickering and silhouette "serration," during the playback. Existing state-of-the-art deinterlacing methods either ignore the temporal information to provide real-time performance but lower visual quality, or estimate the motion for better deinterlacing but with a trade-off of higher computational cost. In this paper, we present the first and novel deep convolutional neural networks (DC-NNs) based method to deinterlace with high visual quality and real-time performance. Unlike existing models for super-resolution problems which relies on the translation-invariant assumption, our proposed DCNN model utilizes the temporal information from both the odd and even half frames to reconstruct only the missing scanlines, and retains the given odd and even scanlines for producing the full deinterlaced frames. By further introducing a layer-sharable architecture, our system can achieve real-time performance on a single GPU. Experiments shows that our method outperforms all existing methods, in terms of reconstruction accuracy and computational performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(d) Deinterlaced results generated by our method. The classical super-resolution method SRCNN reconstruct each frame based on a single field and has large information loss. It also follows the conventional translation-invariant assumption which does not hold for the deinterlacing problem. Therefore, it inevitably generates blurry edges and artifacts, especially around sharp boundaries. In contrast, our method can circumvent this issue and reconstruct frames with higher visual quality and reconstruction accuracy.</p><p>Interlacing is a widely used technique, for television broadcast and video recording, to double the perceived frame rate without increasing the bandwidth. But it presents annoying visual artifacts, such as flickering and silhouette "serration," during the playback. Existing state-of-the-art deinterlacing methods either ignore the temporal information to provide real-time performance but lower visual quality, or estimate the motion for better deinterlacing but with a trade-off of higher computational cost. In this paper, we present the first and novel deep convolutional neural networks (DC-NNs) based method to deinterlace with high visual quality and real-time performance. Unlike existing models for super-resolution problems which relies on the translation-invariant assumption, our proposed DCNN model utilizes the temporal information from both the odd and even half frames to reconstruct only the missing scanlines, and retains the given odd and even scanlines for producing the full deinterlaced frames. By further introducing a layer-sharable architecture, our system can achieve real-time performance on a single GPU. Experiments shows that our method outperforms all existing methods, in terms of reconstruction accuracy and computational performance.</p><p>CCS Concepts: • Computing methodologies → Reconstruction; Neural networks;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Interlacing technique has been widely used in the past few decades for television broadcast and video recording, in both analog and digital ways. Instead of capturing all N scanlines for each frame, only N /2 odd numbered scanlines are captured for the current frame ( <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, upper), and the other N /2 even numbered scanlines are captured for the following frame ( <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, lower). It basically trades the frame resolution for the frame rate, in order to double the perceived frame rate without increasing the bandwidth. Unfortunately, since the two half frames are captured in different time instances, there are significant visual artifacts such as line flickering and "serration" on the silhouette of moving objects ( <ref type="figure" target="#fig_2">Fig. 2(b)</ref>), when the odd and even fields are interlaced displayed. The degree of "serration" depends on the motion of objects and hence is spatially varying. This makes deinterlacing (removal of interlacing artifacts) an ill-posed problem.</p><p>Many deinterlacing methods have been proposed to suppress the visual artifacts. A typical approach is to reconstruct two full frames from the odd and even half frames independently ( <ref type="figure" target="#fig_2">Fig. 2(c)</ref>). However, the result is usually unsatisfactory, due to the large information loss (50% loss) <ref type="bibr">[5,</ref><ref type="bibr">20,</ref><ref type="bibr">21]</ref>. Higher-quality reconstruction can be obtained by first estimating object motion <ref type="bibr">[10,</ref><ref type="bibr">14,</ref><ref type="bibr">17]</ref>. However, motion estimation from half interlacing frames are not reliable, and also computationally expensive. Hence, they are seldomly used in practice, let alone real-time applications.</p><p>In this paper, we propose the first deep convolutional neural networks (DCNNs) method tailormade for the video deinterlacing problem. To our best knowledge, no DCNN-based deinterlacing method exists. One may argue that existing DCNN-based methods for interpolation or super-resolution <ref type="bibr">[4,</ref><ref type="bibr">15]</ref> can be applied to reconstruct the full frames from the half frames, in order to solve the deinterlacing problem. However, such naive approach lacks of utilizing the temporal information between the odd and even half frames, just like the existing intra-field deinterlacing methods <ref type="bibr">[5,</ref><ref type="bibr">20]</ref>. Moreover,  this naive approach follows the conventional translation-invariant assumption. That means, all pixels in the output full frames are processed with the same set of convolutional filters, even though half of the scanlines (odd/even numbered) actually exist in the input half frames. <ref type="figure" target="#fig_3">Fig. 3(b)</ref> shows a full frame, reconstructed by the state-of-the-art DCNN-based super-resolution method, <ref type="bibr">SRCNN [4]</ref>, exhibiting obvious halo artifact. Instead of replacing the potentially error-contaminated pixels from the convolutional filtering with the groundtruth pixels in the input half frames and leading to visual artifacts ( <ref type="figure" target="#fig_3">Fig. 3(c)</ref>), we argue that we should only reconstruct the missing scanlines, and leave the pixels in the original odd/even scanlines intact. All these motivate us to design a novel DCNN model tailored for solving the deinterlacing problem.</p><p>In particular, our newly proposed DCNN architecture circumvents the translation-invariant assumption and takes the temporal information into consideration. Firstly, we only estimate the missing scanlines to avoid modifying the groundtruth pixel values from the odd/even scanlines (input). That is, the output of the neural network system are two half frames containing only the missing scanlines. Unlike most existing methods which ignore the temporal information between the odd and even frames, we reconstruct each half output frame from both the odd and even frames. In other words, our neural network system takes two original half frames as input and outputs two missing half frames (complements).</p><p>Since we have two outputs, two neural networks are needed for training. We further accelerate it by combining the lower-levels of two neural networks [2], as the input are the same and hence the lower-level convolutional filters are sharable. With this improved network structure, we can achieve real-time performance.</p><p>To validate our method, we evaluate it over a rich variety of challenging interlaced videos including live broadcast, legacy movies, and legacy cartoons. Convincing and visually pleasant results are obtained in all experiments ( <ref type="figure" target="#fig_3">Fig. 1 &amp; 3(d)</ref>). We also compare our method to existing deinterlacing methods and DCNN-based models in both visual comparison and quantitative measurements. All experiments confirm that our method not only outperforms existing methods in terms of accuracy, but also speed performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Before introducing our method, we first review existing works related to deinterlacing. They can be roughly classified into tailormade deinterlacing methods, traditional image resizing methods, and DCNN-based image restoration approaches.</p><p>Image/Video Deinterlacing Image/video deinterlacing is a classic vision problem. Existing methods can be classified into two categories: intra-field deinterlacing <ref type="bibr">[5,</ref><ref type="bibr">20,</ref><ref type="bibr">21]</ref> and inter-field deinterlacing <ref type="bibr">[10,</ref><ref type="bibr">14,</ref><ref type="bibr">17]</ref>. Intra-field deinterlacing methods reconstruct two full frames from the odd and even fields independently. Since there is large information loss (half of the data is missing) during frame reconstruction, the visual quality is usually less satisfying. To improve visual quality, inter-field deinterlacing methods incorporate the temporal information between multiple fields from neighboring frames during frame reconstruction. Accurate motion compensation or motion estimation [8] is needed to achieve satisfactory quality. However, accurate motion estimation is hard in general. In addition, motion estimation requires high computational cost, and hence inter-field deinterlacing methods are seldom used in practice, especially for applications requiring real-time processing.</p><p>Traditional Image Resizing Traditional image resizing methods can also be used for deinterlacing by scaling up the height of each field. To scale up an image, cubic <ref type="bibr">[16]</ref> and Lanczos interpolation [6] are frequently used. While they work well for low-frequency components, high-frequency components (e.g. edges) may be over-blurred. More advanced image resizing methods, such as kernel regression <ref type="bibr">[18]</ref> and bilateral filter [9] can improve the visual quality by preserving more high-frequency components. However, these methods may still introduce noise or artifacts if the vertical sampling rate is less than the Nyquist rate. More critically, they only utlize a single field and ignore the temporal information, and hence suffer the same problem as intra-deinterlacing methods.  DCNNs for Image Restoration In recent years, deep convolutional neural networks (DCNNs) based methods have been proposed to solve many image restoration problems. <ref type="bibr">Xie et al. [23]</ref> proposed a DCNN model for image denosing and inpainting. This model recovers the values of corrupted pixels (or missing pixels) by learning the mapping between corrupted and uncorrupted patches. <ref type="bibr">Dong et al. [4]</ref> proposed to adopt DCNN for image super-resolution, which greatly outperforms the state-of-the-art image super-resolution methods. <ref type="bibr">Gharbi et al. [7]</ref> further proposed a DCNN model for joint demosaiking and denosing. It infers the values of three color channels of each pixel from a single noisy measurement. It seems that we can simply re-train these state-of-the-art neural network based methods for our deinterlacing purpose. However, our experiments show that visual artifacts are still unavoidable, as these DCNNs generally follow the conventional translation-invariant assumption and modify the values of all pixels, even in the known odd/even scanlines. Using a larger training dataset or deeper network structure may alleviate this problem, but the computational cost is drastically increased and still there is no guarantee that the values of the known pixels remain intact. Even if we fix the values of the known pixels ( <ref type="figure" target="#fig_3">Fig. 3(c)</ref>), the quality does not improve. In contrast, we propose a novel DCNN tailored for deinterlacing. Our model only estimates the missing pixels instead of the whole frame, and also take the temporal information into account to improve visual quality.</p><formula xml:id="formula_0">I = { , } X odd X even X t X even t X odd X t+1 X even t+1 X odd t+1 conv features F1 . conv features F2 . conv features F3 . conv features F4 1 . _ conv features F4 2 . _ output F5 1 _ output F5 2 _ = { , } = { , } t t+1 t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>Given an input interlaced frame I ( <ref type="figure" target="#fig_5">Fig. 4(a)</ref>), our goal of deinterlacing is to reconstruct two full size original frames X t and X t +1 from I ( <ref type="figure" target="#fig_5">Fig. 4(d)</ref>). We denote the odd field of I as X odd t (blue pixels in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>), and the even field of I as X even t +1 (red pixels in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>). The superscripts, odd and even, denote the odd-or even-numbered half frames. The subscripts, t and t + 1, denote the two fields are captured at two different time instances. Our goal is to reconstruct two missing half frames, X even t (light blue pixels in <ref type="figure" target="#fig_5">Fig. 4(c)</ref>) and X odd t +1 (pink pixels in <ref type="figure" target="#fig_5">Fig. 4(c)</ref>). Note that we retain the known fields X odd t (blue pixels) and X even t +1 (red pixels) in our two output full frames ( <ref type="figure" target="#fig_5">Fig. 4(d)</ref>).</p><p>To estimate the unknown pixels X even t and X odd t +1 from the interlaced frame I, we propose a novel DCNN model ( <ref type="figure" target="#fig_5">Fig. 4(b)</ref> &amp; (c)). The input interlaced frame can be of any resolution, and two half output images are obtained with five convolutional layers. The weights of the convolutional operators are trained from a DCNN model training procedure based on a prepared training dataset. During the training phase, we synthesize a set of interlaced videos from progressive videos of different types as the training pairs. The reason that we need to synthesize interlaced videos for training is that no groundtruth exists for the existing interlaced videos captured by interlaced scan devices. The details of preparing the training dataset and the design of the proposed DCNN are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DCNN-BASED VIDEO DEINTERLACING 4.1 Training Data Preparation</head><p>While there exists a large collection of interlaced videos over the Internet, unfortunately, the ground-truth of these videos is lacking. Therefore, to prepare a training data set, we have to synthesize interlaced videos from existing progressive videos. To enrich our data variety, we collect 33 videos from the Internet and capture 18 videos using progressive scan devices ourselves. The videos are of different genres, ranging from scenic, sports, computer-rendered, to classic movies and cartoons. Then we randomly sample 3 pairs of consecutive frames from each collected video and obtain 153 frame pairs in total. For each pair of consecutive frames, we rescale each frame to the size of 512 × 512 and label them as the pair of original frames X t and X t +1 (ground-truth full frames) ( <ref type="figure">Fig. 5(a)</ref>). Then we synthesize an interlaced frame based on these two original frames as I = {X odd t , X even t +1 }, i.e., the odd lines of I are copied from X t and the even lines of I are copied from X t +1 <ref type="figure" target="#fig_6">(Fig. 5(b) &amp; 6)</ref>. For each triplet ⟨I, X t , X t +1 ⟩ of 512 × 512 resolution, we further divide them into 64×64-resolution patch triplets I p , X t,p , X t +1,p with the sampling stride setting to 64. Note that during patch generation, the parity of the divided patches remain the same as original images. Finally, for each patch triplet I p , X t,p , X t +1,p , we use I p as a training <ref type="figure">Fig. 5</ref>. Training data preparation. (a) Two consecutive frames X t and X t +1 from an input video. (a) An interlaced frame I is synthesized by taking the odd lines from X t and even lines from X t +1 respectively and regarded as the training input. (c) The even lines of X t and the odd lines of X t +1 are regarded as the training output. input <ref type="figure">(Fig. 5(b)</ref>) and the corresponding X even t,p and X odd t +1,p as training outputs <ref type="figure">(Fig. 5(c)</ref>). In particular, we convert patches into the Lab color space and only use the L channel for training. Altogether, we collect 9,792 patch triplets from the prepared videos, where 80% of the triplets are used for training and the rest are used for validation during the training process. Note that, although our model is trained by patches of 64 × 64 resolution, the trained convolutional operators can actually be applied on images of any resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Network Architecture</head><p>With the prepared training dataset, we now present how we design our network structure for deinterlacing. An illustration of our network structure is shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. It contains five convolutional layers. Our goal is to reconstruct the original two frames X t and X t +1 from an input interlaced frame I. In the following, we first explain our design rationales and then describe the architecture in detail.</p><p>The Input/Output Layers One may suggest to utilize the existing neural network (e.g. SRCNN [4]) to learn X t from X odd t and X t +1 from X even t +1 independently. This effectively turns the problem into a super-resolution or image upscaling problem. However, there are two drawbacks.</p><p>First of all, since the two frame reconstruction processes (i.e. from X odd t to X t and X even t +1 to X t +1 ) are independent from each other, the neural network can only estimate the full frame from the known half frame without the temporal information. This inevitably leads to less satisfying results due to the large (50%) information loss. In fact, the two fields in the interlaced frame are temporally correlated. Consider an extreme case where the scene in the two consecutive frames are static. In this scenario, the two consecutive frames are exactly the same, and the interlaced frame should also be artifact-free and exactly equal to the groundtruth we are looking for. However, using this naive super-resolution approach, we have to feed the half frame X odd t (or X even t +1 ) to reconstruct a full frame. It completely ignores the another half frame (which now contains the exact pixel values) and introduces artifacts (due to 50% information loss). <ref type="figure" target="#fig_7">Fig. 7</ref> shows the poor result of one such scenario. In contrast, our proposed neural network takes the whole interlaced frame I as input ( <ref type="figure" target="#fig_5">Fig. 4(a)</ref>). Note that the temporal information is implicitly taken into consideration in our network, since the two fields captured at different time instances are used for reconstructing each single frame. The network may exploit the temporal correlation between fields to improve the visual quality in higher-level convolutional layers.</p><p>Secondly, the standard neural network generally follows the conventional translation-invariant assumption. That means all pixels in the input image are processed with the same set of convolutional filters. However, in our deinterlacing application, half of the pixels in X t and X t +1 actually exist in I and should be directly copied from I. Applying convolutional filters on these known pixels inevitably changes their original colors and leads to clear artifacts ( <ref type="figure" target="#fig_3">Fig. 3(b)</ref> &amp; (c)). In contrast, our neural network only estimates the unknown pixels X even t and X odd t +1 <ref type="figure" target="#fig_5">(Fig. 4(c)</ref>) and copies the known pixels from I to X t and X t +1 directly ( <ref type="figure" target="#fig_5">Fig. 4(d)</ref>).</p><p>Pathway Design Since we estimate two half frames X even t and X odd t +1 from the interlaced frame I, we actually have to train two networks/pathways independently. Separately training two networks is computational costly. Instead of training two networks, one may suggest to train a single network for estimating the two half frames simultaneously by doubling the depth of each convolutional layer. However, this also highly increases the computational cost, since the number of the trained weights are doubled. As reported by [2], deep neural network is to seek good representation of input data, and such representations can be transferred to many other tasks if the input data is similar. For example, the trained features of AlexNet [13] (originally designed for object recognition) can also be used for texture recognition and segmentation <ref type="bibr">[3]</ref>. In fact, the lower-level layers of the convolutional networks are always lower-level feature detectors that can detect edges and other primitives. These lower-level layers in the trained models can be reused for new tasks by training new higher-level layers on top of them. Therefore, in our deinterlacing scenario, it is natural to combine the lower-level convolutional layers to reduce the computation, since the input of the two networks/pathways is completely the same. On top of these weight-sharing lower-level layers, higher-level layers are trained separately for estimating X even t and X odd t +1 respectively. This makes the higher-level layers more adaptable to different objectives. Our method can be regarded as training one neural network for estimating X even t and then fixing the first three convolutional layers and re-training a second neural network for estimating X odd t +1 .</p><p>Detailed Architecture As illustrated in <ref type="figure" target="#fig_5">Fig. 4(b)</ref> &amp; (c), our network contains five convolutional layers with weights. The first, second, and third layers are sequentially connected and shared by both pathways. The first convolutional layer has 64 kernels of size 3×3×1. The second convolutional layer has 64 kernels of size 3 × 3 × 64 and is connected to the output of the first layer. The third convolutional layer has 64 kernels of size 1 × 1 × 64 and is connected to the output of the second layer. The forth and fifth layers branch into two pathways without any connection between them. The forth convolutional layer has 64 kernels of size 3 × 3 × 64 where each pathway has 32 kernels. The fifth convolutional has 2 kernels of size 3 × 3 × 64 where each pathway has 1 kernel. The activations for the first two layers are ReLU functions, while for the rest layers are identify functions. The strides of convolution for the first four layers are 1 pixel. For the last layer, the horizontal stride remains 1 pixel, while the vertical stride is 2 pixels to obtain half-height images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning and Optimization</head><p>Given the training dataset containing a set of triplets I p , X even t,p , X odd t +1,p , the optimal weights W * of our neural network are trained via the following objective function:</p><formula xml:id="formula_1">W * = arg min 1 N p p ∥ X even t,p − X even t,p ∥ 2 2 + ∥ X odd t +1,p − X odd t +1,p ∥ 2 2 + λ T V TV ( X t,p ) + TV ( X t +1,p )<label>(1)</label></formula><p>where N p is the number of training samples, X even t,p and X odd t +1,p are the estimated output of the neural network, TV (·) is the total variation regularizer <ref type="bibr">[1,</ref><ref type="bibr">11]</ref> and λ T V is the regularization scalar. We trained our neural network using Tensorflow on a workstation equipped with a single nVidia TITAN X Maxwell GPU. The standard ADAM optimization method [12] is used to solve Eq. 1. The learning rate is 0.001 and λ T V is set to 2 × 10 −8 in our experiments. The number of epochs is 200 and the batch size for each epoch is 64. It takes about 4 hours to train the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULT AND DISCUSSION</head><p>We evaluate our method on a large collection of interlaced videos downloaded from the Internet or captured by ourselves with interlaced scan cameras. These videos include live sporting videos ("Soccer" in <ref type="figure">Fig. 1</ref> and "Tennis" in <ref type="figure" target="#fig_8">Fig. 8</ref>), scenic videos ("Leaves" in <ref type="figure">Fig. 1</ref> and "Bus" in <ref type="figure" target="#fig_8">Fig. 8</ref>), computer-rendered gameplay videos ("Hunter" in <ref type="figure" target="#fig_8">Fig. 8</ref>), legacy movies ("Haystack" in <ref type="figure" target="#fig_8">Fig. 8</ref>), and legacy cartoons ("Rangers" in <ref type="figure" target="#fig_8">Fig. 8</ref>). Note that, we have no access to the original progressive frames (groundtruth) of these videos. Without groundtruth, we can only compare our method to existing methods visually, but not quantitatively.</p><p>To evaluate quantitatively (with comparison to the groundtruth), we synthesize a set of test interlaced videos from progressive scan videos of different genres. None of these synthetic interlaced videos exist in our training data. <ref type="figure" target="#fig_9">Fig. 9</ref> presents a set of synthetic interlaced videos, including sports ("Basketball"), scenic ("Taxi"), computerrendered ("Roof"), movies ("Jumping"), and cartoons ("Tide" and "Girl"). Due to the page limit, we only present one representative interlaced frame for each video sequence. While two full size frames can be recovered from each single interlaced frame, we only show the first frame in all our results. Please refer to the supplementary materials for more complete results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Comparison</head><p>We first compare our method with the classic bicubic interpolation and the existing DCNN tailored for superresolution, i.e. <ref type="bibr">SRCNN [4]</ref>. Since SRCNN is not designed for deinterlacing, we re-train their model with our prepared dataset for deinterlacing purpose. The results are presented in <ref type="figure" target="#fig_8">Fig. 1 and 8</ref>. "Soccer", "Bus" and "Tennis" are in 1080i format and exhibit severe interlacing artifacts. Besides, the frames also contain motion-blur and video compression artifacts. Since both bicubic interpolation and SRCNN reconstruct each frame from a single field alone, their results are unsatisfactory and exhibit obvious artifacts due to the large information loss. SRCNN performs even worse than the bicubic interpolation, since it follows the conventional translation-invariant assumption which not held in deinterlacing scenario. In comparison, our method can obtain much clearer and sharper results than our competitors. The "Hunter" example shows a moving character from a gameplay where the computer-rendered object contours/boundaries are sharply preserved. Both bicubic interpolation and SRCNN lead to blurry and zig-zag near these sharp edges. In contrast, our method obtains the best reconstruction result in achieving sharp and smooth boundaries. The "Haystack" and "Rangers" examples are both taken from legacy DVDs in interlaced NTSC format. In the "Haystack" example, only the character is moving, while the background remains static. Without considering the temporal information, both bicubic interpolation and SRCNN fails to recover the fine texture of the haystacks and obtain blurry results. In sharp contrast, our method successfully recovers the fine texture by taking two fields into consideration.</p><p>We further compare our method to the state-of-the-art deinterlacing methods, including ELA [5], WLSD <ref type="bibr">[22],</ref><ref type="bibr">and FBA [19]</ref>. ELA is the most widely used deinterlacing methods due to its high performance. It is an intra-field method and uses edge directional correlation to reconstruct the missing scanlines. WLSD is the stateof-the-art intra-field deinterlacing method based on optimization. It generally produces better result than that of ELA, but with a higher computational expense. FBA is the state-of-the-art inter-field method. <ref type="figure" target="#fig_9">Fig. 9</ref> shows the results of all methods for a set of synthetic   interlaced videos, in which we have the groundtruths for quantitative evaluation. Besides the reconstructed frames, we also blow-up the difference images for better visualization. The difference image is simply computed as the pixel-wise absolute difference between the output and the groundtruth. As we can observe, all our competitors generate artifacts surrounding the boundaries. The sharper the boundary is, the more obvious the artifact is. In general, ELA produces the most artifacts since it adopts a simple interpolator and utilizes information from a single field alone. WLSD produces less artifacts as it adopts a more complex optimization-based strategy to fill the missing pixels. But it still only utilizes information of a single field and has large information loss during reconstruction. Though FBA utilizes the temporal information, it still cannot achieve good visual quality because they only rely on simple interpolators. In contrast, our method produces significantly less artifacts than all competitors.</p><p>Quantitative Evaluation We train our neural network by minimizing the loss of Eq. 1 on the training data. The training loss and validation loss throughout the whole training epochs are shown in <ref type="figure" target="#fig_10">Fig. 10</ref>. Both training and validation losses reduce rapidly after the first few epochs and converge in around 50 epochs. We also compare the accuracy of our method to our competitors in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Note that we only compute the PSNR and SSIM for those test videos with groundtruth. We take the average value over all frames of each video sequence in computing both measurements. <ref type="table">Table 1</ref> presents the statistics. Our method outperforms the competitors in terms of both PSNR and SSIM in most cases.</p><p>Timing Statistics Lastly, we compare the running time of our method to our competitors on a workstation with Intel Core CPU i7-5930, 65GB RAM equipped with a nVidia TITAN X Maxwell GPU. The statistics are presented in <ref type="table">Table 2</ref>. Our method achieves the highest performance among all methods in all resolutions. It processes even faster than ELA with apparently better visual quality. ELA and SRCNN have similar performance and are slighter slower than our method. Bicubic interpolation, WLSD, and FBA have much higher computational complexity and are far from real-time processing. Note that ELA is only a CPU method without GPU acceleration. In particular, with a single GPU, our method already achieves realtime performance up to the resolution of 1024 × 768 (33 fps). With one more GPU, our method can also achieve real-time performance for 1920 × 1080-resolution videos. We also test our model without sharing lower-level layers, i.e., two separate networks are needed for reconstructing the two frames. The statistics is shown in the last column in <ref type="table">Table 2</ref>. This strategy roughly triples the computational time while quality is similar to that with sharing low-level layers.</p><p>Limitations Since our method does not explicitly separate the two fields for reconstructing two full frames, the two fields may interfere each other badly when the motion between the two fields are extremely large. The first row in <ref type="figure" target="#fig_11">Fig. 11</ref> presents an example where the interlaced frame has a very large motion, obvious artifacts can be observed. Our method may also fail when the interlaced frame contains very thin horizontal structures. The second row of <ref type="figure" target="#fig_11">Fig. 11</ref> shows an example where a horizontal thin reflection stripe appears on a car. Only one line of the reflection stripe is scanned in the interlaced frame. Our neural network fails to identify it as a result of interlacing, but regards it as the original structures and incorrectly preserves it in the reconstructed frame. This is because this kind of patches is rare and gets diluted by the large amount of common cases. We may relieve this problem by training the neural network with more such training patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present the first DCNN for video deinterlacing. Unlike the conventional DCNNs suffering from the translationinvariant issue, we proposed a novel DCNN architecture by adopting the whole interlaced frame as input and two half frames as output.</p><p>We also propose to share the lower-level convolutional layers for reconstructing the two output frames to boost efficiency. With this strategy, our method achieves real-time deinterlacing on a single GPU for videos of resolution up to 1024 × 768. Experiments show that our method outperforms existing methods, including traditional deinterlacing methods and DCNN-based models re-trained for deinterlacing, in terms of both reconstruction accuracy and computational performance. Since our method takes the whole interlaced frame as the input, frame reconstruction is always influenced by both fields. While this may produce better results in most of the cases, it occasionally leads to visually poorer results when the motion between two fields is extremely large. In this scenario, reconstructing each frame from a single field without considering temporal information may produce better results. A possible solution is to first recognize such large-motion frames, and then decide whether temporal information should be utilized for deinterlacing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Input frames (d) Ours (c) Blown-ups (b) SRCNN (trained with our dataset) Leaves Soccer Fig. 1. (a) Input interlaced frames. (b) Deinterlaced results generated by SRCNN [4] re-trained with our dataset. (c) Blown-ups from (b) and (d) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Two half fields are captured in two distinct time instances. (b) The interlaced display exhibits obvious artifacts on the silhouette of moving car. (c) Two full frames reconstructed from the two half frames independently with an intra-field deinterlacing method ELA [5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) An input interlaced frame. (b) Directly applying SRCNN to deinterlacing introduces blurry and halo artifacts. (c) The visual artifacts are worsen if we retain the pixels from the input odd/even scanlines. (d) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of the proposed convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>A real example of synthesizing an interlaced frame from two consecutive progressive frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Reconstructing two frames from two fields independently leads to inevitable visual artifacts due to the large information loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Comparisons between bicubic interpolation, SRCNN [4] and our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Comparisons between the state-of-the-art deinterlacing tailored methods, including ELA [5], WLSD [22], and FBA [19], with our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Training loss and validation loss of our neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Failure cases. The top row shows a case where our result contains obvious artifacts when the motion of the interlaced frame is too large. The bottom row shows a case where our method fails to identify thin horizontal structures as interlacing artifacts and incorrectly preserves it in the reconstructed frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1. PSNR and SSIM between the deinterlaced frames and groundtruth of all methods.</figDesc><table><row><cell>PSNR/SSIM</cell><cell>Taxi</cell><cell>Roof</cell><cell cols="2">Basketball</cell><cell>Jumping</cell><cell>Tide</cell><cell>Girl</cell></row><row><cell>bicubic</cell><cell>31.56/0.9453</cell><cell>33.11/0.9808</cell><cell cols="2">34.67/0.9783</cell><cell cols="3">37.81/0.9801 31.87/0.9809 29.14/0.9585</cell></row><row><cell>ELA</cell><cell>32.47/0.9444</cell><cell>34.41/0.9839</cell><cell cols="2">32.08/0.9605</cell><cell cols="3">38.82/0.9844 33.89/0.9811 31.62/0.9724</cell></row><row><cell>WLSD</cell><cell cols="4">35.99/0.9746 35.70/0.9883 35.05/0.9794</cell><cell cols="3">38.19/0.9819 34.17/0.9820 32.00/0.9761</cell></row><row><cell>FBA</cell><cell>34.94/0.9389</cell><cell>35.26/0.9815</cell><cell cols="2">33.93/0.9749</cell><cell cols="3">38.27/0.9822 35.15/0.9822 31.78/0.9756</cell></row><row><cell>SRCNN</cell><cell>30.12/0.9214</cell><cell>32.01/0.9749</cell><cell cols="5">29.18/0.9353 36.81/0.97094 33.02/0.9758 27.79/0.9477</cell></row><row><cell>Ours</cell><cell cols="7">38.15/0.9834 35.44/0.9866 36.55/0.9838 39.75/0.9889 35.37/0.9807 35.44/0.9866</cell></row><row><cell cols="5">Average time (s) ELA WLSD FBA Bicubic SRCNN</cell><cell cols="3">Our Methods With sharable layers Without sharable layers</cell></row><row><cell>1920 × 1080</cell><cell cols="3">0.6854 2.9843 4.1486 0.7068</cell><cell>0.3010</cell><cell>0.0835</cell><cell></cell><cell>0.2520</cell></row><row><cell>1024 × 768</cell><cell cols="3">0.0676 1.0643 1.6347 0.2812</cell><cell>0.0998</cell><cell>0.0301</cell><cell></cell><cell>0.0833</cell></row><row><cell>720 × 576</cell><cell cols="3">0.0317 0.4934 0.6956 0.1176</cell><cell>0.0423</cell><cell>0.0204</cell><cell></cell><cell>0.0556</cell></row><row><cell>720 × 480</cell><cell cols="3">0.0241 0.4956 0.7096 0.1110</cell><cell>0.0419</cell><cell>0.0137</cell><cell></cell><cell>0.0403</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 2. Timing statistics for all methods.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image up-sampling using total-variation regularization with a new observation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interlaced to sequential conversion for EDTV applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Signal Processing of HDTV</title>
		<meeting>International Workshop on Signal Processing of HDTV</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="412" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">191</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast image interpolation using the bilateral filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="877" to="890" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weighted fuzzy reasoning scheme for interlaced to progressive conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="842" to="855" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High quality spatially registered vertical temporal filtering for deinterlacing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="182" to="190" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">2065</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction filters in computergraphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><forename type="middle">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced motion compensated deinterlacing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M P</forename><surname>Langlois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1041" to="1048" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">De-Interlacing Using Nonlocal Costs and Markov-Chain-Based Estimation of Interpolation Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhang</forename><surname>Vedadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Shirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient adaptive deinterlacing algorithm with awareness of closeness and similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="17003" to="17004" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moving Least-Squares Method for Interlaced to Progressive Scanning Format Conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1865" to="1872" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">De-Interlacing algorithm using weighted least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
