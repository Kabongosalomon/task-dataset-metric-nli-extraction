<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Weak Perspective for Monocular 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imry</forename><surname>Kissos</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Fritz</surname></persName>
							<email>liorf@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Goldman</surname></persName>
							<email>matang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Meir</surname></persName>
							<email>omermeir@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Oks</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kliger</surname></persName>
							<email>markklig@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Weak Perspective for Monocular 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SMPL</term>
					<term>pose estimation</term>
					<term>perspective projection</term>
					<term>3DPW</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of 3D joints location and orientation prediction from a monocular video with the skinned multi-person linear (SMPL) model. We first infer 2D joints locations with an off-the-shelf pose estimation algorithm. We use the SPIN algorithm and estimate initial predictions of body pose, shape and camera parameters from a deep regression neural network. We then adhere to the SMPLify algorithm which receives those initial parameters, and optimizes them so that inferred 3D joints from the SMPL model would fit the 2D joints locations. This algorithm involves a projection step of 3D joints to the 2D image plane. The conventional approach is to follow weak perspective assumptions which use ad-hoc focal length. Through experimentation on the 3D poses in the wild (3DPW) dataset, we show that using full perspective projection, with the correct camera center and an approximated focal length, provides favorable results. Our algorithm has resulted in a winning entry for the 3DPW Challenge, reaching first place in joints orientation accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicting 3D human joints locations from a monocular image is a challenging task. Estimating the correct depth of locations from a single image may present ambiguities which are hard to solve. Moreover, obtaining annotated data for training is a complicated and cost intensive task. In order to account for the ambiguities, prior knowledge of the human body is utilized in many models. Namely, the skinned multi-person linear (SMPL) model <ref type="bibr" target="#b16">[17]</ref>, obtained by detailed 3D scans of a large number of individuals, provides a strong prior for this task, and has been widely used in the research community <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The SMPLify algorithm <ref type="bibr" target="#b3">[4]</ref>, has been shown to achieve compelling results of full 3D human mesh recovery from a single image. The algorithm fits the SMPL model to the image given locations of 2D joints on the image. The optimization is performed by minimizing an objective function that penalizes the error between projections of the estimated 3D joints onto image plane and input 2D joints. The recently proposed SPIN (SMPL oPtimization IN the loop) model <ref type="bibr" target="#b13">[14]</ref> is a method for regressing SMPL parameters directly from an image. SPIN has introduced a collaboration between an optimization-based SIMPLify algorithm and a deep regression network. A regressed estimate from the network initializes SMPLify with estimated SMPL parameters. The output from SMPLify serves as supervision to train the regression network. This collaboration forms a selfimproving loop, so that a better network provides a better SMPLfy initialization, which leads to, again, better SMPL parameters supervision. In both SIMPLify optimization and SPIN deep network training, one of the main losses, so called reprojection loss, is derived from the error between input 2D joints locations and the projections of the estimated 3D joints onto the image plane. The projection of a 3D point from the world coordinate frame into the 2D image plane requires knowledge of camera intrinsic and extrinsic parameters. Unfortunately the camera parameters are rarely known. The authors of SPIN have chosen the weak perspective camera model in the projection procedure. Thus, it is assumed that the camera is far from the person. This assumption is realized by using unrealistically large focal length. Moreover, the weak perspective assumption facilitates the disregard of the actual location of the input person crop compared to the full resolution image. While this assumption may be reasonable for some data samples, mainly from the COCO dataset <ref type="bibr" target="#b15">[16]</ref>, it does not hold in some other cases, as in the 3DPW dataset <ref type="bibr" target="#b17">[18]</ref>. In fact, the weak perspective assumption may introduce noise into the reprojection loss, and in some degree, undermine its utility as a supervision signal for the estimation of 3D joints.</p><p>To this end, we suggest to look beyond the weak perspective assumptions in the SPIN and SMPLify algorithms. We assume a full perspective projection camera model. That is, we assume that the camera is close enough to the person so that changes in depth can result in changes in the projection to the 2D image plane. We perform the projection to the original image, with respect to correct image center and approximated focal length, in contrast to the projection onto the image patch which was cropped for inference. Additionally, we modify SMPLify camera parameters and global body orientation optimization step by leveraging all joints, rather than torso joints alone, to compute re-projection loss which is used in fitting procedure.</p><p>At the video level the jitter in results of frame-based 3D joints estimation can be reduced by temporal smoothing. We demonstrate that smoothing using the OneEuro filter <ref type="bibr" target="#b6">[7]</ref>, which adaptively changes the filter cut-off frequency, produces more accurate results.</p><p>Recently, a new dataset for 3D human mesh recovery has been collected, 3D poses in the wild (3DPW) <ref type="bibr" target="#b17">[18]</ref>. Using inertial measurement units (IMUs) and 2D joints locations, accurate full 3D human mesh, including 3D joints locations, were obtained for 60 clips in total. We experiment with this dataset, and demonstrate that the SPIN-SMPLify collaborative model with full perspective camera projection procedure achieves state-of-the-art results for joints orientations . Our algorithm was a wining entry for 3DPW Challenge reaching the first place in joints orientation accuracy.</p><p>To summarize, the contribution of our paper is the following,</p><p>-We demonstrate that using full perspective assumptions, one can achieve state-of-the-arts results for 3D joints predictions from a single image -We propose to use all joints within the SMPLify camera translation and global body orientation optimization step, rather than torso joints alone. -We show that simple smoothing procedure result in better 3D joints location estimation at the video level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human pose in 3D is usually represented with a set of joints positions, or with a parametric representation of a body model. Considering the task of 3D human pose inference from a single image, in the first approach, 3D joints locations are either lifted from estimated 2D joints locations on the image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>, or are directly inferred from the image <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. In the second approach, several body models have been proposed, such as the SMPL model <ref type="bibr" target="#b16">[17]</ref> and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. The popular SMPL model is a skinned vertex-based model that is capable of accurately representing a full mesh of the human body using a compact representation of body pose and shape parameters only.</p><p>Recently, it has been shown that SMPL parameters can be directly inferred from an image with a deep neural network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Other methods, such as SMPLify <ref type="bibr" target="#b3">[4]</ref>, consider optimizing these parameters directly given locations of 2D joints on the image. These two paradigms have been recently merged in the SPIN algorithm <ref type="bibr" target="#b14">[15]</ref>, where the SMPLify algorithm is incorporated in-theloop of the training procedure of a deep neural network, and possibly during an inference fine-tuning step. The problem of human pose extraction from videos was investigated in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Our approach closely follows the SPIN method <ref type="bibr" target="#b14">[15]</ref>. To compute reprojection losses of SPIN and SMPLify, authors of <ref type="bibr" target="#b14">[15]</ref> use unrealistically large focal length, that is inherently equivalent to weak perspective assumption. Moreover, the reprojection losses are computed with respect to a resized cropped image of the person. In this paper we attempt to correct shortcomings of the original method related to the weak perspective assumption and projection of 3D joints onto the resized cropped image patch. In <ref type="bibr" target="#b19">[20]</ref>, the authors use corrective rotation to compensate for the perspective distortions from the projection to the cropped patch. However, weak perspective assumption is still used in the projection procedure. In <ref type="bibr" target="#b8">[9]</ref> a neural network directly predicts the parameters of the camera projection model, namely a focal length related scaling parameter and principal point. Yet, weak perspective projection is still assumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>SPIN neural network receives as an input a cropped image I of a person, and outputs body pose and shape parameters, θ and β, respectively. The network also estimates 3 camera parameters, (s, t x , t y ). The translation parameters t x and t y represent the shift of the body kinematic tree root (pelvis) relatively to the origin of the cropped image coordinate frame on x and y axes respectively, while the scale parameter s is used to compute translation parameters t z on z axis. The body pose parameters, θ, represent local rotation transformations of each joint compared to its parent in the kinematic tree. In total, it contains 72 parameters, 3 rotation parameters per each of the 24 joints in the kinematic tree. Additionally, 10 body shape parameters, β, represent linear coefficients of a lowdimensional shape space, learned from a training set of thousands of registered body scans. The SMPL model uses the body pose and shape parameters to compute the locations of 6890 3D mesh vertices. Additionally, a linear joints regressor is used to infer 3D joints locations from the 3D mesh vertices.</p><p>The SMPLify algorithm requires an input of 2D joints locations. The popular choice is 25 joints defined by the OpenPose model <ref type="bibr" target="#b5">[6]</ref>, and we use an off-theshelf implementation of OpenPose to obtain 2D joints locations for each image. The extracted 3D joints from the SMPL model are projected onto the 2D image plane using perspective projection. The joints re-projection loss together with pose and shape prior losses are minimized to estimate body pose, shape, global body orientation and camera translation parameters. The SMPLify optimization is initialized with the output of SPIN neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Projection Formulation</head><p>We assume a pinhole camera model. In order to perform projection of 3D joints onto the 2D image plane, we first need to define the camera intrinsic matrix,</p><formula xml:id="formula_0">K =   f x s k o x 0 f y o y 0 0 1   ,<label>(1)</label></formula><p>where </p><formula xml:id="formula_1">K =   f 0 W/2 0 f H/2 0 0 1   .<label>(2)</label></formula><p>Let us also define T = [t x , t y , t z ] ∈ R 3 as the camera translation in the world coordinate frame. We assume that the camera coordinate frame is aligned with the world frame, i.e., the camera extrinsic matrix is the identity matrix. Moreover, we assume that the camera does not have any radial or tangential distortion. Given a 3D point in the world coordinate system,</p><formula xml:id="formula_2">p 3D = [x, y, z] ∈ R 3 , we compute its 2D projection onto the image plane, p 2D ∈ R 2 , with the following formulation, [ x, y, z] T = K · ([x, y, z] + T ) T ,<label>(3)</label></formula><p>and,</p><formula xml:id="formula_3">p 2D = [ x/ z, y/ z] .<label>(4)</label></formula><p>In SPIN, the person square bounding box is first cropped, and then resized into a fixed resolution of 224 × 224. The projection of 3D joints onto the image plane, and the computation of re-projection losses are performed directly onto a resized cropped image around the person. Since the camera focal length, f , is unknown, the focal length with respect to the resized cropped image patch was defined to be f = 5000. The scale parameter s is converted into the camera translation in the z axis, t z , using the formula,</p><formula xml:id="formula_4">t z = 2 · f Res · s ,<label>(5)</label></formula><p>where Res ≡ 224 is the resized crop resolution. For most images, the resulting t z is very large comparing to changes in the z coordinate of the 3D joints. The inherent assumption here is of weak perspective projection. It is assumed that the person is very far from the camera, and that the possible changes in the z coordinate of 3D joints are negligible compared to the distance from the camera, which is not always true in practice. In fact, this assumption may lead to erroneous projection of the 3D joints onto the image plane, and as a result, a noisy reprojection loss. Moreover, since the parameters (s, t x , t y ) are estimated with respect to the center of the resized cropped image, they do not truly represent the camera translation parameters of the real camera center, which is related to the center of the full resolution image. We propose to consider a more realistic focal length and to project 3D joints directly onto the original full resolution image plane, rather than to the resized cropped low resolution patch. In case the focal length is known or when the camera can be calibrated, the true focal length should be used. In practice, the focal length is usually unknown and the camera is unavailable for calibration. The focal length cannot be optimized as part of the global optimization process, nor can it be estimated by a neural network, since the problem is too unconstrained to optimize it together with camera translation. To overcome this problem, we suggest to use focal length approximation.</p><p>It is known that the focal length can be calculated from the camera field of view (FOV),</p><formula xml:id="formula_5">f = √ W 2 + H 2 2 tan (α/2) ,<label>(6)</label></formula><p>where W, H are the width and height of the original image and α is the diagonal FOV of the camera. Since the FOV of the camera is also unknown, we can simply approximate the focal length by,</p><formula xml:id="formula_6">f ≈ W 2 + H 2 .<label>(7)</label></formula><p>This roughly corresponds to a camera FOV of 55 • . In a 1920 × 1080 resolution camera, we get f ≈ 2200. In Section 4.2 we show that the model is insensitive to the exact value of the focal length within a wide range. We define r = b/Res, where b is the size of the detected person square bounding box in native image resolution, as the resizing factor from the cropped image resolution to the actual input image resolution. Down-sampling the image by the factor r changes the focal length of the camera (in pixels) to be f /r. Since the camera scale parameter s is estimated with respect to the resized image patch, in order to convert it into the correct camera translation in the z axis, t z , we use,</p><formula xml:id="formula_7">t z = 2 · f r · Res · s .<label>(8)</label></formula><p>Finally, the estimated translation parameters t x , t y , which are obtained from the SPIN neural network output, are related to the center of the resized cropped image and not the actual camera center at the full resolution image center. Thus, we need to perform the adequate correction. Let c x , c y be the 2D coordinates of the crop center in the original full resolution image coordinates. We compute the camera center shift parameters using,</p><formula xml:id="formula_8">c x = 2 (c x − W/2) s · b (9) c y = 2 (c y − H/2) s · b ,</formula><p>where b is the bounding box size, which is always square in our case, and W and H are the original image width and height. We use the camera center shift parameters,ĉ x ,ĉ y to compute the camera translation parameter,T , with respect to the true camera location,T</p><formula xml:id="formula_9">= [t x −ĉ x , t y −ĉ y , t z ] ,<label>(10)</label></formula><p>which we substitute with T in <ref type="figure">Equation 3</ref>. The usage of the modified projection formulation is described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SMPLify Camera and Global Orientation Optimization</head><p>The SMPLify algorithm works in two separate optimization steps. The first, optimizes the global orientation (first 3 parameters of θ) and camera translation parameters T . In the second steps, only joints orientations θ and body shape parameters β are optimized.</p><p>Since the camera translation and global orientation are important when assuming full perspective projection, we propose the following modification to the algorithm. We estimate (s, t x , t y ) using SPIN deep regression network. We then initialize optimization of camera translation parameter T withT obtained from modification of (s, t x , t y ) using equations 8-10 . The fitting loss is obtained by projecting predicted 3D joints to them image plane, and comparing them with given 2D joints. In the projection procedure formulated in Equations 2-4 we use the approximated realistic focal length as described in <ref type="bibr">Equation 7</ref>. We also note that in 2 we use the original image width and height. In SPIN, W and H are set according to the input cropped image size, so that they are both 224.</p><p>Finally, while in <ref type="bibr" target="#b13">[14]</ref> this reprojection loss is computed only for the torso joints (hips and shoulders), we found it beneficial to use all joints in our case. Similarly to the joints reprojection loss used in the second step of SMPLify optimization, we weight the contribution of each joint by the squared confidence of its estimate provided by the 2D joints estimation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on 3DPW</head><p>We experiment on the 3D poses in the wild (3DPW) dataset <ref type="bibr" target="#b17">[18]</ref>. We do not use the ground truth 2D joints or bounding box locations. Rather, we first predict 25 2D joints locations with the OpenPose algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, using an off-the-shelf implementation 1 . We then use these predicted joints and match them to the provided in 3DPW ground truth joints in each frame in order to track each person in the clip individually. The bounding box at each frame is obtained from the maximal and minimal values of the predicted joints coordinates. We also use the trained SPIN model <ref type="bibr" target="#b13">[14]</ref> to obtain an initial prediction of body shape and pose, and camera parameters. We then use SMPLify optimization, following its implementation within SPIN, with 100 iterations with a learning rate of 0.01 to optimize these parameters to match the obtained 2D joints. The SMPLify optimization is modified as was described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>To evaluate our model, we consider the 24 joints defined in the SMPL kinematic tree (root is used for matching). We measure 6 metrics on the 3DPW dataset to assess our model, -MPJPE: mean per joint position error (in mm). Average distance from prediction to ground truth joint positions (after root matching).</p><p>-MPJPE PA: mean per joint position error (in mm) after Procrustes alignment (rotation, translation and scale are adjusted). -PCK: percentage of correct joints. A joint is considered correct when it is less than 50mm away from the ground truth. The joints considered here are: shoulders, elbows, wrists, hips, knees and ankles. We compare ourselves to the results of SPIN <ref type="bibr" target="#b13">[14]</ref> and SPIN followed by fine-tuning with its original SMPLify implementation. <ref type="table">Table 1</ref> summarizes the consistent improvements over both methods across all metrics. The modifications to the camera optimization listed in Section 3.2 lead to improvements in MPJPE and MPJAE from 84.174 to 83.154 and from 20.437 to 19.697, respectively. <ref type="table">Table 1</ref>: Results on 3DPW with unknown ground truth person crop. SPIN refers to results obtained from the pretrained network from <ref type="bibr" target="#b13">[14]</ref>, and SPIN+SMPLify refers to fine-tuning the results with the original SMPLify implementation. Our results depict SPIN fine-tuned with our SMPLify implementation. **Our MP-JAE is a current SOTA on the 3DPW dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Focal Length</head><p>We experiment with a set of a few focal length values. We note that the camera focal length used in the computation of the camera translation parameters in Equation 8 is normalized by the resizing factor, r. In the weak perspective implementation of SPIN <ref type="bibr" target="#b13">[14]</ref>, the focal length of the cropped and resized image was assumed to be 5000, which is equivalent to the focal length 5000 · r on the full resolution image. As r is usually around 3, the focal length which was effectively assumed in the original SPIN implementation is about 15000.</p><p>We present the effect of the focal length in pixels units on the joints distance metrics (MPJPE) and joints orientations distance (MPJAE). As can be observed in <ref type="figure">Figure 2</ref>, for a large range of focal length values, the results are maintained. However, as the focal length becomes much smaller or larger, the results are deteriorated substantially. We deduce that a close approximation of the focal length is crucial, however the exact value does not have to be known.  <ref type="figure">Fig. 2</ref>: In (a) we show the effect of the focal length on the joints locations metrics, measured in mm. In (b) we show the effect on the orientations metric, MPJAE, which is the geodesic distance measured between predicted and ground truth orientations, in SO(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Camera Center</head><p>In the original SPIN-SMPLify implementation the camera is assumed to be centered at the bounding box center. As detailed in Section 3.1, we alter the optimization process so that the camera center is at the actual full resolution image center. In <ref type="table" target="#tab_2">Table 2</ref> we present the effect of altering the camera center on the measured metrics. We observe consistent improvements across all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SMPLify Number of Iterations</head><p>We experiment with different number of iterations in the SMPLify optimization process. <ref type="figure">Figure 3</ref> shows the number of SMPLify iterations and the effect on the MPJPE and MPJAE metrics. We observe that as we increase the number  <ref type="figure">Fig. 3</ref>: In (a) we show the effect of the number of SMPLify optimization iterations on the joints locations metrics, measured in mm. In (b) we show the effect on the orientations metric, MPJAE, which is the geodesic distance measured between predicted and ground truth orientations, in SO(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Smoothing</head><p>At the video clip level, we observe considerable jitter in the predicted results. As a final post processing steps, we perform temporal smoothing on the 3D joints locations and orientations with the OneEuro filter <ref type="bibr" target="#b6">[7]</ref>. The advantage of this filter, is that using adaptive cut-off frequency, it maintains small lag in rapid movements, while reducing noise and jitter in slow movements. We summarize the effect of smoothing in <ref type="table" target="#tab_3">Table 3</ref>. While the difference is marginal, it is consistent across all metrics. We note that except of joints and orientations temporal smoothing, we do not enforce any other kinematic, skeletal or shape temporal constraints at the video level. Incorporating additional temporal constraints will be a subject for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Examples from 3DPW</head><p>In <ref type="figure">Figure 4</ref> we show rendered human mesh recovered by our model, and with SPIN (with and without SMPLify fine-tuning) on several extracted frames from the 3DPW dataset. Rendering is done with the same focal length that is used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented the effect of using full perspective projection instead of weak perspective projection within the SMPLify optimization step of SPIN method. Experiments on the 3DPW dataset show that this method provides considerable improvements across all metrics evaluated. While the full perspective projection requires knowledge of the camera focal length, it can be roughly approximated from the image resolution, and we have shown that the model is insensitive to the exact value of the focal length, and that the results are maintained within a wide range of values. Moreover, we have also shown the importance of using the correct camera center, temporal smoothing and provided some modifications to the SMPLify loss function to accommodate the new optimization scheme.</p><p>As future work, one can consider removing the weak perspective assumptions from the network training, so that the results can be maintained within a single forward pass and incorporate additional temporal kinematic, skeletal or shape constraints. <ref type="figure">Fig. 4</ref>: Examples of some mesh recoveries from frames in the 3DPW dataset. We present results of SPIN, SPIN with the original SMPLify fine-tuning (SPIN+SMPLify) and our results. Points of interest are marked on the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPIN SPIN+SMPLify Ours</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The weak perspective assumption may create pose inaccuracies due to incorrect projection onto the image plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-AUC: total area under the PCK-threshold curve. Calculated by computing PCKs by varying from 0 to 200 mm the threshold at which a predicted joint is considered correct. -MPJAE: measures the angle in degrees between the predicted part orientation and the ground truth orientation. The orientation difference is measured as the geodesic distance in SO(3). The 9 parts considered are: left/right upper arm, left/right lower arm, left/right upper leg, left/right lower leg and root. -MPJAE PA: measures the angle in degrees between the predicted part orientation and the ground truth orientation after rotating all predicted orientations by the rotation matrix obtained from the Procrustes alignment step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>f x , f y are the camera focal length values (in pixels), [o x , o y ] is the camera principal point (in pixels) and s k is a skew coefficient. Usually f x ≡ f y ≡ f , [o x , o y ] ≡ [W/2, H/2], where W , H are the image width and height, respectively, and s k ≡ 0. The intrinsic matrix can thus be rewritten as,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of using different camera center definitions on metrics</figDesc><table><row><cell cols="2">Metric</cell><cell></cell><cell></cell><cell cols="7">Camera Center at Bounding Box Camera Center at Image Center</cell></row><row><cell cols="3">MPJPE (↓)</cell><cell></cell><cell>86.103</cell><cell></cell><cell></cell><cell cols="2">83.154</cell><cell></cell></row><row><cell cols="5">MPJPE PA (↓) 59.910</cell><cell></cell><cell></cell><cell cols="2">59.703</cell><cell></cell></row><row><cell cols="2">PCK (↑)</cell><cell></cell><cell></cell><cell>39.208</cell><cell></cell><cell></cell><cell cols="2">42.419</cell><cell></cell></row><row><cell cols="2">AUC (↑)</cell><cell></cell><cell></cell><cell>0.607</cell><cell></cell><cell></cell><cell>0.623</cell><cell></cell><cell></cell></row><row><cell cols="3">MPJAE (↓)</cell><cell></cell><cell>20.586</cell><cell></cell><cell></cell><cell cols="2">19.697</cell><cell></cell></row><row><cell cols="5">MPJAE PA (↓) 19.181</cell><cell></cell><cell></cell><cell cols="2">19.149</cell><cell></cell></row><row><cell cols="11">of iterations, the metrics improve consistently, while at 200 iterations there is</cell></row><row><cell cols="5">slight degradation of MPJPE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>85.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPJPE [mm]</cell><cell>84 84.5</cell><cell></cell><cell></cell><cell></cell><cell>MPJAE</cell><cell>20.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>83.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>50</cell><cell>70</cell><cell>100</cell><cell>200</cell><cell>20 30</cell><cell>50</cell><cell>70</cell><cell>100</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SMPLify iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SMPLify iterations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of using smoothing with OneEuro filter on metrics</figDesc><table><row><cell>Metric</cell><cell cols="2">Without Smoothing With Smoothing</cell></row><row><cell>MPJPE (↓)</cell><cell>84.986</cell><cell>83.154</cell></row><row><cell cols="2">MPJPE PA (↓) 60.677</cell><cell>59.703</cell></row><row><cell>PCK (↑)</cell><cell>42.182</cell><cell>42.419</cell></row><row><cell>AUC (↑)</cell><cell>0.619</cell><cell>0.623</cell></row><row><cell>MPJAE (↓)</cell><cell>20.063</cell><cell>19.697</cell></row><row><cell cols="2">MPJAE PA (↓) 19.435</cell><cell>19.149</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/CMU-Perceptual-Computing-Lab/openpose</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers whose comments and suggestions helped improve and clarify this manuscript. We also thank our colleagues from Amazon Lab 126, Dr. Ilia Vitsnudel, Eli Alshan, Ido Yerushalmi, Liza Potikha, Dr. Ianir Ideses and Dr. Javier Romero from Amazon Body Labs, for multiple useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8387" to="8397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">1 filter: a simple speed-based low-pass filter for noisy input in interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2527" to="2530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xnect: real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Metric-scale truncation-robust heatmaps for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02953</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A simple, fast and highly-accurate algorithm to recover 3d shape from 2d landmarks on a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
