<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<email>lsheng@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>https://yochengliu.github.io/MLIC-KD-WSD/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-Label Image Classification</term>
					<term>Weakly-Supervised Detection</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multilabel classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then <ref type="formula">(2)</ref> construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: The illustration of multi-label image classification (MLIC) and weakly-supervised detection (WSD). We show top-3 predictions, in which correct predictions are shown in blue and incorrect predictions in red. The MLIC model might not predict well due to poor localization for semantic instances. Although the detection results of WSD may not preserve object boundaries well, they tend to locate the semantic regions which are informative for classifying the target object, such that the predictions can still be improved.</p><p>classification (e.g., inter-class similarity and intra-class variation), MLIC is more difficult because predicting the presence of multiple classes usually needs a more thorough understanding of the input image (e.g., associating classes with semantic regions and capturing the semantic dependencies of classes).</p><p>Contemporary methods may simply finetune the multi-label classification networks pre-trained on the single-label classification datasets (e.g., ImageNet <ref type="bibr" target="#b23">[24]</ref>). However, the classifiers trained for global image representations may not generalize well to the images in which objects from multiple classes are distributed in different locations, scales and occlusions. To mitigate this problem, the task of MLIC can be decomposed into multiple independent binary classification tasks, in which one classifier only focuses on one object label. In this way, though very efficient, the semantic dependencies among multiple classes, which is especially important for MLIC <ref type="bibr" target="#b31">[32]</ref>, are ignored (e.g., "cat" is more likely to be misclassified into the category of "dog" than falsely associated to "car"). Therefore, some prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> tried to fix this drawback by explicitly capturing the class dependencies with a RNN or LSTM structure appended after CNN-based models. However, they usually suffer from the difficulty in back-propagating stable gradients <ref type="bibr" target="#b20">[21]</ref>.</p><p>Recently, some object localization techniques <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> are introduced into the MLIC task by simplifying the multi-label classification problem into multi-object detection task. The resulting pipeline usually involves two steps. The hypothesis regions are first proposed using low-level image cues <ref type="bibr" target="#b30">[31]</ref>. Then a neural network is trained to predict class scores on these proposals, and these predictions are aggregated to achieve MLIC task. Even though satisfactory performance can be achieved with sufficiently accurate region proposal algorithms, these methods always have to bear redundant computational cost in the test phase. Thus they are usually not practical for large-scale applications.</p><p>To solve above issues, an effective and efficient multi-class image classification model needs to simultaneously hold three important advantages: (1) locating semantic regions for object-level feature extraction; <ref type="bibr" target="#b1">(2)</ref> capturing semantic dependencies among multiple classes; (3) fewer additional computation and annotation budgets for the practical issue. Following this intuition, weakly-supervised detection (WSD) <ref type="bibr" target="#b1">[2]</ref> may be a feasible solution. It could achieve the detection goal of locating each semantic instance with a specific class using only image-level annotations. <ref type="figure">Figure 1</ref> shows the task illustrations for MLIC and WSD frameworks. The MLIC model might not predict well due to the lack of object-level feature extraction and localization for semantic instances. Although the results detected by WSD may not preserve object boundaries well, they tend to locate the semantic regions which are informative for classifying the target object, such that the predictions can still be improved. Therefore, the localization results of WSD could provide objectrelevant semantic regions while its image-level predictions could naturally capture the class dependencies. These unique advantages are very useful for the MLIC task. The only problem is the huge computational complexity in the WSD pipelines. Is it possible to combine the advantages in WSD with the high efficiency of simple classification network? Knowledge distillation <ref type="bibr" target="#b11">[12]</ref>, a technique that distills knowledge from a large teacher model into a much smaller student model, may provide a good solution to guide the classification model to inherently contain object-level localization ability and mutual class dependencies.</p><p>In this paper, we propose a novel and efficient deep framework to boost MLIC by distilling the unique knowledge from WSD into classification with only image-level annotations. The overall architecture of our framework is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Specifically, our framework works with two steps: (1) we first develop a WSD model with image-level annotations; (2) then we construct an end-to-end knowledge distillation framework by propagating the class-level holistic predictions and the object-level features from RoIs in the WSD model to the MLIC model, where the WSD model is taken as the teacher model (called T-WDet) and the classification model is the student model (called S-Cls). The distillation of object-level features from RoIs focuses on perceiving localizations of semantic regions detected by the WSD model while the distillation of class-level holistic predictions aims at capturing class dependencies predicted by the WSD model. After this distillation, the classification model could be significantly improved and no longer need the WSD model, thus resulting in high efficiency in test phase.</p><p>The main contributions of this work are highlighted as follows:</p><p>• A novel and efficient deep multi-label image classification framework equipped by knowledge distillation is proposed, which distills the unique knowledge from a weakly-supervised detection model into the classification model such that the latter is improved significantly with high efficiency.</p><p>• To our best knowledge, it is the first work that applies knowledge distillation between two different tasks, i.e., weaklysupervised detection and multi-label image classification. • Extensive experiments are conducted on two large-scale public datasets (MS-COCO and NUS-WIDE), and the results show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multi-Label Image Classification (MLIC). The progress of MLIC <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> has been greatly made with deep convolutional neural network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. Some works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> embed label dependencies with the deep model to improve the accuracy of MLIC. CNN-RNN <ref type="bibr" target="#b31">[32]</ref> utilizes RNN combined with CNN to learn a joint image-label embedding for capturing label dependencies. <ref type="bibr" target="#b20">[21]</ref> proposes a regularised embedding layer as the interface between the CNN and RNN to mitigate the difficulty of model training in <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> learn graph structure to model the label dependencies. These methods always require pre-defined label relations. Some other works ensemble multiple deep models with different input scales <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> while suffering high complexity.</p><p>Recently, various methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> have been proposed to locate semantic regions for learning deep attentional representations. For example, MIML-FCN+ <ref type="bibr" target="#b38">[39]</ref> uses bounding boxes from Faster-RCNN <ref type="bibr" target="#b21">[22]</ref> to locate the objects in an image for multiinstance learning. Spatial regularization network <ref type="bibr" target="#b44">[45]</ref> generates class-related attention maps to capture spatial dependencies. <ref type="bibr" target="#b32">[33]</ref> employs a LSTM sub-network to predict labeling scores on the regions located by a spatial transformer layer.</p><p>All the aforementioned methods either require pre-defined label relations or object-level annotations, and they usually add model complexity by the extra modules, both of which result in poor practicality. Weakly-supervised detection (WSD). Recently, many researches on WSD <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46]</ref> have been conducted. Dual-network <ref type="bibr" target="#b7">[8]</ref> is proposed to optimize proposal generation and instance selection in a joint framework. <ref type="bibr" target="#b16">[17]</ref> introduces the domain adaptation techniques for the WSD task. WSDDN <ref type="bibr" target="#b1">[2]</ref> modifies ImageNet pretrained VGG <ref type="bibr" target="#b24">[25]</ref> to operate at image regions, performing simultaneously region selection and classification. In this work, we extend the WSDDN to develop a WSD model into our framework. Knowledge Distillation. Hinton et al. <ref type="bibr" target="#b11">[12]</ref> use a softened version of the output of a large teacher network to teach information to a small student network. FitNets <ref type="bibr" target="#b22">[23]</ref> employs not only the output but also intermediate layer values of the teacher network to train the student network. Attention transfer <ref type="bibr" target="#b41">[42]</ref> forces the student network to be consistent with the teacher network on feature attention maps. These methods focus on the distillation between the same tasks, and they always use the whole feature maps and class-identical soften targets to conduct distillation, which can not locate to semantic regions of the image and are not sensitive to classes. Chen et al. <ref type="bibr" target="#b2">[3]</ref> concentrate on distilling between the same tasks of object detection while our proposed distillation is operated between two different tasks, i.e., from WSD to MLIC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Multi-label image classification aims at obtaining all the semantic classes in an image. Generally, given an image I, the final prediction l k of the k-th class corresponding to I is formulated by</p><formula xml:id="formula_0">l k = I(p k (I|w) &gt; τ k ), k ∈ {1, · · · , K },<label>(1)</label></formula><p>where p k (I|w), estimated by a model with parameters w, denotes the posterior probability of image I including the k-th class. K is the number of given labels, and τ k is the confidence threshold for the k-th class. I(p &gt; τ ) is an indicator function, it takes 1 when p &gt; τ and 0 otherwise. l k is the final label indicator, i.e., l k = 1 means the k-th class is included in the given image and l k = 0 otherwise. In this paper, we propose a novel and efficient framework in which the multi-label image classification (MLIC) task is facilitated by weakly-supervised detection task. In the following, we will present the proposed framework in detail, including (1) weaklysupervised detection (WSD) model, (2) Knowledge distillation from WSD to MLIC, and (3) Implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weakly-Supervised Detection (WSD) Model</head><p>Although any existing WSD methods can be used in our framework, we choose WSDDN <ref type="bibr" target="#b1">[2]</ref> because of its architecture accessibility. Using VGG16 <ref type="bibr" target="#b24">[25]</ref> pre-trained on ImageNet <ref type="bibr" target="#b23">[24]</ref> as backbone network, WSDDN operates on image regions which are outputted by Edge-Boxes (EB) <ref type="bibr" target="#b46">[47]</ref>. In this work, we extend WSDDN to support any popular networks. The architecture of extended WSDDN (called T-WDet) is illustrated at the upper part of <ref type="figure" target="#fig_0">Figure 2</ref>. First, EB algorithm is used to get a lot of proposals R from the input image I. These proposals are inputted to the RoI pooling <ref type="bibr" target="#b21">[22]</ref> module to get RoI-localized features. Note that we replace SPP pooling <ref type="bibr" target="#b9">[10]</ref> by RoI pooing, because the latter keeps the spatial information. Formally, let F conv ∈ R c×h×w denote the last convolutional feature maps of the backbone network, R denote a proposal in R, and s R denote the prior score of R outputted by EB algorithm, F R ∈ R | R |×c×h r ×w r , the obtained RoI features of all the proposals, can be described as</p><formula xml:id="formula_1">F R = s R ⊙ ϕ RoI (F conv ; R), F R = C R ∈R (F R ),<label>(2)</label></formula><p>where ϕ RoI (·) is the operation of RoI pooling, "⊙" is the operation of multiplying each element of ϕ RoI (·) by score s R , and C(·) is the concatenation operation, which concatenates the features of |R| proposals along the fourth dimension. Then, global pooling or several fully connected (fc) layers are adopted to further transform the RoI feature maps F R into feature vectors. The subsequent network is split into two branches. Both of them pass through a fully connected layer, where the output is consistent with the given classes K, to get a logit matrix M ∈ R | R |×K . One branch aims at classification while the other at detection. The classification is achieved by a softmax operation along the first dimension K, and the detection along the second dimension |R|.</p><p>Finally, the element-wise product operation is adopted to fuse the softmax score matrix S c , S d ∈ R | R |×K of the two branches.</p><p>The fused score matrix S is summed along the second dimension |R| to get final class prediction p ∈ R K , which is compatible with the image-level annotations y ∈ R K . The final detection results for each class are obtained by processing each column of S with non-maximum suppression (NMS). T-WDet is also trained in an end-to-end manner. More details can be referred in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Distillation from WSD to MLIC</head><p>In this paper, we argue that there is unique knowledge beyond classification contained in the task of WSD, which could facilitate MLIC. Specifically, on one hand, the detection results of WSD provide localization of semantic regions, which is a powerful cue for classification model to further understand the image. On the other hand, the image-level prediction confidences of WSD naturally capture semantic dependencies among classes, which could be a strong reference for MLIC from the perspective of detection.</p><p>As stated in Section 3.1, we first use a T-WDet model to achieve the goal of detection. The problem locates at how to transfer the unique knowledge from T-WDet model into the classification model. A reasonable solution is knowledge distillation <ref type="bibr" target="#b11">[12]</ref>, which can distill the knowledge in a large teacher model for improving a small student model. Inspired by this idea, we propose a dedicated distillation framework to distill knowledge from a WSD teacher model (T-WDet) for boosting a MLIC student model (S-Cls). This distillation framework works with two stages. The first stage focuses on the feature-level knowledge transfer while the second stage on the prediction-level knowledge transfer. Both of the two stages are included in "step 2" in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Feature-level knowledge transfer. We propose a RoI-aware distillation approach which explicitly distills the localization knowledge from WSD to MLIC at feature level. Specifically, we sum the fused score matrix S in the well-trained T-WDet model along the first dimension K to get a confidence vector s ′ ∈ R | R | . This vector implies a confidence distribution of all the proposals' objectness, i.e., region proposal score, which is a reliable localization importance indicator for MLIC. Since the proposals outputted by EB algorithm are highly overlapped, we take NMS operation for them using the obtained confidences s ′ . Then, with these well-chosen proposals, the knowledge from T-WDet model is distilled into S-Cls model by minimizing the ℓ2 loss of RoI pooled features on selected convolutional layers as</p><formula xml:id="formula_2">L f (w S conv ) = 1 2N n 1 |R ′ n | ∥F T R ′ n ⊖ F S R ′ n ∥ 2 2 ,<label>(3)</label></formula><p>where R ′ n denotes the remaining proposals after performing NMS to R n for image I n and N is the number of training images. "⊖" is the element-wise subtraction operation. F T R ′ n and F S R ′ n denote the RoI pooled features from T-WDet model and S-Cls model, respectively. They can be described as</p><formula xml:id="formula_3">F T R ′ n = C R ∈R ′ n s ′ R ⊙ ϕ RoI (F T conv ; R) , F S R ′ n = C R ∈R ′ n s ′ R ⊙ ϕ RoI (Ψ(F S conv )|w S conv ; R) ,<label>(4)</label></formula><p>where F T conv and F S conv denote the selected convolutional layers in T-WDet model and S-Cls model, respectively. Ψ(F S conv ) is the possibly needed transforming operation, which transforms F S conv to be compatible with F T conv in case the number of their channels is different. In this process, we only update the convolutional parameters w S conv in S-Cls model and s ′ R plays a role as local importance weighting factor for proposal R.</p><p>Our RoI-aware distillation approach explicitly distills the unique knowledge from the detection results of T-WDet model into S-Cls model, i.e., localization of semantic regions and objectness confidence. It is superior to FitNets <ref type="bibr" target="#b22">[23]</ref> and attention transfer <ref type="bibr" target="#b41">[42]</ref>, because both of them transfer knowledge on whole feature map, which is not sensitive to localization and objectness. Our distillation approach can also be operated on multiple layers, then the loss we minimize becomes ℓ L ℓ f (w S conv ).</p><p>Prediction-level knowledge transfer. The final label prediction p of T-WDet model is obtained by summing the score matrix S along the second dimension |R|. It aggregates the confidence of all the proposals over the given classes, which is a powerful reference for classification. Moreover, we observe that the classification accuracy for different classes between T-WDet model and S-Cls model are very different, thus the prediction-knowledge transfer should be of difference over classes. To discriminatively distill the knowledge from T-WDet model to S-Cls model at prediction level, we propose a class-aware distillation approach. Specifically, after initializing the parameters w S of S-Cls model with w S conv pre-trained in the first stage, we then simultaneously minimize two different loss functions for S-Cls model in this stage. The first loss function is the ℓ2 loss of the discriminatively softened predictions of T-WDet model and S-Cls model as</p><formula xml:id="formula_4">L p ′ (w S ) = 1 2N n ∥p ′ T − p ′S (w S )∥ 2 2 ,<label>(5)</label></formula><p>where p ′ T is the softened predictions of T-WDet model, which is calculated by</p><formula xml:id="formula_5">p ′ T = | R | i=1 [σ (M c |t c , K) ⊗ σ (M d |t d , |R|)], M ∈ R | R |×K .<label>(6)</label></formula><p>Here, "⊗" is the the element-wise product operation. σ (M c |t c , K) and σ (M d |t d , |R|) are the softened softmax operation along the first dimension K (classification branch) and the second dimension R (detection branch) on the logit matrix M, respectively. They can be defined as</p><formula xml:id="formula_6">[σ (M c |t c , K)] i j = e m c i j /t c k K k =1 e m c ik /t c k , ∀ i ∈ {1, · · · , |R|}, [σ (M d |t d , |R|)] i j = e m d i j /t d r | R | r =1 e m d r j /t d r , ∀ j ∈ {1, · · · , K },<label>(7)</label></formula><p>where t c k and t d r are the softmax temperature of k-th class and the softmax temperature of r -th proposal, respectively. p ′S (w S ) is the softened sigmoid predictions of S-Cls model, which is calculated by</p><formula xml:id="formula_7">p ′S k (w S ) = δ (m k |t) = 1/(1 + e −m k /t k ),<label>(8)</label></formula><p>where m k and t k are the logit and the sigmoid temperature of k-th class, respectively. We decompose multi-label classification task as multiple binary classification tasks, and we use sigmoid operation to get the final output. forward pass S-Cls model to get p(I n |w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>compute l n by Eq. (1). 12: end for Return: l N Note that all the temperatures are different, and they are learnable in the training phase. This is more reasonable for our task than the class-identical and fixed temperature used in <ref type="bibr" target="#b11">[12]</ref>. Moreover, it also cuts down the laborious costs for tuning the artificial temperatures by this learnable way. Formally, let m denote the input data, t denote the temperature andm denote the output data:m i = m i /t i , then the back-propagation and chain rule are used to compute derivatives w.r.t m and t as</p><formula xml:id="formula_8">∂L p ′ ∂m i = t i ∂L p ′ ∂m i 1 t i , ∂L p ′ ∂t i = m i ∂L p ′ ∂m i (− m i t 2 i</formula><p>).</p><p>The second loss function is the cross entropy with hard label (ground truth) y as</p><formula xml:id="formula_10">L p (w S ) = − 1 N n [y log p + (1 − y)log(1 − p)],<label>(10)</label></formula><p>where p is the normal sigmoid prediction of S-Cls model. In the class-aware distillation stage, we update all the parameters w S of S-Cls model. For one thing, the S-Cls model fits the given hard labels by working as multiple binary classification tasks. For another, it also acquires the knowledge of detection, i.e., semantic dependencies of classes distilled from well-trained T-WDet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>Training. In the training phase, we first train a T-WDet model as stated in Section 3.1. Then, we froze all the parameters of welltrained T-WDet model, and train the S-Cls model using the proposed RoI-aware and class-aware distillation framework. This is operated with two stages. Stage 1: We train S-Cls model to update convolutional parameters w S conv by optimizing the loss in Eq. (3). Stage 2: We update the whole network by optimizing the weighted losses in Eq. (10) and Eq. (5) as</p><formula xml:id="formula_11">L p (w S ) + λL p ′ (w S ),<label>(11)</label></formula><p>where λ is the weighted factor.</p><p>Test. In the test phase, the S-Cls model works without T-WDet model. It is compact as the same as standard classification model, i.e., no any extra computational cost. The normal sigmoid outputs p is taken as its final predictions. The pseudo-code of training and test of S-Cls model can be referred in Algorithm 1.</p><p>To convincingly demonstrate the proposed framework, we use VGG16 pre-trained on ImageNet as backbone network for both T-WDet and S-Cls models. VGG16 is the most popular network used in the literature of MLIC, thus a fair comparison can be made. The RoI pooling size is set to 7 × 7 for the two networks. The image size input to S-Cls model is always 224 × 224. T-WDet model. For training with mini-batch, we take top 500 proposals of EB algorithm, which are sorted by the prior scores of EB. Moreover, we recycle high-score proposals if the candidate number is less than 500. Except mirror flip, we train T-WDet model also using the popular techniques applied in detection area, i.e., training with multiple square scales of 480, 576, 672, 736 and 832. We use the input scale of 672 in the distillation process. Knowledge distillation. In feature-level knowledge transfer, the NMS threshold is set to 0.4 to clean highly overlapped proposals. Moreover, we take top 100 proposals after NMS for training with mini-batch, and again, we recycle them if the candidate number is not enough. In this stage, we only use the conv5_3 layer for knowledge transfer. The transforming operation is set to Ψ(F) = F due to the equal number of channels between two networks. In prediction-level knowledge transfer, the convolutional layers of S-Cls model are initialized with w S conv trained in last stage, fully connected layers are initialized with ImageNet pre-trained parameters, and other layers are initialized with Xavier algorithm <ref type="bibr" target="#b35">[36]</ref>. The value of all the temperatures t are initialized with 1. The weighted factor λ of two losses is set to 1.</p><p>Our framework is implemented using Caffe <ref type="bibr" target="#b13">[14]</ref>. The stochastic gradient descend (SGD) algorithm is employed for the network training, with a batchsize of 32, momentum of 0.9 and weight decay of 0.0005. For feature-level transfer, the learning rate is fixed at 10 −5 and the training continues for about 100 epochs. For predictionlevel transfer, the initial learning rate is set to 10 −4 , and decreased to 1/10 when validation loss gets saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Datasets</head><p>The proposed framework is evaluated on two large-scale datasets with fairly different types of labels: MS-COCO <ref type="bibr" target="#b19">[20]</ref> with 80 object labels and NUS-WIDE <ref type="bibr" target="#b5">[6]</ref> with 81 concept labels. MS-COCO. It contains 122,218 images of 80 object labels, with about 2.9 labels per image. The objects are of high diversity, and they are of severe occlusions. We follow the official split of 82,081 images for training and 40,137 validation images for testing. NUS-WIDE. This dataset contains 269,648 images and 5018 tags from Flickr. There are a total of 1000 tags after removing noisy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All and rare tags. These images are further manually annotated into 81 concepts with 2.4 concepts per image on average. The concepts are quite diverse, including event (e.g., running), scene/location (e.g., airport), object (e.g., animal). We follow the split used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, i.e., 150,000 images for training and 59,347 for testing after removing the images without any labels. Note that both of the two datasets are imbalanced over classes, and the imbalance on NUS-WIDE is even worse. <ref type="table">Table 2</ref>: Quantitative comparison (%) on NUS-WIDE. "w/" and "w/o" indicate "with" and "without" knowledge distillation by the proposed framework, respectively. The values in bold are the best while the values underlined are the second best.</p><formula xml:id="formula_12">Top-3 mAP F1-C F1-O F1-C F1-O CNN-RNN [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All </p><formula xml:id="formula_13">Top-3 mAP F1-C F1-O F1-C F1-O CNN-RNN [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics and Compared Methods</head><p>Evaluation Metrics. We employ three overall metrics for comparison: macro/micro F1 ("F1-C"/"F1-O") and mean average precision (mAP). Macro F1 is evaluated by averaging per-class F1, while micro F1 is evaluated on the results of all the images over all classes. For computing F1, we tune a class-independent confidence threshold, i.e., if the confidence is greater than this threshold, the prediction is taken as positive. We also report top-3 F1 sorted by the prediction confidences. mAP is the mean average precision over classes.</p><p>Generally, mAP is of more reference, because it directly measures ranking quality and does not require choosing the final predictions. Compared Methods. We compare our framework against the following stage-of-the-art deep learning methods: CNN-RNN <ref type="bibr" target="#b31">[32]</ref> and CNN-SREL-RNN <ref type="bibr" target="#b20">[21]</ref> employ CNN combined with RNN for classification; CNN-LSEP <ref type="bibr" target="#b18">[19]</ref> estimates the optimal confidence thresholds for each class; RMAM <ref type="bibr" target="#b32">[33]</ref> and RARLF <ref type="bibr" target="#b4">[5]</ref> locate to image regions for classification, they use very large input size (512×512) and multi-scale and multi-crop tricks in the test phase; MIML-FCN-BB <ref type="bibr" target="#b38">[39]</ref> uses outputs from Faster RCNN <ref type="bibr" target="#b21">[22]</ref> with bounding box annotations for classification; RLSD <ref type="bibr" target="#b42">[43]</ref> employs RNN to capture dependencies at localized regions for classification; Tag-Neighbors <ref type="bibr" target="#b14">[15]</ref> uses CNN to blend information from the image and its neighbors; MCG-CNN-LSTM <ref type="bibr" target="#b42">[43]</ref> employs LSTM to capture dependencies at proposals of MCG <ref type="bibr" target="#b0">[1]</ref>. Note that for fair comparison, we only report the results of methods based on VGG16 network and results without using extra label information (e.g., detailed metadata in NUS-WIDE dataset) or ensemble testing (e.g., fusion of multi-scale and multi-crop test). We also compare with three advanced distillation methods by implementing them following their paper: (1) Distillation <ref type="bibr" target="#b11">[12]</ref>: the t in T-WDet model and S-Cls model are tuned at 1, 5 and 2, 5 for MS-COCO and NUS-WIDE, respectively. (2) FitNets <ref type="bibr" target="#b22">[23]</ref>: we choose the middle layer conv3_3 as hint layer, then training with the setting in (1). (3) Attention transfer <ref type="bibr" target="#b41">[42]</ref>: as suggested in the paper, we choose conv3_3, conv4_3 and conv5_3 as transfer layers, and the transfer is combined with (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>MS-COCO. Experimental results on this dataset are summarised in <ref type="table">Table 1</ref>. With ImageNet pre-training, the simple S-Cls model can also perform well, and it outperforms the state-of-the-arts after knowledge distillation using our framework. Compared with RMAM <ref type="bibr" target="#b32">[33]</ref> and RARLF <ref type="bibr" target="#b4">[5]</ref>, which rely on a large input size and multi-crop test, our S-Cls model still performs better with small input (224×224) and single-forward test. Moreover, our framework with only image-level annotations also outperforms those methods like MIML-FCN-BB, which require bounding box annotations. The framework also shows superior performance over other advanced distillation methods even though all of them get decent results. Some example results are shown at the upper part of <ref type="figure" target="#fig_1">Figure 3</ref>. As can be seen, although the proposals of T-WDet hold poor preserve of object boundaries, they locate at the semantic regions that are very informative for classification, thus the classification results are greatly improved. Taking the 1 st column of the 1 st row as an example, we can see the objects in the image are highly overlapped, resulting in a poor classification result, the top-3 predictions are all wrong. However, after distillation with detected semantic regions, even the occluded objects "fork" and "cup" can be well recognised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUS-WIDE.</head><p>Quantitative results on this dataset are summarised in <ref type="table">Table 2</ref>. The simple S-Cls model again performs well with a backbone network pre-trained on ImageNet, and it also outperforms the state-of-the-arts after knowledge distillation with our framework. Meanwhile, compared with other architectures that add extra modules to the backbone network, e.g., CNN-RNN <ref type="bibr" target="#b31">[32]</ref>, CNN-SREL-RNN <ref type="bibr" target="#b20">[21]</ref> and RLSD <ref type="bibr" target="#b42">[43]</ref>, our framework performs better with higher efficiency at the same time. Moreover, the proposed framework also outperforms other advanced distillation methods. We also show some example results at the lower part of <ref type="figure" target="#fig_1">Figure 3</ref>. As it shows, the T-WDet model can still locate semantic regions even with the concept label, and the classification results are again improved by distilling these informative regions into the classification model. Taking the 2 nd column of the 1 st row as an example, the classification model only recognises correctly with one concept "sky" by a global perception of this image in top-3 predictions. However, other concepts like "waterfall" and "rainbow" are recognised after the distillation with our framework. This demonstrates the effectiveness and robustness of our framework simultaneously.</p><p>The improvements over each class on two datasets are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. As it shows, on one hand, the improvements on MS-COCO are relatively even to the classes, while NUS-WIDE focuses on the classes in which the number of images is fewer. This demonstrates the effectiveness of our framework to mitigate the problem of data imbalance, since the NUS-WIDE dataset is very imbalanced (the number of images on "sky" is 53k while quite a few concepts only hold hundreds of images). On the other hand, the improvements on MS-COCO focus on small objects like "bottle", "fork", "apple" and so on, which may be difficult for the classification model to pay attention. This indicates the importance of semantic regions where T-WDet model is distilled into S-Cls model, which is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Moreover, on NUS-WIDE, the improvements focus on scenes (e.g., "rainbow"), events (e.g., "earthquake") and objects (e.g., "book"), which demonstrates the robustness of our framework to the types of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Overall ablation study. The results of overall ablation study on two datasets are summarised in <ref type="table">Table 3</ref>. The T-WDet model achieves very good performance on MS-COCO while slightly better performance on NUS-WIDE. The main reason is that the clean object labels on MS-COCO are quite suitable for detection task while the noisy concept labels are not. Moreover, the S-Cls model is improved on both datasets after knowledge distillation by our framework, which verifies its effectiveness. Component-wise ablation study. We also perform componentwise ablation study on MS-COCO to carefully evaluate the contribution of the critical components of our framework. The baseline is the VGG16-based S-Cls model trained with sigmoid logistic loss as Eq. 10. The results are summarised in <ref type="table" target="#tab_3">Table 4</ref>. It improves a little when directly applying the distillation methods proposed by <ref type="bibr" target="#b11">[12]</ref>. After adding our class-aware distillation approach to baseline, the performance is improved much more (from 71.3 to 72.1). This demonstrates that our discriminative knowledge distillation at prediction level is superior than class-identical distillation <ref type="bibr" target="#b11">[12]</ref>.</p><p>We then add feature-level knowledge distillation followed with class-aware distillation in the way of two-stage training. The performance is improved considerably (from 72.3 to 73.8) when we take NMS operation to all the proposals based on their objectness confidences, and it improves again when weighting the localized features with these confidences. This demonstrates that the objectness confidence obtained from T-WDet model is a reliable indicator of semantic of the proposal.</p><p>We also analyse our framework by the experiment using supervised detection results. Specifically, we replace the EB proposals   <ref type="table">Table 3</ref>: Overall ablation study on two datasets (%). "w/" and "w/o" indicate "with" and "without" knowledge distillation by the proposed framework, respectively.  input to T-WDet model by the detection results from Faster RCNN <ref type="bibr" target="#b21">[22]</ref>. All the hyper-parameters are set to the same as the source code of <ref type="bibr" target="#b21">[22]</ref>, which results in 100 proposals for each image. The results are summarised in <ref type="table" target="#tab_4">Table 5</ref>. As can be seen, the classification performance of T-WDet is improved from 78.6 to 81.1 when using the supervised detection results. After distillation with our framework, S-Cls model is improved to 76.3 compared with EB proposals to 74.6, where the gap is not obvious. This further demonstrates the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, a novel and efficient deep framework for multi-label image classification has been proposed. It boosts classification by distilling the unique knowledge from weakly-supervised detection (WSD) into classification with only image-level annotations. The proposed framework works with two steps. A WSD model is first developed, then an end-to-end knowledge distillation framework is constructed via feature-level distillation from RoIs and class-level distillation from predictions, where the WSD model is the teacher model and the classification model is the student model. The featurelevel distillation from RoIs learns to perceiving semantic regions detected by the WSD model while class-level distillation aims at capturing class dependencies in the predictions of the WSD model. Thanks to this effective distillation, the classification model could be significantly improved without the WSD model in the test phase, thus resulting in high efficiency. Extensive experiments on two large-scale public datasets (MS-COCO and NUS-WIDE) show that the proposed framework outperforms the state-of-the-arts on both performance and efficiency. In the future, we will explore the complementary cues that could facilitate weakly-supervised detection to further boost the multi-label classification task.</p><p>ACKNOWLEDGEMENT This work was supported by the National Natural Science Foundation of China under grant 91646207.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture of our framework. The proposed framework works with two steps: (1) we first develop a WSD model as teacher model (called T-WDet) with only image-level annotations y; (2) then the knowledge in T-WDet is distilled into the MLIC student model (called S-Cls) via feature-level distillation from RoIs and prediction-level distillation from whole image, where the former is conducted by optimizing the loss ℓ L ℓ f while the latter is conducted by optimizing the loss L p and L p ′ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example results on two datasets. The green bounding boxes in images are the top-10 proposals detected by T-WDet model, which is sorted by objectness confidences s ′ in Eq. 4. The text on the right of images are the top-3 classification results of S-Cls model "without" and "with" knowledge distillation using our framework, where correct predictions are shown in blue and incorrect predictions in red.Table 1: Quantitative comparison (%) on MS-COCO. "w/" and "w/o" indicate "with" and "without" knowledge distillation by the proposed framework, respectively. The values in bold are the best while the values underlined are the second best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). The improvements over each class on MS-COCO.(b). The improvements over each concept on NUS-WIDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The improvements of S-Cls model over each class/concept on two datasets after knowledge distillation with our framework. "*k" indicates the number (divided by 1000) of images including this class/concept. The classes/concepts in horizontal axis are sorted by the number "*k" from large to small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Training and Test of S-Cls model TRAINING Input: image data and label data (I N , y N ). Output: parameters w of S-Cls model. Initialize: w, λ and training hyper-parameters.</figDesc><table><row><cell cols="2">Stage 1: Feature-Level Knowledge Transfer.</cell></row><row><cell cols="2">1: Repeat:</cell></row><row><cell>2:</cell><cell>compute L f (w S conv ) by Eq. (3), Eq. (4).</cell></row><row><cell cols="2">3: 4: Until: L f (w S update w S conv by gradient back-propagation. conv ) converges.</cell></row><row><cell cols="2">Stage 2: Prediction-Level Knowledge Transfer.</cell></row><row><cell cols="2">5: Repeat:</cell></row><row><cell>6:</cell><cell>compute L p (w S ) + λL p ′ (w S ) by Eq. (5), Eq. (10), Eq. (11).</cell></row><row><cell>7:</cell><cell>update w S by gradient back-propagation.</cell></row><row><cell cols="2">8: Until: L p (w S ) converges.</cell></row><row><cell cols="2">Return: w</cell></row></table><note>S TEST Input: image data I N .Output: prediction l N .Initialize: parameters w of S-Cls model, confidence threshold τ .9: for n = 1 to N do 10:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Component-wise ablation study (%).</figDesc><table><row><cell>Method</cell><cell>mAP</cell></row><row><cell>Baseline (Sigmoid-Logistic)</cell><cell>70.9</cell></row><row><cell>+Distillation [12]</cell><cell>71.3</cell></row><row><cell>+Class-aware distillation</cell><cell>72.1</cell></row><row><cell>+NMS proposals transfer+Class-aware transfer</cell><cell>73.8</cell></row><row><cell>+RoI-aware transfer+Class-aware transfer</cell><cell>74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The comparison of region proposals from EdgeBoxes<ref type="bibr" target="#b46">[47]</ref> and Faster-RCNN<ref type="bibr" target="#b21">[22]</ref> (%).</figDesc><table><row><cell>Method</cell><cell>mAP</cell></row><row><cell>Baseline (Sigmoid-Logistic)</cell><cell>70.9</cell></row><row><cell>T-WDet (EdgeBoxes [47])</cell><cell>78.6</cell></row><row><cell>S-Cls</cell><cell>74.6</cell></row><row><cell>T-WDet (Faster RCNN [22])</cell><cell>81.1</cell></row><row><cell>S-Cls</cell><cell>76.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale Combinatorial Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly Supervised Deep Detection Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Efficient Object Detection Models with Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Order-Free RNN with Visual Attention for Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Shang-Fu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent Attentional Reinforcement Learning for Multi-label Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NUS-WIDE: A Real-World Web Image Database from National University of Singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Survey on Multi-label Classification for Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Devkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankirti</forename><surname>Shiravale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="39" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Dual-Network Progressive Approach to Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Convolutional Ranking for Multilabel Image Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the Knowledge in a Neural Network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Structured Inference Neural Networks with Label Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Love Thy Neighbors: Image Annotation by Exploiting Image Metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4624" to="4632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly Supervised Object Localization with Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional Graphical Lasso for Multi-label Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2977" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving Pairwise Ranking for Multi-label Image ClassificationImproving Pairwise Ranking for Multi-label Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3617" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollãąr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic Regularisation for Recurrent Image Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2872" to="2880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3524" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning graph structure for multi-label image classification via clique generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4100" to="4109" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, Chunhua Shen, and Junbin Gao</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Patch Learning for Weakly Supervised Object Classification and Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="446" to="459" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-Label Classification: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic Image Annotation via Label Transfer in the Semantic Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiberio</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation as Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CNN-RNN: A Unified Framework for Multi-label Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilabel Image Recognition by Recurrently Discovering Attentional Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CNN: Single-label to Multi-label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive Low-Rank Multi-Label Active Learning for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1336" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glorot</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research. PP</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Determinantal Point Process for Large-Scale Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luntian</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MIML-FCN+: Multi-instance Multi-label Learning via Fully Convolutional Networks with Privileged Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploit Bounding Box Annotations for Multi-label Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Deep Latent Spaces for Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chieh</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jen</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2838" to="2834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pay More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multilabel Image Classification with Regional Latent Semantic Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06998</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Spatial Regularization with Image-level Supervisions for Multilabel Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Soft Proposal Networks for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Edge Boxes: Locating Object Proposals from Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
