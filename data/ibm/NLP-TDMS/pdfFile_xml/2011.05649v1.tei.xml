<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENT NEURAL ARCHITECTURE SEARCH FOR END-TO-END SPEECH RECOGNITION VIA STRAIGHT-THROUGH GRADIENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahuan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Processing and Machine Intelligence (SPMI) Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Processing and Machine Intelligence (SPMI) Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Processing and Machine Intelligence (SPMI) Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENT NEURAL ARCHITECTURE SEARCH FOR END-TO-END SPEECH RECOGNITION VIA STRAIGHT-THROUGH GRADIENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-NAS</term>
					<term>Straight-Through</term>
					<term>End-to-end ASR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS), the process of automating architecture engineering, is an appealing next step to advancing endto-end Automatic Speech Recognition (ASR), replacing expertdesigned networks with learned, task-specific architectures. In contrast to early computational-demanding NAS methods, recent gradient-based NAS methods, e.g., DARTS (Differentiable AR-chiTecture Search), SNAS (Stochastic NAS) and ProxylessNAS, significantly improve the NAS efficiency. In this paper, we make two contributions. First, we rigorously develop an efficient NAS method via Straight-Through (ST) gradients, called ST-NAS. Basically, ST-NAS uses the loss from SNAS but uses ST to back-propagate gradients through discrete variables to optimize the loss, which is not revealed in ProxylessNAS. Using ST gradients to support subgraph sampling is a core element to achieve efficient NAS beyond DARTS and SNAS. Second, we successfully apply ST-NAS to endto-end ASR. Experiments over the widely benchmarked 80-hour WSJ and 300-hour Switchboard datasets show that the ST-NAS induced architectures significantly outperform the human-designed architecture across the two datasets. Strengths of ST-NAS such as architecture transferability and low computation cost in memory and time are also reported.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Building Automatic Speech Recognition (ASR) systems historically was an expertise-intensive task and involved a complex pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which consists of phonetic decision trees and multiple stages of alignments and model updating. Recently, there are increasing interests in developing end-to-end ASR systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> to reduce expert efforts and simplify the system. The progress largely relies on utilizing deep neural networks (DNNs) of various architectures, which is generally known as deep learning. The success of deep learning is largely due to its automation of the feature engineering process: hierarchical feature extractors are automatically learned from data rather than manually designed. This success has been accompanied, however, by a rising demand for architecture engineering.</p><p>Various neural architectures, e.g., 2D Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b6">[7]</ref>, which is also known as (a.k.a.) VGG-Net, 1D dilated CNN <ref type="bibr" target="#b7">[8]</ref> (a.k.a. TDNN), ResNet <ref type="bibr" target="#b8">[9]</ref> and so on, are manually designed by experts through intuitions plus laborious trial and error experiments. Hyper-parameters for an architecture (e.g., kernel size, stride, and dilation of CNN) are set empirically, which may not be optimal for the specific task at hand, since naive grid-search is highly This work is supported by NSFC 61976122. expensive. Neural Architecture Search (NAS) <ref type="bibr" target="#b9">[10]</ref>, the process of automating architecture engineering, is thus an appealing next step to advancing end-to-end ASR.</p><p>Early NAS methods are computationally demanding despite their remarkable performance. For example, it takes 2000 GPU days of reinforcement learning and 3150 GPU days of evolution to obtain a state-of-the-art (SOTA) architecture for image classification over CIFAR- <ref type="bibr">10 [11]</ref> and ImageNet <ref type="bibr" target="#b11">[12]</ref> respectively . Some recent NAS methods, e.g., DARTS (Differentiable ARchiTecture Search) <ref type="bibr" target="#b12">[13]</ref>, SNAS (Stochastic NAS) <ref type="bibr" target="#b13">[14]</ref> and ProxylessNAS <ref type="bibr" target="#b14">[15]</ref>, significantly improve the NAS efficiency and can obtain SOTA architecture over CIFAR-10 within one GPU day. Note that NAS methods roughly can be categorized according to three dimensions -search space, search method, and performance evaluation method <ref type="bibr" target="#b15">[16]</ref>. Some common features shared by these efficient NAS methods are: representing the search space as a weighted directed acyclic graph (DAG) <ref type="bibr" target="#b0">1</ref> , using gradient-based search methods to learn the edge weights, and weight sharing in performance evaluation of candidate architectures (i.e., sub-graphs of the DAG). The final architecture is derived from the learned edge weights. Notably, DARTS uses the Softmax trick to relax the search space to be continuous and performs gradient search over the whole super-network, while SNAS uses the Gumbel-Softmax trick <ref type="bibr" target="#b19">[20]</ref>. These are less efficient in both memory and computation than ProxylessNAS, which uses discrete search spaces and does sub-graph sampling. To back-propagate gradients through the discrete variables, which index the sampled edges, ProxylessNAS uses an ad-hoc trick, analogous to BinaryConnect <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this paper, we make two contributions. First, we observe that ProxylessNAS essentially uses the Straight-Through (ST) estimator <ref type="bibr" target="#b21">[22]</ref> for gradient approximation, which is missed in the Proxyless-NAS paper. To back-propagate gradients through discrete variables, the basic idea of ST is that the sampled discrete index is used for forward computation, and the continuous Softmax probability of the sampled index is used for backward gradient calculation. Using ST gradients to support sub-graph sampling is a core element to achieve efficient NAS beyond DARTS and SNAS. Based on ProxylessNAS and also the above observation, we develop an efficient NAS method via ST gradients, called ST-NAS. In contrast to ProxylessNAS whose NAS objective definition is not clearly shown, ST-NAS uses the NAS objective definition from SNAS and is more rigorous. Compared to SNAS, ST-NAS does not use the Gumbel-Softmax trick and uses the more efficient ST gradients. The development of the ST-NAS method is the first contribution of this paper. <ref type="table">Input  CTC/CTC-CRF  loss   macro-DAG   ?  ?  ?</ref> super-network sampling step <ref type="figure">Fig. 1</ref>. An overview of our ST-NAS method, which illustrates the concepts of macro-DAG, candidate operations (OP1, OP2, OP3), super-network, and sub-graph sampling. Here we plot the macro-DAG used in our ASR experiments, which has a serial structure. More complicated macro-DAGs are possible in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS model</head><p>Second, we apply ST-NAS to end-to-end ASR. Compared to the application of NAS techniques in computer vision tasks, there are limited studies in applying NAS to speech recognition tasks. In <ref type="bibr" target="#b22">[23]</ref>, NAS via policy gradient based reinforcement learning <ref type="bibr" target="#b16">[17]</ref> is applied to keyword spotting. In <ref type="bibr" target="#b23">[24]</ref>, evolution-based NAS is applied to keyword spotting and language identification. DARTS are used in <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref> for speaker recognition and Connectionist Temporal Classification (CTC) based speech recognition respectively. The NAS methods used in these previous studies are not as efficient as NAS via ST gradients. This work represents the first to introduce ST gradient based NAS into end-to-end speech recognition, which is the second contribution of this paper.</p><p>We evaluate the ST-NAS method in end-to-end ASR experiments over the widely benchmarked 80-hour WSJ and 300-hour Switchboard datasets and show that the ST-NAS induced architectures significantly outperform the human-designed architecture (TDNN-D in <ref type="bibr" target="#b26">[27]</ref>) across the two datasets. Notably, our ST-NAS induced model obtains the lowest word error rate (WER) of 2.77%/5.68% on WSJ eval92/dev93 among all published endto-end ASR results, to the best of our knowledge. Our NAS implementation is based on the CAT toolkit [28] -a PyTorch-based ASR toolkit, which offers two benefits. First, it enables us to seamlessly integrate the NAS code with the ASR code to flexibly use PyTorch functionalities. Second, the CAT toolkit supports the end-to-end CTC-CRF loss, which is defined by a CRF (conditional random field) with CTC topology and has been shown to perform significantly better than CTC <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. Additionally, we show that the architectures learned by the CTC loss based ST-NAS are transferable to be retrained under the CTC-CRF loss, i.e., these architectures achieve close performance to the architectures searched under the CTC-CRF loss. This enables us to reduce the cost of running NAS to search the architecture, since CTC-CRF is somewhat more expensive than CTC. We also show that the model transferred from the WSJ experiment performs close to the model searched over Switchboard and better than the comparably-sized TDNN-D. We release the code 2 for reproducible NAS study, as it is found in <ref type="bibr" target="#b28">[29]</ref> that reproducibility is crucial to foster NAS research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND RELATED WORK</head><p>Recent gradient-based NAS methods such as DARTS <ref type="bibr" target="#b12">[13]</ref>, SNAS <ref type="bibr" target="#b13">[14]</ref> and ProxylessNAS <ref type="bibr" target="#b14">[15]</ref> significantly improve over previous reinforcement learning and evolution based NAS methods with reduced computational cost. In these methods, the search space is represented as a DAG, which consists of nodes (numbering by 0, 1, · · · , N ) and directed edges (pointing from lower-numbered nodes to higher-numbered). The i-th node, denoted by nodei, represents a latent representation xi (e.g., a feature map in CNNs). A directed edge leaving nodei is associated with an operation that transforms xi. An intermediate node (i.e., after the input node x0) is computed as follows:</p><formula xml:id="formula_0">xj = i∈A j Ωij(xi)<label>(1)</label></formula><p>where Aj is the set of parent nodes of nodej, Ωij denotes the operation that connects nodei to nodej in the computation flow. Suppose that Ωij can take from K different candidate operations {o (k) ij , k = 1, · · · , K} (e.g., different convolutions), where each candidate operation is associated a weight α (k) ij , called architecture weight. It can be easily seen that by sampling one of the K candidate operations for each connected pair of nodes, we obtain a candidate architecture. The task of NAS therefore is reduced to learning the architecture weights. The final architecture is derived from the learned architecture weights, e.g., by selecting the largest weighted candidate operation for each connected pair of nodes (i, j). Different NAS methods mainly differ in how to learn the architecture weights</p><formula xml:id="formula_1">α = {α (k)</formula><p>ij }, together with the operation parameters θ, which are used to define the operations {o</p><formula xml:id="formula_2">(k) ij }.</formula><p>Note that it is often a practice to plot all the candidate operations {o (k) ij , k = 1, · · · , K} between each connected nodei and nodej in the DAG and call the resulting expanded DAG a super-network, e.g., as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and also shown in <ref type="figure">Fig. 1</ref>. Then each edge in the super-network represents a candidate operation, and a candidate architecture corresponds to a sub-graph in the super-network. Each connected pair of nodes together with the K edges between them is called a searching block. To be differentiated from the supernetwork, the initial DAG is referred to as the macro-DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DARTS</head><p>Instead of searching over a discrete set of sub-graphs, DARTS relaxes the categorical choice of a particular operation o (k) ij on edge (i, j) to a mixture of all possible K candidate operations, by defining the following mixed operation:</p><formula xml:id="formula_3">Ω DARTS ij (xi) = K k=1 π (k) ij o (k) ij (xi)<label>(2)</label></formula><p>where πij = (π <ref type="bibr" target="#b0">(1)</ref> ij , · · · , π (K) ij ) denotes the mixing probabilities of the K operations defined by a Softmax over the architecture weights:</p><formula xml:id="formula_4">π (k) ij = exp(α (k) ij ) K k =1 exp(α (k ) ij )<label>(3)</label></formula><p>Here we add DARTS as the superscript to denote the particular operation Ωij used in DARTS, which is plugged in Eq. (1) to define the computation flow. In this manner, DARTS relaxes the search space to be continuous and directly incorporates the architecture weights α into the super-network forward computation to define the loss L DARTS (α, θ), together with the operation parameters θ. Thus α and θ can be jointly trained via the standard backpropagation algorithm by optimizing L DARTS (α, θ) <ref type="bibr" target="#b2">3</ref> .</p><p>A limitation of DARTS is its expensive computation in both memory and time, which is around K times of training a single model. The mixed operation of DARTS requires to store the whole super-network in memory, and all edges are involved in the forward and backward computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">SNAS</head><p>Note that DARTS uses the super-network in NAS training, but after NAS training, uses the extracted sub-graph in inference. This incurs an inconsistency, which harms the performance. SNAS uses the same super-network setup as in DARTS, but proposes to optimize the expected performance of all sub-graphs sampled with pα(z):</p><formula xml:id="formula_5">L(α, θ) = E z∼pα(z) [L θ (z)]<label>(4)</label></formula><p>where z = {zij} denotes the collection of the independent onehot random variable vectors 4 for all pairs of connected nodes in the super-network, indexing, which edge is sampled. Thus a sample z represents a sampled sub-graph, and L θ (z) denotes the loss evaluated under the sampled sub-graph z. The one-hot vector zij = (z <ref type="bibr" target="#b0">(1)</ref> ij , · · · , z (K) ij ) for connected nodei and nodej is assumed to follow the categorical distribution Cat(π</p><formula xml:id="formula_6">(1) ij , · · · , π (K) ij ).</formula><p>The loss L(α, θ) is not directly differentiable w.r.t. the architecture weights α, since we cannot pass the gradient through the discrete random variable zij to αij = (α</p><formula xml:id="formula_7">(1) ij , · · · , α (K) ij ).</formula><p>To sidestep this, SNAS relaxes the discrete one-hot variable zij to be a continuous random variable computed by the Gumbel-Softmax (G-S) function <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_8">y (k) ij = exp((α (k) ij + g (k) ij )/τ ) K k =1 exp((α (k ) ij + g (k ) ij )/τ ) (5) where g (k) ij = − log(− log(u (k) ij )), and {u (k)</formula><p>ij } are independent and identical distributed (i.i.d.) samples from u ∼ Uniform(0, 1). τ is the temperature, which is gradually annealed to be close to zero.</p><p>The soften one-hot variable yij = (y <ref type="bibr" target="#b0">(1)</ref> ij , · · · , y (K) ij ) follows the G-S distribution. It is shown in <ref type="bibr" target="#b19">[20]</ref> that as the temperature τ approaches 0, yij from the G-S distribution become one-hot, and the G-S distribution becomes identical to the categorical distribution Cat(π <ref type="bibr" target="#b0">(1)</ref> ij , · · · , π (K) ij ). Thus, SNAS uses the following surrogate loss to approximate the loss defined in Eq. <ref type="formula" target="#formula_5">(4)</ref>:</p><formula xml:id="formula_9">L(α, θ) = E y∼pα(y) [L θ (y)] (6) which becomes directly differentiable w.r.t. α.</formula><p>Here L θ (y) is defined by the computation flow still according to Eq. (1) but with the following soften operation:</p><formula xml:id="formula_10">Ω SNAS ij (xi) = K k=1 y (k) ij o (k) ij (xi)<label>(7)</label></formula><p>It can be seen that the computation cost of SNAS in both memory and time is identical to that of DARTS. The difference is that while DARTS uses the Softmax trick for continuous relaxation, SNAS uses the Gumbel-Softmax trick with annealed temperature. When τ approaches 0, the objectives in NAS training and in model inference becomes consistent. The Gumbel-Softmax trick is also used in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref> for NAS.</p><p>(α, θ) w.r.t. α are alternately optimized, which are defined over training data and validation data respectively. <ref type="bibr" target="#b3">4</ref> As usual, categorical variables are encoded as K-dimensional one-hot vectors lying on the corners of the (K − 1)-dimensional simplex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">ProxylessNAS</head><p>Using the same super-network setup as in DARTS, ProxylessNAS proposes to use binary gates (essentially a one-hot vector) zij for each edge to define the operation to reduce the memory footprint:</p><formula xml:id="formula_11">Ωij(xi) = K k=1 z (k) ij o (k) ij (xi),<label>(8)</label></formula><p>Here we use the same zij from SNAS as it carries the same meaning -indexing, which edge is sampled. In this manner, the architecture weights αij are not directly involved in the computation flow as defined in Eq. (1) and thereby we cannot directly calculate the gradient of αij. Motivated by BinaryConnect <ref type="bibr" target="#b20">[21]</ref>, ProxylessNAS proposes the following gradient approximation:</p><formula xml:id="formula_12">∂L ∂α (k) ij = K k =1 ∂L ∂π (k ) ij ∂π (k ) ij ∂α (k) ij ≈ K k =1 ∂L ∂z (k ) ij ∂π (k ) ij ∂α (k) ij<label>(9)</label></formula><p>By using the sampled sub-graph z, ProxylessNAS reduces the memory footprint and backward computation cost, compared to DARTS and SNAS. We leave the detailed comparison of SNAS, ProxylessNAS and our method to Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>Our NAS method is based on the following two observations from the review of existing gradient-based NAS methods. First, ProxylessNAS is an interesting method, but the loss L used in ProxylessNAS is not explicitly shown in the original paper <ref type="bibr" target="#b14">[15]</ref>. We observe that the ProxylessNAS loss L in Eq. <ref type="formula" target="#formula_12">(9)</ref> is in fact the loss L θ (z) as defined in SNAS. Second, we observe that ProxylessNAS essentially uses the Straight-Through (ST) estimator <ref type="bibr" target="#b21">[22]</ref> for gradient approximation -a simple yet effective technique to back-propagate gradients through discrete variables. This point is also missed in the ProxylessNAS paper.</p><p>In the following, we first introduce the ST gradient estimator, and then present our NAS method via ST gradients, called ST-NAS. Basically, ST-NAS uses the loss from SNAS but optimizes the loss using the ST gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Straight-Through gradient</head><p>To optimize the loss L(α, θ), we sample a sub-graph, represented by z, according to the architecture weights α. Specifically, for each edge (i, j), zij is independently sampled from Cat(π <ref type="bibr" target="#b0">(1)</ref> ij , · · · , π (K) ij ), which is defined by the architecture weights αij as in Eq. (3). Then the forward computation is conducted over the sampled sub-graph to obtain the loss L θ (z) as a Monte Carlo estimate of L(α, θ), by executing the operation as defined in Eq. <ref type="bibr" target="#b7">(8)</ref>. During the backward computation, we encounter the problem that we cannot pass the gradient through the one-hot sample zij to αij.</p><p>The basic idea of ST is that the sampled discrete zij is used for forward computation, but the continuous probability πij is used for backward gradient calculation, namely approximating ∂zij ≈ ∂πij 5 . Specifically, we compute the gradient w.r.t. αij as follows: <ref type="bibr" target="#b4">5</ref> Clearly, this is the approximation used by Eq. (9) in ProxylessNAS.   An illustration of the forward and backward computation performed locally between connected nodei and nodej is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We compare different gradient-based NAS methods in <ref type="table" target="#tab_0">Table 1</ref>. Compared to training a single model, both DARTS and SNAS are around K times more expensive in both memory and time. Note that as shown in Eq. (10), the gradient w.r.t. αij involves all the K features {o (k ) ij (xi), k = 1, · · · , K}. Regarding this, the backward computation complexity in ProxylessNAS and ST-NAS can be reduced to be O(1), but the forward computation complexity is still O(K). As shown in <ref type="table" target="#tab_0">Table 1</ref>, the memory cost in ProxylessNAS and ST-NAS is far less than KC1, where C1 denotes the memory size for training a single model.</p><formula xml:id="formula_13">∂L θ (z) ∂α (k) ij = ∂L θ (z) ∂xj ∂xj ∂α (k) ij = ∂L θ (z) ∂xj i∈A j ∂Ωij(xi) ∂α (k) ij ∂Ωij(xi) ∂α (k) ij = K k =1 ∂z (k ) ij ∂α (k) ij o (k ) ij (xi) ≈ K k =1 ∂π (k ) ij ∂α (k) ij o (k ) ij (xi)<label>(10)</label></formula><formula xml:id="formula_14">DARTS L DARTS (α, θ) continuous KC1 O(K) SNASL(α, θ) continuous KC1 O(K) Proxyless Lθ(z) ST C1 + (K − 1)C2 O(1) ST-NAS L(α, θ) ST C1 + (K − 1)C2 O(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The NAS procedure</head><p>Here we present the whole NAS procedure, which uses the ST gradients and is illustrated in <ref type="figure">Fig. 1</ref>. First, we need to choose the macro-DAG and the set of possible candidate operations, which together define the super-network, as the search space. The setup of macro-DAG, candidate operations and the super-network is flexible and depends on the target task. One example setup for the end-to-end ASR task is described in Section 4. Given this setup, we divide the NAS procedure into three stages: super-network initialization (also called warm-up), architecture search and retraining.</p><p>There are two sets of learnable parameters: the architecture weights and operation parameters, denoted by α and θ respectively. We split the original dataset into a training set and a validation set.</p><p>Super-network initialization. Note that the single set of operation parameters θ is shared for all sampled sub-graphs, and thus plays a critical role in later architecture search. It is found in our experiments that an initialization stage to warm up θ is beneficial. Basically, this initialization stage is similar to the following architecture search stage, except that only θ is trained. Specifically, for each minibatch from the training data, we uniformly sample a sub-graph, update θ using the standard gradient descent. After each training epoch, we evaluate the super-network over the validation data (which is to be detailed below) to monitor the convergence. The effect of this initialization stage on the performance of the searched architecture is detailed in Section 4.5.</p><p>Architecture search. After completing the super-network initialization, we hold θ, reset the optimizer and run the architecture search stage, which involves a bi-level optimization problem <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_15">min α L val (α, θ * (α)) s.t. θ * (α) = arg min θ Ltrain(α, θ)<label>(11)</label></formula><p>where Ltrain and L val are the losses over the training and validation data respectively. Solving the above bi-level optimization problem is difficult. In practice, updating α and θ alternately by stochastically optimizing L val and Ltrain over minibatches respectively is found to work reasonably well, as shown in many previous NAS methods and given in Algorithm 1. Note that as the validation set is usually smaller than the training set, we cyclically use the validation set in drawing validation minibatches in Step 1 of Algorithm 1. For evaluating the super-network over the validation data to monitor convergence, we average the validation losses over the minibatches in the whole validation set (not cyclically). Specifically, for each validation minibatch, we sample a sub-graph and calculate the validation loss over the sampled sub-graph. In this manner, we evaluate the expected performance of the current super-network.</p><p>Retraining. After the convergence of the architecture search, we derive a single model by selecting the top-1 edge between connected nodes and prune others from the super-network. The derived single model is trained from scratch to yield the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Experiments are conducted on the 80-hour WSJ and the 300hour Switchboard datasets. Input features are 40 dimension fbank with delta and delta-delta features (120 dimensions in total). The features are augmented by 3-fold speed perturbation, and are mean and variance normalized. We use the CAT toolkit <ref type="bibr" target="#b27">[28]</ref> for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">NAS settings and hyper-parameters</head><p>We conduct NAS for the acoustic model. Unless otherwise stated, we follow the basic settings in <ref type="bibr" target="#b5">[6]</ref>. The denominator graph uses a phone-based 4-gram language model, and the language model in  ij ) for the searching blocks in the NAS run that yields the derived single model in (a). decoding is a word-based 4-gram language model. For NAS settings, we design a super-network by serially connecting 3 searching blocks, 1 subsampling TDNN layer, 3 searching blocks, and 1 fullyconnected layer, as shown in <ref type="figure" target="#fig_4">Fig. 3(a)</ref>. Although more complex super-networks can be designed, this super-network is inspired by the "TDNN-D" of <ref type="bibr" target="#b26">[27]</ref>. It can be easily seen that the "TDNN-D" is a sub-graph in the super-network, thus lies in our search space.</p><p>The subsampling layer is a TDNN layer with convolution kernel size 3, dilation 1, and stride 3, which leads to a subsampling of factor 3. The candidate operations in each searching block are TDNNs with different configurations. The number of hidden units in all TDNN layers is 640. Convolution strides of candidate TDNNs are all set to 1. Layer normalization and dropout with probability 0.5 are applied after each TDNN layer.</p><p>By default, during the warm-up and architecture search stages, the super-network is trained with CTC. This is called NAS with CTC. The derived single model is then retrained with CTC-CRF. We run the 3-stage NAS procedure as described in Section 3.2 with the following hyper-parameters. We use the Adam optimizer (in PyTorch) with its default arguments unless otherwise stated. It can be seen that for simplicity and reproducibility, we only introduce a few extra hyper-parameters compared to training a single model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">WSJ</head><p>The candidate operations of WSJ experiments are shown in <ref type="figure" target="#fig_4">Fig. 3(a)</ref>, with which the search space contains 4096 sub-graphs in total. Eval92 and dev93 sets are both for test and excluded from training  <ref type="bibr" target="#b1">2</ref> Obtained based on our implementation of the "TDNN-D" in <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b2">3</ref> Random search is a competitive baseline for NAS <ref type="bibr" target="#b28">[29]</ref> but still inferior to our ST-NAS.</p><p>set and validation set. In warm-up and architecture search, the original training data are split into 90%:10% proportions for training and validation. For retraining, 5% sentences of the original training set are for validation and the others for training. The experiments run on 4 NVIDIA GTX1080 GPUs. To reduce the randomness, we conduct NAS with CTC for 5 times with different random seeds. The retraining uses the CTC-CRF loss with the CTC loss (weighted by 0.01). For comparison, NAS with fully CTC-CRF (namely the warm-up and architecture are trained with CTC-CRF) is conducted 3 times due to the expensive computation of CTC-CRF. Random search is conducted 5 times. We compare our searched models to various human-designed DNN architectures in end-to-end ASR, trained with CTC, CTC-CRF or attention-based losses. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the model searched by ST-NAS with CTC and retrained with CTC-CRF achieves the lowest WER of 2.77%/5.68% on WSJ eval92/dev93, outperforming all other end-to-end ASR models. This model obtains significant improvement over both BLSTM and VGG-BLSTM with CTC-CRF, On dev93, this model shows an 8.8% relative improvement over BLSTM and is close to VGG-BLSTM. Notably, the number of parameters in our searched models on average is around 11.9 million, 11.8% less than BLSTM and 25.6% less than VGG-BLSTM.</p><p>There are some ablation results in <ref type="table" target="#tab_3">Table 2</ref>. First, for the models searched by NAS with CTC, retraining with CTC-CRF loss achieves improvement over retraining with CTC loss. Second, compared to the models searched and retrained all with CTC-CRF (namely NAS with fully CTC-CRF), the models searched with CTC but retrained with CTC-CRF perform equally well. In summary, these results show that the architectures searched by NAS with CTC are transferable to be retrained with CTC-CRF. This enables us to reduce the cost of running NAS to search the architecture, since CTC-CRF is somewhat expensive than CTC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Switchboard</head><p>In the Switchboard experiment, we add extra "TDNN-3-1" and "TDNN-3-2" candidate operations, in addition to those shown in <ref type="figure" target="#fig_4">Fig. 3(a)</ref> for WSJ. The entire search space contains 46656 subgraphs. The retraining uses the CTC-CRF loss with the CTC loss (weighted by 0.1). In warm-up and architecture search, the original training data are split into 95%:5% proportions for training and validation. For retraining, we take around 5 hours of speech as validation set and the rest for training, following the setting in CAT <ref type="bibr" target="#b27">[28]</ref>. The Eval2000 data, including the Switchboard evaluation dataset (SW) and Callhome evaluation dataset (CH), are for testing. We conduct a single run of ST-NAS on 4 NVIDIA P100 GPUs, and do not conduct multiple random searches due to time limitation.</p><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, our ST-NAS models outperform both the TDNN-D-Small and TDNN-D-Large. Notably, the model transferred from the WSJ experiment performs close to the model searched over Switchboard; and compared to the comparably-sized TDNN-D-Large, it obtains 13.7% and 8.6% relative WER reductions on SW and CH respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Complexity analysis</head><p>To complement </p><p>which is far less than C1. SequenceLen denotes the average length of sequences, which is around 850.  For time complexity, warm-up and architecture search are trained with CTC, which is much faster than CTC-CRF in retraining. Thus the extra time cost is limited. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the total time of running the 3-stage NAS procedure is less than 3 times of training a single model from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effect of super-network initialization</head><p>The effect of the super-network initialization (i.e., warm-up) seems to be overlooked in previous NAS studies. We run architecture search from two differently initialized super-networks (A and B), retrain the two searched models, and compare the performance of the two retrained models, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. Experiments are conducted under the same settings as in Section 4.2 on the WSJ dataset. Supernetwork A represents a randomly-initialized super-network, without any warm-up training, and super-network B is obtained when the warm-up converges. It can be seen that sufficient warm-up helps the architecture search stage to find a better final model, which achieves lower validation loss in retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>NAS is an appealing next step to advancing end-to-end ASR. In this paper, we review existing gradient-based NAS methods and develop an efficient NAS method via Straight-Through (ST) gradients, called ST-NAS. Basically, ST-NAS uses the loss from SNAS but optimizes the loss using the ST gradients. We successfully apply ST-NAS to end-to-end ASR. Experiments over WSJ and Switchboard show that the ST-NAS induced architectures significantly outperform the human-designed architecture across the two datasets. Strengths of ST-NAS such as architecture transferability and low computation cost in memory and time are also reported. Remarkably, the ST-NAS method is flexible and can be further explored by using different macro-DAGs, candidate operations and super-networks for ASR, not limited to the example setup in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2 https://github.com/thu-spmi/ST-NAS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Computation flow of different NAS methods locally between connected nodei and nodej when K = 4. Solid and dashed lines denote the forward and backward computations respectively. (a) For DARTS and SNAS, the forward is fully continuous and the backward is fully differentiable. (b) For ProxylessNAS and ST-NAS, the forward from πij to zij involves sampling and the backward uses the ST gradient to flow from zij to αij.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label></label><figDesc>Computation costs are estimated relative to training a single model. K denotes the number of possible operations for each connected pair of nodes. The forward computation complexity of all methods is O(K). C1 denotes the memory size for training a single model. C2 denotes the average memory size for storing the output features for all connected pairs of nodes in a sub-graph. Usually we have C2 C1 (see numerics in Section 4.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>(a) shows the super-network for WSJ experiments. Labels of candidate operations over edges are formatted in "-{half of context}-{dilation}". In Switchboard experiments, we add extra "TDNN-3-1" and "TDNN-3-2" candidate operations to each searching blocks. The solid lines in (a) indicate one of the derived single model from the 5 runs of NAS on WSJ. (b) shows the evolution of architecture probabilities (i.e., π (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>(a) The two points A and B in the loss curve during warmup, which represent differently initialized super-networks. (b) The curves of validation losses for retraining the two models, obtained by running architecture search starting from A and B respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different gradient-based NAS methods.</figDesc><table><row><cell>Methods</cell><cell>Loss</cell><cell>α gradient</cell><cell>Memory</cell><cell>Backward computation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Architecture Search while not converged do for each training minibatch in an epoch do Step 1. Freeze θ, draw a validation minibatch, sample a subgraph, run the forward computation 6 over the super-network under the validation minibatch, and update α with the ST gradients; Step 2. Freeze α, sample a sub-graph, run the forward computation over the sampled sub-graph under the training minibatch, and update θ with the standard gradients; end for Evaluate the super-network over the validation data to monitor convergence.</figDesc><table><row><cell>end while</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>WERs on the 80-hour WSJ.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>eval92</cell><cell>dev93</cell></row><row><cell cols="2">EE-Policy-CTC [32]</cell><cell>5.53</cell><cell>9.21</cell></row><row><cell cols="2">SS-LF-MMI [33]</cell><cell>3.0</cell><cell>6.0</cell></row><row><cell cols="2">EE-LF-MMI [34]</cell><cell>3.0</cell><cell>-</cell></row><row><cell></cell><cell>FC-SR [35] 1</cell><cell>3.5</cell><cell>6.8</cell></row><row><cell cols="2">ESPRESSO [36]</cell><cell>3.4</cell><cell>5.9</cell></row><row><cell>CTC</cell><cell>BLSTM ST-NAS</cell><cell cols="2">4.93 4.72±0.03 8.82±0.07 8.57</cell></row><row><cell></cell><cell>BLSTM [6]</cell><cell>3.79</cell><cell>6.23</cell></row><row><cell></cell><cell>VGG-BLSTM [28]</cell><cell>3.2</cell><cell>5.7</cell></row><row><cell>CTC-CRF</cell><cell>TDNN-D 2 [27]</cell><cell>2.91</cell><cell>6.24</cell></row><row><cell></cell><cell>Random search 3</cell><cell cols="2">2.82±0.01 5.71±0.03</cell></row><row><cell></cell><cell>ST-NAS</cell><cell cols="2">2.77±0.00 5.68±0.01</cell></row><row><cell cols="2">ST-NAS with fully CTC-CRF</cell><cell cols="2">2.81±0.01 5.74±0.02</cell></row></table><note>1 FC-SR uses dev93 as validation set and eval92 for test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>WERs on the 300-hour Switchboard. All experiments are trained with CTC-CRF. TDNN-D-Small is with the hidden size of 640, which is the same as that of our searched models. TDNN-D-Large is with the hidden size of 800.<ref type="bibr" target="#b1">2</ref> Randomly taken from one of the 5 runs of NAS with CTC over WSJ, and retrained on Switchboard.with 26.9% and 13.4% relative WER reductions respectively on eval92.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>SW</cell><cell>CH</cell><cell>Params</cell></row><row><cell></cell><cell>TDNN-D-Small</cell><cell cols="2">15.2 26.8</cell><cell>7.64M</cell></row><row><cell></cell><cell>TDNN-D-Large</cell><cell cols="3">14.6 25.5 11.85M</cell></row><row><cell>ST-NAS</cell><cell cols="4">Transferred from WSJ 2 Searched on Switchboard 12.6 23.2 15.98M 12.5 23.2 11.89M</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 ,</head><label>1</label><figDesc>this section provides numerical complexity analysis in memory and time for running ST-NAS. In the WSJ experiment, there are 6 searching blocks in our super-network and we run on 4 GPUs with data parallel in PyTorch. Training a single model requires around C1 = 3.5GB memory on each GPU. The quantity C2 inTable 1can be calculated as follows:</figDesc><table /><note>C2 = 6 × MinibatchSize × SequenceLen × HiddenUnitsNum × 4 Byte ÷ GPUNum = 6 × 64 × 850 × 640 × 4 Byte ÷ 4 ≈ 209MB</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The estimated run-time for the three stages in the ST-NAS procedure, averaged over the 5 runs in WSJ.</figDesc><table><row><cell>Stage</cell><cell>warm-up</cell><cell>architecture search</cell><cell>retraining</cell></row><row><cell>Epochs</cell><cell>65.2</cell><cell>22</cell><cell>28.6</cell></row><row><cell>Minutes/epoch</cell><cell>11</cell><cell>31</cell><cell>27.2</cell></row><row><cell>Total time (minute)</cell><cell>717.2</cell><cell>682</cell><cell>777.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When used to represent the entire architecture, the DAG is often called the super-network<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> or the over-parameterized network<ref type="bibr" target="#b14">[15]</ref>. When the search space is defined over smaller building blocks<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>, the DAG is called the cell block or the cell, that could be stacked in some way via a preset or learned meta-architecture to form the entire architecture.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In fact, L DARTS train (α, θ) with respect to (w.r.t.) θ and L DARTS val</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In this forward computation, we calculate {o(k )ij (x i ), k = 1, · · · , K}, but use Ω ij (x i ) as defined in Eq.<ref type="bibr" target="#b7">(8)</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The IBM 2004 conversational telephony system for rich transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent NN: First results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CRF-based single-stage acoustic modeling with CTC topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5676" to="5680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">ProxylessNAS: Direct neural architecture search on target task and hardware,&quot; in ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning time/memoryefficient deep architectures with budgeted super networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Veniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3492" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BinaryConnect: training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanpierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic adaptive neural architecture search for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Véniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Schwander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2842" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving keyword spotting and language identification via neural architecture search at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Mazzawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavi</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleks</forename><surname>Kracun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Violette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">AutoSpeech: Neural architecture search for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03215</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DARTS-ASR: Differentiable architecture search for multilingual speech recognition and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Yang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kuang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hungyi</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low latency acoustic modeling using temporal convolution and LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CAT: A CTC-CRF based ASR toolkit bridging the hybrid and the end-to-end approaches towards data efficiency and low latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Keyu An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four GPU hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Benoit Colson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5819" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flat-start single-stage discriminatively trained HMM-Based models for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Sameti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1949" to="1961" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition using lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Sameti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Espresso: A fast endto-end neural speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
