<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-15">15 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<email>matthias.fey@udo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Graphics</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
								<address>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-15">15 Apr 2019</date>
						</imprint>
					</monogr>
					<note>Published as a workshop paper at ICLR 2019 JUST JUMP: DYNAMIC NEIGHBORHOOD AGGREGATION IN GRAPH NEURAL NETWORKS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive nodeclassification experiments, we demonstrate the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND RELATED WORK</head><p>Graph neural networks (GNNs) have become the de facto standard for representation learning on relational data <ref type="bibr" target="#b9">Gilmer et al., 2017;</ref><ref type="bibr" target="#b0">Battaglia et al., 2018)</ref>. GNNs follow a simple neighborhood aggregation procedure motivated by two major perspectives: The generalization of classical CNNs to irregular domains <ref type="bibr" target="#b21">(Shuman et al., 2013)</ref>, and their strong relations to the <ref type="bibr" target="#b25">Weisfeiler &amp; Lehman (1968)</ref> algorithm <ref type="bibr" target="#b27">(Xu et al., 2019;</ref><ref type="bibr" target="#b18">Morris et al., 2019)</ref>. Many different graph neural network variants have been proposed and significantly advanced the state-of-the-art in this field <ref type="bibr" target="#b6">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b14">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b17">Monti et al., 2017;</ref><ref type="bibr" target="#b9">Gilmer et al., 2017;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017;</ref><ref type="bibr" target="#b7">Fey et al., 2018)</ref>.</p><p>Most of these approaches focus on novel kernel formulations, however, deeply stacking those layers usually result in gradually decreasing performance despite having, in principal, access to a wider range of information <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>. <ref type="bibr" target="#b26">Xu et al. (2018)</ref> blame the strongly varying speed of expansion on this phenomenon, caused by locally differing graph structures, and hence propose to node-adaptively jump back to earlier representations if those fit the task at hand more precisely.</p><p>Inspired by these so-called Jumping Knowledge networks <ref type="bibr" target="#b26">(Xu et al., 2018)</ref>, we explore a highly dynamic neighborhood aggregation (DNA) procedure based on scaled dot-product attention <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> which is able to aggregate neighboring node representations of differing locality. We show that this approach, when additionaly combined with grouped linear projections, outperforms traditional stacking of GNN layers, even when those are enhanced by Jumping Knowledge.</p><p>We briefly give a formal overview of the related work before we propose our method in more detail:</p><p>Graph Neural Networks (GNNs) operate over graph structured data G = (V, E) and iteratively update node features h</p><formula xml:id="formula_0">(t−1) v of node v ∈ V in layer t − 1 by aggregating localized information via h (t) v = f (t) Θ h (t−1) v , h (t−1) w w∈N (v) , e.g., h (t) v = σ   Θ (t) w∈N (v)∪{v} C (t−1) v,w h (t−1) w   ,<label>(1)</label></formula><p>from the neighbor set N (·) through a differentiable function f</p><p>Θ parametrized by weights Θ (t) . In current implementations, C</p><formula xml:id="formula_2">(t−1) v,w</formula><p>is either defined to be static <ref type="bibr" target="#b27">(Xu et al., 2019)</ref>, structure- <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017)</ref> or data-dependent .</p><p>Published as a workshop paper at ICLR 2019 GNN layers are typically stacked sequentially, but can be optionally enhanced by skip connections, e.g., h <ref type="bibr" target="#b3">(Cangea et al., 2018)</ref>, or updated using Gated Recurrent Units via h <ref type="bibr" target="#b4">(Cho et al., 2014;</ref><ref type="bibr" target="#b16">Li et al., 2016)</ref>. After T layers, h (T ) v holds the T -hop subgraph representation centered around node v.</p><formula xml:id="formula_3">(t) v ← h (t) v + Θ (t) s h (t−1) v</formula><formula xml:id="formula_4">(t) v ← GRU( h (t−1) v , h (t) v )</formula><p>Jumping Knowledge (JK) networks enable deeper GNNs by introducing layer-wise jump connections and selective aggregations to leverage node-adaptive neighborhood ranges <ref type="bibr" target="#b26">(Xu et al., 2018)</ref>. Given layer-wise representations h</p><formula xml:id="formula_5">(1) v , . . . , h (T ) v of node v, its final output representation is obtained by either (1) concatenation, (2) pooling or (3) summation h (1) v . . . h (T ) v max h (1) v , . . . , h (T ) v T t=1 α (t) v h (t) v<label>(2)</label></formula><p>where scorings α (t) v are obtained from a bi-directional LSTM <ref type="bibr" target="#b12">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><p>Attention modules weight the values of a set of key-value pairs K, V ∈ R n×d according to a given query q ∈ R d by computing scaled dot-products between key-query pairs and using the softmax-normalized results as weighting coefficients <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>:</p><formula xml:id="formula_6">Attention( q, K, V ) = softmax q K √ d V<label>(3)</label></formula><p>In practice, the attention function is usually performed h times (with each head learning separate attention weights and attending to different positions) and the results are concatenated.</p><p>Grouped operations control the channel-wise connections between an input X ∈ R n×c and an output Y ∈ R n×d to reduce the number of parameters by g, the number of groups <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref>. If c = g, the operation is performed independently over every channel <ref type="bibr" target="#b5">(Chollet, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>Closely related to the JK networks <ref type="bibr" target="#b26">(Xu et al., 2018)</ref>, we are seeking for a way to node-adaptively craft receptive-fields for a specific task at hand. JK nets achieve this by dynamically jumping to the most representive layer-wise embedding after a fixed range of node representations were obtained. Hence, Jumping Knowledge can not guarantee that higher-order features will not become "washed out" in later layers, but instead will just fall back to more localized information preserved from earlier representations. In addition, fine-grained details may still get lost very early on in expanderlike subgraph structures <ref type="bibr" target="#b26">(Xu et al., 2018)</ref>.</p><p>In contrast, we propose to allow jumps to earlier knowledge immediately while aggregating information from neighboring nodes. This results in a highly-dynamic receptive-field in which neighborhood information is potentially gathered from representations of differing locality. Each node's representation controls its own spread-out, possibly aggregating more global information in one branch, and falling back to more local information in others.</p><p>Formally, we allow each node-neighborhood pair (v, w) ∈ E to attend to all its former representations h</p><formula xml:id="formula_7">(1) w , . . . , h (t−1) w while using its output h (t)</formula><p>v←w for aggregation:</p><formula xml:id="formula_8">h (t) v = f (t) Θ h (t) v←v , h (t) v←w w∈N (v) where h (t) v←w = Attention h (t−1) v Θ (t) Q , h (1) w , . . . , h (t−1) w Θ (t) K , h (1) w , . . . , h (t−1) w Θ (t) V (4) with Θ (t) Q , Θ (t) K , Θ<label>(t)</label></formula><p>V ∈ R d×d denoting trainable symmetric projection matrices. A scheme of this layer is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. By ensuring that former information is preserved, our operator can be stacked deep by design, in particular without the need of JK nets.</p><p>In practice, we replace the single attention module by multi-head attention with a user-defined number of heads h while maintaining the same number of parameters. We implemented f  w , either preserving current state, previous state, or no state at all. In addition, self-attention is applied to retain central node information. graph convolutional operator from <ref type="bibr" target="#b14">Kipf &amp; Welling (2017)</ref>, although any other GNN layer may be applicable. Due to being already projected, we do not transform incoming node embeddings in f (t)</p><formula xml:id="formula_9">(t) Θ as the t=1 t=2 h v + h (3) v←v + + + N (v) f (3) Θ , + self-</formula><p>Θ . Furthermore, we incorporate an additional parameter to the softmax distribution of the attention module to allow the model to refuse the aggregation of individual neighboring embeddings in order to preserve fine-grained details (cf. <ref type="figure" target="#fig_0">Figure 1</ref>). Instead of actually overparametrizing the resulting distribution, we restrict this parameter to be fixed <ref type="bibr" target="#b10">(Goodfellow et al., 2016)</ref>. This results in a softmax function of the form</p><formula xml:id="formula_10">softmax( x) i = exp(x i ) 1 + j exp(x j ) ∈ R n , where x ∈ R n .<label>(5)</label></formula><p>Feature dimensionality. In order to leverage the attention module, input and output feature dimensionality are forced to remain equal across all layers. We found this to be only a weak constraint since this is already common practice <ref type="bibr" target="#b9">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b26">Xu et al., 2018)</ref>.</p><p>Regularization. We apply dropout <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> to the softmax-normalized attention weights and use grouped linear projections with g groups to reduce the number of parameters from d 2 to d 2 /g, where g must be chosen so that max(g, h) is divisible by min(g, h). The grouped projections regulate the attention heads by forcing them to only have a local influence on other attention heads (or even restricting them to have no influence at all). We observed that these adjustments greatly help the model to avoid overfitting while still maintaining large effective hidden sizes.</p><p>Runtime. Our proposed operator does scale linearly in the number of previously seen node representations for each edge, i.e. O(|E|T ). To account for large T , we suggest to restrict the inputs of the attention module to a fixed-sized subset of former representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluate our approach on 8 transductive benchmark datasets: the tasks of classifying academic papers (Cora, CiteSeer, PubMed, Cora Full) <ref type="bibr" target="#b19">(Sen et al., 2008;</ref>, active research fields of authors (Coauthor CS, Coauthor Physics) <ref type="bibr" target="#b20">(Shchur et al., 2018)</ref> and product categories (Amazon Computers, Amazon Photo) <ref type="bibr" target="#b20">(Shchur et al., 2018)</ref>. We randomly split nodes into 20%, 20% and 60% for training, validation and testing. Descriptions and statistics of all datasets can be found in Appendix A. The code with all its evaluation examples is integrated into the PyTorch Geometric 1 library . Setup. We compare our DNA approach to GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref> and GAT  with and without Jumping Knowledge, closely following the network architectures of <ref type="bibr" target="#b26">Xu et al. (2018)</ref>: We first project node features separately into a lower-dimensional space, apply a number of GNN layers ∈ {1, 2, 3, 4, 5} with effective hidden size ∈ {16, 32, 64, 128} and ReLU non-linearity, and perform the final prediction via a fully-connected layer. All models were implemented using grouped linear projections and evaluated with the number of groups ∈ {1, 8, 16}.</p><p>We use the Adam optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 0.005 and stop training early with a patience value of 10. We apply a fixed dropout rate of 0.5 before and after GNN layers and add a 2 regularization of 0.0005 to all model parameters. For our proposed model and GAT, we additionaly tune the number of heads ∈ {8, 16} and set the dropout rate of attention weights to 0.8. Hyperparameter configurations of the best performing models with respect to the validation set are reported in Appendix B.</p><p>Results. <ref type="table" target="#tab_1">Table 1</ref> shows the average classification accuracy over 10 random data splits and initializations. Our DNA approach outperforms traditional stacking of GNN layers (JK-None) and even exceeds the performance of Jumping Knowledge in most cases. Noticeably, the use of grouped linear projections greatly improves attention-based approaches, especially when combined with a large effective hidden size. We noticed gains in accuracy up to 3 percentage points when comparing the best results of g = 1 to g &gt; 1, both for GAT and DNA, especially when combined with a large effective hidden size. Best hyperparameter configurations (cf. Appendix B) show advantages in using increased feature dimensionalities across all datasets. For GCN, we found those gains to be negligible. Similar to JK nets, our approach benefits from an increased amount of stacked layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QUALTIVATE ANALYSIS ON CORA</head><p>We use the (normalized) influence score <ref type="bibr" target="#b26">(Xu et al., 2018)</ref> to visualize the differences in aggregation starting at a node which is correctly classified by DNA, but is incor- <ref type="figure">Figure 2</ref>: Influence distributions of different 5-layer GNNs starting at the squared node. Due to visibility, we visualize only its 2-hop neighborhood.  <ref type="figure">Figure 2)</ref>. While the node embedding of GCN JK-Pool is nearly exclusively influenced by its central node and a node nearby, DNA aggregates localized information even from nodes far away. <ref type="figure" target="#fig_1">Figure 3</ref> signals that aggregations typically attend to earlier representations. This verifies that nearby information is indeed often sufficient to classify most nodes. However, there are some nodes that do make heavy usage of information retrieved from latter representations, indicating the merits of a dynamic neighborhood aggregation procedure.</p><formula xml:id="formula_11">I v (w) = 1 ∂ h (T ) v ∂ h (0) w 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced a dynamic neighborhood aggregation (DNA) scheme which computes new embeddings for a node by attending to all previous embeddings of its neighbors. This dynamic aggregation allows the model to learn to use specific receptive fields and depths for a given task and naturally solves the problems of exponential spread-outs and "washed out" representations when naively stacking GNN layers. In contrast to JK nets, our DNA scheme enables fine-grained node representations in which both local and global information can effectively be combined across different neighborhood branches. Finally, we showed empirically that grouped operations can be an effective regularizer for attention heads which can additionally enable the usage of larger feature dimensionalities in GNNs. Coauthor CS and Coauthor Physics <ref type="bibr" target="#b20">(Shchur et al., 2018)</ref> are co-authorship graphs where nodes are authors which are connected by an edge if they co-authored a paper. Given paper keywords for each author's paper as node features, the task is to map each author to its most active field of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASETS</head><p>Amazon Computers and Amazon Photo <ref type="bibr" target="#b20">(Shchur et al., 2018)</ref> are segments of the Amazon copurchase graph where nodes represent goods which are linked by an edge if these goods are frequently bought together. Node feature encode product reviews as bag-of-word feature vectors, and class labels are given by product category.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPERPARAMETER CONFIGURATIONS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Given current node representation h (2) v as query, a node-adaptive embedding h (3) v←w gets computed for all neighbors w ∈ N (v) based on their former representations h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Final attention weight distribution of a 5-layer DNA-GNN.∅ t = 1 t = 2 t = 3 t = 4 t = GCN JK-Pool (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of our DNA approach, in comparison to GCN and GAT with and without Jumping Knowledge. Accuracy and standard deviations are computed from 10 random data splits. ± 0.98 73.87 ± 0.81 86.93 ± 0.25 62.55 ± 0.60 92.90 ± 0.14 95.90 ± 0.16 89.32 ± 0.20 93.11 ± 0.27 JK-Concat 83.99 ± 0.72 73.77 ± 0.89 87.52 ± 0.25 65.62 ± 0.49 95.44 ± 0.32 96.71 ± 0.15 90.27 ± 0.28 94.74 ± 0.29 JK-Pool 84.36 ± 0.62 73.86 ± 0.97 87.61 ± 0.27 65.14 ± 0.81 95.47 ± 0.21 96.74 ± 0.17 90.30 ± 0.37 94.64 ± 0.24 JK-LSTM 80.46 ± 0.88 72.92 ± 0.69 87.38 ± 0.29 55.39 ± 0.40 94.40 ± 0.28 96.55 ± 0.08 90.06 ± 0.23 94.54 ± 0.30 GAT JK-None 86.35 ± 0.74 73.70 ± 0.53 86.76 ± 0.25 65.70 ± 0.32 93.54 ± 0.17 96.21 ± 0.08 88.02 ± 1.39 93.00 ± 0.42 JK-Concat 84.70 ± 0.57 73.97 ± 0.46 88.73 ± 0.30 66.18 ± 0.47 95.12 ± 0.18 96.66 ± 0.09 89.67 ± 0.59 94.93 ± 0.31 JK-Pool 83.91 ± 0.87 73.42 ± 0.71 88.44 ± 0.33 61.52 ± 1.17 94.84 ± 0.16 96.62 ± 0.06 89.42 ± 0.47 94.80 ± 0.24 JK-LSTM 78.08 ± 1.53 71.84 ± 1.20 87.85 ± 0.26 55.41 ± 0.35 94.09 ± 0.23 96.45 ± 0.05 87.26 ± 1.82 94.47 ± 0.33 DNA g = 1 83.88 ± 0.50 73.37 ± 0.83 87.80 ± 0.25 63.72 ± 0.44 94.02 ± 0.17 96.49 ± 0.10 90.52 ± 0.40 94.89 ± 0.26 g = 8 85.86 ± 0.45 74.19 ± 0.66 88.04 ± 0.17 66.50 ± 0.42 94.46 ± 0.15 96.58 ± 0.09 90.99 ± 0.40 94.96 ± 0.24 g = 16 86.15 ± 0.57 74.50 ± 0.62 88.04 ± 0.22 66.64 ± 0.47 94.64 ± 0.15 96.53 ± 0.10 90.81 ± 0.38 95.00 ± 0.19</figDesc><table><row><cell></cell><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora Full</cell><cell>Coauthor CS</cell><cell>Coauthor Physics</cell><cell>Amazon Computers</cell><cell>Amazon Photo</cell></row><row><cell>GCN</cell><cell>JK-None</cell><cell>83.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics of the transductive node-classification experiments.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="3">Edges Features Classes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>1,433</cell><cell>7</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell>4,552</cell><cell>3,703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell>Cora Full</cell><cell>19,793</cell><cell>63,421</cell><cell>8,710</cell><cell>70</cell></row><row><cell>Coauthor CS</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell></row><row><cell>Coauthor Physics</cell><cell cols="2">34,493 247,962</cell><cell>8,415</cell><cell>5</cell></row><row><cell cols="3">Amazon Computers 13,752 245,861</cell><cell>767</cell><cell>10</cell></row><row><cell>Amazon Photo</cell><cell cols="2">7,650 119,081</cell><cell>745</cell><cell>8</cell></row><row><cell cols="5">Cora, CiteSeer, PubMed and Cora Full (Sen et al., 2008; Bojchevski &amp; Günnemann, 2018) are ci-</cell></row><row><cell cols="5">tation network datasets where nodes represent documents, and edges represent (undirected) citation</cell></row><row><cell cols="4">links. The networks contain bag-of-words feature vectors for each document.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameter configuration (number of layers / effective hidden size / number of groups) of the best GCN models with respect to the validation set.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell cols="2">CiteSeer PubMed</cell><cell>Cora Full</cell><cell cols="2">Coauthor Coauthor CS Physics</cell><cell>Amazon Computers</cell><cell>Amazon Photo</cell></row><row><cell>JK-None</cell><cell>1/128/16</cell><cell>1/128/8</cell><cell>1/16/1</cell><cell cols="2">1/128/16 1/128/16</cell><cell>1/32/16</cell><cell>1/128/16</cell><cell>1/64/16</cell></row><row><cell cols="2">JK-Concat 2/128/8</cell><cell>2/64/8</cell><cell>2/16/16</cell><cell>2/128/8</cell><cell>2/128/1</cell><cell>3/64/1</cell><cell>1/128/1</cell><cell>3/128/1</cell></row><row><cell>JK-Pool</cell><cell>2/128/1</cell><cell>2/128/1</cell><cell>2/16/16</cell><cell cols="2">5/128/16 5/128/16</cell><cell>5/64/1</cell><cell>1/128/8</cell><cell>3/128/16</cell></row><row><cell>JK-LSTM</cell><cell>1/128/8</cell><cell>1/128/1</cell><cell>2/16/8</cell><cell>1/128/8</cell><cell>1/64/1</cell><cell>1/64/8</cell><cell>1/128/16</cell><cell>1/64/1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameter configuration (number of layers / effective hidden size / number of groups / number of heads) of the best GAT models with respect to the validation set.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora Full</cell><cell cols="2">Coauthor Coauthor CS Physics</cell><cell>Amazon Computers</cell><cell>Amazon Photo</cell></row><row><cell>JK-None</cell><cell cols="2">3/128/16/8 1/128/16/8</cell><cell>1/64/8/8</cell><cell cols="2">1/128/16/8 1/128/8/8</cell><cell>1/128/1/8</cell><cell cols="2">1/128/1/16 1/128/8/16</cell></row><row><cell cols="2">JK-Concat 2/128/1/8</cell><cell>2/128/1/8</cell><cell cols="3">5/128/16/8 5/128/8/16 3/128/1/8</cell><cell>2/128/1/8</cell><cell cols="2">2/128/8/16 2/128/8/16</cell></row><row><cell>JK-Pool</cell><cell cols="6">5/128/1/16 4/128/1/16 3/128/16/16 2/128/1/16 2/128/1/16 1/128/1/8</cell><cell cols="2">2/128/1/16 2/128/8/16</cell></row><row><cell cols="3">JK-LSTM 1/128/1/16 1/128/1/16</cell><cell>2/16/1/8</cell><cell>1/128/1/8</cell><cell>1/64/1/16</cell><cell>1/64/1/8</cell><cell>1/64/1/16</cell><cell>1/64/1/8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter configuration (number of layers / effective hidden size / number of heads) of the best DNA models with respect to the validation set.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell cols="2">CiteSeer PubMed</cell><cell>Cora Full</cell><cell cols="2">Coauthor Coauthor CS Physics</cell><cell>Amazon Computers</cell><cell>Amazon Photo</cell></row><row><cell>g = 1</cell><cell>1/128/16</cell><cell>2/128/8</cell><cell>2/16/8</cell><cell cols="2">2/128/8 1/128/16</cell><cell>1/32/16</cell><cell>2/128/8</cell><cell>1/64/8</cell></row><row><cell>g = 8</cell><cell>4/64/8</cell><cell>3/128/16</cell><cell>2/64/8</cell><cell>3/128/8</cell><cell>1/64/8</cell><cell>1/64/8</cell><cell>2/128/16</cell><cell>1/128/8</cell></row><row><cell cols="2">g = 16 4/128/8</cell><cell>4/128/8</cell><cell>2/64/16</cell><cell cols="2">2/128/8 1/128/16</cell><cell>1/128/16</cell><cell>1/128/16</cell><cell>1/128/16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rusty1s/pytorch_geometric</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been supported by the German Research Association (DFG) within the Collaborative Research Center SFB 876, Providing Information by Resource-Constrained Analysis, project A6. I thank Jan E. Lenssen for proofreading and helpful advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of attributed graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<editor>NeurIPS-W</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<title level="m">Weisfeiler and Leman go neural: Higher-order graph neural networks. In AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-W</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
