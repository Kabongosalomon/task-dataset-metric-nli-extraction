<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Komorowski</surname></persName>
							<email>jacek.komorowski@pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Wysoczańska</surname></persName>
							<email>monika.wysoczanska.dokt@pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
							<email>tomasz.trzcinski@pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Tooploox</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multimodal place recognition</term>
					<term>global descriptor</term>
					<term>deep metric learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a discriminative multimodal descriptor based on a pair of sensor readings: a point cloud from a LiDAR and an image from an RGB camera. Our descriptor, named MinkLoc++, can be used for place recognition, relocalization and loop closure purposes in robotics or autonomous vehicles applications. We use late fusion approach, where each modality is processed separately and fused in the final part of the processing pipeline. The proposed method achieves state-ofthe-art performance on standard place recognition benchmarks. We also identify dominating modality problem when training a multimodal descriptor. The problem manifests itself when the network focuses on a modality with a larger overfit to the training data. This drives the loss down during the training but leads to suboptimal performance on the evaluation set. In this work we describe how to detect and mitigate such risk when using a deep metric learning approach to train a multimodal neural network. Our code is publicly available on the project website. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Place recognition is an important task in computer vision with a broad range of practical applications, such as loopclosure in SLAM <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or autonomous driving <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Early methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[8]</ref> mostly rely on images from RGB cameras and are affected by illumination changes and different weather conditions. To mitigate this problem and improve the robustness, autonomous vehicles are nowadays equipped with additional sensors, such as LiDAR. LiDAR provides information about the geometry of the observed scene in the form of a sparse point cloud. It's more robust to adverse environmental conditions than RGB camera and can provide reliable data in limited visibility conditions (e.g. at night). On the other hand, LiDARs cannot capture fine details of the observed scene, such as a texture of visible buildings or other objects, that might be helpful in localization task. The intuitive approach to improve place recognition performance is to combine these two modalities. Such multimodal approach is an interesting machine learning problem that has gained attention recently but remains underexplored <ref type="bibr" target="#b9">[9]</ref>. The main research question is how to efficiently combine different modalities to fully exploit their complementary character.</p><p>Place recognition is often formulated as an instance retrieval problem. Given a query instance, such as an RGB image, the aim is to find its closest match, with a known location, in a large-scale database. This idea is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In this work, we focus on learning a discriminative global descriptor for large-scale place recognition purposes using multimodal data: an image from an RGB camera and a 3D point cloud from a LiDAR. We use a late fusion approach to obtain our multimodal descriptor, where two modalities are processed separately and merged in the final part of the processing pipeline. Our method yields the state-of-the-art results on the Oxford RobotCar <ref type="bibr" target="#b10">[10]</ref> and KITTI datasets <ref type="bibr" target="#b11">[11]</ref>. In our initial experiments we observed an interesting behaviour, when training the multimodal network using late fusion of two unimodal descriptors. The network trained with two modalities performs worse compared to using only one modality. Further investigation shows, that this counterintuitive result is caused by different susceptibility to overfitting of two feature extraction subnetworks. The network learns to focus on the modality giving a smaller training errorbut because of much larger overfitting, producing poor results on the evaluation set. The same phenomenon is observed in the context of training multimodal classifiers in <ref type="bibr" target="#b12">[12]</ref>, where authors propose so-called Gradient-Blending approach based on overfitting-to-generalization ratio of each modality. In this work we use deep metric learning approach with a triplet loss and propose a similar solution based on extending the loss function with additional terms based on individual modalities.</p><p>In summary the main contributions of our work are as follows.</p><p>• We design a discriminative multimodal descriptor for place recognition purposes based on two modalities: a 3D point cloud from a LiDAR scan and an image from RGB camera. The proposed method advances state of the art. • We propose an effective solution to the problem of dominating modality which adversely affects the discriminability of a multimodal descriptor. The problem happens when one modality exhibits significantly higher overfitting to the training set than the other one. The remainder of this paper is organized as follows: Sec.II gives a brief overview of the related work. Sec.III introduces our method for multimodal place recognition. Sec.IV shows the experimental results and Sec.V concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image-based place recognition</head><p>The majority of image retrieval methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[13]</ref> builds upon local features aggregation techniques such as Fisher Vectors <ref type="bibr" target="#b14">[14]</ref> or VLAD <ref type="bibr" target="#b15">[15]</ref>. Previously used handcrafted features are currently replaced with convolutional neural networks (CNNs), such as VGG <ref type="bibr" target="#b17">[16]</ref> or ResNet <ref type="bibr" target="#b18">[17]</ref>. In <ref type="bibr" target="#b4">[5]</ref> authors introduce NetVLAD, an end-to-end learnable method explicitly targeting the large-scale place recognition task. Its main component is a generalized VLAD layer for local feature aggregation. It can be plugged into any CNN-based feature extractor and trained using backpropagation. Similarly, Generalized Mean Pooling (GeM) proposed in <ref type="bibr" target="#b19">[18]</ref> is another aggregation method that can be appended a CNN backbone and trained end-to-end. GeM layer generalizes max and average pooling and proved successful in instance retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point cloud-based place recognition</head><p>PointNetVLAD <ref type="bibr" target="#b20">[19]</ref> is the first deep network to generate a global point cloud descriptor for large-scale place recognition. The method uses PointNet architecture <ref type="bibr" target="#b21">[20]</ref> to extract local features, followed by NetVLAD aggregation layer. However PointNet-based architecture is not well suited to capture local geometric structures and extract informative local features. LPD-Net <ref type="bibr" target="#b22">[21]</ref> addresses this weakness by enhancing an input point cloud with handcrafted features and using graph neural networks to extract local contextual information. EPC-Net <ref type="bibr" target="#b23">[22]</ref> improves upon LPD-Net by using proxy point convolutional neural network. SOE-Net <ref type="bibr" target="#b24">[23]</ref> improves extraction of local contextual information by introducing the PointOE module, which captures local geometric structures from eight spatial orientations.</p><p>All of the previously mentioned methods are based on unordered set of points representation. MinkLoc3D <ref type="bibr" target="#b25">[24]</ref> uses an alternative representation and is based on sparse voxelized representation. It uses 3D convolutional architecture modelled upon a Feature Pyramid Network (FPN) <ref type="bibr" target="#b26">[25]</ref> design pattern to extract informative local features. Local features are aggregated using GeM <ref type="bibr" target="#b19">[18]</ref> layer into a discriminative global descriptor. MinkLoc3D yields state-of-the-art results on standard benchmarks outperforming other other point cloudbased global descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LiDAR and RGB camera fusion for multimodal place recognition</head><p>Methods for multimodal place recognition can be categorized by a modality fusion strategy. The first group creates augmented images based on structural (3D) information, thus approaches modality fusion problem in the early stages of the processing pipeline. CORAL, a bi-modal descriptor presented in <ref type="bibr" target="#b27">[26]</ref>, builds an elevation image from an input point cloud. Elevation image is enhanced with projected RGB image features extracted at multiple scales and processed using a deep neural network. Extracted local features are aggregated using NetVLAD layer. In <ref type="bibr" target="#b1">[2]</ref> an input RGB image is concatenated with LiDAR intensity image and processed using a convolutional neural network.</p><p>The most common approach in multimodal place recognition is late fusion. In <ref type="bibr" target="#b28">[27]</ref> a point cloud descriptor is obtained using PointNetVLAD architecture, with NetVLAD aggregation layer altered to ignore non-informative 3D points. Imagebased descriptor is extracted using ResNet50 <ref type="bibr" target="#b18">[17]</ref> architecture. Two unimodal descriptors are concatenated and processed by a fully-connected layer to produce the multimodal descriptor. <ref type="bibr" target="#b29">[28]</ref> enhances late fusion with a global channel attention layer (GCA). In <ref type="bibr" target="#b30">[29]</ref> image-based place recognition is augmented with structural cues constructed from RGB video stream using structure-from-motion. The reconstructed 3D point segments are discretized into a regular voxel grid and processed using 3D convolutional neural network. Features extracted from a 3D point cloud and an RGB image are merged to form the multimodal descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD DESCRIPTION</head><p>This section presents our MinkLoc++ method to compute a discriminative global descriptor for place recognition purposes. The descriptor is based on two modalities: an image from an RGB camera and a point cloud from a LiDAR installed on a moving vehicle. Place recognition is performed by searching the database of geo-tagged pairs of sensor readings (RGB image and 3D point cloud) for descriptors closest to the descriptor of the query pair. The idea is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>We use late fusion approach where modalities are processed separately and combined in the final part of the processing pipeline. The late fusion approach is highly flexible and robust if one sensor fails.</p><p>The high-level network architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The network consists of two branches: the upper branch computes a point cloud descriptor D P C ∈ R k and the lower branch computes an RGB image descriptor D RGB ∈ R k . In our experiments we use k = 128.</p><p>The upper branch uses an enhanced version of Min-kLoc3D <ref type="bibr" target="#b25">[24]</ref> architecture, that proved successful in point cloud-based place recognition task. We improve the design  by incorporating ECA <ref type="bibr" target="#b31">[30]</ref> channel attention mechanism originally proposed for 2D convolutional networks. The input point cloud is quantized into a sparse voxelized representation and processed by a point cloud feature extraction block, shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. It is a 3D convolutional network based on a Feature Pyramid Network <ref type="bibr" target="#b32">[31]</ref> design pattern. First, the input point cloud is processed in the bottom-up part direction producing 3D feature maps with decreasing spatial resolution and increasing receptive field. Each block in the bottomup pass consists of a series of 3D convolutions with skip connections followed by ECA <ref type="bibr" target="#b31">[30]</ref> channel attention layer. See <ref type="figure" target="#fig_5">Fig. 4</ref> for details. Then, the transposed convolution upsamples the feature map generated by the last convolutional block. Upsampled feature map is concatenated with features from the corresponding layer of the bottom-up pass using a lateral connection. This design produces a feature map with relatively high spatial resolution and large receptive field. Our initial experiments proved, that pooling 3D features from feature map with higher spatial resolution is advantageous over using lower resolution maps computed by deeper networks. The resultant sparse 3D feature map F P C is pooled using a generalizedmean (GeM) pooling <ref type="bibr" target="#b19">[18]</ref> layer to produce a point cloud descriptor</p><formula xml:id="formula_0">D P C ∈ R k .</formula><p>The lower branch uses first four blocks of a pre-trained ResNet18 <ref type="bibr" target="#b18">[17]</ref> to extract a 2D feature map with 256 channels. The number of channels is reduced to k using a convolutional layer with 1x1 filter, producing an RGB image feature map F RGB . The feature map is pooled using a GeM pooling to produce an RGB image descriptor D RGB ∈ R k .</p><p>Descriptor aggregation block combines a point cloud descriptor D P C and an image descriptor D RGB to produce a final 2k-dimensional multimodal descriptor. We use a simple concatenation along channels dimension to merge two descriptors. In ablation study we evaluate other options and discuss why this simplistic aggregation strategy gives better results than more sophisticated approaches.    </p><formula xml:id="formula_1">C 32 3 k 1s C 32 3 k 1s A Conv2 C 64 2 k 2s C 64 3 k 1s C 64 3 k 1s A Conv3 C 64 2 k 2s C 64 3 k 1s C 64 3 k 1s A 1x1x1Conv2, 3 C 128 1 k 1s 1x1x1Conv3 C 128 1 k 1s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network training</head><p>To train the network to produce discriminative global descriptors we use a deep metric learning approach <ref type="bibr" target="#b33">[32]</ref> with a triplet margin loss <ref type="bibr" target="#b34">[33]</ref>. Each mini-batch element is a pair of readings: a 3D point cloud and a corresponding RGB image. Mini-batch elements are arranged into triplets consisting of an anchor, a positive and a negative example. The positive example is a batch element similar to the anchor and a negative example is a batch element dissimilar to the anchor. Two elements are similar if they show the same place, that is if they were acquired at locations at most 10 meters apart. Elements are dissimilar, if they represent different locations, that is they were captured at locations more than 50 meters apart. If a distance between locations is within 10-50 meters range, the elements are considered neither similar nor dissimilar. To increase the training efficiency, we use batch-hard negative mining <ref type="bibr" target="#b34">[33]</ref> to build informative triplets.</p><p>The loss function L, defined by Eq. 1, is a sum of three terms: the first is based on the fused multimodal descriptor D, the second on the a point cloud descriptor D P C and third on an RGB image descriptor D RGB .</p><formula xml:id="formula_2">L = (1 − α − β)L F + αL P C + βL RGB ,<label>(1)</label></formula><p>where α, β, γ are experimentally chosen weights and each component L F , L P C , L RGB is a triplet margin loss function <ref type="bibr" target="#b34">[33]</ref> defined as:</p><formula xml:id="formula_3">L (a i , p i , n i ) = max {d(a i , p i ) − d(a i , n i ) + m, 0} ,<label>(2)</label></formula><p>where d(x, y) = ||x − y|| 2 is an Euclidean distance between descriptors x and y; a i , p i , n i are descriptors of an anchor, a positive and a negative element in i-th triplet and m is a margin. For each batch, L F is computed from triplets constructed from multimodal descriptors D; L P C from triplets constructed from point cloud descriptors D P C ; and L RGB from triplets constructed from RGB image descriptors D RGB . <ref type="figure" target="#fig_1">Fig. 2</ref> shows the relation of each loss component to the processing pipeline. The rationale for using multiple terms in the loss function is the dominating modality problem discovered during our initial experiments. We noticed that a multimodal descriptor, trained with a standard triplet loss, had an inferior performance on the evaluation set compared to the best unimodal descriptor. This counter-intuitive result is caused by the fact that one modality (RGB image in our case) performs much better on the training set and much worse on the evaluation set than the other one (3D point cloud), due to much larger overfitting. Thus, the network learns to focus on the modality with smaller training error, but greater evaluation error, largely ignoring the other one. We mitigate this problem by extending the loss function with additional terms L P C and L RGB based on unimodal descriptors D P C and D RGB produced by two unimodal subnetworks (see <ref type="figure" target="#fig_1">Fig. 2</ref>). This simple solution allows balancing the influence each modality and improves performance of the fused multimodal descriptor.</p><p>The same phenomenon was observed in the context of training multimodal classifiers in <ref type="bibr" target="#b12">[12]</ref>. It proposes Gradient-Blending, where weights of each loss term are based on overfitting-to-generalization ratio of each single modality subnetwork. This approach cannot be directly applied when training a multimodal descriptor using a deep metric learning approach. When batch-hard negative mining is used, the loss function often fluctuates or stagnates for a number of epochs as harder and harder triplets are constructed. Instead, the number of active triplets (i.e. triplets with non-zero loss) constructed from each unimodal descriptor is much better indicator of overfitting. Hence, we find the dominating modality (RGB image) by examining the ratios of active triplets in training and validation set using each unimodal descriptor. Weights α and β in Eq. 1 are chosen to maximize the performance on the validation set. It should be noted, that it's sufficient to tune only one parameter in Eq. 1, that is the weight for the nondominating modality. The weight for the dominating modality can be set to 0, as parameters of its descriptor extraction subnetwork will nevertheless be optimized during the learning due to their influence on the fused multimodal loss L F term. See ablation study section for evidence how multi-head loss improves discriminability of the multimodal descriptor.</p><p>The loss function is minimized using a stochastic gradient descent with Adam optimizer. Similar to <ref type="bibr" target="#b25">[24]</ref>, we use dynamic batch sizing. The training starts with a small batch size (32 elements) and the batch size is gradually increased up to 128 elements as the training progress and the network learns more discriminative embeddings. The number of active triplets (i.e. triplets with non-zero loss) is monitored and when falls below the threshold, the batch size is increased. This prevents the training process from collapse, which could happen in early epochs when mining hard triplets from a large batch, and allows using larger batch to mine more difficult triplets later on.</p><p>We use data augmentation to improve generalization and reduce the overfitting. For RGB images, we use photometric distortions, random crop and random erasing augmentation <ref type="bibr" target="#b35">[34]</ref>. For 3D points clouds, we use a random jitter and random points removal. We also adapted random erasing augmentation to remove 3D points within the randomly selected frontoparallel cuboid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>In all experiments we quantize 3D point coordinates with 0.01 quantization step. As point coordinates in the Oxford RobotCar dataset are normalized to be within [−1, 1] range, this gives up to 200 voxels in each spatial direction. The initial learning rate for network parameters in the RGB image feature extraction block is set to 10 −4 and for all other parameters to 10 −3 . In all experiments we train the network for 50 epochs, reducing the learning rate by 10 at the end of 30 epoch. The dimensionality of point cloud and RGB image descriptors is set to k = 128, and the multimodal descriptor has 2k = 256 dimensions. To prevent embedding collapse in early epochs of training, we use a dynamic batch sizing strategy. The initial batch size is set to 8. When the number of active triplets falls below 70% of the current batch size, the batch is increased by 40% until the maximum size of 160 elements is reached. To limit overfitting we use L 2 weight regularization with λ = 10 −3 coefficient. The coefficients of the loss terms in Eq. 1 are α = 0.5, β = 0.</p><p>All experiments are performed on a server with a single nVidia RTX 2080Ti GPU, 12 core AMD Ryzen Threadripper 1920X processor and 64 GB of RAM. We use PyTorch 1.7 <ref type="bibr" target="#b36">[35]</ref> deep learning framework, MinkowskiEngine 0.4.3 <ref type="bibr" target="#b37">[36]</ref> autodifferentiation library for sparse tensors and Pytorch Metric Learning library 0.9.96 <ref type="bibr" target="#b38">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>This section describes results of an experimental evaluation of our global descriptor and comparison with state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and evaluation methodology</head><p>Our method is trained and evaluated on a modified Oxford RobotCar dataset <ref type="bibr" target="#b10">[10]</ref>. The dataset is created using a suite sensors (RGB cameras, LiDAR sensors, GPS/INS) mounted on the car travelling multiple times through the same route in the city of Oxford at different times of day and year. We build our dataset by enhancing a dataset of 3D point clouds introduced in <ref type="bibr" target="#b20">[19]</ref>, constructed from RobotCar LiDAR scans. Point clouds are build from consecutive 2D LiDAR scans during the 20 m. drive of a vehicle. The ground plane is removed and point clouds are downsampled to 4096 points. For each point cloud we find in an original RobotCar dataset corresponding RGB images with the closest timestamps taken from the center camera. Each image is downsampled from 1280x960 to 320x200 resolution. During training, to increase data diversity and limit overfitting, we randomly sample from 15 closest RGB images. In evaluation we use only one RGB image with the closest timestamp. Exemplary dataset elements are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Elements are similar (positive examples) if they are at most 10m apart and dissimilar (negative examples) if they are at least 50m apart. Elements between 10 and 50m apart are neither similar nor dissimilar. The dataset is split into disjoint training (21.7k elements) and test (3k elements) areas based on UTM coordinates.</p><p>We follow the same evaluation protocol and use the same train/test split (so called Baseline scenario), as introduced in <ref type="bibr" target="#b20">[19]</ref> and used in later works <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b25">[24]</ref>. An element (point cloud with a corresponding RGB image) from a test dataset is taken as a query. Pairs of point clouds and RGB images from other traversals covering the same region but captured at different times form the database. Recall@N measures the percentage of correctly localized queries using top-N elements returned from the database. Localization is correct, if at least one of top-N retrieved database elements is within 25 meter distance threshold from the ground truth position of the query element. In addition to Average Recall@1 (AR@1) we report Average Recall@1% (AR@1%), which is calculated taking into account top-k matches, where k is 1% of the database size.</p><p>Additionally, we evaluate the generalization ability of our methods using KITTI Odometry dataset. The dataset consists of RGB camera images, as well as LIDAR scans acquired with Velodyne laser scanner. Therefore, the characteristics of input point clouds differs significantly from the scans in Oxford RobotCar dataset. We follow the same procedure as introduced in <ref type="bibr" target="#b27">[26]</ref>. We take Sequence 00 which visits the same places repeatedly and construct the reference database using the data gathered during the first 170 seconds. The rest is used as localization queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation results</head><p>We compare the performance of our multimodal descriptor with point cloud-only descriptors: PointNetVLAD <ref type="bibr" target="#b20">[19]</ref>, PCAN <ref type="bibr" target="#b39">[38]</ref>, LPD-Net <ref type="bibr" target="#b22">[21]</ref>, EPC-Net <ref type="bibr" target="#b23">[22]</ref>, SOE-Net <ref type="bibr" target="#b24">[23]</ref>, MinkLoc3D <ref type="bibr" target="#b25">[24]</ref> and multimodal descriptors based on a 3D point cloud and an RGB image: CORAL <ref type="bibr" target="#b27">[26]</ref>, PIC-Net <ref type="bibr" target="#b29">[28]</ref>. Where the source code or pretrained model is available (PointNetVAD, PCAN, LPD-Net, MinkLoc3D) we run the evaluation by ourselves, otherwise (EPC-Net, SOE-Net, CORAL, PIC-Net) we show results reported by authors on the same dataset and with identical evaluation protocol.</p><p>Evaluation results on the modified Oxford RobotCar dataset (Baseline scenario) are shown in Tab. I. Our MinkLoc++ model outperforms other multimodal methods. The improvement in AR@1% seems moderate (99.1 our method versus 98.2 for PIC-Net) but this metric is close to 100% and there's very little room for improvement. We also compared MinkLoc++ trained using only 3D modality to a number of unimodal, point cloud-based descriptors. Our method outperforms all of them, including recently proposed EPC-Net and SOE-Net. It can be noticed, that overall improvement from using multimodal approach compared to 3D-based unimodal is limited. The best unimodal descriptor has AR@1 94.5 compared to 96.7 obtained by multimodal MinkLoc++. This can be explained by two factors. First, the results are close to 100% and even 2 p.p. improvement is worth noticing. Second, RobotCar dataset was acquired at varying environmental conditions, with a number of images taken in limited visibility conditions (at night, very sunny or rainy day). 3D modality based on LiDAR scans is more reliable source of data than an RGB image.</p><p>To verify robustness of our method to different environmental conditions, we run additional evaluation on RobotCar Seasons <ref type="bibr" target="#b41">[40]</ref> benchmark. This is a challenging image-based dataset, prepared from Oxford RobotCar, with different splits corresponding to different atmospheric conditions and times of a day (e.g. snow, rain, dawn or night). We extend this dataset with 3D information to allow testing our multimodal descriptor. For each image, we find LiDAR readings with corresponding timestamps in the in original RobotCar dataset <ref type="bibr" target="#b10">[10]</ref> and use them to construct 3D point clouds. We compute descriptors for each element in the extended dataset and use them to retrieve elements from each dataset split. We compare our method with two image-based and two point cloud-based global descriptors. Tab. III reports the percentage of correctly localized queries   for each dataset split. The query is correctly localized if the pose of best matching element from the database is within 5 meter and 10 • threshold from the ground truth pose. In day conditions MinkLoc++ consistently outperforms all unimodal methods, except for rainy weather. The rain adversely affects LiDAR performance causing worse results. The improvement over image-based methods in good visibility conditions is only moderate and additional modality (3D) does not significantly improve the results. In night conditions, point cloud-based methods and our multimodal method show their potential and overcome image-based methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization evaluation</head><p>To test the generalization ability of our method, we evaluate the model trained on RobotCar dataset on KITTI dataset. We compare our results with <ref type="bibr" target="#b27">[26]</ref> by reproducing the evaluation procedure described in this paper. Our multimodal MinkLoc++ descriptor outperforms other methods. Our method can efficiently fuse two modalities to boost the performance. It must be noted, that point clouds in Kitti dataset have very different characteristic than point clouds in our training dataset. The former are build from a single 360 • sweep of a 3D LiDAR, the latter are constructed by merging multiple scans from a 2D LiDAR during the 20m. drive of a vehicle. This explains worse performance of our method using solely 3D modality compared to the results on RobotCar dataset. <ref type="figure">Fig. 6</ref> shows an example of a successful retrieval case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>In this section, we investigate the effects of the design choices on our multimodal descriptor performance. In all experiments, unless otherwise noted, the network is trained using the same dataset and hyperparameters as described in previous sections.</p><p>First, we compare our multi-head loss function with a straightforward approach where the loss is based only on the fused multimodal descriptor. Tab. IV shows the number of active triplets, at the end of the training, constructed from each unimodal descriptor and the performance of a fused multimodal descriptor, for different combination of weights in the loss function given by Eq. 1. Setting α and β to 0 (first row), means that only one term L F , based on the fused multimodal descriptor, is used. In this case, when using a training data there's much less (−52) active triplets for RGB image modality than for 3D modality. On the contrary, on validation data, there's more (+24) active triplets for RGB modality than 3D. This suggests, that the network overfits much more to RGB modality, which is the dominating modality. Because of the larger overfit, during the training the network concentrates on RGB modality, largely ignoring the other one. This leads to suboptimal performance of the fused multimodal descriptor on the evaluation set. The solution is to increase the weight of the loss term corresponding to the non-dominating modality (L P C in our case). This balances both modalities during the training and increases performance of the fused multimodal descriptor. Note, that the best performance of the multimodal descriptor is achieved when the difference between the number of active triplets for two unimodal descriptors is similar in both training and validation sets (rows 2 and 3 in Tab. IV with α = 0.5 and α = 0.8). Using multi-head loss function allows increasing the performance from 94.4% AR@1 (for α = β = 0) to 96.7% (for α = 0.5, β = 0). In earlier experiments we tried limiting overfitting for RGB modality by regularization or more aggressive data augmentation. But this had only a limited impact and didn't allow solving the above mentioned problem.</p><p>In Tab. V we compare discriminativity of the multimodal descriptor with its unimodal counterparts. To make a fair comparison we increase dimensionality of each unimodal descriptor to 256, the same size as the multimodal descriptor. Using multimodal approach improves discriminability of the global descriptor (96.7 AR@1) compared to its unimodal counterparts (86.8 for RGB and 94.5 for 3D). As expected, an RGB-based descriptor has the worst performance. Oxford RobotCar is a challenging dataset and image acquisition conditions vary significantly between different traversals. The same place visited during a sunny day and at dusk or at night looks differently. The improvement over 3D modality was more limited.</p><p>Tab. VI shows the impact of a feature pooling method on the performance of each unimodal descriptor. The following methods are evaluated: maximum activations of convolutions (MAC) <ref type="bibr" target="#b43">[42]</ref>, sum-pooled convolutional features (SPoC) <ref type="bibr" target="#b44">[43]</ref>, generalized-mean pooling (GeM) <ref type="bibr" target="#b19">[18]</ref>, NetVLAD <ref type="bibr" target="#b4">[5]</ref> and NetVLAD with Context Gating <ref type="bibr" target="#b45">[44]</ref> (NetVLAD-CG). For RGB modality, all examined methods, except for SPoC, performed similarly, with AR@1 varying between 86.1 and 86.9. For 3D modality, generalized-main pooling, with one trainable parameter, yields the best results (94.5 AR@1). Hence, we choose GeM as a feature pooling method for both modalities. It can be noted that more sophisticated feature aggregation methods (NetVLAD and NetVLAD with Context Gating) didn't improved the performance. This can be explained by the fact, that NetVLAD layer with a large number of trainable parameters (about 6 mln. in our configuration) increases overfitting which prevents an improvement on the evaluation set.</p><p>We also investigate different approaches for aggregation of two unimodal descriptors to form the multimodal descriptor. Two basic strategies are concatenation and summation of unimodal descriptors and the results are shown in the second column ('-') in Tab. VII. After this initial step, we optionally process the merged descriptor using a fully connected layer (column FC in the table) or two-layer MLP (column MLP). Interestingly, simple concatenation or addition of unimodal descriptors gives the best results. When concatenation is followed by a fully connected layer, AR@1 drops by 9 p.p. to 87.8. This behavior can again be attributed to the dominating modality problem. When a fully connected layer or MLP is used to process merged descriptors, the benefits or using multi-head loss function, with additional loss term for each modality, are lost. Added layers learn to extract information from dominating modality (RGB image in our case) from merged data and focus on it. Due to larger overfitting to the training set this leads to noticeably worse performance on the evaluation set. It can be noted that adding 2-layer MLP drops AR@1 to 85.0 (86.61), which is comparable with the performance of the unimodal descriptor based on the dominating RGB modality (86.8 AR@1). V. CONCLUSION In this paper we present MinkLoc++, a multimodal descriptor for place recognition purposes based on an input from a LiDAR scanner and RGB camera. Experimental evaluation proves that the proposed method outperforms state of the art on standard benchmarks. The performance of our method can be attributed to two factors. First, we use an efficient 3D convolutional network, based on sparse voxelized representation, for producing a discriminative point cloud descriptor. Enhancing MinkLoc3D <ref type="bibr" target="#b25">[24]</ref> architecture with a ECA channel attention <ref type="bibr" target="#b31">[30]</ref> improves results on 3D modality. Second, we show how to efficiently train multimodal descriptor avoiding dominating modality problem. Straightforward approach of computing the loss based only on the fused multimodal descriptor leads to suboptimal results. This is caused by the fact, that one modality (RGB image in our case) performs better on the training set, and much worse on the validation set, due to larger overfitting. The network learns to focus on the modality with better training performance leading to worse results on the evaluation set. We mitigate this problem by extending the loss function with additional terms based on unimodal descriptors computed in intermediary steps of the processing pipeline. This simple solution balances both modalities and improves performance of the fused descriptor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The project was funded by POB Research Centre for Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative Program -Research University (ID-UB) 1 https://github.com/jac99/MinkLocMultimodal [0.24,…, 0.41] Multimodal place recognition. MinkLoc++ computes a global descriptor from a pair of sensor readings: a 3D point cloud from LiDAR and an image from a RGB camera. Localization is performed by searching the database for a geo-tagged pair of readings with the closest descriptor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>High level MinkLoc++ architecture. Sensor readings (3D point cloud and RGB image) are processed by separate descriptor extraction networks, consisting of a local feature extraction block followed by a feature pooling layer. Resultant point cloud descriptor D P C and image descriptor D RGB are aggregated to produce a fused multimodal descriptor D. Green arrows show intermediary values used in the different terms of the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture of a point cloud feature extraction block. The input point cloud is quantized into a sparse 3D tensor and processed by a 3D convolutional network with FPN<ref type="bibr" target="#b32">[31]</ref> architecture, producing a point cloud feature map F P C Numbers in parentheses (e.g. 1/32) denote a stride and a number of channels of a feature map produced by each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Layers in the point cloud feature extraction block. All convolutions in Conv0 . . . 3 blocks are followed by batch norm and ReLU non-linearity. C denotes a 3D convolution with a number of filters given as the top-right index, t decorator indicates a transposed convolution, lower k shows a filter size and lower s a stride. A is ECA<ref type="bibr" target="#b31">[30]</ref> channel attention and . . . enclosures a residual block with a skip connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Example of successful retrieval using multimodal approach. (a) is the query element, (b) incorrect retrieval using 3D modality and (c) incorrect retrieval using RGB modality (d) correct retrieval using both modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Example of successful retrieval using multimodal approach. The top left is the query example. The top right is a successful retrieval based on multimodal input. The bottom left is incorrect retrieval using RGB input, and the bottom right is the one based on 3D input. Example of unsuccessful retrieval using both modalities. (a) is the query element, (b) incorrect match to the query and (c) the closest true match. Geometry of true and false matches is very similar and appearance of the query differs from the true match due to lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">EVALUATION RESULTS OF PLACE RECOGNITION METHODS ON ROBOT</cell></row><row><cell cols="3">CAR DATASET.</cell><cell></cell></row><row><cell></cell><cell cols="4">Modality AR@1% AR@1</cell></row><row><cell>PointNetVLAD [19]</cell><cell>3D</cell><cell></cell><cell>80.3</cell><cell>63.3</cell></row><row><cell>PCAN [38]</cell><cell>3D</cell><cell></cell><cell>83.8</cell><cell>70.7</cell></row><row><cell>DH3D-4096 [39]</cell><cell>3D</cell><cell></cell><cell>84.3</cell><cell>73.3</cell></row><row><cell>LPD-Net [21]</cell><cell>3D</cell><cell></cell><cell>94.9</cell><cell>86.4</cell></row><row><cell>EPC-Net [22]</cell><cell>3D</cell><cell></cell><cell>94.7</cell><cell>86.2</cell></row><row><cell>SOE-Net [23]</cell><cell>3D</cell><cell></cell><cell>96.4</cell><cell>89.3</cell></row><row><cell>MinkLoc3D [24]</cell><cell>3D</cell><cell></cell><cell>97.9</cell><cell>93.8</cell></row><row><cell>MinkLoc++ (our)</cell><cell>3D</cell><cell></cell><cell>98.2</cell><cell>93.9</cell></row><row><cell>CORAL [26]</cell><cell cols="2">3D + RGB</cell><cell>96.1</cell><cell>88.9</cell></row><row><cell>PIC-Net [28]</cell><cell cols="2">3D + RGB</cell><cell>98.2</cell><cell>-</cell></row><row><cell>MinkLoc++ (our)</cell><cell cols="2">3D + RGB</cell><cell>99.1</cell><cell>96.7</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell cols="5">EVALUATION RESULTS OF PLACE RECOGNITION METHODS ON KITTI</cell></row><row><cell cols="3">ODOMETRY DATASET.</cell><cell></cell></row><row><cell></cell><cell cols="3">Modality AR@1%</cell></row><row><cell cols="2">PointNetVLAD [19]</cell><cell>3D</cell><cell>72.4</cell></row><row><cell>LPD-Net [21]</cell><cell></cell><cell>3D</cell><cell>74.6</cell></row><row><cell>MinkLoc++ (our)</cell><cell></cell><cell>3D</cell><cell>72.6</cell></row><row><cell>MinkLoc++ (our)</cell><cell></cell><cell>RGB</cell><cell>76.2</cell></row><row><cell>CORAL [26]</cell><cell cols="2">3D + RGB</cell><cell>76.4</cell></row><row><cell>MinkLoc++ (our)</cell><cell cols="2">3D + RGB</cell><cell>82.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERCENTAGE</head><label>III</label><figDesc>OF CORRECTLY LOCALIZED QUERIES ON ROBOTCAR SEASONS BENCHMARK.TABLE IV IMPACT OF WEIGHTS OF EACH UNIMODAL LOSS TERM IN EQ.1 ON THE NUMBER OF ACTIVE TRIPLETS FOR EACH MODALITY AND THE PERFORMANCE (AR%1) OF THE MULTIMODAL DESCRIPTOR.TABLE VII IMPACT OF UNIMODAL DESCRIPTORS AGGREGATION METHOD ON THE DISCRIMINABILITY (AR@1) OF THE MULTIMODAL DESCRIPTOR. (-) ONLY CONCATENATE/ADD, FC = WITH A FULLY CONNECTED LAYER, MLP = WITH 2 LAYER MLP.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">day conditions</cell><cell>night conditions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dawn dusk</cell><cell>overcast summer</cell><cell>overcast winter</cell><cell>rain snow sun</cell><cell>night night-rain</cell></row><row><cell></cell><cell></cell><cell cols="3">DenseVLAD [41]</cell><cell>RGB</cell><cell></cell><cell>92.5 94.2</cell><cell>92.0</cell><cell>93.3 96.9 90.2 80.2</cell><cell>19.9</cell><cell>25.5</cell></row><row><cell></cell><cell></cell><cell cols="3">NetVLAD [5]</cell><cell>RGB</cell><cell></cell><cell>82.6 92.9</cell><cell>95.2</cell><cell>92.6 96.0 91.8 86.7</cell><cell>15.5</cell><cell>16.4</cell></row><row><cell></cell><cell></cell><cell cols="3">LPD-Net [21]</cell><cell>3D</cell><cell></cell><cell>79.7 79.9</cell><cell>79.7</cell><cell>73.8</cell><cell>-</cell><cell>-82.3</cell><cell>77.3</cell><cell>32.8</cell></row><row><cell></cell><cell></cell><cell cols="3">MinkLoc3D [24]</cell><cell>3D</cell><cell></cell><cell>89.2 88.3</cell><cell>90.3</cell><cell>83.1 66.3 86.3 87.4</cell><cell>86.1</cell><cell>58.0</cell></row><row><cell></cell><cell></cell><cell cols="3">MinkLoc++ (our)</cell><cell cols="2">3D+RGB</cell><cell>93.6 94.4</cell><cell>96.3</cell><cell>94.4 79.1 94.3 88.0</cell><cell>80.6</cell><cell>59.1</cell></row><row><cell></cell><cell></cell><cell cols="3">Active triplets train</cell><cell cols="3">Active triplets val</cell><cell></cell></row><row><cell>α β</cell><cell cols="3">D P C D RGB</cell><cell cols="3">D P C D RGB</cell><cell cols="2">AR@1</cell></row><row><cell>0 0</cell><cell></cell><cell>87</cell><cell cols="2">35 -52</cell><cell>95</cell><cell cols="2">119 +24</cell><cell>94.4</cell></row><row><cell>0.2 0</cell><cell></cell><cell>51</cell><cell>56</cell><cell>+5</cell><cell>83</cell><cell cols="2">124 +41</cell><cell>96.0</cell></row><row><cell>0.5 0</cell><cell></cell><cell>25</cell><cell cols="2">66 +41</cell><cell>75</cell><cell cols="2">121 +46</cell><cell>96.7</cell></row><row><cell>0.8 0</cell><cell></cell><cell>19</cell><cell cols="2">73 +54</cell><cell>71</cell><cell cols="2">123 +52</cell><cell>96.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">IMPACT OF EACH MODALITY ON THE DISCRIMINABILITY OF MINKLOC++</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">DESCRIPTOR.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Modality</cell><cell cols="3">AR@1% AR@1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>RGB</cell><cell></cell><cell></cell><cell>95.9</cell><cell>86.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">3D point cloud</cell><cell>98.3</cell><cell>94.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">RGB + 3D</cell><cell></cell><cell>99.1</cell><cell>96.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">IMPACT OF A FEATURE POOLING METHOD ON A DISCRIMINABILITY</cell></row><row><cell></cell><cell></cell><cell cols="5">(AR@1) OF UNIMODAL DESCRIPTORS.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">SPoC MAC GeM NetVLAD NetVLAD-GC</cell><cell></cell></row><row><cell cols="2">RGB</cell><cell cols="3">76.9 86.9 86.8</cell><cell>86.9</cell><cell></cell><cell>86.1</cell><cell></cell></row><row><cell>3D</cell><cell></cell><cell cols="3">86.7 91.6 94.5</cell><cell>89.2</cell><cell></cell><cell>89.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">FC MLP</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Concatenation 96.7 87.8 86.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Addition</cell><cell cols="3">96.7 90.7 85.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SegMatch: Segment based place recognition in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of camera-based and 3d lidar-based place recognition across weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Żywanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banaszczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale place recognition based on camera-lidar fused descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/20/10/2870" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual odometry and place recognition fusion for vehicle position tracking in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouerghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boutteau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Savatier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tlili</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/1424-8220/18/4/939" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fab-map: Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0278364908090961</idno>
		<ptr target="https://doi.org/10.1177/0278364908090961" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bags of binary words for fast place recognition in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1188" to="1197" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual place recognition with probabilistic voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hinzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: Integrating language, vision and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P17-5002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes training multimodal classification networks hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.07589" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image Classification with the Fisher Vector: Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-00830491" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<ptr target="http://lear.inrialpes.fr/pubs/2010/JDSP10" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural network based image classification using small training sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnetvlad: Deep point cloud based retrieval for large-scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lpd-net: 3d point cloud learning for large-scale place recognition and environment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2831" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient 3d point cloud feature learning for large-scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02374</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Soenet: A self-attention and orientation encoding network for point cloud based place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12430</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minkloc3d: Point cloud based large-scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<ptr target="http://arxiv.org/abs/1612.03144" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Coral: Colored structural representation for bi-modal place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale place recognition based on camera-lidar fused descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2870</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pic-net: Point cloud and image collaboration network for large-scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Augmenting visual place recognition with structural cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oertel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cieslewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5534" to="5541" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep metric learning for visual understanding: An overview of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pcan: 3d attention map learning using contextual information for point cloud based retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dh3d: Deep hierarchical 3d descriptors for robust large-scale 6dof relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Benchmarking 6dof outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8601" to="8610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICL 2016-RInternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregating local deep features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Yandex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
