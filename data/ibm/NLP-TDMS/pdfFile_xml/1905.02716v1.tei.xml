<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><forename type="middle">C K</forename><surname>Chan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects:</p><p>(1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable convolutions, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github.com/xinntao/EDVR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we describe our winning solution in the NTIRE 2019 challenges on video restoration and enhancement. The challenge releases a valuable benchmark, known as REalistic and Diverse Scenes dataset (REDS) <ref type="bibr" target="#b25">[26]</ref>, for the aforementioned tasks. In comparison to existing datasets, videos in REDS contain larger and more complex motions, making it more realistic and challenging. The competition enables fair comparisons among different algorithms and promotes the progress of video restoration.</p><p>Image restoration tasks such as super-resolution (SR) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref> and deblurring <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref> have experienced significant improvements over the last few years thanks to deep learning. The successes encourage the community to further attempt deep learning on the more challenging video restoration problems. Earlier studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11]</ref> treat video restoration as a simple exten-Bicubic RCAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDVR (Ours) DUF</head><p>Image SR Video SR <ref type="figure">Figure 1</ref>. A comparison between image super-resolution and video super-resolution (×4). RCAN <ref type="bibr" target="#b51">[52]</ref> and DUF <ref type="bibr" target="#b9">[10]</ref> are the state-ofthe-art methods of image and video super-resolution, respectively. sion of image restoration. The temporal redundancy among neighboring frames is not fully exploited. Recent studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref> address the aforementioned problem with more elaborated pipelines that typically consist of four components, namely feature extraction, alignment, fusion, and reconstruction. The challenge lies in the design of the alignment and fusion modules when a video contains occlusion, large motion, and severe blurring. To obtain high-quality outputs, one has to (1) align and establish accurate correspondences among multiple frames, and (2) effectively fuse the aligned features for reconstruction. Alignment. Most existing approaches perform alignment by explicitly estimating optical flow field between the reference and its neighboring frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b12">13]</ref>. The neighboring frames are warped based on the estimated motion fields. Another branch of studies achieve implicit motion compensation by dynamic filtering <ref type="bibr" target="#b9">[10]</ref> or deformable convolution <ref type="bibr" target="#b39">[40]</ref>. REDS imposes a great challenge to existing alignment algorithms. In particular, precise flow estimation and accurate warping can be challenging and timeconsuming for flow-based methods. In the case of large motions, it is difficult to perform motion compensation either explicitly or implicitly within a single scale of resolution. Fusion. Fusing features from aligned frames is another critical step in the video restoration task. Most existing methods either use convolutions to perform early fusion on all frames <ref type="bibr" target="#b1">[2]</ref> or adopt recurrent networks to gradually fuse multiple frames <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6]</ref>. Liu et al. <ref type="bibr" target="#b21">[22]</ref> propose a temporal adaptive network that can dynamically fuse across different temporal scales. None of these existing methods consider the underlying visual informativeness on each frame -different frames and locations are not equally informative or beneficial to the reconstruction, as some frames or regions are affected by imperfect alignment and blurring. Our Solution. We propose a unified framework, called EDVR, which is extensible to various video restoration tasks, including super-resolution and deblurring. The cores of EDVR are (1) an alignment module known as Pyramid, Cascading and Deformable convolutions (PCD), and (2) a fusion module known as Temporal and Spatial Attention (TSA).</p><p>The PCD module is inspired by TDAN <ref type="bibr" target="#b39">[40]</ref> in using deformable convolutions to align each neighboring frame to the reference frame at the feature level. Different from TDAN, we perform alignment in a coarse-to-fine manner to handle large and complex motions. Specifically, we use a pyramid structure that first aligns features in lower scales with coarse estimations, and then propagates the offsets and aligned features to higher scales to facilitate precise motion compensation, similar to the notion adopted in optical flow estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, we cascade an additional deformable convolution after the pyramidal alignment operation to further improve the robustness of alignment.</p><p>The proposed TSA is a fusion module that helps aggregate information across multiple aligned features. To better consider the visual informativeness on each frame, we introduce temporal attention by computing the element-wise correlation between the features of the reference frame and each neighboring frame. The correlation coefficients then weigh each neighboring feature at each location, indicating how informative it is for reconstructing the reference image. The weighted features from all frames are then convolved and fused together. After the fusion with temporal attention, we further apply spatial attention to assign weights to each location in each channel to exploit cross-channel and spatial information more effectively.</p><p>We participated in all the four tracks in the video restoration and enhancement challenges <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>, including video super-resolution (clean/blur) and video deblurring (clean/compression artifacts). Thanks to the effective alignment and fusion modules, our EDVR has won the champion in all the four challenging tracks, demonstrating the effectiveness and the generalizability of our method. In addition to the competition results, we also report comparative results on existing benchmarks of video super-resolution and deblurring. Our EDVR shows superior performance to state-of-the-art methods in these video restoration tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Restoration. Since the pioneer work of SRCNN <ref type="bibr" target="#b4">[5]</ref>, deep learning methods have brought significant improvements in image and video super-resolution <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref>. For video super-resolution, temporal alignment plays an important role and has been extensively studied. Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref> use optical flow to estimate the motions between images and perform warping. However, accurate flow is difficult to obtain given occlusion and large motions. TOFlow <ref type="bibr" target="#b47">[48]</ref> also reveals that the standard optical flow is not the optimal motion representation for video restoration. DUF <ref type="bibr" target="#b9">[10]</ref> and TDAN <ref type="bibr" target="#b39">[40]</ref> circumvent the problem by implicit motion compensation and surpass the flow-based methods. Our EDVR also enjoys the merits of implicit alignment, with a pyramid and cascading architecture to handle large motions.</p><p>Video deblurring also benefits from the development of learning-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. Several approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref> directly fuse multiple frames without explicit temporal alignment, because the existence of blur increases the difficulty of motion estimation. Unlike these approaches, we attempt to acquire information from multiple frames using alignment, with a slight modification that an image deblurring module is added prior to alignment when there is a blur. Deformable Convolution. Dai et al. <ref type="bibr" target="#b2">[3]</ref> first propose deformable convolutions, in which additional offsets are learned to allow the network to obtain information away from its regular local neighborhood, improving the capability of regular convolutions. Deformable convolutions are widely used in various tasks such as video object detection <ref type="bibr" target="#b0">[1]</ref>, action recognition <ref type="bibr" target="#b52">[53]</ref>, semantic segmentation <ref type="bibr" target="#b2">[3]</ref>, and video super-resolution <ref type="bibr" target="#b39">[40]</ref>. In particular, TDAN <ref type="bibr" target="#b39">[40]</ref> uses deformable convolutions to align the input frames at the feature level without explicit motion estimation or image warping. Inspired by TDAN, our PCD module adopts deformable convolution as a basic operation for alignment. Attention Mechanism. Attention has proven its effectiveness in many tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">52]</ref>. For example, in video SR, Liu et al. <ref type="bibr" target="#b21">[22]</ref> learn a set of weight maps to weigh the features from different temporal branches. Nonlocal operations <ref type="bibr" target="#b43">[44]</ref> compute the response at a position as a weighted sum of the features at all positions for capturing long-range dependencies. Motivated by the success of these works, we employ both temporal and spatial attention in our TSA fusion module to allow different emphases on different temporal and spatial locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given 2N +1 consecutive low-quality frames I [t−N :t+N ] , we denote the middle frame I t as the reference frame and the other frames as neighboring frames. The aim of video restoration is to estimate a high-quality reference frameÔ t , which is close to the ground truth frame O t . The overall framework of the proposed EDVR is shown in <ref type="figure">Fig. 2</ref>. It is a generic architecture suitable for several video restoration  <ref type="figure">Figure 2</ref>. The EDVR framework. It is a unified framework suitable for various video restoration tasks, e.g., super-resolution and deblurring. Inputs with high spatial resolution are first down-sampled to reduce computational cost. Given blurry inputs, a PreDeblur Module is inserted before the PCD Align Module to improve alignment accuracy. We use three input frames as an illustrative example. tasks, including super-resolution, deblurring, denoising, deblocking, etc.</p><p>Take video SR as an example, EDVR takes 2N +1 lowresolution frames as inputs and generates a high-resolution output. Each neighboring frame is aligned to the reference one by the PCD alignment module at the feature level. The TSA fusion module fuses image information of different frames. The details of these two modules are described in Sec. 3.2 and Sec. 3.3. The fused features then pass through a reconstruction module, which is a cascade of residual blocks in EDVR and can be easily replaced by any other advanced modules in single image SR <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b51">52]</ref>. The upsampling operation is performed at the end of the network to increase the spatial size. Finally, the high-resolution framê O t is obtained by adding the predicted image residual to a direct upsampled image.</p><p>For other tasks with high spatial resolution inputs, such as video deblurring, the input frames are first downsampled with strided convolution layers. Then most computation is done in the low-resolution space, which largely saves the computational cost. The upsampling layer at the end will resize the features back to the original input resolution. A PreDeblur module is used before the alignment module to pre-process blurry inputs and improve alignment accuracy.</p><p>Though a single EDVR model could achieve stateof-the-art performance, we adopt a two-stage strategy to further boost the performance in NTIRE19 competition. Specifically, we cascade the same EDVR network but with shallower depth to refine the output frames of the first stage. The cascaded network can further remove the severe motion blur that cannot be handled by the preceding model. The details are presented in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Alignment with Pyramid, Cascading and Deformable Convolution</head><p>We first briefly review the use of deformable convolution for alignment <ref type="bibr" target="#b39">[40]</ref>, i.e., aligning features of each neighboring frame to that of the reference one. Differ-ent from optical-flow based methods, deformable alignment is applied on features of each frame, denoted by F t+i , i∈[−N :+N ]. We use the modulated deformable module <ref type="bibr" target="#b53">[54]</ref>. Given a deformable convolution kernel of K sampling locations, we denote w k and p k as the weight and the pre-specified offsets for the k-th location, respectively. For instance, a 3×3 kernel is defined with K=9 and p k ∈{(−1, −1), (−1, 0), · · · , (1, 1)}. The aligned features F a t+i at each position p 0 can then be obtained by:</p><formula xml:id="formula_0">F a t+i (p 0 ) = K k=1 w k · F t+i (p 0 + p k + ∆p k ) · ∆m k . (1)</formula><p>The learnable offset ∆p k and the modulation scalar ∆m k are predicted from concatenated features of a neighboring frame and the reference one:</p><formula xml:id="formula_1">∆P t+i = f ( [F t+i , F t ] ), i ∈ [−N : +N ]<label>(2)</label></formula><p>where ∆P={∆p}, f is a general function consisting several convolution layers, and [·, ·] denotes the concatenation operation. For simplicity, we only consider learnable offsets ∆p k and ignore modulation ∆m k in the descriptions and figures. As p 0 + p k + ∆p k is fractional, bilinear interpolation is applied as in <ref type="bibr" target="#b2">[3]</ref>.</p><p>To address complex motions and large parallax problems in alignment, we propose PCD module based on wellestablished principles in optical flow: pyramidal processing <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref> and cascading refinement <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, as shown with black dash lines in <ref type="figure" target="#fig_1">Fig. 3</ref>, to generate feature F l t+i at the l-th level, we use strided convolution filters to downsample the features at the (l−1)-th pyramid level by a factor of 2, obtaining L-level pyramids of feature representation. At the l-th level, offsets and aligned features are predicted also with the ×2 upsampled offsets and aligned features from the upper (l+1)-th level, respectively (purple dash lines in <ref type="figure" target="#fig_1">Fig. 3)</ref>: where (·) ↑s refers to upscaling by a factor s, DConv is the deformable convolution described in Eqn. 1, and g is a general function with several convolution layers. Bilinear interpolation is adopted to implement the ×2 upsampling. We use three-level pyramid, i.e., L=3, in EDVR. To reduce computational cost, we do not increase channel numbers as spatial sizes decrease. Following the pyramid structure, a subsequent deformable alignment is cascaded to further refine the coarsely aligned features (the part with light purple background in <ref type="figure" target="#fig_1">Fig. 3</ref>). PCD module in such a coarse-to-fine manner improves the alignment to the sub-pixel accuracy. We demonstrate the effectiveness of PCD in Sec. 4.3. It is noteworthy that the PCD alignment module is jointly learned together with the whole framework, without additional supervision <ref type="bibr" target="#b39">[40]</ref> or pretraining on other tasks like optical flow <ref type="bibr" target="#b47">[48]</ref>.</p><formula xml:id="formula_2">∆P l t+i = f ( [F t+i , F t ], (∆P l+1 t+i ) ↑2 ),<label>(3)</label></formula><formula xml:id="formula_3">(F a t+i ) l = g( DConv(F l t+i , ∆P l t+i ), ((F a t+i ) l+1 ) ↑2 ),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion with Temporal and Spatial Attention</head><p>Inter-frame temporal relation and intra-frame spatial relation are critical in fusion because 1) different neighboring frames are not equally informative due to occlusion, blurry regions and parallax problems; 2) misalignment and unalignment arising from the preceding alignment stage adversely affect the subsequent reconstruction performance. Therefore, dynamically aggregating neighboring frames in pixel-level is indispensable for effective and efficient fusion. In order to address the above problems, we propose TSA fusion module to assign pixel-level aggregation weights on each frame. Specifically, we adopt temporal and spatial attentions during the fusion process, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>The goal of temporal attention is to compute frame sim-  ilarity in an embedding space. Intuitively, in an embedding space, a neighboring frame that is more similar to the reference one, should be paid more attention. For each frame i∈[−N :+N ], the similarity distance h can be calculated as:</p><formula xml:id="formula_4">h(F a t+i , F a t ) = sigmoid( θ(F a t+i ) T φ(F a t ) ),<label>(5)</label></formula><p>where θ(F a t+i ) and φ(F a t ) are two embeddings, which can be achieved with simple convolution filters. The sigmoid activation function is used to restrict the outputs in [0, 1], stabilizing gradient back-propagation. Note that for each spatial location, the temporal attention is spatial-specific, i.e., the spatial size of h(F a t+i , F a t ) is the same as that of F a t+i .</p><p>The temporal attention maps are then multiplied in a pixel-wise manner to the original aligned features F a t+i . An extra fusion convolution layer is adopted to aggregate these attention-modulated featuresF a t+i :</p><formula xml:id="formula_5">F a t+i = F a t+i h(F a t+i , F a t ),<label>(6)</label></formula><p>F fusion = Conv( [F a t−N , · · · ,F a t , · · · ,F a t+N ] ), <ref type="bibr" target="#b6">(7)</ref> where and [·, ·, ·] denote the element-wise multiplication and concatenation, respectively. Spatial attention masks are then computed from the fused features. A pyramid design is employed to increase the attention receptive field. After that, the fused features are modulated by the masks through element-wise multiplication and addition, similar to <ref type="bibr" target="#b44">[45]</ref>. The effectiveness of TSA module is presented in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Two-Stage Restoration</head><p>Though a single EDVR equipped with PCD alignment module and TSA fusion module could achieve state-of-theart performance, it is observed that the restored images are not perfect, especially when the input frames are blurry or severely distorted. In such a harsh circumstance, motion compensation and detail aggregation are affected, resulting in inferior reconstruction performance.</p><p>Intuitively, coarsely restored frames would greatly mitigates the pressure for alignment and fusion. Thus, we employ a two-stage strategy to further boost the performance. Specifically, a similar but shallower EDVR network is cascaded to refine the output frames of the first stage. The benefits are two-fold: 1) it effectively removes the severe motion blur that cannot be handled in the preceding model, improving the restoration quality; 2) it alleviates the inconsistency among output frames. The effectiveness of twostage restoration is illustrated in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Datasets and Details</head><p>Training datasets. Previous studies on video processing <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34]</ref> are usually developed or evaluated on private datasets. The lack of standard and open video datasets restricts fair comparisons. REDS <ref type="bibr" target="#b25">[26]</ref> is a newly proposed high-quality (720p) video dataset in the NTIRE19 Competition. REDS consists of 240 training clips, 30 validation clips and 30 testing clips (each with 100 consecutive frames). During the competition, since the test ground truth is not available, we select four representative clips (with diverse scenes and motions) as our test set, denoted by REDS4 1 . The remaining training and validation clips are re-grouped as our training dataset (a total of 266 clips). To be consistent with our methods and process in the competition, we also adopt this configuration in this paper.</p><p>Vimeo-90K <ref type="bibr" target="#b47">[48]</ref> is a widely used dataset for training, usually along with Vid4 <ref type="bibr" target="#b20">[21]</ref> and Vimeo-90K testing dataset (denoted by Vimeo-90K-T) for evaluation. We observe dataset bias when the distribution of training sets deviates from that of testing sets. More details are presented in Sec. 4.3. Training details. The PCD alignment module adopts five residual blocks (RB) to perform feature extraction. We use 40 RBs in the reconstruction module and 20 RBs for the second-stage model. The channel size in each residual block is set to 128. We use RGB patches of size 64×64 and 256×256 as inputs for video SR and deblurring tasks, respectively. Mini-batch size is set to 32. The network takes five consecutive frames (i.e., N=2) as inputs unless otherwise specified. We augment the training data with random horizontal flips and 90 • rotations. We only adopt Charbonnier penalty function <ref type="bibr" target="#b16">[17]</ref> as the final loss, defined by</p><formula xml:id="formula_6">L= Ô t −O t 2 +ε 2 ,</formula><p>where ε is set to 1×10 −3 . We train our model with Adam optimizer <ref type="bibr" target="#b13">[14]</ref> by setting <ref type="bibr" target="#b0">1</ref> Specifically, REDS4 contains the 000, 011, 015 and 020 clips. β 1 =0.9 and β 2 =0.999. The learning rate is initialized as 4×10 −4 . We initialize deeper networks by parameters from shallower ones for faster convergence. We implement our models with the PyTorch framework and train them using 8 NVIDIA Titan Xp GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-art Methods</head><p>We compare our EDVR with several state-of-the-art methods on video SR and video deblurring respectively. Two-stage and self-ensemble strategies <ref type="bibr" target="#b19">[20]</ref> are not used. In the evaluation, we include all the input frames and do not crop any border pixels except the DUF method <ref type="bibr" target="#b9">[10]</ref>. We crop eight pixels near image boundary for DUF due to its severe boundary effects. Video Super-Resolution.</p><p>We compare our EDVR method with nine algorithms: RCAN <ref type="bibr" target="#b51">[52]</ref>, DeepSR <ref type="bibr" target="#b18">[19]</ref>, BayesSR <ref type="bibr" target="#b20">[21]</ref>, VESPCN <ref type="bibr" target="#b1">[2]</ref>, SPMC <ref type="bibr" target="#b36">[37]</ref>, TOFlow <ref type="bibr" target="#b47">[48]</ref>, FRVSR <ref type="bibr" target="#b31">[32]</ref>, DUF <ref type="bibr" target="#b9">[10]</ref> and RBPN <ref type="bibr" target="#b5">[6]</ref> on three testing datasets: Vid4 <ref type="bibr" target="#b20">[21]</ref>, Vimeo-90K-T <ref type="bibr" target="#b47">[48]</ref> and REDS4. Most previous methods use different training sets and different down-sampling kernels, making the comparisons difficult. Each testing dataset has different characteristics. Vid4 is commonly used in video SR. The data has limited motion. Visual artifacts also exist on its ground-truth (GT) frames. Vimeo-90K-T is a much larger dataset with various motions and diverse scenes. REDS4 consists of high-quality images but with larger and more complex motions. We observe dataset bias when training and testing sets diverge a lot. Hence, we train our models on Vimeo-90K when evaluated on Vid4 and Vimeo-90K-T.</p><p>The quantitative results on Vid4, Vimeo-90K-T and REDS4 are shown in <ref type="table">Table 1</ref>, <ref type="table">Table 2 and Table 3</ref> (Left), respectively. On Vid4, EDVR achieves comparable performance to DUF and outperforms other methods by a large margin. On Vimeo-90K-T and REDS, EDVR is significantly better than the state-of-the-art methods, including DUF and RBPN. Qualitative results on Vid4 and Vimeo-90K-T are presented in <ref type="figure">Fig. 5 and Fig. 6</ref>, respectively. On both datasets, EDVR recovers more accurate textures compared to existing methods, especially in the second image of <ref type="figure">Fig. 6</ref>, where the characters can be correctly identified only in the outputs of EDVR. Video Deblurring. We compare our EDVR method with four algorithms: DeepDeblur <ref type="bibr" target="#b26">[27]</ref>, DeblurGAN <ref type="bibr" target="#b15">[16]</ref>, SRN-Deblur <ref type="bibr" target="#b38">[39]</ref> and DBN <ref type="bibr" target="#b33">[34]</ref> on the REDS4 dataset. Quantitative results are shown in <ref type="table">Table 3</ref> (Right). Our EDVR outperforms the state-of-the-art methods by a large margin. We attribute this to both the effectiveness of our method and the challenging REDS dataset that contains complex blurring. Visual results are presented in <ref type="figure" target="#fig_5">Fig. 7</ref>, while most methods are able to address small blurring, only EDVR can successfully recover clear details from extremely blurry images.         <ref type="figure" target="#fig_7">Fig. 9</ref>, we present the flow between the reference and neighboring frames, together with the temporal atten-SR-Blur Track Clip 011</p><p>GT Bicubic EDVR (S1) EDVR (S2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SR-Clean Track</head><p>Clip 000</p><p>GT Bicubic EDVR (S1) EDVR (S2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDVR (S1) EDVR (S2)</head><p>Deblur-Clean Track Clip 015</p><p>GT Input</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDVR (S1) EDVR (S2)</head><p>Deblur-Comp. Track Clip 020 <ref type="figure">Figure 10</ref>. Qualitative results of our EDVR method on the four tracks in the NTIRE 2019 video restoration and enhancement challenges. tion of each frame. It is observed that the frames and regions with lower flow magnitude tend to have higher attention, indicating that the smaller the motion is, the more informative the corresponding frames and regions are. Dataset Bias. As shown in <ref type="table" target="#tab_1">Table 4</ref> (Right), we conduct different settings of training and testing datasets for video super-resolution. The results show that there exists a large dataset bias. The performance decreases 0.5-1.5 dB when the distribution of training and testing data mismatch. We believe that the generalizability of video restoration methods is worth investigating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on REDS Dataset</head><p>We participated in all the four tracks in the NTIRE19 video restoration and enhancement challenges <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>. Quantitative results are presented in <ref type="table" target="#tab_4">Table 5</ref>. Our EDVR wins the champions and outperforms the second place by a large margin in all tracks. In the competition, we adopt selfensemble as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20]</ref>. Specifically, during the test time, we flip and rotate the input image to generate four augmented inputs for each sample. We then apply the EDVR method on each, reverse the transformation on the restored outputs and average for the final result. The two-stage restoration strategy as described in Sec. 3.4 is also used to boost the performance. As shown in <ref type="table">Table 6</ref>, we observe that the two-stage <ref type="table">Table 6</ref>. Evaluation on REDS4 for all the four competition tracks. '+' and '-S2' denote the self-ensemble strategy and two-stage restoration strategy, respectively. restoration largely improves the performance around 0.5 dB (EDVR(+) vs. EDVR-S2(+)). While the self-ensemble is helpful in the first stage (EDVR vs. EDVR+), it only brings marginal improvement in the second stage (EDVR-S2 vs. EDVR-S2+). Qualitative results are shown in <ref type="figure">Fig. 10</ref>. It is observed that the second stage helps recover clear details in challenging cases, e.g., the inputs are extremely blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced our winning approach in the NTIRE 2019 video restoration and enhancement challenges. To handle the challenging benchmark released in the competition, we propose EDVR, a unified framework with unique designs to achieve good alignment and fusion quality in various video restoration tasks. Thanks to the PCD alignment module and TSA fusion module, EDVR not only wins all four tracks in the NTIRE19 Challenges but also demonstrates superior performance to existing methods on several benchmarks of video super-resolution and deblurring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>PCD alignment module with Pyramid, Cascading and Deformable convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>TSA fusion module with Temporal and Spatial Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Qualitative comparison on the Vid4 dataset for 4× video SR. Zoom in for best view. Qualitative comparison on the Vimeo-90K-T dataset for 4× video SR. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison on the REDS4 dataset for video deblurring. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Ablation on the PCD alignment module. Compared with the results without PCD alignment, the flow of the PCD outputs is much smaller and cleaner, indicating that the PCD module can successfully handle large and complex motions. Flow field color coding scheme is shown in the right. The direction and magnitude of the displacement vector are represented by hue and color intensity, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Ablation on the TSA fusion module. The frames and regions of lower flow magnitude tend to have more attention, indicating that the corresponding frames and regions are more informative.4.3. Ablation StudiesPCD Alignment Module. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Quantitative comparison on Vid4 for 4× video SR. Red and blue indicates the best and the second best performance, respectively. Y or RGB denotes the evaluation on Y (luminance) or RGB channels. '*' means the values are taken from their publications. Quantitative comparison on Vimeo-90K-T for 4× video SR. ' †' means the values are taken from<ref type="bibr" target="#b47">[48]</ref>. '*' means the values are taken from their publications.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Bicubic</cell><cell cols="9">RCAN [52] VESPCN* [2] SPMC [37] TOFlow [48] FRVSR* [32] DUF [10] RBPN* [6] EDVR (Ours)</cell></row><row><cell>Clip Name</cell><cell></cell><cell cols="2">(1 Frame)</cell><cell cols="2">(1 Frame)</cell><cell cols="2">(3 Frames)</cell><cell cols="5">(3 Frames) (7 Frames) (recurrent) (7 Frames) (7 Frames) (7 Frames)</cell></row><row><cell cols="2">Calendar (Y)</cell><cell cols="4">20.39/0.5720 22.33/0.7254</cell><cell>-</cell><cell></cell><cell cols="4">22.16/0.7465 22.47/0.7318</cell><cell>-</cell><cell>24.04/0.8110 23.99/0.807 24.05/0.8147</cell></row><row><cell>City (Y)</cell><cell></cell><cell cols="4">25.16/0.6028 26.10/0.6960</cell><cell>-</cell><cell></cell><cell cols="4">27.00/0.7573 26.78/0.7403</cell><cell>-</cell><cell>28.27/0.8313 27.73/0.803 28.00/0.8122</cell></row><row><cell>Foliage (Y)</cell><cell></cell><cell cols="4">23.47/0.5666 24.74/0.6647</cell><cell>-</cell><cell></cell><cell cols="4">25.43/0.7208 25.27/0.7092</cell><cell>-</cell><cell>26.41/0.7709 26.22/0.757 26.34/0.7635</cell></row><row><cell>Walk (Y)</cell><cell></cell><cell cols="4">26.10/0.7974 28.65/0.8719</cell><cell>-</cell><cell></cell><cell cols="4">28.91/0.8761 29.05/0.8790</cell><cell>-</cell><cell>30.60/0.9141 30.70/0.909 31.02/0.9152</cell></row><row><cell cols="2">Average (Y)</cell><cell cols="11">23.78/0.6347 25.46/0.7395 25.35/0.7557 25.88/0.7752 25.89/0.7651 26.69/0.822 27.33/0.8318 27.12/0.818 27.35/0.8264</cell></row><row><cell cols="6">Average (RGB) 22.37/0.6098 24.02/0.7192</cell><cell>-/-</cell><cell></cell><cell cols="4">24.39/0.7534 24.41/0.7428</cell><cell>-/-</cell><cell>25.79/0.8136</cell><cell>-/-</cell><cell>25.83/0.8077</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bicubic</cell><cell cols="8">RCAN [52] DeepSR  † [19] BayesSR  † [21] TOFlow [48] DUF [10] RBPN* [6] EDVR (Ours)</cell></row><row><cell cols="2">Test on</cell><cell></cell><cell cols="2">(1 Frame)</cell><cell>(1 Frame)</cell><cell cols="3">(7 Frames)</cell><cell cols="2">(7 Frames)</cell><cell cols="2">(7 Frames) (7 Frames) (7 Frames)</cell><cell>(7 Frames)</cell></row><row><cell cols="13">RGB Channels 29.79/0.8483 33.61/0.9101 25.55/0.8498 24.64/0.8205 33.08/0.9054 34.33/0.9227</cell><cell>-/-</cell><cell>35.79/0.9374</cell></row><row><cell cols="2">Y Channel</cell><cell></cell><cell cols="4">31.32/0.8684 35.35/0.9251</cell><cell>-/-</cell><cell></cell><cell></cell><cell>-/-</cell><cell cols="2">34.83/0.9220 36.37/0.9387 37.07/0.9435 37.61/0.9489</cell></row><row><cell cols="13">Table 3. Quantitative comparison on REDS4. Left: 4× Video SR (clean); Right: Video deblurring (clean). Test on RGB channels.</cell></row><row><cell>Method</cell><cell cols="2">Clip 000</cell><cell cols="2">Clip 011</cell><cell>Clip 015</cell><cell>Clip 020</cell><cell></cell><cell>Average</cell><cell></cell><cell>Method</cell><cell></cell><cell>Clip 000</cell><cell>Clip 011</cell><cell>Clip 015</cell><cell>Clip 020</cell><cell>Average</cell></row><row><cell>Bicubic</cell><cell cols="9">24.55/0.6489 26.06/0.7261 28.52/0.8034 25.41/0.7386 26.14/0.7292</cell><cell cols="3">DeblurGAN [16] 26.57/0.8597 22.37/0.6637 26.48/0.8258 20.93/0.6436 24.09/0.7482</cell></row><row><cell>RCAN [52]</cell><cell cols="9">26.17/0.7371 29.34/0.8255 31.85/0.8881 27.74/0.8293 28.78/0.8200</cell><cell cols="3">DeepDeblur [27] 29.13/0.9024 24.28/0.7648 28.58/0.8822 22.66/0.6493 26.16/0.8249</cell></row><row><cell cols="10">TOFlow [48] 26.52/0.7540 27.80/0.7858 30.67/0.8609 26.92/0.7953 27.98/0.7990</cell><cell cols="3">SRN-Deblur [39] 28.95/0.8734 25.48/0.7595 29.26/0.8706 24.21/0.7528 26.98/0.8141</cell></row><row><cell>DUF [10]</cell><cell cols="9">27.30/0.7937 28.38/0.8056 31.55/0.8846 27.30/0.8164 28.63/0.8251</cell><cell>DBN [34]</cell><cell cols="2">30.03/0.9015 24.28/0.7331 29.40/0.8878 22.51/0.7039 26.55/0.8066</cell></row><row><cell cols="10">EDVR (Ours) 28.01/0.8250 32.17/0.8864 34.06/0.9206 30.09/0.8881 31.09/0.8800</cell><cell cols="3">EDVR (Ours) 36.66/0.9743 34.33/0.9393 36.09/0.9542 32.12/0.9269 34.80/0.9487</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell>Bicubic</cell><cell></cell><cell>RCAN</cell><cell>SPMC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TOFlow</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FRVSR</cell><cell>DUF</cell><cell>EDVR (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Ablations on: Left: PCD and TSA modules (Experiments here adopt a smaller model with 10 RBs in the reconstruction module and the channel number is set to 64). FLOPs [25] are calculated on an image with the HR size of 1280 × 720. Right: the bias between the training and testing datasets. .8800 25.37/0.7956 34.33/0.9246 Vimeo-90K(7 frames) 30.49/0.8700 25.83/0.8077 35.79/0.9374</figDesc><table><row><cell cols="3">Model PCD? (1 DConv) (4 DConv) Model 1 Model 2</cell><cell>Model 3 Model 4</cell><cell>Train</cell><cell>Test</cell><cell>REDS4</cell><cell>Vid4 [21] Vimeo90k [48]</cell></row><row><cell>TSA? PSNR FLOPs</cell><cell>29.78 640.2G</cell><cell>29.98 932.9G</cell><cell>30.39 939.3G 936.5G 30.53</cell><cell cols="3">REDS (5 frames) 31.09/0∆ 0.60/0.0100 -0.46/-0.0121 -1.46/-0.0128</cell></row><row><cell></cell><cell>Input Image</cell><cell></cell><cell>Before Alignment</cell><cell cols="3">After Alignment (w/o PCD)</cell><cell>After Alignment (w/ PCD)</cell></row><row><cell>Frame 28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Flow Magnitude</cell><cell>2.86</cell><cell></cell><cell>2.84</cell><cell></cell><cell>0.15</cell><cell>0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Model 3 is nearly 0.4 dB better than Model 2 with roughly the same computational cost, demonstrating the effectiveness of PCD alignment module. InFig. 8, we show representative features before and after different alignment modules, and depict the flow (derived by PWCNet<ref type="bibr" target="#b34">[35]</ref>) between reference and neighboring features. Compared with the flow without PCD alignment, the flow of the PCD outputs is much smaller and cleaner, indicating that the PCD module can successfully handle large and complex motions.</figDesc><table><row><cell>(Left), our</cell></row><row><cell>baseline (Model 1) only adopts one deformable convolution</cell></row><row><cell>for alignment. Model 2 follows the design of TDAN [40] to</cell></row><row><cell>use four deformable convolutions for alignment, achieving</cell></row><row><cell>an improvement of 0.2 dB. With our proposed PCD mod-ule, TSA Attention Module. As shown in Table 4 (Left), with the TSA attention module, Model 4 achieves 0.14 dB per-</cell></row><row><cell>formance gain compared to Model 3 with similar computa-</cell></row><row><cell>tions. In</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Top5 methods in the NTIRE 2019 challenges on video restoration and enhancement. Red and blue indicates the best and the second best performance, respectively.</figDesc><table><row><cell></cell><cell>SR</cell><cell></cell><cell></cell><cell>Deblur</cell></row><row><cell>Method</cell><cell>Clean</cell><cell>Blur</cell><cell>Clean</cell><cell>Compression</cell></row><row><cell cols="5">EDVR (Ours) 31.79/0.8962 30.17/0.8647 36.96/0.9657 31.69/0.8783</cell></row><row><cell>2nd method</cell><cell>31.13/0.8811</cell><cell>-/-</cell><cell cols="2">35.71/0.9522 29.78/0.8285</cell></row><row><cell>3rd method</cell><cell cols="4">31.00/0.8822 27.71/0.8067 34.09/0.9361 29.63/0.8261</cell></row><row><cell>4th method</cell><cell cols="4">30.97/0.8804 28.92/0.8333 33.71/0.9363 29.19/0.8190</cell></row><row><cell>5th method</cell><cell cols="4">30.91/0.8782 28.98/0.8307 33.46/0.9293 28.33/0.7976</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.8800 31.54/0.8888 31.23/0.8818 31.56/0.8891 Blur 28.88/0.8361 29.41/0.8503 29.14/0.8403 29.49/0.8515 Deblur Clean 34.80/0.9487 36.37/0.9632 35.27/0.9526 36.49/0.9639 Comp. 30.24/0.8567 31.00/0.8734 30.46/0.8599 31.06/0.8741</figDesc><table><row><cell cols="2">Track</cell><cell>EDVR</cell><cell>EDVR-S2</cell><cell>EDVR+</cell><cell>EDVR-S2+</cell></row><row><cell>SR</cell><cell cols="2">Clean 31.09/0</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Yapeng Tian for providing the core codes of TDAN <ref type="bibr" target="#b39">[40]</ref>. This work is supported by SenseTime Group Limited, Joint Lab of CAS-HK, the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14241716, 14224316. 14209217), and Singapore MOE AcRF Tier 1 (M4012082.020).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitken</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acosta</forename><surname>Alejandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dictionary-based multiple frame video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07414</idno>
		<title level="m">A lightweight optical flow cnn-revisiting data fidelity and regularization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dynamic video deblurring using a locally adaptive blur model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2374" to="2387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07064</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenges on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring: methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simultaneous stereo video deblurring and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00850</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Space-time super-resolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Super-resolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1958" to="1975" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">TDAN: Temporally deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02898</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCVW</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09078</idno>
		<title level="m">Video enhancement with task-oriented flow</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Crafting a toolchain for image restoration by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Path-restore: Learning network path selection for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial spatio-temporal learning for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="291" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
