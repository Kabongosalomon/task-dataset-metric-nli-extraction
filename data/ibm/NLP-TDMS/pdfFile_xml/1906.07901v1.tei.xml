<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Abstractive Summarization for How2 Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Palaskar</surname></persName>
							<email>spalaska@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
							<email>libovicky@ufal.mff.cuni.czsgella@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Amazon AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
							<email>fmetze@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Abstractive Summarization for How2 Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to "compress" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, with the growing popularity of video sharing platforms, there has been a steep rise in the number of user-generated instructional videos shared online. With the abundance of videos online, there has been an increase in demand for efficient ways to search and retrieve relevant videos <ref type="bibr" target="#b14">(Song et al., 2011;</ref><ref type="bibr">Wang et al., 2012;</ref><ref type="bibr" target="#b3">Otani et al., 2016;</ref><ref type="bibr" target="#b19">Torabi et al., 2016)</ref>. Many cross-modal search applications rely on text associated with the video such as description or title to find relevant content. However, often videos do not have text meta-data associated with them or the existing ones do not provide clear information of the video content and fail to capture subtle differences between related videos <ref type="bibr">(Wang et al., 2012)</ref>. We address this by aiming to generate a short text summary of the video that describes the most salient content of the * *Work done while SG was at University of Edinburgh video. Our work benefits users through better contextual information and user experience, and video sharing platforms with increased user engagement by retrieving or suggesting relevant videos to users and capturing their attention.</p><p>Summarization is a task of producing a shorter version of the content in the document while preserving its information and has been studied for both textual documents (automatic text summarization) and visual documents such as images and videos (video summarization). Automatic text summarization is a widely studied topic in natural language processing <ref type="bibr">(Luhn, 1958;</ref><ref type="bibr">Kupiec et al., 1995;</ref><ref type="bibr">Mani, 1999)</ref>; given a text document the task is to generate a textual summary for applications that can assist users to understand large documents. Most of the work on text summarization has focused on single-document summarization for domains such as news <ref type="bibr" target="#b7">(Rush et al., 2015;</ref><ref type="bibr" target="#b1">Nallapati et al., 2016;</ref><ref type="bibr" target="#b11">See et al., 2017;</ref><ref type="bibr" target="#b2">Narayan et al., 2018)</ref> and some on multi-document summarization <ref type="bibr">(Goldstein et al., 2000;</ref><ref type="bibr">Lin and Hovy, 2002;</ref><ref type="bibr">Woodsend and Lapata, 2012;</ref><ref type="bibr">Cao et al., 2015;</ref><ref type="bibr">Yasunaga et al., 2017)</ref>. Video summarization is the task of producing a compact version of the video (visual summary) by encapsulating the most informative parts <ref type="bibr" target="#b0">(Money and Agius, 2008;</ref><ref type="bibr">Lu and Grauman, 2013;</ref><ref type="bibr">Gygli et al., 2014;</ref><ref type="bibr" target="#b15">Song et al., 2015;</ref><ref type="bibr" target="#b8">Sah et al., 2017)</ref>. Multimodal summarization is the combination of textual and visual modalities by summarizing a video document with a text summary that summarizes the content of the video. Multimodal summarization is a more recent challenge with no benchmarking datasets yet. <ref type="bibr">Li et al. (2017)</ref> collected a multimodal corpus of 500 English news videos and articles paired with manually annotated summaries. The dataset is small-scale and has news articles with audio, video, and text summaries, but there are no human annotated audio-transcripts. today we are going to show you how to make spanish omelet . i 'm going to dice a little bit of peppers here . i 'm not going to use a lot , i 'm going to use very very little . a little bit more then this maybe . you can use red peppers if you like to get a little bit color in your omelet . some people do and some people do n't …. t is the way they make there spanish omelets that is what she says . i loved it , it actually tasted really good . you are going to take the onion also and dice it really small . you do n't want big chunks of onion in there cause it is just pops out of the omelet . so we are going to dice the up also very very small . so we have small pieces of onions and peppers ready to go .</p><p>how to cut peppers to make a spanish omelette; get expert tips and advice on making cuban breakfast recipes in this free cooking video .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transcript Video</head><p>Figure 1: How2 dataset example with different modalities. "Cuban breakfast" and "free cooking video" is not mentioned in the transcript, and has to be derived from other sources.</p><p>Related tasks include image or video captioning and description generation, video story generation, procedure learning from instructional videos and title generation which focus on events or activities in the video and generating descriptions at various levels of granularity from single sentence to multiple sentences <ref type="bibr">(Das et al., 2013;</ref><ref type="bibr" target="#b5">Regneri et al., 2013;</ref><ref type="bibr" target="#b6">Rohrbach et al., 2014;</ref><ref type="bibr">Zeng et al., 2016;</ref><ref type="bibr">Zhou et al., 2018;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr">Gella et al., 2018)</ref>. A closely related task to ours is video title generation where the task is to describe the most salient event in the video in a compact title that is aimed at capturing users attention <ref type="bibr">(Zeng et al., 2016)</ref>. <ref type="bibr">Zhou et al. (2018)</ref> present the YouCookII dataset containing instructional videos, specifically cooking recipes, with temporally localized annotations for the procedure which could be viewed as a summarization task as well although localized with time alignments between video segments and procedures.</p><p>In this work, we study multimodal summarization with various methods to summarize the intent of open-domain instructional videos stating the exclusive and unique features of the video, irrespective of modality. We study this task in detail using the new How2 dataset <ref type="bibr" target="#b9">(Sanabria et al., 2018)</ref> which contains human annotated video summaries for a varied range of topics. Our models generate natural language descriptions for video content using the transcriptions (both user-generated and output of automatic speech recognition systems) as well as visual features extracted from the video. We also introduce a new evaluation metric (Content F1) that suits this task and present detailed results to understand the task better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multimodal Abstractive Summarization</head><p>The How2 dataset <ref type="bibr" target="#b9">(Sanabria et al., 2018)</ref> contains about 2,000 hours of short instructional videos, spanning different domains such as cooking, sports, indoor/outdoor activities, music, etc. Each video is accompanied by a human-generated transcript and a 2 to 3 sentence summary is available for every video written to generate interest in a potential viewer.</p><p>The example in <ref type="figure">Figure 1</ref> shows the transcript describes instructions in detail, while the summary is a high-level overview of the entire video, mentioning that the peppers are being "cut", and that this is a "Cuban breakfast recipe", which is not mentioned in the transcript. We observe that text and vision modalities both contain complementary information, thereby when fused, helps in generating richer and more fluent summaries. Additionally, we can also leverage the speech modality by using the output of a speech recognizer as input to a summarization model instead of a human-annotated transcript.</p><p>The How2 corpus contains 73,993 videos for training, 2,965 for validation and 2,156 for testing. The average length of transcripts is 291 words and of summaries is 33 words. A more general comparison of the How2 dataset for summarization as compared with certain common datasets is given in <ref type="bibr" target="#b9">(Sanabria et al., 2018)</ref>.</p><p>Video-based Summarization. We represent videos by features extracted from a pre-trained action recognition model: a <ref type="bibr">ResNeXt-101 3D Convolutional Neural Network (Hara et al., 2018)</ref> trained to recognize 400 different human actions in the Kinetics dataset <ref type="bibr">(Kay et al., 2017)</ref>. These features are 2048 dimensional, extracted for every 16 non-overlapping frames in the video. This results in a sequence of feature vectors per video rather than a single/global one. We use these sequential features in our models described in Section 3. 2048-dimensional feature vector representing all text a single video.</p><p>Speech-based Summarization. We leverage the speech modality by using the outputs from a pretrained speech recognizer that is trained with other data, as inputs to a text summarization model. We use the state-of-the-art models for distantmicrophone conversational speech recognition, AS-pIRE <ref type="bibr" target="#b4">(Peddinti et al., 2015)</ref> and <ref type="bibr">EESEN (Miao et al., 2015;</ref><ref type="bibr">Le Franc et al., 2018)</ref>. The word error rate of these models on the How2 test data is 35.4%. This high error mostly stems from normalization issues in the data. For example, recognizing and labeling "20" as "twenty" etc. Handling these effectively will reduce the word error rates significantly. We accept these as is for this task.</p><p>Transfer Learning. Our parallel work <ref type="bibr" target="#b10">Sanabria et al. (2019)</ref> demonstrates the use of summarization models trained in this paper for a transfer learning based summarization task on the Charades dataset (Sigurdsson et al., 2016) that has audio, video, and text (summary, caption and question-answer pairs) modalities similar to the How2 dataset. <ref type="bibr" target="#b10">Sanabria et al. (2019)</ref> observe that pre-training and transfer learning with the How2 dataset led to significant improvements in unimodal and multimodal adaptation tasks on the Charades dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarization Models</head><p>We study various summarization models. First, we use a Recurrent Neural Network (RNN) Sequenceto-Sequence (S2S) model <ref type="bibr" target="#b18">(Sutskever et al., 2014)</ref> consisting of an encoder RNN to encode (text or video features) with the attention mechanism (Bahdanau et al., 2014) and a decoder RNN to generate summaries. Our second model is a Pointer-Generator (PG) model <ref type="bibr" target="#b20">(Vinyals et al., 2015;</ref><ref type="bibr">Gülçehre et al., 2016)</ref> that has shown strong performance for abstractive summarization <ref type="bibr" target="#b1">(Nallapati et al., 2016;</ref><ref type="bibr" target="#b11">See et al., 2017)</ref>. As our third model, we use hierarchical attention approach of Libovický and Helcl 2017 originally proposed for multimodal machine translation to combine textual and visual <ref type="bibr">videoframes············ResNeXtfeatures(w/RNN:7;</ref><ref type="bibr">w/oRNN:6,</ref><ref type="bibr">8,</ref><ref type="bibr">8,</ref><ref type="bibr">9)attention⊕hier.attn.(8,</ref><ref type="bibr">9)</ref>w...RNNdecoder video frames · · · · · · · · · · · · ResNeXt features (w/ RNN: 7; w/o <ref type="bibr">RNN: 6,</ref><ref type="bibr">8,</ref><ref type="bibr">9)</ref> attention RNN over transcript <ref type="bibr">(3-5, 8, 9)</ref> attention ⊕ hier. attn. <ref type="bibr">(8, 9) w</ref> . . . modalities to generate text. The model first computes the context vector independently for each of the input modalities (text and video). In the next step, the context vectors are treated as states of another encoder, and a new vector is computed. When using a sequence of action features instead of a single averaged vector for a video, the RNN layer helps capture context. In <ref type="figure" target="#fig_0">Figure 2</ref> we present the building block of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate the summaries using the standard metric for abstractive summarization ROUGE-L (Lin and Och, 2004) that measures the longest common sequence between the reference and the generated summary. Additionally, we introduce the Content F1 metric that fits the template-like structure of the summaries. We analyze the most frequently occurring words in the transcription and summary. The words in transcript reflect the conversational and spontaneous speech while the words in the summaries reflect their descriptive nature. For examples, see <ref type="table" target="#tab_3">Table A1</ref> in Appendix A.2.</p><p>Content F1. This metric is the F1 score of the content words in the summaries based over a monolingual alignment, similar to metrics used to evaluate quality of monolingual alignment <ref type="bibr" target="#b16">(Sultan et al., 2014)</ref>. We use the METEOR toolkit (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) to obtain the alignment. Then, we remove function words and task-specific stop words that appear in most of the summaries (see Appendix A.2) from the reference and the hypothesis. The stop words are easy to predict and thus increase the ROUGE score. We treat remaining content words from the reference  <ref type="formula">(7)</ref> 3.58 3.30 3.71 3.80 Text-and-Video (8) 3.89 3.74 3.85 3.94 and the hypothesis as two bags of words and compute the F1 score over the alignment. Note that the score ignores the fluency of output.</p><p>Human Evaluation. In addition to automatic evaluation, we perform a human evaluation to understand the outputs of this task better. Following the abstractive summarization human annotation work of Grusky et al. <ref type="formula">(2018)</ref>, we ask our annotators to label the generated output on a scale of 1 − 5 on informativeness, relevance, coherence, and fluency. We perform this on randomly sampled 500 videos from the test set.</p><p>We evaluate three models: two unimodal (textonly (5a), video-only (7)) and one multimodal <ref type="bibr">(text-and-video (8)</ref>). Three workers annotated each video on Amazon Mechanical Turk. More details about human evaluation are in the Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>As a baseline, we train an RNN language model <ref type="bibr" target="#b17">(Sutskever et al., 2011)</ref> on all the summaries and randomly sample tokens from it. The output ob-tained is fluent in English leading to a high ROUGE score, but the content is unrelated which leads to a low Content F1 score in <ref type="table">Table 1</ref>. As another baseline, we replace the target summary with a rule-based extracted summary from the transcription itself. We used the sentence containing words "how to" with predicates learn, tell, show, discuss or explain, usually the second sentence in the transcript. Our final baseline was a model trained with the summary of the nearest neighbor of each video in the Latent Dirichlet Allocation (LDA; <ref type="bibr">Blei et al., 2003)</ref> based topic space as a target. This model achieves a similar Content F1 score as the rulebased model which shows the similarity of content and further demonstrates the utility of the Content F1 score.</p><p>We use the transcript (either ground-truth transcript or speech recognition output) and the video action features to train various models with different combinations of modalities. The text-only model performs best when using the complete transcript in the input (650 tokens). This is in contrast to prior work with news-domain summarization <ref type="bibr" target="#b1">(Nallapati et al., 2016)</ref>. We also observe that PG networks do not perform better than S2S models on this data which could be attributed to the abstractive nature of our summaries and also the lack of common n-gram overlap between input and output which is the important feature of PG networks. We also use the automatic transcriptions obtained from a pretrained automatic speech recognizer as input to the summarization model. This model achieves competitive performance with the video-only models (described below) but degrades noticeably than ground-truth transcription summarization model. This is as expected due to the large margin of ASR errors in distant-microphone open-domain speech recognition.</p><p>We trained two video-only models: the first one uses a single mean-pooled feature vector representation for the entire video, while the second one applies a single layer RNN over the vectors in time.</p><p>Note that using only the action features in input reaches almost competitive ROUGE and Content F1 scores compared to the text-only model showing the importance of both modalities in this task. Finally, the hierarchical attention model that combines both modalities obtains the highest score.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report human evaluation scores on our best text-only, video-only and multimodal models. In three evaluation measures, the multimodal models with the hierarchical attention reach the best scores. Model hyperparameter settings, attention analysis and example outputs for the models described above are available in the Appendix.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we analyze the word distributions of different system generated summaries with the human annotated reference. The density curves show that most model outputs are shorter than human annotations with the action-only model (6) being the shortest as expected. Interestingly, the two different uni-modal and multimodal systems with groundtruth text and ASR output text features are very similar in length showing that the improvements in Rouge-L and Content-F1 scores stem from the difference in content rather than length. Example presented in <ref type="table" target="#tab_1">Table A2</ref> Section A.3 shows how the outputs vary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We present several baseline models for generating abstractive text summaries for the open-domain videos in How2 data. Our presented models include a video-only summarization model that performs competitively with a text-only model. In the future, we would like to extend this work to generate multidocument (multi-video) summaries and also build end-to-end models directly from audio in the video instead of text-based output from pretrained ASR. We define and show the quality of a new metric, Content F1, for evaluation of the video summaries that are designed as teasers or highlights for viewers, instead of a condensed version of the input like traditional text summaries. We restrict the input length to 600 tokens for all experiments except the best text-only model in the section Experiments and Results. We use vocabulary the 20,000 most frequently occurring words which showed best results in our experiments, largely outperforming models using subword-based vocabularies. We ran all experiments with the nmtpytorch toolkit <ref type="bibr">(Caglayan et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set Words</head><p>Transcript <ref type="bibr">the, to, and, you, a, it, that, of, is, i, going, we, in, your, this, 's, so, on Summary in, a, this, to, free, the, video, and, learn, from, on, with, how, tips, for, of, expert, an</ref>   <ref type="table" target="#tab_3">Table A1</ref> shows the frequent words in transcripts (input) and summaries (output). The words in transcripts reflect conversational and spontaneous speech while words in the summary reflect their descriptive nature. <ref type="table" target="#tab_1">Table A2</ref> shows example outputs from our different text-only and text-and-video models. The text-only model produces a fluent output which is close to the reference. The action features with the RNN model, which sees no text in the input, produces an in-domain ("fly tying"' and "fishing") abstractive summary that involves more details like "equipment" which is missing from the text-based models but is relevant. The action features without RNN model belongs to the relevant domain but contains fewer details. The nearest neighbor model is related to "knot tying" but not related to "fishing". The scores for each of these models reflect their respective properties. The random baseline output shows the output of sampling from the random language model based baseline. Although it is a fluent output, the content is incorrect. Observing other outputs of the model we noticed that although predictions were usually fluent leading to high scores, there is scope to improve them by predicting all details from the ground truth summary, like the subtle selling point phrases, or by using the visual features in a different adaptation model. <ref type="figure" target="#fig_3">Figure A1</ref> shows an analysis of the attention distributions using the hierarchical attention model in an example video of painting. The vertical axis denotes the output summary of the model, and the horizontal axis denotes the input time-steps (from the transcript). We observe less attention in the first 1 Random Baseline 27.5 8.3 learn tips on how to play the bass drum beat variation on the guitar in this free video clip on music theory and guitar lesson . <ref type="table" target="#tab_1">Table A2</ref>: Example outputs of ground-truth text-and-video with hierarchical attention <ref type="formula">(8)</ref>, text-only with groundtruth (5a), text-only with ASR output (5c), ASR output text-andv-video with hierarchical attention (9), action features with RNN <ref type="formula">(7)</ref> and action features only (6) models compared with the reference, the topic-based next neighbor (2b) and random baseline (1). Arranged in the order of best to worst summary in this table.  part of the video where the speaker is introducing the task and preparing the brush. In the middle half, the camera focuses on the close-up of brush strokes with hand, to which the model pays higher attention over consecutive frames. Towards the end, the close up does not contain the hand but only the paper and brush, where the model again pays less attention which could be due to unrecognized ac-tions in the close-up. There are black frames in the very end of the video where the model learns not to pay any attention. In the middle of the video, there are two places with a cut in the video when the camera shifts angle. The model has learned to identify these areas and uses it effectively. From this particular example, we see the model using both modalities very effectively in this task of the summarization of open-domain videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Frequent Words in Transcripts and Summaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Output Examples from Different Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Attention Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Human Evaluation Details</head><p>To understand the outputs generated for this task better, we ask workers on Amazon Mechanical Turk to compare outputs of unimodal and multimodal models with the ground-truth summary and assign a score between 1 (lowest) and 5 (highest) for four metrics: informativeness, relevance, coherence and fluency of generated summary. The annotators were shown the ground-truth summary and a candidate summary (without knowledge of the type of modality used to generate it). Each example was annotated by three workers. Annotation was restricted to English speaking countries. 129 annotators participated in this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Building blocks of the sequence-to-sequence models, gray numbers in brackets indicate which components are utilized in which experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Word distribution in comparison with the human summaries for different unimodal and multimodal models. Density curves show the length distributions of human annotated and system produced summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A1 :</head><label>A1</label><figDesc>Visualizing Attention over Video Features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation scores on 4 different measures of Informativeness (INF), Relevance (REL), Coherence (COH), Fluency (FLU).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Walid Aransa, Fethi Bougares, and Loïc Barrault. 2017. Nmtpy: A flexible toolkit for advanced neural machine translation systems. The Prague Bulletin of Mathematical Linguistics, 109:15-28. Paul Natsev, et al. 2017. The kinetics human action video dataset. CoRR.</figDesc><table><row><cell>Ozan Caglayan, Mercedes García-Martínez, Adrien Meng Wang, Richang Hong, Guangda Li, Zheng-Jun</cell><cell>Will Kay, Joao Carreira, Karen Simonyan, Brian</cell></row><row><cell>Zha, Shuicheng Yan, and Tat-Seng Chua. 2012. Event driven web video summarization by tag local-Bardet, Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou. 2015. Ranking with recursive neural net-works and its application to multi-document summa-ization and key-shot identification. IEEE Transac-tions on Multimedia, 14(4):975-985. Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear program-ming. In Proceedings of the 2012 Joint Conference rization. In Twenty-ninth AAAI conference on artifi-cial intelligence. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-on Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning, pages 233-243.</cell><cell>Zhang, Chloe Hillier, Sudheendra Vijaya-narasimhan, Fabio Viola, Tim Green, Trevor Back, Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th annual international ACM SIGIR confer-</cell></row><row><cell>cehre, Dzmitry Bahdanau, Fethi Bougares, Holger</cell><cell>ence on Research and development in information</cell></row><row><cell>Schwenk, and Yoshua Bengio. 2014. Learning Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu,</cell><cell>retrieval, pages 68-73. ACM.</cell></row><row><cell>phrase representations using rnn encoder-decoder Ayush Pareek, Krishnan Srinivasan, and Dragomir R.</cell><cell></cell></row><row><cell>for statistical machine translation. In Proceedings of Radev. 2017. Graph-based neural multi-document</cell><cell>Adrien Le Franc, Eric Riebling, Julien Karadayi,</cell></row><row><cell>the 2014 Conference on Empirical Methods in Natu-summarization. In Proceedings of the 21st Confer-</cell><cell>W Yun, Camila Scaff, Florian Metze, and Alejand-</cell></row><row><cell>ral Language Processing (EMNLP). ence on Computational Natural Language Learning</cell><cell>rina Cristia. 2018. The aclew divime: An easy-to-</cell></row><row><cell>(CoNLL 2017), pages 452-462.</cell><cell>use diarization tool. In Interspeech, pages 1383-</cell></row><row><cell>P. Das, C. Xu, R. F. Doell, and J. J. Corso. 2013. A</cell><cell>1387. Interspeech, ISCA.</cell></row><row><cell>thousand frames in just a few words: Lingual de-scription of videos through latent topics and sparse object stitching. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, and Min Sun. 2016. Generation for user generated videos. In European conference on com-puter vision, pages 609-625. Springer.</cell><cell>Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing Zong. 2017. Multi-modal summariza-tion for asynchronous collection of text, image, au-dio and video. In Proceedings of the 2017 Con-</cell></row><row><cell>Michael Denkowski and Alon Lavie. 2014. Meteor</cell><cell>ference on Empirical Methods in Natural Language</cell></row><row><cell>universal: Language specific translation evaluation Jianguo Zhang, Pengcheng Zou, Zhao Li, Yao Wan,</cell><cell>Processing, pages 1092-1102.</cell></row><row><cell>for any target language. In Proceedings of the ninth Ye Liu, Xiuming Pan, Yu Gong, and Philip S Yu.</cell><cell></cell></row><row><cell>workshop on statistical machine translation, pages 2018. Product title refinement via multi-modal</cell><cell>Jindřich Libovický and Jindřich Helcl. 2017. Attention</cell></row><row><cell>376-380. Association for Computational Linguis-generative adversarial learning. arXiv preprint</cell><cell>strategies for multi-source sequence-to-sequence</cell></row><row><cell>tics. arXiv:1811.04498.</cell><cell>learning. In Proceedings of the 55th Annual Meet-</cell></row><row><cell></cell><cell>ing of the Association for Computational Linguistics</cell></row><row><cell>Spandana Gella, Mike Lewis, and Marcus Rohrbach. Luowei Zhou, Chenliang Xu, and Jason J. Corso. 2018. 2018. A dataset for telling the stories of social media videos. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 968-974. Towards automatic learning of procedures from web instructional videos. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pages 7590-7598.</cell><cell>(Volume 2: Short Papers), pages 196-202. Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization. In Proceedings of 40th Annual Meeting of the Association for Compu-tational Linguistics, pages 457-464.</cell></row><row><cell>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and</cell><cell></cell></row><row><cell>Mark Kantrowitz. 2000. Multi-document summa-rization by sentence extraction. In Proceedings A Appendix</cell><cell>Chin-Yew Lin and Franz Josef Och. 2004. Auto-matic evaluation of machine translation quality us-</cell></row><row><cell>of the 2000 NAACL-ANLP Workshop on Automatic summarization, pages 40-48. Association for Com-A.1 Experimental Setup</cell><cell>ing longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Meeting of the</cell></row><row><cell>putational Linguistics. Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with In all our experiments, the text encoder consists of 2 bidirectional layers of the encoder with 256 Gated Recurrent Units (GRU; Cho et al. 2014) and</cell><cell>Association for Computational Linguistics, pages 605-612. Association for Computational Linguis-tics.</cell></row><row><cell>diverse extractive strategies. CoRR. Çaglar Gülçehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of the 54th An-2 layers of the decoder with Conditional Gated Recurrent Units (CGRU; Sennrich et al. 2017). We optimize the models with the Adam Optimizer (Kingma and Ba, 2014) with learning rate 4 · 10 −4</cell><cell>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Zheng Lu and Kristen Grauman. 2013. Story-driven summarization for egocentric video. In Proceedings of the IEEE Conference on Computer Vision and Pat-tern Recognition, pages 2714-2721.</cell></row><row><cell>puter vision, pages 505-520. Springer. from user videos. In European conference on com-der, and Luc Van Gool. 2014. Creating summaries Michael Gygli, Helmut Grabner, Hayko Riemenschnei-nual Meeting of the Association for Computational Linguistics, ACL 2016, Volume 1: Long Papers. halved after each epoch when the validation perfor-mance does not increase for maximum 50 epochs.</cell><cell>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved marization, pages 65-72. marization. MIT press. ation measures for machine translation and/or sum-Inderjeet Mani. 1999. Advances in automatic text sum-of the acl workshop on intrinsic and extrinsic evalu-correlation with human judgments. In Proceedings Hans Peter Luhn. 1958. The automatic creation of lit-erature abstracts. IBM Journal of research and de-velopment, 2(2):159-165.</cell></row><row><cell></cell><cell>Yajie Miao, Mohammad Gowayyed, and Florian Metze.</cell></row><row><cell>Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.</cell><cell>David M Blei, Andrew Y Ng, and Michael I Jordan. 2015. Eesen: End-to-end speech recognition us-</cell></row><row><cell>2018. Can spatiotemporal 3d cnns retrace the his-</cell><cell>2003. Latent dirichlet allocation. Journal of ma-ing deep rnn models and wfst-based decoding. In</cell></row><row><cell>tory of 2d cnns and imagenet? In Proceedings of</cell><cell>chine Learning research, 3(Jan):993-1022. Automatic Speech Recognition and Understanding</cell></row><row><cell>the IEEE Conference on Computer Vision and Pat-</cell><cell>(ASRU), 2015 IEEE Workshop on, pages 167-174.</cell></row><row><cell>tern Recognition (CVPR), pages 6546-6555.</cell><cell>IEEE.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A1 :</head><label>A1</label><figDesc>Most frequently occurring words in Transcript and Summaries.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and learn how to tie thread to a hook to help with fly tying as explained by out expert in this free how -to video on fly tying tips and techniques . 48.9 learn from our expert how to attach thread to fly fishing for fly fishing in this free how -to video on fly tying tips and techniques . 24.8 learn from our expert how to do a double half hitch knot in this free video clip about how to use fly fishing . 2b Next Neighbor 31.8 17.9 use a sheep shank knot to shorten a long piece of rope . learn how to tie sheep shank knots for shortening rope in this free knot tying video from an eagle scout .</figDesc><table><row><cell cols="2">No. Model</cell><cell></cell><cell cols="2">R-L C-F1 Output</cell></row><row><cell>-</cell><cell>Reference</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">8 54.9 5a Text-only (Ground-Ground-truth text + Action Feat. 53.9 47.4 learn from our expert how to tie a thread for fly fishing in this free</cell></row><row><cell></cell><cell>truth)</cell><cell></cell><cell></cell><cell>how -to video on fly tying tips and techniques .</cell></row><row><cell>9</cell><cell cols="2">ASR output + Ac-</cell><cell cols="2">46.3 34.7 learn how to tie a fly knot for fly fishing in this free how-to video on</cell></row><row><cell></cell><cell>tion Feat.</cell><cell></cell><cell></cell><cell>fly tying tips and techniques .</cell></row><row><cell cols="3">5c ASR output</cell><cell cols="2">46.1 34.7 learn tips and techniques for fly fishing in this free fishing video on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>techniques for and making fly fishing nymphs .</cell></row><row><cell>7</cell><cell cols="2">Action Features +</cell><cell cols="2">46.3 34.9 learn about the equipment needed for fly tying , as well as other fly</cell></row><row><cell></cell><cell>RNN</cell><cell></cell><cell></cell><cell>fishing tips from our expert in this free how -to video on fly tying tips and techniques .</cell></row><row><cell>6</cell><cell>Action</cell><cell>Features</cell><cell>38.5</cell></row><row><cell></cell><cell>only</cell><cell></cell><cell></cell></row></table><note>watch</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.clsp.jhu.edu/workshops/18-workshop/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was mostly conducted at the 2018 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, 1 hosted and sponsored by Johns Hopkins University. Shruti Palaskar received funding from Facebook and Amazon grants. Jindřich Libovický received funding from the Czech Science Foundation, grant no. 19-26934X. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) supported by NSF grant ACI-1548562 and the Bridges system supported by NSF award ACI-1445606, at the Pittsburgh Supercomputing Center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video summarisation: A conceptual framework and survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Money</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ça</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Glar Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1028</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">280</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning joint representations of videos and sentences with web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naokazu</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="651" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jhu aspire system: Robust lvcsr with tdnns, ivector adaptation and rnn-lms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/asru.2015.7404842</idno>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00207</idno>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Grounding action descriptions in videos</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition -36th German Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
	<note>Annemarie Friedrich, Manfred Pinkal, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic text summarization of long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagan</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Kulhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Prud&amp;apos;hommeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ptucha</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv.2017.115</idno>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How2: a large-scale dataset for multimodal language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Palaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL). NIPS</title>
		<meeting>the Workshop on Visually Grounded Interaction and Language (ViGIL). NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cmu sinbad&apos;s submission for the dstc7 avsd challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Palaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Dialog System Technology Challenges Workshop at AAAI</title>
		<meeting>7th Dialog System Technology Challenges Workshop at AAAI<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple feature hashing for real-time large scale near-duplicate video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
		<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Md Arafat Sultan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumner</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00178</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno>abs/1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
