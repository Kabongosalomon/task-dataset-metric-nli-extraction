<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using Error-Bounded Lossy Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sian</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
							<email>tao@cs.ua.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
							<email>cappello@mcs.anl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sian</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Alabama Tuscaloosa</orgName>
								<address>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Argonne National Laboratory Lemont</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Alabama Tuscaloosa</orgName>
								<address>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The University of Alabama Tuscaloosa</orgName>
								<address>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Argonne National Laboratory Lemont</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The Univer-sity of Alabama</orgName>
								<address>
									<postCode>35487</postCode>
									<settlement>Tuscaloosa</settlement>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using Error-Bounded Lossy Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307681.3326608</idno>
					<note>* Corresponding author: ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or affiliate of the United States government. As such, the United States government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for government purposes only. HPDC &apos;19, June 22-29, 2019, Phoenix, AZ, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6670-0/19/06. . . $15.00 ACM Reference Format: Dingwen Tao, and Franck Cappello. 2019. DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using Error-Bounded Lossy Compression. In The 28th Interna-tional Symposium on High-Performance Parallel and Distributed Computing (HPDC &apos;19), June 22-29, 2019, Phoenix, AZ, USA. ACM, New York, NY, USA, 12 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Mathematics of computing → Coding theory KEYWORDS Neural Networks</term>
					<term>Deep Learning</term>
					<term>Lossy Compression</term>
					<term>Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's deep neural networks (DNNs) are becoming deeper and wider because of increasing demand on the analysis quality and more and more complex applications to resolve. The wide and deep DNNs, however, require large amounts of resources (such as memory, storage, and I/O), significantly restricting their utilization on resource-constrained platforms. Although some DNN simplification methods (such as weight quantization) have been proposed to address this issue, they suffer from either low compression ratios or high compression errors, which may introduce an expensive fine-tuning overhead (i.e., a costly retraining process for the target inference accuracy). In this paper, we propose DeepSZ: an accuracyloss expected neural network compression framework, which involves four key steps: network pruning, error bound assessment, optimization for error bound configuration, and compressed model generation, featuring a high compression ratio and low encoding time. The contribution is threefold. (1) We develop an adaptive approach to select the feasible error bounds for each layer. (2) We build a model to estimate the overall loss of inference accuracy based on the inference accuracy degradation caused by individual decompressed layers.</p><p>(3) We develop an efficient optimization algorithm to determine the best-fit configuration of error bounds in order to maximize the compression ratio under the user-set inference accuracy constraint. Experiments show that DeepSZ can compress AlexNet and VGG-16 on the ImageNet dataset by a compression ratio of 46× and 116×, respectively, and compress LeNet-300-100 and LeNet-5 on the MNIST dataset by a compression ratio of 57× and 56×, respectively, with only up to 0.3% loss of inference accuracy. Compared with other state-of-the-art methods, DeepSZ can improve the compression ratio by up to 1.43×, the DNN encoding performance by up to 4.0× with four V100 GPUs, and the decoding performance by up to 6.2×.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ABSTRACT Today's deep neural networks (DNNs) are becoming deeper and wider because of increasing demand on the analysis quality and more and more complex applications to resolve. The wide and deep DNNs, however, require large amounts of resources (such as memory, storage, and I/O), significantly restricting their utilization on resource-constrained platforms. Although some DNN simplification methods (such as weight quantization) have been proposed to address this issue, they suffer from either low compression ratios or high compression errors, which may introduce an expensive fine-tuning overhead (i.e., a costly retraining process for the target inference accuracy). In this paper, we propose DeepSZ: an accuracyloss expected neural network compression framework, which involves four key steps: network pruning, error bound assessment, optimization for error bound configuration, and compressed model generation, featuring a high compression ratio and low encoding time. The contribution is threefold. <ref type="bibr" target="#b0">(1)</ref> We develop an adaptive approach to select the feasible error bounds for each layer. <ref type="bibr" target="#b1">(2)</ref> We build a model to estimate the overall loss of inference accuracy based on the inference accuracy degradation caused by individual decompressed layers. <ref type="bibr" target="#b2">(3)</ref> We develop an efficient optimization algorithm to determine the best-fit configuration of error bounds in order to maximize the compression ratio under the user-set inference accuracy constraint. Experiments show that DeepSZ can compress AlexNet and VGG-16 on the ImageNet dataset by a compression ratio of 46× and 116×, respectively, and compress LeNet-300-100 and LeNet-5 on the MNIST dataset by a compression ratio of 57× and 56×, respectively, with only up to 0.3% loss of inference accuracy. Compared with other state-of-the-art methods, DeepSZ can improve the compression ratio by up to 1.43×, the DNN encoding performance by up to 4.0× with four V100 GPUs, and the decoding performance by up to 6.2×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) have rapidly evolved to the state-ofthe-art technique for many artificial intelligence tasks in various science and technology areas, for instance, image and vision recognition <ref type="bibr" target="#b34">[35]</ref>, recommender systems <ref type="bibr" target="#b44">[45]</ref>, nature language processing <ref type="bibr" target="#b6">[7]</ref>, and time series classification <ref type="bibr" target="#b47">[48]</ref>. DNNs contain millions of parameters in an unparalleled representation, which is efficient for modeling complexity nonlinearities. Thus, using either deeper or larger DNNs can be an effective way to improve data analysis. As pointed by Wang et al. <ref type="bibr" target="#b45">[46]</ref>, the deep learning community has been acknowledging that increasing the scales of DNNs can improve the inference accuracy of image recognition tasks. A 9-layer AlexNet <ref type="bibr" target="#b20">[21]</ref>, for example, proposed by Krizhevsky et al., won the 2012 ILSVRC (ImageNet Large-Scale Visual Recognition Challenge) <ref type="bibr" target="#b22">[23]</ref> with a top-5 accuracy of 83%. In 2014 ILSVRC, a 22-layer GoogLeNet <ref type="bibr" target="#b36">[37]</ref> proposed by Szegedy et al. further improved the record of top-5 accuracy to 93.3%. He et al. proposed a 152-layer ResNet <ref type="bibr" target="#b17">[18]</ref>, which refreshed the record to 96.43% in 2015 ILSVRC. This trend suggests that the networks will go larger in the future.</p><p>The ever-increasing growth of networks is bringing more and more challenges to resource-limited platforms, such as mobile phones and wireless sensors. For instance, a typical use case is to train DNNs in the cloud using high-performance accelerators, such as graphic processing units (GPUs) or field-programmable gate arrays (FPGAs), and distribute the trained DNN models to edge devices for inferences <ref type="bibr" target="#b41">[42]</ref>. According to the data released by GSMA <ref type="bibr" target="#b13">[14]</ref>, 0.8 billion users will be still using 2G networks (with a theoretical maximum transfer speed of 1 Mbit/s) by 2020. Consequently, one practical challenge is to deliver multiple latest DNN models (i.e., an order of tens to hundreds of megabytes for each model) from cloud to edge devices through bandwidth-limited networks. An issue also arises in a wireless sensor network with DNN models, which is usually deployed in a region of interest for tracking objects or abnormal detection. Since large DNN models must be stored in external DRAM (e.g., embedded flash-based storage) and frequently fetched for inferences, they consume large amounts of energy <ref type="bibr" target="#b14">[15]</ref>. However, sensor nodes are usually equipped with small batteries and it is unfeasible to recharge them or deploy new nodes in many cases. Therefore, how to maintain/extend the lifetime of sensor networks with large DNNs becomes challenging <ref type="bibr" target="#b46">[47]</ref>. Compressing neural networks provides an effective way to reduce the burden of these problems. Most approaches, however, have focused on simplification methods, such as network pruning <ref type="bibr" target="#b15">[16]</ref> and quantization <ref type="bibr" target="#b14">[15]</ref>, which suffer from limited compression quality. A straightforward idea is to leverage existing lossy compression and encoding techniques <ref type="bibr" target="#b32">[33]</ref> to significantly improve the ratio for compressing DNNs. The existing compression strategies applied on DNNs, however, do not have error expected features, which may greatly distort the data, leading to expensive fine-tuning overhead (i.e., extra, costly retraining process).</p><p>In this paper, we propose DeepSZ: a lossy compression framework for DNNs. DeepSZ is composed of four key steps: network pruning, error bound assessment, optimization of error bound configuration, and compressed model generation. Unlike traditional compression methods used on DNNs, we perform error-bounded lossy compression on the pruned weights, an approach that can significantly reduce the data size while restricting the loss of inference accuracy. Specifically, we adapt the SZ lossy compression framework developed by us previously <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> to fit the context of DNN compression. In this compression framework, each data point's value would be predicted based on its neighboring data points by an adaptive, best-fit prediction method (either a Lorenzo predictor or linear regression-based predictor <ref type="bibr" target="#b27">[28]</ref>). Then, each floating-point weight value would be converted to an integer number by a linearscaling quantization based on the difference between the real value and predicted value and a specific error bound. Huffman encoding or other lossless compression such as Zstd <ref type="bibr" target="#b48">[49]</ref> and Blosc <ref type="bibr" target="#b4">[5]</ref> would be applied to significantly reduce the data size thereafter. SZ can get a much higher compression ratio on the compression of nonzero weights than other state-of-the-art compressors such as ZFP <ref type="bibr" target="#b28">[29]</ref> can, especially because of efficient linear-scaling quantization, which contrasts with the simple vector quantization applied to the original weights in other related work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. Moreover, our SZ compressor can control errors in more sophisticated ways, such as relative error bound and peak-signal-to-noise ratio (PSNR).</p><p>Designing an efficient lossy compression framework for DNNs raises two important challenging issues to resolve. (1) How can we determine an appropriate error bound for each layer in the neural network? Specifically, we have to explore a feasible range of error bounds for each layer, under which the lossy compression should still get a high inference accuracy for users. (2) How can we maximize the overall compression ratio regarding different layers in the DNN under user-specified loss of inference accuracy? Considering the heterogeneous and diverse data features across multiple layers, we have to explore the best-fit error bounds on the compression of different layers. A straightforward idea is to traverse all the possible error-bound combinations on different layers, which would definitely lead to an extremely high time-complexity. To address this issue, we develop a dynamic strategy to efficiently determine the best-fit error bound for each layer.</p><p>The contributions of this work are summarized as follows.</p><p>• We propose a novel, accuracy-loss expected framework, called DeepSZ, by applying our previously developed SZ error-bounded lossy compression to compress DNNs. To the best of our knowledge, this is the first attempt to do so. • We propose an adaptive method to select the feasible range of error bounds for each layer. We also develop an effective model to estimate the overall inference accuracy loss based on the forward-propagation results with individual layers reconstructed from the error-bounded lossy compressor. • We develop a dynamic algorithm to optimize the combined configuration regarding different layers' error bounds, significantly reducing the overall size of the neural network. In addition, because of our careful design of the accurate error control, our solution also effectively eliminates the costly retraining overhead that was generally introduced by other DNN compression methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> The rest of the paper is organized as follows. In Section 2, we discuss the background and motivation of our research. In Section 3, we describe the design methodologies of the DeepSZ framework in detail. In Section 4, we provide a detailed analysis and comparison of DeepSZ and two other state-of-the-art approaches. In Section 5, we present the evaluation results on four well-known DNNs with multiple GPUs. In Section 6, we discuss related work. In Section 7, we summarize our conclusions and present ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we present some background information about neural networks and lossy compression for floating-point data and discuss our research motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Networks</head><p>Neural networks have been widely studied and used in recent years and have produced dramatic improvements in many scientific and engineering aspects, such as computer vision <ref type="bibr" target="#b33">[34]</ref> and natural language processing <ref type="bibr" target="#b6">[7]</ref>. Each neural network is composed of multiprocessing layers. Among the various kinds of layers, convolutional layers and fully connected layers (denoted by fc-layers) have contributed the most to the recent progress in the deep learning community, especially in vision-related tasks such as image classification and object detection. One convolutional layer consists of a set of filters that slide in the input dataset and perform convolution with the signal in the sliding window. fc-layers are connected by a dense weight matrix and forward the signals by a matrix-matrix multiplication. The filters in the convolutional layers and the weight matrices in the fc-layers dominate the storage space of the neural networks, which will become larger as the networks become deeper or wider. In neural networks, the forward pass refers to the calculation process, which traverses from the first layer to the last layer. The backward pass refers to the process to update the weights by stochastic gradient descent, which traverses from the last layer backward to the first layer. During the training period, both forward and backward passes are performed, whereas only the forward pass is performed for testing. In the following discussion, test refers to the forward pass process on the test dataset for generating the inference accuracy of the neural network. According to prior studies <ref type="bibr" target="#b2">[3]</ref>, although convolutional layers occupy most of the computation time (∼95%) because of the expensive convolution operations, they take up little storage space (∼5%). On the other hand, fc-layers require large storage space (∼95%) because of the large dense matrices, while consuming little computation time (∼5%). This phenomenon is also verified in our experiments. For demonstration purposes, we present the breakdown of storage and computational overhead for four well-known networks. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the fc-layers take the majority of the networks' storage space (i.e., 89.4% ∼ 96.1%) in all three cases; however, they have much lower computational cost (i.e., about 1% ∼ 2% for VGG-16 and AlexNet and 20% for LeNet-5) than the convolutional layers do. Hence, we are motivated to leverage lossy compression techniques in order to trade computation time for storage space on the fc-layers in resource-constrained scenarios, our aim is to significantly reduce the storage requirements of neural networks while introducing little computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lossy Compression for Floating-Point Data</head><p>Floating-point data compression has been studied for decades. The data compressors can be split into two categories: lossless and lossy. Lossless compressors such as GZIP <ref type="bibr" target="#b9">[10]</ref>, FPZIP <ref type="bibr" target="#b30">[31]</ref>, and BlosC <ref type="bibr" target="#b1">[2]</ref> cannot significantly reduce the floating-point data size because of the significant randomness of the ending mantissa bits. The compression ratios of lossless compression are generally limited to 2:1, according to recent studies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Many lossy compressors supporting floating-point data were proposed originally for visualization. Hence, many lossy compressors employ the techniques directly inherited from lossy compression of images, such as variations of wavelet transforms, coefficient prioritization, and vector quantization. Lossy compressors for image processing are designed and optimized considering human perception, such as JPEG2000 <ref type="bibr" target="#b40">[41]</ref>. While such compressors may be adequate for scientific visualization, they do not provide pointwise error controls on demand. For example, most lossy compressors designed for visualization do not provide control of a global upper bound on the compression error (the maximum compression error, or L inf norm of the compression error).</p><p>A new generation of lossy compression techniques for floatingpoint data has been developed recently. SZ, ZFP, and MGARD 1 are three typical error-bounded compressors. SZ <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> predicts each data point's value by its neighboring data points in a multidimensional space with an adaptive predictor (using either a Lorenzo predictor <ref type="bibr" target="#b18">[19]</ref> or linear regression <ref type="bibr" target="#b27">[28]</ref>). Next, it performs an error-controlled linear-scaling quantization to convert all floating-point values to an array of integer numbers. And then it performs a customized Huffman coding and lossless compression to shrink the data size significantly. ZFP <ref type="bibr" target="#b28">[29]</ref> splits the whole dataset into many small blocks and compresses the data in each block separately by four steps: alignment of exponent, orthogonal transform, fixed-point integer conversion, and bit-plane-based embedded coding. MGARD uses multigrid methods to compress multidimensional floating-point data <ref type="bibr" target="#b0">[1]</ref>. Many independent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> have showed that SZ outperforms the other two compressors in terms of compression ratio, especially on 1-D floating-point datasets; note that the datasets to compress in our case are 1-D floating-point arrays after conversion.</p><p>Today's lossy compression techniques have been used in HPC scientific applications for saving storage space and reducing the I/O cost of saving data. However, how to effectively and efficiently utilize error-bounded lossy compressors to significantly reduce the neural network size and encoding time, while still maintaining a high inference accuracy, remains an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN METHODOLOGIES</head><p>In this section, we describe in detail DeepSZ, our proposed lossy compression framework for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of DeepSZ Framework</head><p>The general workflow of the DeepSZ framework is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. As illustrated in the figure, DeepSZ consists of four key steps: network pruning, error bound assessment, optimization of the error bound configuration, and generation of the compressed model. The first step is to adopt network pruning in order to reduce the network complexity and mitigate the overfitting problem caused by the large number of parameters in the network. The second step is to apply the error-bounded lossy compression to the pruned fc-layers and assess the impacts of different error bounds on the inference accuracy for different fc-layers. Based on the inference accuracy degradation, DeepSZ will identify the feasible range of error bounds for each fc-layer and collect the results of inference accuracy degradation and compressed layer size based on these bounds. This step can effectively narrow the range of the best-fit error bounds for each layer. Note that we focus only on the fclayers in this work because it dominates most of the storage space, as discussed in Section 2. The third step is to determine the bestfit error bound for each fc-layer based on the narrowed feasible range generated from the second step. DeepSZ will compress the network as much as possible while satisfying the user-set inference accuracy requirement. The fourth step is to generate the compressed network based on the optimized error bounds and best-fit lossless compressor. In the remainder of this section, we discuss each step of DeepSZ in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Pruning</head><p>An fc-layer in DNNs can be represented by a floating-point matrix.</p><p>Each nonzero element in the matrix represents the weight of one connection between previous layer and current layer. Previous studies on modern neural network models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> have shown that most of the weights in fc-layers are redundant and can be pruned without any impact on the inference accuracy. Moreover, network pruning is an effective way to prevent the DNN model from overfitting. We build our pruning method on top of prior state-of-the-art techniques <ref type="bibr" target="#b14">[15]</ref>, which can prune DNNs without loss of inference accuracy. We first set up thresholds for each fc-layer and prune their weights based on these thresholds: every weight below these thresholds will be removed. The thresholds are set based on the predefined pruning ratios suggested by previous studies <ref type="bibr" target="#b15">[16]</ref>. Then, we retrain the network with masks (i.e., zero weights are marked as unchanged) on fc-layers such that the weights that have been pruned can be kept zero. This pruning method is called magnitude threshold plus retraining (Magnitude). Note that this process can start from well-trained networks; more details can be found in <ref type="bibr" target="#b15">[16]</ref>. We note that Reagen et al. <ref type="bibr" target="#b32">[33]</ref> presented another pruning method, dynamic network surgery (DNS), and evaluated its performance on several networks. The time overhead of DNS applied to large networks (such as VGG-16) is very high, however, because DNS needs to iteratively prune the weights and retrain the network based on increasing thresholds. In contrast, Magnitude has relatively lower time overhead thus can be well applied to large neural networks. Therefore, we focus on the Magnitude method in this paper.</p><p>After the network pruning, the weight matrix becomes sparse, so it can be represented by a sparse matrix format, such as compressed sparse row (CSR) or compressed sparse column (CSC) format. Unlike the traditional format that uses three 1-D arrays (e.g., arrays</p><p>for nonzero values, the extents of rows, and column indices in CSR), we only use two 1-D arrays to represent one fc-layer after the pruning. One array is named data array 2 ; it is used to store the floating-point weights (32 bits per value); the other one is named index array; it is used to store the index differences between two consequent nonzero weights (8 bits per value). Similar to <ref type="bibr" target="#b14">[15]</ref>, if the index difference exceeds 256 (i.e., 2 8 ), we additionally save a zero padding to data array and 255 to index array. Here we use sparse matrix representation because the inference accuracy can be dropped sharply (i.e., to 20% on the tested networks) if the lossy compression is applied to the matrices of pruned weights (i.e., 2-D arrays) based on our experiments. Note that the real compression ratio after the pruning step (i.e., original size divided by the CSR size) is always lower than the compression ratio that we set for the pruning (i.e., one divided by the pruning ratio), because after the pruning every nonzero weight will be represented by 40 bits (8 for index and 32 for data), which is slightly larger than the original 32 bits. Based on our evaluation results, the pruning step can typically reduce the size of fc-layers by about 8× to 20× if the pruning ratio is set to be around 4% to 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error Bound Assessment</head><p>SZ lossy compression usually has a much higher compression ratio on 1-D datasets than other lossy compression methods do <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>. Our floating-point datasets are 1-D data arrays, as described in Section 3.2. For demonstration purposes, we evaluated SZ and ZFP 3 on the 1-D data arrays of each fc-layer in AlexNet and VGG-16. The compression ratios are presented in <ref type="figure">Figure 2</ref>. The figure shows that SZ consistently outperforms ZFP in terms of compression ratios on the tested fc-layers with absolute error bounds of 10 −2 , 10 −3 , and 10 −4 . Although SZ has higher compression and decompression times than does ZFP <ref type="bibr" target="#b38">[39]</ref>, they are still much lower than the time overheads of forward or backward pass in neural networks. Taking these facts into consideration, we propose to use SZ lossy compression in our DeepSZ framework.</p><p>Error-bounded lossy compression can provide high compression ratio but can also bring bounded errors to neural networks, leading to possible loss of inference accuracy. Thus, before adopting SZ to compress fc-layers, we need to find the best-fit error bound</p><formula xml:id="formula_0">SZ-fc6 ZFP-fc6 SZ-fc7 ZFP-fc7 SZ-fc8 ZFP-fc8 0 5 10 15 20 compression ratio VGG-16 1E-2 1E-3 1E-4 SZ-fc6 ZFP-fc6 SZ-fc7 ZFP-fc7 SZ-fc8 ZFP-fc8</formula><p>AlexNet <ref type="figure">Figure 2</ref>: Compression ratios of SZ and ZFP lossy compression on fc-layer in AlexNet and VGG-16.  for each layer. Our idea is to narrow the best-fit error bounds by identifying a feasible error bound range for each fc-layer with a high compression ratio and also bounded loss of inference accuracy (detailed in this subsection) and then fine tune the best-fit error bound within the range (detailed in next subsection). To this end, we need to understand the impact of different error bounds of each fc-layer on the overall inference accuracy. Specifically, we use SZ to compress each fc-layer's data array with different error bounds and use its decompressed array to reconstruct the fc-layer while leaving the other fc-layers uncompressed or unchanged. Based on the reconstructed network (only one fc-layer is modified), we can perform the forward pass on the test data to generate the inference accuracy. Thus, we can get a series of inference accuracies based on different error bounds for each fc-layer in the network. For example, <ref type="figure" target="#fig_1">Figure 3</ref> presents the accuracies based on the absolute error bounds from 10 −1 to 10 −4 for the three fc-layers in AlexNet. Note that in order to reduce the time overhead, we choose only a group of error bounds for compression, decompression, and checking inference accuracy rather than all the error bounds shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In the following text we will describe how to determine the error bounds.</p><p>In our solution, we test the inference accuracy with only one compressed layer in every test, instead of using a brute-force method to search all possible test cases (i.e., all combinations of error bounds across all layers), for the following two reasons. <ref type="bibr" target="#b0">(1)</ref> We observe that the fc-layers in neural networks usually have independent characteristics in the context of SZ lossy compression. That is, two reconstructed fc-layers based on SZ affect the overall accuracy independently. Thus, the overall loss of inference accuracy can be composed of (and thus estimated by) the losses of inference accuracy introduced by individual layers. We will discuss more details in Section 3.4. (2) Checking the inference accuracy in a test with only one reconstructed layer using multiple error bounds has much Specifically, our solution has a linear time complexity compared with the brute-force method with an exponential time complexity. VGG-16, for instance, has three fc-layers. Assume we have 10 candidate error bounds for each layer. Then the brute-force method needs to check 1,000 test cases, each involving one compression, one decompression, and one forward-pass test. By comparison, our solution has only 30 test cases to check, thus reducing the testing time to 3% compared with the brute-force method.</p><p>We propose an algorithm to identify the feasible range of error bounds and collect the results about inference accuracy degradation and compressed layer size based on these bounds for each fc-layer. We present the pseudo-code in Algorithm 1. The inputs of the algorithm include the architecture of the neural network, the pruned weights of the network, and the user-set loss of inference accuracy. Lines 12-21 show the main loop of this algorithm. Specifically, when the loss of inference accuracy exceeds a criterion of 0.1% (called distortion criterion) in terms of absolute percentage, we treat the reconstructed network to be distorted. We search the feasible range of error bounds by checking the accuracy based on multiple error bounds. The error bound to check starts from a certain value (i.e., the default value is 10 −3 ; the reason will be discussed in Section 5.1) and will be increased by an order of magnitude (10×) each time. As the accuracy drops below the criterion (i.e., 0.1%) at a certain error bound eb ′ , we set the starting point of the range to be eb ′ /10. Note that the default value of 10 −3 can be further decreased, such as 10 −4 , based on different neural networks.</p><p>From Lines 1-10, we determine the ending point of the range based on the accuracy of reconstructed network. The ending point is the first error bound that the accuracy drops below the user's expected accuracy ϵ * . Once the feasible range is generated (as shown in <ref type="figure" target="#fig_1">Figure 3</ref>), we conduct the tests with the error bounds in the feasible range and collect the sizes of compressed layer and accuracy degradation results. Lines 6-8 describe how we choose the error bounds within the range. For example, if the range is [8 × 10 −3 , 3 × 10 −2 ], we test the error bounds of 8 × 10 −3 , 9 × 10 −3 , 10 −2 , 2 × 10 −3 , and 3 × 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization of Error Bound Configuration</head><p>Depending on the number of input and output neurons, the sizes of different fc-layers vary dramatically. For example, the largest fc-layer of VGG-16 is fc6 (i.e., 25,088×4,096), which is 25× larger than the smallest layer fc8 (i.e., 1,000×4,096). Compressing larger fc-layers with a higher error bound can lead to a higher compression ratio, which can benefit the overall ratio. However, the higher error bound also brings more errors to the network, which may degrade the overall accuracy in turn. Therefore, how to determine the optimal error bound for each fc-layer is an important problem.</p><p>From our experiments we discovered that the overall accuracy loss in the neural network exhibits an approximate linearity in terms of the accuracy degraded in each fc-layer in the context of SZ lossy compression when the targeted loss of inference accuracy is lower than 2% based on our tested neural networks. In other words, the overall accuracy loss Λ ⋆ is approximately equal to the sum of each layer's accuracy degradation ∆ ℓ , as shown in Equation <ref type="formula" target="#formula_1">(1)</ref>, where eb ℓ is a given arbitrary error bound for each fc-layer ℓ:</p><formula xml:id="formula_1">∆ ⋆ = ∆ ℓ,eb ℓ , ∆ ⋆ &lt; 2%.<label>(1)</label></formula><p>This observation can be explained by a theoretical analysis on the independent impact of compression error introduced in each fc-layer on the output of a neural network. Assume t 1 and t 2 to be the original outputs of two successive fc-layers. Assume ∆W 1 and ∆W 2 to be the compression errors introduced to the weights of these two fc-layers, respetively. When computations pass these two fc-layers, the outputs of the two fc-layer would be</p><formula xml:id="formula_2">t ⋆ 1 = f ((W 1 + ∆W 1 )t 0 + b 1 ), t ⋆ 2 = f ((W 2 + ∆W 2 )t ⋆ 1 + b 2 ),</formula><p>(2) where f represents the activation function of fc-layers, W represents the weights, and b represents the bias. For most of neural networks, the activation function f used in fc-layers is rectified linear unit (a.k.a., ReLU), which is max(0, x) (where x is the input to a neuron). In this case, due to ∆W 1 &lt;&lt; W 1 and ∆W 2 &lt;&lt; W 2 , Equation (2) can be simplified to</p><formula xml:id="formula_3">t ⋆ 1 = f (W 1 t 0 + b 1 ) + f ∆W 1 t 0 = t 1 + f ∆W 1 t 0 ,<label>(3)</label></formula><formula xml:id="formula_4">t ⋆ 2 = t 2 + f W 2 ∆W 1 t 0 + f ∆W 2 t 1 .<label>(4)</label></formula><p>The final output T ⋆ of the network would be</p><formula xml:id="formula_5">T ⋆ = T + f ∆W 1 t 0 Π n i=2 (W i ) + f ∆W 2 t 1 Π n i=3 (W i ).<label>(5)</label></formula><p>where n is the total number of fc-layers and T is the original output. We can clearly observe that two errors ∆W 1 and ∆W 2 have independent impacts on the final output T ⋆ . Therefore, we can conclude that compression error introduced in each fc-layer would have independent impact on final network's output. In order to assure ∆W &lt;&lt; W , we will choose the error bound to be lower than 0.1 in our experiments. Finally, the relationship between final output and accuracy loss is approximately linear, which will be experimentally demonstrated in Section 5.2. We propose Algorithm 2 to determine the best-fit error bound for each layer. The inputs include the accuracy degradation and compressed size of each fc-layer based on our tested error bounds, end for 20 end procedure which are outputted by our previous error bound assessment (Section 3.3) and an expected loss of accuracy set by users. Algorithm 2 can minimize the total size of the compressed fc-layers while ensuring the sum of the accuracy degradation of each layer to be within the expected accuracy loss. Specifically, the first part of Algorithm 2 (Lines 2-14) finds the minimum total size of compressed fc-layers with different combinations of error bounds before a certain layer by using a variation of Knapsack algorithm. Then, the algorithm traces back to determine the error bound for each fc-layer (Lines <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. More specifically, we save the minimal size of all fc-layers (before ℓ-layer) and the accuracy loss of ϵ * to the variable S (ℓ;ϵ ) . After we find the minimum compressed size of all the fc-layers under the constraint of the overall accuracy loss (Lines 13-14), we trace back from the minimal S (ℓ;ϵ ) to identify the error bound combination for each layer (Lines <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>.</p><p>Besides this optimization of the compression ratio by an expected accuracy loss (i.e., expected-accuracy mode), DeepSZ can optimize the overall accuracy with an expected compression ratio (i.e., expected-ratio mode).The algorithm of the fixed-rate mode is similar to Algorithm 2 but just reverses the compressed size and accuracy degradation. Based on these two modes, we can fine tune the balance between accuracy loss and compression ratio for a neural network, which is much more flexible than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Generation of Compressed Model</head><p>The last step in our framework is to generate the compressed model by using SZ lossy compression on the data arrays with the error bounds (obtained in Step 3) and the best-fit lossless compression on the index arrays. The index array represents the locations of nonzero weights, which need to be compressed losslessly. DeepSZ provides three state-of-the-art lossless compressors: Gzip <ref type="bibr" target="#b9">[10]</ref>, Zstandard <ref type="bibr" target="#b48">[49]</ref>, and Blosc <ref type="bibr" target="#b48">[49]</ref>. More lossless compressors can be integrated into the framework in the future. In our experiments, we identified that Zstandard always leads to the highest compression ratio compared with the other two compressors, as shown in  . After these four steps of DeepSZ, the compressed neural network model is generated. In this paper, we use encoding to refer this whole process of generating compressed DNNs and decoding to refer the process of reconstructing DNNs. Once the network is needed for forward pass, it must be decoded. During the decoding, DeepSZ will decompress the data arrays using the SZ lossy compression and the index arrays using the bestfit lossless compression (e.g., Zstandard). Then, the sparse matrix can be reconstructed based on the decompressed data array and index array for each fc-layer. Finally, the whole neural network can be decoded. Note that the computational cost of the decoding in DeepSZ is relatively low compared to that of the forward pass with a batch of images. We will analyze the performance overhead of our decoding in detail and compare it with other state-of-the-art methods next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DETAILED ANALYSIS AND COMPARISON</head><p>In this section, we analyze DeepSZ in detail and compare it with two other state-of-the-art solutions: Weightless <ref type="bibr" target="#b32">[33]</ref> and Deep Compression <ref type="bibr" target="#b14">[15]</ref>. Our analyses focus on both performance and storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Analysis of DeepSZ</head><p>For Algorithm 1 in DeepSZ, the computational cost is focused mostly on performing the tests with different error bounds to check the corresponding accuracies, while compression and decompression both cost negligible time overhead. Let us take AlexNet as an example. Compressing and decompressing one data array (about tens of megabytes per fc-layer's data array, as shown in <ref type="table" target="#tab_5">Table 2c</ref>), and reconstructing the network based on the decompressed layer typically take no more than one second on an Nvidia Tesla V100 GPU, <ref type="bibr" target="#b3">4</ref> whereas testing the reconstructed network with 50,000 images in the ImageNet dataset will take about 55 seconds (10 seconds for data transfer, 5 seconds for initialization, and 40 seconds for forward computations). In this case, DeepSZ needs to perform 12 tests on each fc-layer, -bringing the total to 36 tests. In contrast, the 36 tests require performing forward passes of 1.8 million images considering 50,000 images in the ImageNet test data. Based on our experiments, the execution time of one epoch is about 42 times higher than that of one test with an Nvidia V100 GPU on AlexNet 5 . Thus, the workload of 1.8 million images in the test is equivalent to training about <ref type="bibr">6 7</ref> epochs of data (i.e., one test is <ref type="bibr" target="#b3">4</ref> Based on a recent study <ref type="bibr" target="#b38">[39]</ref>, SZ's compression and decompression rate are about 80 MB/sec and 150 MB/sec, respectively, with the error bound of 10 −3 on a 2.3 GHz Intel Core i7 processor. <ref type="bibr" target="#b4">5</ref>  , which is much less than that of traditional methods with retraining (typically O(5 · M) to O(10 · M) in the ImageNet dataset). It is worth noting that the scalability of test (i.e., embarrassing parallelism) is higher than that of training in parallel, thus, the time complexity of DeepSZ compared with training will be further reduced with increasing scale.</p><p>For DeepSZ's decoding, the computational cost is also comparatively low because it performs an O(n pruned ) lossy decompression with SZ, an O(n pruned ) lossless decompression with the best-fit lossless compressor (e.g., Zstandard) and an O(n) sparse-dense matrix conversion. Here we denote the number of pruned weights by n pruned and the number of original weights by n. Overall, the time complexity of DeepSZ's decoding is O(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Weightless</head><p>DeepSZ has four major advantages over Weightless. (1) Weightless has higher time overhead than does DeepSZ for encoding. After Weightless reconstructs the layer based on the Bloomier filter, the inference accuracy can drop dramatically. For example, the inference accuracy drops about 3% when compressing fc6 in VGG-16 using Weightless. Thus, Weightless requires retraining the other layers to recover the overall inference accuracy, whereas DeepSZ does not require any retraining. (2) Weightless has higher time overhead than does DeepSZ on decoding. To decode one element, Weightless has to calculate four hash functions based on all the values (including zero values) in the pruned matrix and check the hash table to determine the value of this element, leading to much higher time overhead compared with DeepSZ. (3) Weightless can compress only one layer (usually the largest layer). By contrast, DeepSZ can compress all fc-layers, leading to higher overall compression ratio. (4) DeepSZ provides two modes to users. Even for the fixed-accuracy mode, users can set an expected loss of inference accuracy in DeepSZ and get as high a compression ratio as possible, whereas Weightless is unable to provide such flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Deep Compression</head><p>Similar to Weightless, Deep Compression also requires retraining the whole network to mitigate the inference accuracy loss caused by its quantization. Deep Compression adopts a simple quantization technique on the pruned weights. It quantizes all the nonzero   weights to a group of floating-point values based on a code-book. The number of these values in the code-book is always 2 k , where k refers to the number of bits used to represent one weight. Using 5 bits per weight, for example, can map every nonzero weights to a 32-value code-book. Unlike Deep Compression applying a simple quantization to the weights, DeepSZ applies an error-bounded linear-scaling quantization to the difference between the predicted weight and real weight based on a best-fit prediction method, leading to higher compression ratios and fine-granularity error controls. Similar to Weightless, Deep Compression has lower flexibility than DeepSZ has in terms of the balance between the ratio and inference accuracy. Since the number of floating-point values the code-book can represent is always 2 k , the inference accuracy under Deep Compression may drop significantly (shown in Section 5.2) with increasing compression ratios (or lower bit rates), leading to unbounded inference accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate our proposed DeepSZ framework by comparing it with state-of-the-art methods. actual accuracy loss <ref type="figure">Figure 6</ref>: Approximate linearity relationships of error introduced by different fc-layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>We conduct our evaluation on a single core of an MacBook Pro with Intel Core i7-8750H Processors (with 32 GB of memory) and parallel experiments using four Nvidia Tesla V100 GPUs (each with 16 GB of memory) on the node of the Pantarhei cluster at the University of Alabama. The four GPUs are connected via NVLink <ref type="bibr" target="#b11">[12]</ref>. We implement DeepSZ based on the Caffe deep learning framework <ref type="bibr" target="#b19">[20]</ref> (v1.0) and SZ lossy compression library (v2.0) <ref type="bibr" target="#b27">[28]</ref>. We evaluate DeepSZ on four well-known neural networks: LeNet-300-100 <ref type="bibr" target="#b24">[25]</ref>, LeNet-5 <ref type="bibr" target="#b23">[24]</ref>, AlexNet <ref type="bibr" target="#b20">[21]</ref>, and VGG-16 <ref type="bibr" target="#b34">[35]</ref>. We train/test LeNet300-100 and LeNet-5 on the MNIST dataset and AlexNet and VGG-16 on the ImageNet dataset 6 , respectively. These neural networks and datasets are commonly used in evaluation studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>. We present the details of their architectures in <ref type="table" target="#tab_1">Table 1</ref>. Note that the fc-layers occupy most of the storage space (i.e., 89.4% ∼ 96.1%). We use the default solver (i.e., stochastic gradient descent (SGD)) in Caffe for all training. We set the expected loss of inference accuracy to 0.2% for two LeNets and 0.4% for AlexNet and VGG-16, without loss of generality. We also set the expected loss of inference accuracy to zero and demonstrate the flexibility of DeepSZ. We note that in an fc-layer of a neural network, weights are floating-point numbers between -1.0 and 1.0; more generally, for a trained network, such as AlexNet and VGG-16, the value ranges of their weights are typically between -0.3 and +0.3. Thus, the absolute error bounds in the order of 10 −1 are relatively large compared with the weight values. Consequently, using the error bounds in the order of 10 −1 would significantly affect the overall inference accuracy (i.e., dropped to less than 20%), as illustrated in <ref type="figure" target="#fig_8">Figure 5</ref>. We also note that the absolute error bound of 10 −4 can maintain the inference accuracy without any loss for these networks. Thus, we set 10 −3 to be the default value for initial point of the error bound to be checked. It is worth pointing out that the output of a neural network for image recognition is a vector of probabilities for different types. Based on these probabilities, a neural network can predict the type of an image. Thus, the inference accuracy in the context of image recognition and neural network is the precision ratio. Similar to previous studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>, no recall ratio and F1 score would be measured in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Linearity of Accuracy Loss.</head><p>We first experimentally demonstrate the approximate linearity of accuracy loss on fc-layers based on AlexNet and VGG-16. We set different combinations of error bounds (i.e., within 0.1) for fc-layers and compare our expected accuracy loss (based on Equation <ref type="formula" target="#formula_1">(1)</ref>) with the actual accuracy loss, as shown in <ref type="figure">Figure 6</ref>. Specifically, x-axis shows our expected accuracy loss, which is the sum of accuracy degradation in each fc-layer, and y-axis shows the actual accuracy loss. We can observe a clear linear relationship when the overall accuracy loss is lower than 2%. Therefore, we can use this approximate linearity of accuracy loss to perform the following optimization of error bound configurations discussed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Compression</head><p>Ratio. We then present the experimental results of DeepSZ in terms of compression ratio and compare with Deep Compression and Weightless.</p><p>LeNet-300-100 and LeNet-5 on MNIST. First, we evaluate DeepSZ on LeNet300-100 and LeNet-5 with the MNIST dataset <ref type="bibr" target="#b25">[26]</ref>. LeNet-300-100 contains only three fc-layers (i.e., ip1, ip2, and ip3). LeNet-5 contains three convolutional layers and two fc-layers (i.e., ip1 and ip2). The fc-layers dominate: 100% and 95.3% of the overall sizes of LeNet-300-100 and LeNet-5, respectively, as shown in <ref type="table" target="#tab_1">Table 1</ref>. DeepSZ first prunes the network with the pruning ratios suggested by <ref type="bibr" target="#b14">[15]</ref> (as shown in <ref type="table" target="#tab_5">Table 2a</ref> and 2b) and stores the pruned weights in the data arrays and index arrays. After the pruning step, LeNet-300-100 and LeNet-5 can be reduced by 9.7× and 9.8×, respectively. Note that the compression ratio is slightly different from the pruning ratio because every nonzero pruned weight requires 40 bits instead of 32 bits, as discussed in Section 3.2. Then, DeepSZ deploys the error bound assessment step to the pruned network and gets the feasible ranges of error bounds for fc-layers. The feasible ranges are [10 −2 , 3 × 10 −2 ], [10 −2 , 8 × 10 −2 ], and [10 −2 , 2 × 10 −1 ] for ip1, ip2, and ip3 of LeNet-300-100, respectively, as shown in <ref type="figure" target="#fig_8">Figure 5a</ref>. The ranges are [10 −2 , 3 × 10 −2 ] and [10 −2 , 9 × 10 −2 ] for ip1 and ip2 of LeNet-5, respectively, as shown in <ref type="figure" target="#fig_8">Figure 5b</ref>. DeepSZ then optimizes the configuration of the error bounds based on Algorithm 2. The final error bounds of ip1, ip2, and ip3 of LeNet-300-100 are 2 × 10 −2 , 3 × 10 −2 , and 4 × 10 −2 , respectively. The final error bounds of ip1 and ip2 of LeNet-5 are 3 × 10 −2 and 8 × 10 −2 , respectively. DeepSZ then adopts the best-fit lossless compressor-Zstandard-to compress the index arrays. As shown in <ref type="table" target="#tab_5">Table 2a</ref> and 2b, DeepSZ can compress fc-layers of LeNet-300-100 by 55.8× and the fc-layers of LeNet-5 by 57.3× with no loss of inference accuracy. We note that each fc-layer has a threshold of error bound, after which the inference accuracy begins to drop sharply. This phenomenon is also true for other networks, such as AlexNet and VGG-16. It demonstrates that how to determine the proper error bounds for each fc-layer is a critical problem, for which DeepSZ provides an efficient, fine-tuning solution (see Section 3.3 and Section 3.4).</p><p>AlexNet on ImageNet. We next evaluate DeepSZ on a much larger network, AlexNet, with the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref>. AlexNet contains five convolutional layers and three fc-layers (i.e., fc6, fc7, and fc8). The fc-layers take up 96.1% of the overall storage space, as shown in <ref type="table" target="#tab_1">Table 1</ref>. After the pruning, the network can be reduced to 10.1%, as shown in <ref type="table" target="#tab_5">Table 2c</ref>. After the second assessment and third optimization step, DeepSZ uses 7×10 −3 , 7×10 −3 , and 5×10 −3 as the error bound for fc6, fc7, and fc8, respectively, as shown in <ref type="figure" target="#fig_8">Figure  5d</ref>. DeepSZ can compress AlexNet by 45.5× with only 0.13% loss of top-1 accuracy, as shown in <ref type="table" target="#tab_6">Table 3</ref>. Note that the top-5 accuracy is not decreased but, rather, is increased by 0.18%. We can further set the expected inference accuracy loss to zero. DeepSZ then can compress AlexNet by 36.5× with no loss of inference accuracy (the error bound of 2 × 10 −3 for all fc6, fc7, and fc8).</p><p>VGG-16 on ImageNet. We now apply DeepSZ on VGG-16, which contains one large fc-layer (i.e., fc6) and two relatively small fclayers (i.e., fc7 and fc8). The pruning ratios are set to relatively low values, leading to a much higher compression ratio after the pruning (i.e., 20.9×), as shown in <ref type="table" target="#tab_5">Table 2d</ref>. DeepSZ then uses 10 −2 , 9 × 10 −3 , and 5 × 10 −3 as the error bound for fc6, fc7, and fc8, respectively. By leveraging DeepSZ, we can achieve a compression ratio of 115.6× with only 0.25% loss of inference accuracy on VGG-16, as shown in <ref type="table" target="#tab_6">Table 3</ref>. Similar to AlexNet, we can also set the expected inference accuracy loss to zero. DeepSZ then can compress VGG-16 by 92.7× with no loss of inference accuracy (with the error bound of 3 × 10 −3 for fc6 and fc7 and 10 −3 for fc8).</p><p>In summary, DeepSZ can compress the fc-layers in the tested neural networks with compression ratios of 57× to 116× while maintaining a loss of inference accuracy less than 0.3% (within the user-set expected loss of 0.4%), as shown in <ref type="table" target="#tab_6">Table 3</ref>. Note that the top-5 accuracy is usually not displayed on the LeNet-5 because its top-1 accuracy (i.e., &gt; 99%) is relatively high. DeepSZ can improve the overall compression ratio by 21% to 43%, compared with the second-best solution, as shown in <ref type="table">Table 4</ref>. This table also illustrates that DeepSZ can deliver a high compression ratio for each fc-layer.</p><p>Even compared with the Weightless method, which can compress only one layer, DeepSZ can still achieve a comparable compression ratio. We note that compression ratio is not available for some layers in Weightless, because (1) Weightless <ref type="bibr" target="#b32">[33]</ref> does not provide their open source code and (2) the Weightless paper <ref type="bibr" target="#b32">[33]</ref> showed evaluation results only for the largest two layers in LeNet-5 and VGG-16, without any results for AlexNet. We also note that Deep Compression uses 5 bits per pruned weights, whereas DeepSZ can compress the networks to 2.0 ∼ 3.3 bits per pruned weights. If we also set similar bit width for Deep Compression's quantization (i.e., the number of bits based on DeepSZ compressed layers), the inference accuracy will drop sharply by 1.56% for AlexNet and 2.81% for VGG-16, as shown in <ref type="table" target="#tab_7">Table 5</ref>. Note that the inference accuracy degradation is not available for Weightless for LeNet-5 and AlexNet, because Weightless does not provide these results in <ref type="bibr" target="#b32">[33]</ref> (the paper does show the inference accuracy degradation and encoding time overhead for VGG-16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Performance Evaluation.</head><p>As discussed in Section 4, DeepSZ is faster than the other methods theoretically in terms of both encoding and decoding. We now present the time overhead of DeepSZ on the four neural networks, as shown in <ref type="figure" target="#fig_9">Figure 7</ref>. The figure illustrates that DeepSZ has lower encoding and decoding time overheads than do Deep Compression and Weightless. We <ref type="bibr" target="#b6">7</ref> The accuracy is slightly increased by 0.03% in this case. We investigated the times of the last three steps (i.e., spent mainly in the time of compression, decompression, and tests) for DeepSZ's encoding on GPUs. We do not include the pruning time because all three methods have the same pruning process and the time overheads are the same. <ref type="figure" target="#fig_9">Figure 7a</ref> shows the encoding time with the three solutions. We normalize the other two compression methods compared with DeepSZ in <ref type="figure" target="#fig_9">Figure 7a</ref>, because compared with AlexNet and VGG-16, LeNet-5 features much smaller encoding time. Specifically, DeepSZ takes &lt;1 min, 8 min, and 16 min on encoding LeNet-5, AlexNet, and VGG-16, respectively. Deep Compression takes 4 min, 14 min, and 38 min on encoding LeNet-5, AlexNet, and VGG-16, respectively. Due to lack of source code, we estimate the encoding time of Weightless based on the number of epochs (for retraining) shown in the paper and the time of one epoch based on our experimental platform. Weightless takes about 113 min on encoding VGG-16; again, Weightless does not present the encoding time (i.e., the number of epochs) of LeNet-5 or AlexNet in the paper. DeepSZ can improve the encoding performance by 1.8× to 4.0× compared with the second-best solution. We note that for the Deep Compression and Weightless methods, it is difficult to determine the initial parameters of the solver in order to retrain the network. It could take much longer time than the optimal performance overhead if users are not familiar with the characteristics of the network.</p><p>We also investigated the times of lossless decompression, SZ lossy decompression, and sparse matrix reconstruction for DeepSZ's decoding on CPU. As we can see in <ref type="figure" target="#fig_9">Figure 7b</ref>, DeepSZ outperforms the second-best solution by 4.5× to 6.2× for decoding. Specifically, DeepSZ takes 2.7 ms, 296 ms, and 341 ms on decoding LeNet-5, AlexNet, and VGG-16, respectively; Deep Compression takes 13.9 ms, 1,832 ms, and 1,565 ms on decoding LeNet-5, AlexNet, and VGG-16, respectively; and Weightless takes 520 ms, 1,300 ms, and 22,800 ms on decoding LeNet-5, AlexNet, and VGG-16, respectively, as shown in the paper <ref type="bibr" target="#b7">8</ref> . More specifically, for example, DeepSZ spends 26 ms in lossless decompression, 108 ms in SZ lossy decompression, and 162 ms in reconstructing the sparse matrix on AlexNet. As a comparison, the time for one forward pass with 50 images per batch takes 1,100 ms on AlexNet. This demonstrates that the time overhead of DeepSZ's decoding is comparatively low compared with typical forward pass. Therefore, once the network is needed for inference, DeepSZ can quickly decompress the compressed data and reconstruct the network without much delay. Note that the decoding time of Weightless relies on the number of nonpruned weights, whereas the decoding times of DeepSZ and Deep <ref type="bibr" target="#b7">8</ref> The paper evaluated its decoding time on an Intel Core i7-6700K Processor, which has similar processing power to our processor.  Compression depend on the number of pruned weights. This difference can explain the following two observations. (1) DeepSZ and Deep Compression have similar decoding time on AlexNet and VGG-16 because they have similar numbers of pruned weights (i.e., 6.5 million for AlexNet and 5.8 million for VGG-16).</p><p>(2) Weightless spends more time on VGG-16 than AlexNet for decoding because the largest fc-layer of VGG-16 (i.e., fc6 of 25,088×4,096) is much larger than that of AlexNet (i.e., fc6 of 9,216×4,096).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Most of neural networks have significant redundancy in their parameters according to a well-known research study <ref type="bibr" target="#b7">[8]</ref>. Such redundant information may cause significant waste of computation, memory, and storage resources. In general, two types of methods have been proposed to resolve this issue: (1) modifying the network structures to reduce the complexity of parameters and (2) compressing a trained network by removing redundant information.</p><p>Modifying the structures of networks by adopting specialized structure or loss function can reduce the memory footprint while training the larger-scale networks with the same resources. For example, Vanhoucke et al. <ref type="bibr" target="#b43">[44]</ref> exploited a fixed-point representation of activations with 8-bit integer rather than 32-bit floating point. Denton et al. <ref type="bibr" target="#b8">[9]</ref> proposed using low-rank tensor approximations to reduce the number of parameters by up to a factor of 13 for a single layer while keeping the inference accuracy loss of 1% compared with the original network. Arora et al. <ref type="bibr" target="#b3">[4]</ref> theoretically studied using random-like sparse networks with +1/0/-1 weights for interesting properties. Chen et al. <ref type="bibr" target="#b5">[6]</ref> proposed a network architecture, named HashedNets, that uses a low-cost hash function to randomly group connection weights into hash buckets, such that all connections within the same hash bucket share a single value.</p><p>Compressing neural networks is an alternative strategy to reduce the model size. For example, Gong et al. <ref type="bibr" target="#b12">[13]</ref> compressed fc-layers by using vector quantization, which achieved a compression ratio of 24 with 1% inference accuracy loss. Recently, two state-of-the-art works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> have been designed for compressing the network with high compression ratio and inference accuracy. Han et al. proposed a three-step approach, named Deep Compression, that contains pruning, quantization, and encoding. Deep Compression, however, may degrade the inference accuracy significantly in the course of each forward-propagation because of its vector quantization design, such that the network has to to be retrained over and over again in order to reach the target inference accuracy, thus resulting in a high execution time overhead. Reagen et al. proposed a lossy compression method, named Weightless, by adopting a Bloomier filter to compress the weights lossily. For encoding, the Bloomier filter needs to construct a hash table, which is O(n log n) in time complexity; for decoding, in order to decompress one value, the Bloomier filter typically needs to calculate four hashing functions. The time complexity is O(n) for the best case but O(n 2 ) for the worst case. Here n is the number of values for encoding/decoding. Therefore, the Weightless method suffers from a relatively high time overhead because of the expensive Bloomier filter. Moreover, it was applied to only one fc-layer instead of the whole neural network. Tung et al. <ref type="bibr" target="#b42">[43]</ref> proposed a method named CLIP-Q that uses weight pruning and quantization, which is similar to Deep Compression. Unlike Deep Compression that separates pruning and quantization, CLIP-Q combines them at the same step in a single framework and can be performed in parallel with network finetuning. Moreover, it adopts much higher pruning ratio than Deep Compression in order to achieve higher overall compression ratio. In this paper, we mainly compare our proposed DeepSZ with both Deep Compression and Weightless approaches comprehensively.</p><p>Unlike the first type of method that requires modification of the network structure and full retraining, the second type of method is more general and efficient. Therefore, we focus on compressing welltrained neural networks without modifying the network structure for high reduction ratio and inference accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a novel lossy compression framework, called DeepSZ, for effectively compressing sparse weights in deep neural networks. Unlike traditional methods, DeepSZ can avoid the costly retraining process after compression, leading to a significant performance improvement in encoding DNNs. We develop a series of approaches to efficiently determine the best-fit error bound for each layer in the network, maximizing the overall compression ratio with user acceptable loss of inference accuracy. Experimental results based on the tested neural networks show that DeepSZ can achieve compression ratios of up to 116× and can outperform the second-best approach by up to 1.43×. Our experiments with four Nvidia Tesla V100 GPUs demonstrate that DeepSZ can obtain 1.8× to 4.0× performance improvement in encoding compared with the previous state-of-the-art. DeepSZ can improve the decoding performance by 4.5× to 6.2× compared with the second-best solution. DeepSZ also can provide high flexibility to balance the compression ratio and inference accuracy. We plan to first evaluate our proposed DeepSZ on more neural network architectures. We also will further improve the SZ compression algorithm to achieve a higher reduction ratio in compressing DNNs. Moreover, we hope to use DeepSZ for improving GPU memory utilization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " E v U A 7 s 3 s C T r D A c 0 N e U L z 0 Q o v g D M = " &gt; A A A C C H i c Z V B L S g N B E K 2 J v x h / U Z d u G q P g I o Q Z E X Q Z c O M y g v l A M o a e T k / S p G d 6 6 K 4 R w p A L e A K 3 e g J 3 4 t Z b e A D v Y U + S h T E F T b 1 + 9 e r D C x I p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L a N S z X i T K a l 0 J 6 C G S x H z J g q U v J N o T q N A 8 n Y w v s 3 r 7 S e u j V D x A 0 4 S 7 k d 0 G I t Q M I q W e u z x x A i Z A 4 N U 9 8 s V t + b O g q w C b w E q s I h G v / z T G y i W R j x G J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 0 L Y x p x 4 2 e z q 6 f k 3 D I D E i p t X 4 x k x v 7 t y G h k z C Q K q j Z H F E d 5 z p X G t u Z / s y T O G V R K m i r y 2 N j 5 y 8 s x v P E z E S e p r b L 5 7 j C V B B X J X S E D o T l D O b G A M i 3 s + Y S N q K Y M r X c l 6 4 v 3 3 4 V V 0 L q s e W 7 N u 7 + q 1 M 8 W D h X h B E 7 h A j y 4 h j r c Q Q O a w E D D C 7 z C m / P s v D s f z u d c W n A W P c e w F M 7 X L z b N m y c = &lt; / l a t ex i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E v U A 7 s 3 s C T r D A c 0 N e U L z 0 Q o v g D M = " &gt; A A A C C H i c Z V B L S g N B E K 2 J v x h / U Z d u G q P g I o Q Z E X Q Z c O M y g v l A M o a e T k / S p G d 6 6 K 4 R w p A L e A K 3 e g J 3 4 t Z b e A D v Y U + S h T E FT b 1 + 9 e r D C x I p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L a N S z X i T K a l 0 J 6 C G S x H z J g q U v J N o T q N A 8 n Y w v s 3 r 7 S e u j V D x A 0 4 S 7 k d 0 G I t Q M I q W e u z x x A i Z A 4 N U 9 8 s V t + b O g q w C b w E q s I h G v / z T G y i W R j x G J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 0 L Y x p x 4 2 e z q 6 f k 3 D I D E i p t X 4 x k x v 7 t y G h k z C Q K q j Z H F E d 5 z p X G t u Z / s y T O G V R K m i r y 2 N j 5 y 8 s x v P E z E S e p r b L 5 7 j C V B B X J X S E D o T l D O b G A M i 3 s + Y S N q K Y M r X c l 6 4 v 3 3 4 V V 0 L q s e W 7 N u 7 + q 1 M 8 W D h X h B E 7 h A j y 4 h j r c Q Q O a w E D D C 7 z C m / P s v D s f z u d c W n A W P c e w F M 7 X L z b N m y c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4= " E v U A 7 s 3 s C T r D A c 0 N e U L z 0 Q o v g D M = " &gt; A A A C C H i c Z V B L S g N B E K 2 J v x h / U Z d u G q P g I o Q Z E X Q Z c O M y g v l A M o a e T k / S p G d 6 6 K 4 R w p A L e A K 3 e g J 3 4 t Z b e A D v Y U + S h T E F T b 1 + 9 e r D C x I p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L a N S z X i T K a l 0 J 6 C G S x H z J g q U v J N o T q N A 8 n Y w v s 3 r 7 S e u j V D x A 0 4 S 7 k d 0 G I t Q M I q W e u z x x A i Z A 4 N U 9 8 s V t + b O g q w C b w E q s I h G v / z T G y i W R j x GJ q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 0 L Y x p x 4 2 e z q 6 f k 3 D I D E i p t X 4 x k x v 7 t y G h k z C Q K q j Z H F E d 5 z p X G t u Z / s y T O G V R K m i r y 2 N j 5 y 8 s x v P E z E S e p r b L 5 7 j C V B B X J X S E D o T l D O b G A M i 3 s + Y S N q K Y M r X c l 6 4 v 3 3 4 V V 0 L q s e W 7 N u 7 + q 1 M 8 W D h X h B E 7 h A j y 4 h j r c Q Q O a w E D D C 7 z C m / P s v D s f z u d c W n A W P c e w F M 7 X L z b N m y c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E v U A 7 s 3 s C T r D A c 0 N e U L z 0 Q o v g D M = " &gt; A A A C C H i c Z V B L S g N B E K 2 J v x h / U Z d u G q P g I o Q Z E X Q Z c O M y g v l A M o a e T k / S p G d 6 6 K 4 R w p A L e A K 3 e g J 3 4 t Z b e A D v Y U + S h T E F T b 1 + 9 e r D C x I p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L a N S z X i T K a l 0 J 6 C G S x H z J g q U v J N o T q N A 8 n Y w v s 3 r 7 S e u j V D x A 0 4 S 7 k d 0 G I t Q M I q W e u z x x A i Z A 4 N U 9 8 s V t + b O g q w C b w E q s I h G v / z T G y i W R j x G J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 0 L Y x p x 4 2 e z q 6 f k 3 D I D E i p t X 4 x k x v 7 t y G h k z C Q K q j Z H F E d 5 z p X G t u Z / s y T O G V R K m i r y 2 N j 5 y 8 s x v P E z E S e p r b L 5 7 j C V B B X J X S E D o T l D O b G A M i 3 s + Y S N q K Y M r X c l 6 4 v 3 3 4 V V 0 L q s e W 7 N u 7 + q 1 M 8 W D h X h B E 7 h A j y 4 h j r c Q Q O a w E D D C 7 z C m / P s v D s f z u d c W n A W P c e w F M 7 X L z b N m y c = &lt; / l a t e x i t &gt; Overview of DeepSZ framework for neural network compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Inference accuracy of different error bounds on the fclayers in AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 4 terminate the Check 5 else 6 eb • ← eb • +base 7 base ← 10 × 20 return</head><label>145671020</label><figDesc>Error bound assessment for fc-layers in deep neural networks. Notation: layer: ℓ; Accuracy Degradation: ∆; error bound: eb; threshold: τ ; Expected Accuracy Loss: ϵ ⋆ ; the given network: N; size of compressed layer ℓ with eb: σ (ℓ;eb)Global: N, ϵ ⋆ , all σ (ℓ;eb) , all ∆ (ℓ;eb) 1 procedure Check(ℓ • , eb • , base) 2 test ∆ (ℓ • ;eb • ) on N and update σ (ℓ • ;eb • ) 3 if ∆ (ℓ • ;eb • ) &gt; ϵ ⋆ then base if eb • = 10 × base 8 Check(ℓ • , eb • , base) ErrorBoundAssessment13 for ℓ ← layers in network do 14 for β ← [1E-3,1E-2,1E-1] do ▷ can be pushed to 1E-4 etc. 15 if ∆ (ℓ;β ) &gt; 0.1% then 16 Check(ℓ, 1 × β /10, β /10) all σ (ℓ;eb) , all ∆ (ℓ;eb) 21 end for 22 end procedure less computational cost than does a brute-force method involving every possible combination of error bounds across multiple layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 2 S 3 S 8 S 13 let</head><label>223813</label><figDesc>Optimization of Error Bound ConfigurationNotation: layer: ℓ; error bound: eb; accuracy: ϵ ; AccuracyDegradation at ℓ with eb: ∆ (ℓ;eb) ; size: σ ; TotalSize: S ; ExpectAccuracy: ϵ ⋆ Input:∆ (ℓ;eb) , ϵ ⋆ , σ (ℓ;eb) Output: eb ℓ 1 procedure OptimizeErrorBound (ℓ;∆) ← maximum (ℓzero;ϵ ) ← zero4 for ℓ ← layers in network do 5 for eb ← tested error bounds do 6 for ϵ ← [0 . . . 100] × ϵ ⋆ do 7 if S (ℓprev;ϵ) + σ (ℓ;eb) &lt; S ℓ;ϵ +∆ (ℓ, eb) then ℓ;ϵ +∆ (ℓ, eb) ← S (ℓprev;ϵ ) + σ (ℓ;eb) ℓ ⋆ be the final layer in the network 14 find minimum S (ℓ ⋆ ;∆) 15 for ℓ ← layers in network do ▷ tracing back 16 for eb ← tested error bound do 17 eb ℓ ← eb if S (ℓ;ϵ ) − σ (ℓ;eb) = S ℓprev;ϵ −∆ (ℓ;eb) 18 end for 19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Compression ratios of different layers' index arrays with different lossless compressors on AlexNet and VGG-16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4. After these four steps of DeepSZ, the compressed neural network model is generated. In this paper, we use encoding to refer this whole process of generating compressed DNNs and decoding to refer the process of reconstructing DNNs. Once the network is needed for forward pass, it must be decoded. During the decoding, DeepSZ will decompress the data arrays using the SZ lossy compression and the index arrays using the bestfit lossless compression (e.g., Zstandard). Then, the sparse matrix can be reconstructed based on the decompressed data array and index array for each fc-layer. Finally, the whole neural network can be decoded. Note that the computational cost of the decoding in DeepSZ is relatively low compared to that of the forward pass with a batch of images. We will analyze the performance overhead of our decoding in detail and compare it with other state-of-the-art methods next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>One epoch contains one forward pass and one backward pass of the 1.28 million images in the training dataset and takes about 15.7 minutes for AlexNet on a single Nvidia Tesla V100 GPU, based on our experiments.about 1 42 epochs). Therefore, the time complexity of Algorithm 1 is O c ·k ·M 42 , where c is the number of tests per layer (e.g., 12 for AlexNet), k is the number of fc-layers (e.g., 3 for AlexNet), and O(M) is the time complexity for training one epoch of data. For AlexNet, for example, we can set c to 12 and k to 3; hence, the time complexity is about O( 6·M 7 ). For Algorithm 2 in DeepSZ, because of our optimization in Algorithm 1, the input dimension of the algorithm is very small (i.e., O(c · k), for example 36 pairs of inference accuracy degradation and compressed size for 3-fc-layer AlexNet. Based on the time complexity of the Knapsack algorithm, the time complexity of Algorithm 2 is O(100·ϵ ⋆ ·c·k). We note that O(100·ϵ ⋆ ·c·k) is far smaller than O c ·k ·M 42 because O M 42 is much larger than O(100 · ϵ). Thus, the computational cost of Algorithm 2 would be relatively small compared with multiple tests of inference accuracy in Step 2. Overall, we conclude that the time complexity of DeepSZ's encoding is O c ·k ·M 42</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Inference accuracy of different error bounds on the fclayers in LeNet300-100, LeNet-5, AlexNet, and VGG-16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Time breakdown of encoding and decoding with different lossy compression techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Architectures of example nerual networks.</figDesc><table><row><cell>Neural</cell><cell>LeNet-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LeNet-5</cell><cell>AlexNet</cell><cell>VGG-16</cell></row><row><cell>Networks</cell><cell>300-100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv layers</cell><cell>0</cell><cell>3</cell><cell>5</cell><cell>13</cell></row><row><cell>fc-layers</cell><cell>3</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell cols="5">ip1/fc6 300 × 784 500 × 800 4096 × 9216 4096 × 25088</cell></row><row><cell cols="2">ip2/fc7 100 × 300</cell><cell cols="2">10 × 500 4096 × 4096</cell><cell>4096 × 4096</cell></row><row><cell>ip3/fc8</cell><cell>10 × 100</cell><cell cols="2">-1000 × 4096</cell><cell>1000 × 4096</cell></row><row><cell>conv fwd time</cell><cell>0 ms</cell><cell>0.5 ms</cell><cell>116.5 ms</cell><cell>149.8 ms</cell></row><row><cell>fc fwd time</cell><cell>0.30 ms</cell><cell>0.12 ms</cell><cell>2.5 ms</cell><cell>1.7 ms</cell></row><row><cell>total size</cell><cell>1.1 MB</cell><cell>1.7 MB</cell><cell>243.9 MB</cell><cell>553.4 MB</cell></row><row><cell>fc-layers' size (%)</cell><cell>100%</cell><cell>95.3%</cell><cell>96.1%</cell><cell>89.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>fc-layers' compression statistics for 4 NNs (a) fc-layers' compressing statistics for LeNet-300-100</figDesc><table><row><cell></cell><cell></cell><cell>Pruning</cell><cell></cell><cell>DeepSZ</cell></row><row><cell cols="2">Layer Original Size</cell><cell></cell><cell>CSR Size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ratio</cell><cell></cell><cell>Compressed</cell></row><row><cell>ip1</cell><cell>941 KB</cell><cell>8%</cell><cell>94 KB</cell><cell>15.2 KB</cell></row><row><cell>ip2</cell><cell>120 KB</cell><cell>9%</cell><cell>14 KB</cell><cell>1.6 KB</cell></row><row><cell>ip3</cell><cell>4 KB</cell><cell>26%</cell><cell>1.3 KB</cell><cell>0.7 KB</cell></row><row><cell>overall</cell><cell>1056 KB</cell><cell>8.25%</cell><cell>109 KB (9.7 ×)</cell><cell>19.1 KB (55.8 ×)</cell></row><row><cell></cell><cell cols="4">(b) fc-layers' compressing statistics for LeNet-5</cell></row><row><cell></cell><cell></cell><cell>Pruning</cell><cell></cell><cell>DeepSZ</cell></row><row><cell cols="2">Layer Original Size</cell><cell></cell><cell>CSR Size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ratio</cell><cell></cell><cell>Compressed</cell></row><row><cell>ip1</cell><cell>1600 KB</cell><cell>8%</cell><cell>160 KB</cell><cell>27.3 KB</cell></row><row><cell>ip2</cell><cell>20 KB</cell><cell>19%</cell><cell>4.8 KB</cell><cell>0.93 KB</cell></row><row><cell>overall</cell><cell>1620 KB</cell><cell>8.1%</cell><cell>165 KB (9.8 ×)</cell><cell>28.27 KB (57.3 ×)</cell></row><row><cell></cell><cell cols="4">(c) fc-layers' compressing statistics for AlexNet</cell></row><row><cell></cell><cell></cell><cell>Pruning</cell><cell></cell><cell>DeepSZ</cell></row><row><cell cols="2">Layer Original Size</cell><cell></cell><cell>CSR size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ratio</cell><cell cols="2">Compressed</cell></row><row><cell>fc6</cell><cell>151.0 MB</cell><cell>9%</cell><cell>17.0 MB</cell><cell>2.77 MB</cell></row><row><cell>fc7</cell><cell>67.1 MB</cell><cell>9%</cell><cell>7.5 MB</cell><cell>1.44 MB</cell></row><row><cell>fc8</cell><cell>16.4 MB</cell><cell>25%</cell><cell>5.1 MB</cell><cell>0.94 MB</cell></row><row><cell>overall</cell><cell>234.5 MB</cell><cell>10.1%</cell><cell>29.6 MB (7.9 ×)</cell><cell>5.15 MB (45.5 ×)</cell></row><row><cell></cell><cell cols="4">(d) fc-layers' compressing statistics for VGG-16</cell></row><row><cell></cell><cell></cell><cell>Pruning</cell><cell></cell><cell>DeepSZ</cell></row><row><cell cols="2">Layer Original Size</cell><cell></cell><cell>CSR Size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ratio</cell><cell cols="2">Compressed</cell></row><row><cell>fc6</cell><cell>411.0 MB</cell><cell>3%</cell><cell>15.4 MB</cell><cell>2.70 MB</cell></row><row><cell>fc7</cell><cell>67.1 MB</cell><cell>4%</cell><cell>3.4 MB</cell><cell>0.75 MB</cell></row><row><cell>fc8</cell><cell>16.4 MB</cell><cell>24%</cell><cell>4.8 MB</cell><cell>0.83 MB</cell></row><row><cell>overall</cell><cell>494.5 MB</cell><cell>3.8%</cell><cell>23.6 MB (20.9 ×)</cell><cell>4.28 MB (115.6 ×)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Inference accuracy of DeepSZ compressed LeNet-5, AlexNet, and VGG-16.</figDesc><table><row><cell cols="2">Neural Network</cell><cell>Top-1</cell><cell></cell><cell>Top-5</cell><cell>fc-layers'</cell><cell>Compress</cell></row><row><cell></cell><cell></cell><cell>Accuracy</cell><cell cols="2">Accuracy</cell><cell>Size</cell><cell>Ratio</cell></row><row><cell cols="2">LeNet-300-100 original</cell><cell>98.35%</cell><cell></cell><cell>-</cell><cell>1056 KB</cell></row><row><cell cols="2">LeNet-300-100 DeepSZ</cell><cell>98.31%</cell><cell></cell><cell>-</cell><cell>19.1 KB</cell><cell>55.8×</cell></row><row><cell cols="2">LeNet-5 original</cell><cell>99.13%</cell><cell></cell><cell>-</cell><cell>1620 KB</cell></row><row><cell cols="2">LeNet-5 DeepSZ</cell><cell>99.16%</cell><cell></cell><cell>-</cell><cell>28.3 KB</cell><cell>57.3 ×</cell></row><row><cell cols="2">AlexNet original</cell><cell>57.41%</cell><cell></cell><cell>80.40%</cell><cell>234.5 MB</cell></row><row><cell cols="2">AlexNet DeepSZ</cell><cell>57.28%</cell><cell></cell><cell>80.58%</cell><cell>5.15 MB</cell><cell>45.5 ×</cell></row><row><cell cols="2">VGG-16 original</cell><cell>68.05%</cell><cell></cell><cell>88.34%</cell><cell>494.5 MB</cell></row><row><cell cols="2">VGG-16 DeepSZ</cell><cell>67.80%</cell><cell></cell><cell>88.20%</cell><cell>4.277 MB</cell><cell>115.6 ×</cell></row><row><cell cols="7">Table 4: Comparison of compression ratios of different techniques</cell></row><row><cell cols="6">on LeNet-300-100, LeNet-5, AlexNet, and VGG-16.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Compression Ratio</cell></row><row><cell>Neural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>Layer</cell><cell cols="4">Deep Weight-DeepSZ Compression less</cell><cell>Improve-ment</cell></row><row><cell></cell><cell>ip1</cell><cell></cell><cell>43.1</cell><cell>60.1</cell><cell>61.81</cell><cell>1.43×</cell></row><row><cell>LeNet-</cell><cell>ip2</cell><cell></cell><cell>32.9</cell><cell>64.3</cell><cell>37.97</cell><cell>1.15×</cell></row><row><cell>300-100</cell><cell>ip3</cell><cell></cell><cell>7.9</cell><cell>-</cell><cell>5.6</cell><cell>0.71×</cell></row><row><cell></cell><cell>overall</cell><cell></cell><cell>41.0</cell><cell>7.6</cell><cell>55.77</cell><cell>1.36×</cell></row><row><cell></cell><cell>ip1</cell><cell></cell><cell>40.8</cell><cell>74.2</cell><cell>58.5</cell><cell>1.43×</cell></row><row><cell>LeNet-5</cell><cell>ip2</cell><cell></cell><cell>16.3</cell><cell>-</cell><cell>21.5</cell><cell>1.32×</cell></row><row><cell></cell><cell>overall</cell><cell></cell><cell>40.1</cell><cell>39.0</cell><cell>57.3</cell><cell>1.43×</cell></row><row><cell></cell><cell>fc6</cell><cell></cell><cell>41.8</cell><cell>-</cell><cell>54.4</cell><cell>1.30×</cell></row><row><cell>AlexNet</cell><cell>fc7 fc8</cell><cell></cell><cell>40.7 17.1</cell><cell>--</cell><cell>46.5 17.5</cell><cell>1.14× 1.02×</cell></row><row><cell></cell><cell>overall</cell><cell></cell><cell>37.7</cell><cell>-</cell><cell>45.5</cell><cell>1.21×</cell></row><row><cell></cell><cell>fc6</cell><cell cols="2">119.0</cell><cell>157.0</cell><cell>152.1</cell><cell>1.28×</cell></row><row><cell>VGG-16</cell><cell>fc7 fc8</cell><cell></cell><cell>80.0 19.1</cell><cell>85.8 -</cell><cell>90.0 19.8</cell><cell>1.13× 1.04×</cell></row><row><cell></cell><cell>overall</cell><cell></cell><cell>95.8</cell><cell>5.9</cell><cell>115.6</cell><cell>1.21×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Inference accuracy degradation of different techniques based on comparable compression ratio.</figDesc><table><row><cell></cell><cell>quantization</cell><cell>Bloomier Filter</cell><cell>SZ</cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(Deep Compression)</cell><cell>(Weightless)</cell><cell>(DeepSZ)</cell></row><row><cell>LeNet-300-100</cell><cell>0.22%</cell><cell>-</cell><cell>0.12%</cell></row><row><cell>LeNet-5</cell><cell>0.30%</cell><cell>-</cell><cell>−0.03% 7</cell></row><row><cell>AlexNet</cell><cell>1.56%</cell><cell>-</cell><cell>0.13%</cell></row><row><cell>VGG-16</cell><cell>2.81%</cell><cell>&gt;3.0%</cell><cell>0.25%</cell></row><row><cell cols="4">note that the time results of LeNet-300-100 are almost identical to</cell></row><row><cell cols="4">those of LeNet-5; hence, because of space limitations, we present</cell></row><row><cell cols="2">the time overheads only for LeNet-5.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Normalized encoding time with different solutions on GPUs.</figDesc><table><row><cell></cell><cell cols="2">LeNet-5</cell><cell></cell><cell cols="2">AlexNet</cell><cell></cell><cell cols="2">VGG-16</cell></row><row><cell>2.5 5.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nomalized encoding time</cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell>n/a</cell><cell></cell><cell></cell><cell>n/a</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DeepSZ</cell><cell>Deep Compre -ssion</cell><cell>Weight -less</cell><cell>DeepSZ</cell><cell>Deep Compre -ssion</cell><cell>Weight -less</cell><cell>DeepSZ</cell><cell>Deep Compre -ssion</cell><cell>Weight -less</cell></row><row><cell>10 1 10 1 10 3</cell><cell cols="2">Deep Compre -ssion lossless SZ LeNet-5 (a) DeepSZ</cell><cell cols="3">Weight -less Codebook based quantization DeepSZ Deep Compre -ssion AlexNet</cell><cell>Weight -less</cell><cell cols="2">CSR Bloomier filter DeepSZ Deep Compre -ssion VGG-16</cell><cell>Weight -less</cell><cell>decoding time (ms)</cell></row></table><note>(b) Breakdown of decoding time with different solutions on CPU.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Some compressors such as ISABELA<ref type="bibr" target="#b21">[22]</ref> were designed with pointwise error control, but tests<ref type="bibr" target="#b10">[11]</ref> have shown that the maximum error could be much larger than the user-set error bound.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that the nonzero floating-point weights will be condensed into a 1-D array or linked list regardless of the sparse matrix representation format.<ref type="bibr" target="#b2">3</ref> Many recent studies, such as<ref type="bibr" target="#b39">[40]</ref>, have demonstrated that SZ and ZFP are two leading lossy compressors for floating-point data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the Exascale Computing Project (ECP), Project Number: 17-SC-20-SC, a collaborative effort of two DOE organizations -the Office of Science and the National Nuclear Security Administration, responsible for the planning and preparation of a capable exascale ecosystem, including software, applications, hardware, advanced system engineering and early testbed platforms, to support the nation's exascale computing imperative. This material was based upon work supported by the U.S. Department of Energy, Office of Science, under contract DE-AC02-06CH11357, and also supported by the National Science Foundation under Grant No. 1619253. We gratefully acknowledge the support from Alabama Water Institute (AWI), Remote Sensing Center (RSC), and Center for Complex Hydrosystems Research (CCHR).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilevel techniques for compression and reduction of scientific dataâĂŤthe univariate case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Tugluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Klasky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing and Visualization in Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blosc, an extremely fast, multi-threaded, meta-compressor library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>F Alted</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fused-layer CNN accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Provable bounds for learning some deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="http://blosc.org/.Online" />
	</analytic>
	<monogr>
		<title level="j">Blosc compressor</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
	<note>Yann LeCun, and Rob Fergus</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">GZIP file format specification version 4.3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Deutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast error-bounded lossy HPC data compression with SZ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultra-performance Pascal GPU and NVLink interconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Danskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Half of the worlds population connected to the mobile internet by 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gsma</surname></persName>
		</author>
		<ptr target="https://www.gsma.com/newsroom/press-release/half-worlds-population-connected-mobile-internet-2020-according-gsma/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Out-of-core compression and decompression of large n-dimensional scalar fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Ibarria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Rossignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Szymczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caffe -Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressing the incompressible with ISABELA: In-situ reduction of spatio-temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Ethier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagiza F</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Parallel Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large Scale Visual Recognition Challenge</title>
		<ptr target="http://www.image-net.org/challenges/LSVRC/.Online" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">LeNet-5, convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/lenet20" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist2" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Error-Controlled Lossy Compression Optimized for High Compression Ratios of Scientific Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fixed-rate compressed floating-point arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lindstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2674" to="2683" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Error Distributions of Lossy Floating-Point Compressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lindstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joint Statistical Meetings</title>
		<imprint>
			<biblScope unit="page" from="2574" to="2589" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast and efficient compression of floating-point data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Isenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1245" to="1250" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and modeling lossy compression schemes on HPC scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Suchyta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Podhorszki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="348" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weightless: Lossy weight encoding for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Reagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Adolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Henry A Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data compression for the exascale computing era-survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhang</forename><surname>Seung Woo Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Hendrix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Keng</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomputing Frontiers and Innovations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="88" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">In-depth exploration of single-snapshot lossy compression techniques for N-body simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="486" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Significantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimizing Lossy Compression Rate-Distortion from Automatic Online Selection between SZ and ZFP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">JPEG2000 image compression fundamentals, standards and practice: image compression fundamentals, standards and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Taubman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Marcellin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed deep neural networks over the cloud, the edge and end devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 37th International Conference on Distributed Computing Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CLIP-Q: Deep network compression learning by in-parallel pruning-quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7873" to="7882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on CPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
		<meeting>Deep Learning and Unsupervised Feature Learning NIPS Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Superneurons: dynamic GPU memory management for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Shuaiwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kraska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="41" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compression and storage schemes in a sensor network with spatial and temporal coding techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Chiun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chee</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VTC Spring 2008-IEEE Vehicular Technology Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="148" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Time series classification using multi-channels deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web-Age Information Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="298" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zstandard</surname></persName>
		</author>
		<ptr target="http://facebook.github.io/zstd/.Online" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
