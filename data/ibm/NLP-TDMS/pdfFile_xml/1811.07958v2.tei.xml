<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tukey-Inspired Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><forename type="middle">A</forename><surname>Griffin</surname></persName>
							<email>griffb@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
							<email>jjcorso@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tukey-Inspired Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the problem of strictly unsupervised video object segmentation, i.e., the separation of a primary object from background in video without a user-provided object mask or any training on an annotated dataset. We find foreground objects in low-level vision data using a John Tukey-inspired measure of "outlierness." This Tukeyinspired measure also estimates the reliability of each data source as video characteristics change (e.g., a camera starts moving). The proposed method achieves state-of-the-art results for strictly unsupervised video object segmentation on the challenging DAVIS dataset. Finally, we use a variant of the Tukey-inspired measure to combine the output of multiple segmentation methods, including those using supervision during training, runtime, or both. This collectively more robust method of segmentation improves the Jaccard measure of its constituent methods by as much as 28%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding remains a focus area in vision. Video object segmentation (VOS), a critical sub-problem, supports learning object class models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>, scene parsing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48]</ref>, action recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, and video editing applications <ref type="bibr" target="#b4">[5]</ref>. Despite the utility of VOS, finding general solutions remains hotly in focus, especially in cases without human annotation or other supervision provided for training or inference. Most unsupervised methods make use of a measurable property, such as salient object motion <ref type="bibr" target="#b35">[36]</ref>, generic object appearance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>, or rigid background elements <ref type="bibr" target="#b55">[56]</ref>. However, a primary challenge in VOS is the variability of video characteristics: cameras can be static or moving; backgrounds and objects can be rigid or dynamic; objects can leave view, change scale, or become occluded; and unique elements like rippling water cause peculiar visual effects. In these circumstances, specific data sources become unreliable, degrading VOS performance.</p><p>We propose a new, strictly unsupervised VOS method called Tukey-Inspired Segmentation (TIS), which separates the primary foreground object in a video from background. In Exploratory Data Analysis <ref type="bibr" target="#b52">[53]</ref>, John Tukey provides a statistical method to find outliers in data. Given that foreground objects typically exhibit a measurable difference relative to the surrounding background, we use Tukey's statistical method to identify candidate foreground objects in low-level vision data. We also develop our own "outlierness" scale to quantitatively determine each data source's ability to reveal foreground objects. By weighting and combining foreground candidates from multiple data sources, our output segmentation mitigates problems associated with changing video characteristics (see <ref type="figure" target="#fig_0">Figure 1</ref>). In addition, we use a variant of our "outlierness" scale to estimate the reliability of segmentations from multiple VOS frameworks, which we weight and combine to generate new, more reliable segmentations. Using our TIS method to find foreground objects in low-level vision data, we set a new precedent for unsupervised methods on the DAVIS benchmark dataset. Using our TIS variant to combine segmentations, we achieve better performance on DAVIS than all prior approaches, whether unsupervised or supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning-based methods have become commonplace for VOS benchmarks. However, supervised methods require annotated data, which are exceptionally tedious and costly for segmentation. For example, the state-of-the-art On-AVOS <ref type="bibr" target="#b53">[54]</ref> has three distinct types of supervision: first, it trains its base network using the ImageNet <ref type="bibr" target="#b9">[10]</ref>, Microsoft COCO <ref type="bibr" target="#b26">[27]</ref>, and PASCAL datasets <ref type="bibr" target="#b11">[12]</ref>; second, it fine-tunes using the DAVIS dataset <ref type="bibr" target="#b37">[38]</ref>; and, third, it re- The outlier scale α acts as a saliency weighting that adapts to frame-to-frame video characteristics.</p><p>In these examples, we focus on optical flow magnitude with outliers depicted as black pixels (middle row). Flow distributions are offset from the median (bottom row) and include the interquartile range (solid lines) and outlier thresholds (dotted lines). In the Bus video, the camera follows the bus while the background moves; from the resulting bimodal distribution, we reliably track the bus as a salient flow outlier (α = 0.93). In Mallard-Water, due to dynamically flowing background elements, the mallard is difficult to track using flow magnitude (α = 0.05). In Dance-Twirl, α = 0.86 when the camera is relatively stationary and α = 0 when the camera is moving.</p><p>quires a user-provided object mask at the beginning of each video. Other DAVIS benchmark leaders, such as OSVOS-S <ref type="bibr" target="#b30">[31]</ref>, RGMP <ref type="bibr" target="#b33">[34]</ref>, OSVOS <ref type="bibr" target="#b3">[4]</ref>, and MSK <ref type="bibr" target="#b36">[37]</ref>, have similar requirements. To support learning, some researchers develop their own weakly-annotated (FSEG <ref type="bibr" target="#b19">[20]</ref>) or synthetic training data (LMP <ref type="bibr" target="#b48">[49]</ref>). Other methods require supervision through components, such as features from a previously-trained network (OFL <ref type="bibr" target="#b51">[52]</ref>) or a previouslytrained boundary detection algorithm (ARP <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b56">57]</ref>). Finally, some supervised algorithms are not trained, but still need a user-provided mask at runtime <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. In contrast, our TIS method has no labeling or training requirements. This design constraint supports application areas where user-provided masks are impractical and applicationspecific datasets for training are unavailable.</p><p>Multiple benchmarks are available to evaluate VOS methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref>, including the Densely Annotated VIdeo Segmentation (DAVIS) dataset <ref type="bibr" target="#b37">[38]</ref>. DAVIS evaluates VOS methods across many challenge categories, including multiple instances of occlusions, objects leaving view, scalevariation, appearance change, edge ambiguity, camerashake, interacting objects, and dynamic background (among others); these challenges frequently occur simultaneously. Using our strictly unsupervised TIS method, we achieve a Jaccard measure (or intersect over union) of 67.6 and, using the combinational variant, a Jaccard measure of 74.9, which is a 17% improvement over previous unsupervised results on DAVIS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Contributions</head><p>Our primary contribution is a video object segmentation method that utilizes low-level processes without any training or annotation requirements, neither at training or runtime. Our method achieves the highest performance among all known unsupervised methods and higher performance than many supervised methods on the DAVIS video object segmentation benchmark for single objects, which is our focus. We also develop a variant of our initial approach that adaptively combines segmentations from multiple methods, generating new segmentations that achieve the highest DAVIS performance in every category of unsupervised and supervised approaches. We provide source code for the current work at https://github.com/griffbr/TIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tukey-Inspired Segmentation</head><p>We derive two Tukey-inspired methods of segmentation. In Section 3.1, we find foreground objects directly from general image data. We identify foreground candidates using John Tukey's statistical measure of outliers, then we use our own measure of "outlierness" to determine the reliability of each data source for identifying foreground objects. In Section 3.2, we combine a set of binary images that represent previous estimates of foreground object locations, which enables the simultaneous utilization of multiple segmentation methods. To improve accuracy, we use a second "outlierness" measure to determine the reliability of each source segmentation and weight its relative contribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Foreground Candidates in Image Data</head><p>Most foreground objects exhibit a measurable difference in data relative to background elements. We identify this difference using outliers in vision data (e.g., optical flow). Given an image D containing data values d p ∈ R for each pixel p, we calculate lower and upper outlier thresholds:</p><formula xml:id="formula_0">O 1 = Q 1 − k(Q 3 − Q 1 ) O 3 = Q 3 + k(Q 3 − Q 1 ),<label>(1)</label></formula><p>where Q 1 and Q 3 are the lower and upper quartiles for all d p ∈ D and Q 3 − Q 1 is the interquartile range (Q 2 is the median). k is a constant that scales the outlier thresholds, O 1 and O 3 ; as suggested by John Tukey in <ref type="bibr" target="#b52">[53]</ref>, we use k = 1.5 to find "outliers." We use each set of outlier data</p><formula xml:id="formula_1">O := {d p ∈ D|d p &lt; O 1 ∨ d p &gt; O 3 }<label>(2)</label></formula><p>to identify pixels corresponding to foreground objects. We also define our own quantitative measure of "outlierness" for each data source D:</p><formula xml:id="formula_2">α := dp∈O |d p | dp∈D |d p | .<label>(3)</label></formula><p>α ∈ [0, 1] is proportional to the magnitude of data in O relative to D. By calculating α for each data source, we approximate the frame-to-frame capacity of each source to track foreground objects in a variety of video settings (see <ref type="figure" target="#fig_1">Figure 2</ref>). Accordingly, we use α to weight each input data source for our example VOS implementation in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Foreground Objects from Binary Images</head><p>As an alternative to finding foreground objects in image data, we derive a second outlier scale that enables us to combine segmentation masks from multiple methods and weight their relative contribution according to their frame-to-frame reliability (see <ref type="figure" target="#fig_3">Figure 3</ref>). Given a set of N M binary masks for the same video frame, assume each mask M consists of pixel-level labels, p ∈ {0, 1}, where p = 1 indicates pixel p is a foreground-object location. Compared to image data, the "outlierness" of data within each M is relatively meaningless ( p ∈ {0, 1} implies that for p ∈ M :</p><formula xml:id="formula_3">Q 1 , Q 2 , Q 3 ∈ {0, 1}</formula><p>). Instead, we measure "outlierness" across the set of binary images using the total number of foreground pixels in each M ,</p><formula xml:id="formula_4">N p := p ∈M p .<label>(4)</label></formula><p>Using N p , the outlier scale for each mask is defined as</p><formula xml:id="formula_5">α M := max N p −O1 Q2−O1 , 0 if N p &lt; Q 2 max N p −O3 Q2−O3 , 0 otherwise ,<label>(5)</label></formula><p>where quartiles Q 1 , Q 2 , and Q 3 and thresholds O 1 and O 3 are found using (1) on the set of N p from all N M masks.</p><p>We use α M ∈ [0, 1] to weight each mask based on proximity to the median number of foreground pixels across all N M masks, with outliers being scaled at 0. The intuition behind this weighting is simple. If an individual mask is near the median, it is representative of the collective consensus of segmentation masks for the approximate size of the foreground object, and it is likely more reliable. Alternatively, if an individual mask is an outlier, it is likely unreliable for the current video frame.</p><p>To generate the output segmentation mask, we define an image-level estimate of "foregroundness" as</p><formula xml:id="formula_6">F := N M i=1 α Mi M i N M i=1 α Mi ,<label>(6)</label></formula><p>where α Mi is the outlier scale for the ith mask M i . Note that F 's corresponding pixel-level values f p ∈ [0, 1]. The final output mask M for the Tukey-inspired combination of binary images is found using</p><formula xml:id="formula_7">p = 1 if f p &gt; 0.5 0 otherwise .<label>(7)</label></formula><p>Remark: In <ref type="figure" target="#fig_3">Figure 3</ref> and Section 5.2, we refer to this binary image-based segmentation method as TIS M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tukey-Inspired Segmentation using Motion and Visual Saliency Outliers</head><p>We implement the method of finding foreground objects in image data from Section 3.1 using motion and visual saliency data. For motion saliency, x and y optical flow components are found using the method from <ref type="bibr" target="#b28">[29]</ref>; in addition, the flow magnitude (i.e., |x 2 +y 2 |) and flow angle (i.e., arctan( y x )) are also calculated. For each flow measure, the outlier thresholds and scales are calculated on a frame-toframe basis using (1)-(3). The pixel-level motion saliency measure, d ms p , is defined for each flow component as</p><formula xml:id="formula_8">d ms p := 0 if d p / ∈ O ∨ α f &lt; 0.5 α f |d p f − Q 2 f | otherwise ,<label>(8)</label></formula><p>where d p f is the initial pixel-level flow component value with corresponding frame-to-frame median Q 2 f , outlier scale α f , and a 0.5 minimum scale requirement. The intuition behind <ref type="formula" target="#formula_8">(8)</ref> is as follows. First, whether a foreground object is moving with a fixed camera or vice versa, the foreground object's deviation from the frame's median optical flow will generally be salient (see Bus in <ref type="figure" target="#fig_1">Figure 2</ref>). Second, the absolute value enables a positive "foregroundness" contribution regardless of a flow component's sign. Finally, the minimum scale requirement will remove the influence of less reliable flow components.</p><p>We found that visual saliency is less useful than optical flow for most video segmentation cases. However, the product of visual saliency and optical flow is beneficial for videos with dynamic background elements (e.g., Mallard-Water in <ref type="figure" target="#fig_1">Figure 2</ref>). Thus, we include a pixel-level visual saliency measure, d vs p , defined as</p><formula xml:id="formula_9">d vs p := (d pv ) k 4 i=1 max(α fi , 0.5)|d p f i − Q 2 f i |, (9)</formula><p>where d pv ∈ [0, 1] is a pixel-level visual saliency-based scale (found using <ref type="bibr" target="#b31">[32]</ref>), k is an exponential scale that adjusts the relative sharpness of (d pv ) k ∈ [0, 1], d p f i is the ith flow component with corresponding median Q 2 f i and outlier scale α fi , and the minimum applied scale of 0.5 ensures that visual saliency features are available even if α fi = 0 ∀i. Three d vs p measures are used altogether, with k = {1, 1 2 , 1 3 }. To generate the segmentation mask, we combine the four d ms p and three d vs p saliency measures for a pixel-level estimate of "foregroundness," defined as</p><formula xml:id="formula_10">f p := 7 i=1 d i p ,<label>(10)</label></formula><p>where d i p is the ith measure. Using <ref type="formula" target="#formula_0">(10)</ref>, we generate a mask M for each video frame with pixel-level labels</p><formula xml:id="formula_11">p = 1 if f p &gt; βδ p 0 otherwise ,<label>(11)</label></formula><p>where p = 1 indicates a foreground object and β ∈ R is the sum of the mean and standard deviation of f p in the current frame. δ p is the pixel-level previous-mask "discount"</p><formula xml:id="formula_12">δ p := 1 2 if p,i−1 = 1 1 otherwise .<label>(12)</label></formula><p>In simple words, if f p is greater than the pixel-level mean and standard deviation of "foregroundness" in the current frame, pixel p is considered a foreground object location. In addition, wherever p corresponds to a mask position in the previous frame, a half-threshold discount is applied, which encourages frame-to-frame continuity and gradually increasing accuracy of the segmentation mask. Finally, the output segmentation mask assumes a single foreground object hypothesis, so the mask in each frame is the single continuous segment with the greatest f p sum. Remark: In the remainder of the paper, we refer to this image data-based segmentation method as TIS 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Consensus-based Boundary Refinement</head><p>To improve the boundary accuracy of (11), we use supervoxel consensus voting. This choice is motivated by previous VOS work in <ref type="bibr" target="#b13">[14]</ref> that relates the "foregroundness" of superpixels to nearest neighbors across frames. Supervoxels, on the other hand, inherently exist across many frames and have shown promising results for relating features <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> and detecting object boundaries <ref type="bibr" target="#b58">[59]</ref>. Given a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervoxel Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Consensus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground Consensus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Video</head><p>Tukey-Inspired Saliency</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervoxel Consensus Refinement</head><p>Output Segmentation <ref type="figure">Figure 4</ref>. Consensus-based Boundary Refinement. Our Tukeyinspired segmentation (TIS0) uses saliency outliers in image data to form a quantitative estimate of "foregroundness" and a corresponding object mask. Using supervoxels generated from the input video, we refine TIS0 with a supervoxel-based consensus for both background and foreground elements, improving the boundary accuracy of the TIS0 segmentation.</p><p>set of non-overlapping supervoxels that cover all video pixels, we use TIS 0 to build an internal consensus within the bounds of each supervoxel, and then relate this consensus across the video to refine the boundary of the initial TIS 0 mask. This refinement process is depicted in <ref type="figure">Figure 4</ref>. First, using p ∈ {0, 1} from <ref type="formula" target="#formula_0">(11)</ref>, the local consensus within each supervoxel S is defined as</p><formula xml:id="formula_13">f L S := 1 N p p∈S (2 p − 1),<label>(13)</label></formula><p>where N p is the number of pixels p ∈ S. Note that f L S ∈ [−1, 1], where positive or negative values imply a foreground object or background.</p><p>Next, the non-local consensus among each S is found as</p><formula xml:id="formula_14">f NL S = NNL i=1 w i f L Si ,<label>(14)</label></formula><p>where N NL is the number of nearest-neighbors contributing to the non-local consensus for supervoxel S and w i determines the relative weight of each neighbor. Because the total number of supervoxels, N S , changes with each video, we set N NL = N S 100 . Nearest neighbor weight w i is set as</p><formula xml:id="formula_15">w i = 1 R(S, S i ) 2 ,<label>(15)</label></formula><p>where R ∈ R calculates the city-block distance between the mean-LAB color of local supervoxel S and the ith nearest neighbor S i . To ensure that all three LAB distances are meaningful, video-wide LAB values are linearly mapped between 0 and 1. R is squared to reduce the influence of supervoxels outside of the primary "clique." Finally, to refine the TIS 0 segmentation, we add <ref type="formula" target="#formula_0">(13)</ref> and <ref type="bibr" target="#b13">(14)</ref> to the initial estimate of "foregroundness" from (10): where f L S are f NL S are the local and non-local consensus for the supervoxel containing p and w 0 determines the relative weight of f L S to f NL S . Essentially, f p improves the f p -based boundary by adding to supervoxels with a consistent foreground object consensus while subtracting from supervoxels with a consistent background consensus. The refined TIS 0 segmentation mask is found using pixel-level labels</p><formula xml:id="formula_16">f p := f p + w 0 f L S + f NL S ,<label>(16)</label></formula><formula xml:id="formula_17">p = 1 if f p &gt; 0 0 otherwise .<label>(17)</label></formula><p>Remarks: (a) f p in <ref type="formula" target="#formula_0">(16)</ref>   <ref type="formula" target="#formula_0">(17)</ref> is limited to the two segments with the greatest f p sum in each frame. This improves VOS for objects with partial occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We evaluate our TIS segmentation methods on two VOS benchmark datasets: the Densely Annotated VIdeo Segmentation (DAVIS) dataset <ref type="bibr" target="#b37">[38]</ref> and the Georgia Tech Segmentation and Tracking Dataset (SegTrackv2) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref>. The DAVIS 2016 dataset includes 50 diverse videos, 30 training and 20 validation, all of which have ground truth annotations matching the single object hypothesis (unlike DAVIS 2017-18). The SegTrackv2 dataset has fewer videos than DAVIS, and only a subset match the single object hypothesis. SegTrackv2 also contains videos with different resolutions, which span from 76,800 to 230,400 pixels per frame.</p><p>Three standard benchmark measures evaluate the performance of our segmentation method: region similarity J , contour accuracy F, and temporal stability T , which are all calculated using the definitions provided in <ref type="bibr" target="#b37">[38]</ref>. Region similarity (also known as the intersect over union or Jaccard index <ref type="bibr" target="#b12">[13]</ref>) provides an intuitive, scale-invariant evaluation <ref type="table">Table 2</ref>. Complete DAVIS Results for State-of-the-Art Unsupervised Methods. Methods are unsupervised, so we compare using training and validation videos. Object recall measures the fraction of sequences scoring higher than 0.5, and decay quantifies the performance loss (or gain) over time <ref type="bibr" target="#b37">[38]</ref>. TIS0 exhibits the best decay performance; TISS and TISM achieve top results in all other categories. for the number of mislabeled foreground pixels with respect to a ground truth annotation. Given a foreground mask M and ground truth annotation G, J = M ∩G M ∪G . Contour accuracy evaluates the boundary of a segmentation by measuring differences between the closed set of contours for M and G. Finally, temporal stability is a measure based on the consistency of a mask between video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">TIS 0 Foreground Objects from Image Data</head><p>We evaluate our Tukey-inspired measure for finding foreground objects in image data using the TIS 0 implementation from Section 4. To improve boundary accuracy, we refine TIS 0 with supervoxel consensus (Section 4.1) using segmentation by weighted aggregation (SWA) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> and hierarchical graph-based (GBH) <ref type="bibr" target="#b17">[18]</ref> supervoxels (both generated using the LIBSVX library <ref type="bibr" target="#b59">[60]</ref>). Both TIS 0 refinement configurations are detailed in <ref type="table" target="#tab_0">Table 1</ref> with additional analysis provided in <ref type="bibr" target="#b16">[17]</ref>.</p><p>DAVIS results for TIS 0 -based methods are compared with state-of-the-art methods in <ref type="table" target="#tab_1">Tables 2 and 3</ref>. TIS 0 exhibits the best decay performance for J and F and achieves a higher J than all previous unsupervised methods on the DAVIS validation set. Multiple methods have better contour accuracy than TIS 0 , but the refinement process from Section 4.1 solves this problem. TIS S , which improves TIS 0 with consensus-based boundary refinement, outperforms all unsupervised segmentation methods for both J and F.</p><p>SegTrackv2 results are provided in <ref type="table" target="#tab_2">Table 4</ref>. We select videos from SegTrackv2 that use a single object hypothesis (like DAVIS). SegTrackv2 uses videos with different resolutions, causing individual hierarchy levels to have dramatically different supervoxel quantities from one video to the next. Accordingly, we change supervoxel hierarchy levels for TIS S and TIS L G between videos. Besides NLC, ITS, and FAM, TIS L G achieves top results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">TIS M Foreground Objects from Binary Images</head><p>We evaluate our Tukey-inspired measure for finding foreground objects in binary images (Section 3.2) by combining the segmentation methods listed in <ref type="table" target="#tab_4">Table 5</ref>. Each combination set includes a certain level of supervision: strictly unsupervised (TIS M ), training on an annotated dataset (TIS T M ), user-provided object mask at runtime (TIS R M ), or both user-provided masks and dataset training (TIS RT M ). Some source segmentation methods are only available on the DAVIS Validation Set; TIS M configurations using these validation methods are only evaluated on the validation set and are distinguished by a V (e.g., TIS V M ). TIS M -based methods achieve top DAVIS results for unsupervised segmentation and all categories of supervised segmentation, as shown in <ref type="table" target="#tab_1">Table 3</ref> and <ref type="figure" target="#fig_5">Figure 5</ref>. For the complete dataset, the relative J increases over previous results in each category are: 17% for unsupervised (TIS M to NLC), 6% for training (TIS T M to ARP), 18% for runtime (TIS R M to BVS), and 4% for runtime and training (TIS RT M to MSK). For the validation set, the relative increases are: 28% for unsupervised (TIS V M to FST), 6% for training (TIS TV M to PDB), 28% for runtime (TIS RV M to BVS), and 2% for runtime and training (TIS V M 5 to OnAVOS). The greatest increases for TIS M combinations over constituent methods occur for categories with a lower J score (see <ref type="figure" target="#fig_5">Figure 5</ref>).</p><p>To further evaluate TIS M , we compare against two additional statistics-based methods for combining segmentations. First, we test a mean-based combination (mean M )  <ref type="table" target="#tab_0">NLC  ITS  FAM  HPF  KEY  FST  FSEG  HVS  Birdfall  62  54  23  74  73  66  58  49  18  38  57  Frog  78  50  61  83  80  81  58  0  54  57  67  Girl  69  70  65  91  86  82  69  88  55  67  32  Monkey  58  57  34  71  83  69  69  79  65  8  62  Parachute  88  88  67  94  96  90  94  96  76  52  69  Soldier  56  53  49  83  76  83  6  67  4  7  67  Worm  77  58  52  81  82  82  84  84  73  51</ref>   ). We postulate that the TIS M combination method is well-suited for the larger, more variable set of segmentations, where the elimination of poorer-performing outliers and promotion of reliable inliers is critical. This is evidenced by TIS V M 19 and the other large TIS M sets in <ref type="table" target="#tab_4">Table 5</ref>  methods, the calculation of quartiles and outliers for scaling is less meaningful; consequently, we find that the small-set TIS V M 5 and mean M have similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Future Work</head><p>We develop a Tukey-inspired segmentation methodology to discover foreground objects in general image data. Our approach automatically adjusts its reliance on each data source according to the frame-to-frame characteristics of a video, addressing a primary challenge in video object segmentation of maintaining reliable measures with changing video characteristics. In addition, our Tukey-inspired measure for finding foreground objects in image data sets a new precedent on the DAVIS dataset for unsupervised segmentation methods. Our current implementation used optical flow and visual saliency data, but the method can incorporate additional sources of image data to accommodate new applications.</p><p>We apply a variant of our Tukey-inspired measure to combine the output of other segmentation methods, generating a new and collectively more robust method of segmentation. Our combination method achieves better performance on the DAVIS dataset than all prior segmentation methods and represents a new paradigm in video object segmentation. In real-world applications, it is difficult to know which individual segmentation methods will perform best for various videos. On the other hand, by using our approach, multiple methods can be implemented simultaneously, and only the most reliable methods will be used for segmentation in any given video frame. This extension was particularly effective when constitute methods had variable performance and a large gap for improvement, indicating that the Tukey-inspired combination can be a viable tool for cutting-edge applications where performance has not yet reached its potential. Furthermore, we found that attempting to combine segmentations using "non-Tukey" methods can result in worse performance than some of the source segmentations. Finally, our combination method can easily incorporate new, better-performing segmentation methods as they are developed by the research community.</p><p>Given that the current results are restricted to a single object hypothesis, we are currently working on extending this approach to multiple objects. We are also exploring how our method of weighting input data based on "outlierness" can improve performance in supervised learning applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Tukey-inspired segmentation of DAVIS's Parkour video, which has a moving camera, a dynamic foreground object with scale-variation, and occlusions (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>arXiv:1811.07958v2 [cs.CV] 30 Nov 2018 Foreground Candidates in Image Data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Foreground Objects from Binary Images. In this example, 34 segmentation methods are combined using the TISM method (i.e., NM = 34). The binary image outlier scale αM functions as a confidence weighting for each mask M and changes each video frame. Masks exhibiting a strong N p consensus with the group are considered reliable (middle left), while masks with too many or too few foreground pixels are considered unreliable outliers (top and bottom left, αM = 0). The N p distribution for the current video frame (bottom right) includes the interquartile range (solid lines) and outlier thresholds (dotted lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is scaled s.t. f p ∈ [0, 1]. (b) If using only local consensus, w 0 = 1 in (16) and all non-local weights are zeroed. (c) If using local and non-local consensus, w 0 = 1 3 and non-local weights are uniformly scaled s.t. NNL i=1 w i = 2 3 . (d) The refined mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>outperforming mean M . On the other hand, on a smaller, less variable set of segmentation Visual Comparison of Segmentation Methods on Complete DAVIS Dataset. Note the correlation between J and F. The TISM segmentation methods improve performance across all categories of supervised and unsupervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>DAVIS Results for TIS0 with and without Supervoxel Consensus Refinement. GBH-based refinement performs better when using only the local consensus. Higher numbers are better for rows labeled with ↑ (e.g., J mean) and worse for rows with ↓.</figDesc><table><row><cell></cell><cell cols="3">Configuration ID</cell></row><row><cell>Configuration</cell><cell cols="2">TIS S TIS L G</cell><cell>TIS 0</cell></row><row><cell cols="4">Supervoxels Used SWA GBH None</cell></row><row><cell>Hierarchy Level</cell><cell>6</cell><cell>2</cell><cell>N/A</cell></row><row><cell>Local Consensus</cell><cell>Yes</cell><cell>Yes</cell><cell>N/A</cell></row><row><cell>Non-Local Consensus</cell><cell>Yes</cell><cell>No</cell><cell>N/A</cell></row><row><cell>Measure</cell><cell cols="3">DAVIS Results</cell></row><row><cell cols="2">Region Similarity: J Mean ↑ 67.6</cell><cell>65.3</cell><cell>58.6</cell></row><row><cell cols="2">Contour Accuracy: F Mean ↑ 63.9</cell><cell>61.2</cell><cell>47.5</cell></row><row><cell cols="2">Temporal Stability: T Mean ↓ 31.0</cell><cell>31.8</cell><cell>30.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>DAVIS Validation Set Results. Runtime supervision requires a user-provided annotation frame for each segmentation video. Training supervision includes dataset training and dataset-trained components (e.g., OFL and ARP). The online benchmark distinguishes by runtime supervision only. TIS0-based methods achieve top unsupervised results; TISM -based methods achieve top results in all categories. Mean 88.1 86.7 86.1 85.6 83.4 82.4 81.5 76.5 60.0 81.8 77.2 76.2 71.2 62.6 56.2 55.8 55.2 F Mean 87.5 83.8 84.9 87.5 85.0 79.5 82.0 71.2 58.8 76.6 74.5 70.6 66.4 59.6 45.6 51.1 55.2</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Current Results</cell><cell></cell><cell></cell><cell>[56]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Measure</cell><cell>TIS M</cell><cell cols="2">TIS S</cell><cell>TIS 0</cell><cell cols="2">NLC</cell><cell>BGM</cell><cell>FST</cell><cell>KEY</cell><cell>MSG</cell><cell>CVOS</cell><cell>TRC</cell><cell>SAL</cell></row><row><cell cols="2">Mean ↑</cell><cell>74.9</cell><cell cols="2">67.6</cell><cell>58.6</cell><cell cols="2">64.1</cell><cell>62.5</cell><cell>57.5</cell><cell>56.9</cell><cell>54.3</cell><cell>51.4</cell><cell>50.1</cell><cell>42.6</cell></row><row><cell cols="2">J Recall ↑</cell><cell>90.1</cell><cell cols="2">84.7</cell><cell>75.9</cell><cell cols="2">73.1</cell><cell>70.0</cell><cell>65.2</cell><cell>67.1</cell><cell>63.6</cell><cell>58.1</cell><cell>56.0</cell><cell>38.6</cell></row><row><cell cols="2">Decay ↓</cell><cell>5.8</cell><cell></cell><cell>4.0</cell><cell>2.3</cell><cell cols="2">8.6</cell><cell>-</cell><cell>4.4</cell><cell>7.5</cell><cell>2.8</cell><cell>12.7</cell><cell>5.0</cell><cell>8.4</cell></row><row><cell cols="2">Mean ↑</cell><cell>69.0</cell><cell cols="2">63.9</cell><cell>47.5</cell><cell cols="2">59.3</cell><cell>59.3</cell><cell>53.6</cell><cell>50.3</cell><cell>52.5</cell><cell>49.0</cell><cell>47.8</cell><cell>38.3</cell></row><row><cell cols="2">F Recall ↑</cell><cell>83.8</cell><cell cols="2">78.5</cell><cell>48.8</cell><cell cols="2">65.8</cell><cell>66.2</cell><cell>57.9</cell><cell>53.4</cell><cell>61.3</cell><cell>57.8</cell><cell>51.9</cell><cell>26.4</cell></row><row><cell cols="2">Decay ↓</cell><cell>9.0</cell><cell></cell><cell>5.7</cell><cell>1.4</cell><cell cols="2">8.6</cell><cell>-</cell><cell>6.5</cell><cell>7.9</cell><cell>5.7</cell><cell>13.8</cell><cell>6.6</cell><cell>7.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Supervision Required</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Runtime &amp; Training</cell><cell></cell><cell></cell><cell cols="2">Runtime</cell><cell>Training</cell><cell></cell><cell>Unsuperivsed</cell></row><row><cell>Measure</cell><cell>TIS V M 5</cell><cell>TIS RTV M</cell><cell>OnAVOS</cell><cell>OSVOS-S</cell><cell>CINM</cell><cell>FAVOS</cell><cell>RGMP</cell><cell>TIS RV M</cell><cell>BVS TIS TV M</cell><cell cols="2">PDB ARP TIS V M</cell><cell>TIS S TIS 0 FST CUT</cell></row><row><cell>J</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>SegTrackv2 Results. Other results are directly from citation or comparative studies in<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">62]</ref>. Videos are single-object only.</figDesc><table><row><cell>Unsupervised</cell><cell>Supervised</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>DAVIS Results for Multiple TISM Configurations. Bold font indicates best performance for a given set of methods combined. With the exception of "Experimental Sets," TISM configurations use all applicable methods from the online benchmark (davischallenge.org).</figDesc><table><row><cell>Configuration</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the DARPA MediFor program under contract FA8750-16-C-0168.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime</head><p>TIS M set + BVS <ref type="bibr" target="#b32">[33]</ref>, FCP <ref type="bibr" target="#b38">[39]</ref>, JMP <ref type="bibr" target="#b14">[15]</ref>, HVS <ref type="bibr" target="#b17">[18]</ref>, SEA <ref type="bibr">[</ref> that averages all source masks together and outputs a foreground label where the pixel-level mean is higher than 0.5; this is equivalent to setting α Mi = 1 in <ref type="bibr" target="#b5">(6)</ref>. Second, we test a median-based combination (median M ) that outputs the source mask with the median number of foreground pixels (N p in <ref type="formula">(4)</ref>). For the baseline comparison in <ref type="table">Table 5</ref>, TIS M has the highest combined J and F score for all categories of supervision but one (mean M over TIS V M ). In addition, TIS M is the only combination method that achieves higher J scores than all source segmentation methods.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in A cnn-based higher-order spatiotemporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient multilevel brain tumor segmentation with integrated bayesian model classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>El-Saden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="629" to="640" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Category Independent Object Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jumpcut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Video object segmentation using supervoxel-based gerrymandering. CoRR, abs/1704.05165</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation in video by efficient selection of highly probable positive features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5095" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7474" to="7483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="740" to="755" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiclass semantic video segmentation with object-level active inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human action segmentation with hierarchical supervoxel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What makes a patch distinct?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchy and adaptivity in segmenting visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">442</biblScope>
			<biblScope unit="issue">7104</biblScope>
			<biblScope unit="page" from="810" to="813" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dilated pyramid convolution and deeper bidirectional convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action localization in videos through context walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting the where and what of actors and actions through online action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discriminative segment annotation in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motion words for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Taralova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>2017. 7</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Motion coherent tracking using multi-label mrf optimization. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video segmentation with background motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wehrwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to detect motion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2578" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Libsvx: A supervoxel library and benchmark for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="272" to="290" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flattening supervoxel hierarchies by the uniform entropy slice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast appearance modeling for automatic primary video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="503" to="515" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
