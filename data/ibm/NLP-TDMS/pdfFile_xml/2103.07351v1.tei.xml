<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular Quasi-Dense 3D Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Hsu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Monocular Quasi-Dense 3D Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Monocular 3D Detection</term>
					<term>Monocular 3D Tracking</term>
					<term>Multiple Object Tracking</term>
					<term>Quasi-Dense Similarity Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A reliable and accurate 3D tracking framework is essential for predicting future locations of surrounding objects and planning the observer's actions in numerous applications such as autonomous driving. We propose a framework that can effectively associate moving objects over time and estimate their full 3D bounding box information from a sequence of 2D images captured on a moving platform. The object association leverages quasi-dense similarity learning to identify objects in various poses and viewpoints with appearance cues only. After initial 2D association, we further utilize 3D bounding boxes depth-ordering heuristics for robust instance association and motion-based 3D trajectory prediction for re-identification of occluded vehicles. In the end, an LSTM-based object velocity learning module aggregates the long-term trajectory information for more accurate motion extrapolation. Experiments on our proposed simulation data and real-world benchmarks, including KITTI, nuScenes, and Waymo datasets, show that our tracking framework offers robust object association and tracking on urban-driving scenarios. On the Waymo Open benchmark, we establish the first camera-only baseline in the 3D tracking and 3D detection challenges. Our quasi-dense 3D tracking pipeline achieves impressive improvements on the nuScenes 3D tracking benchmark with near five times tracking accuracy of the best vision-only submission among all published methods. Our code, data and trained models are available at https://github.com/SysCV/qd-3dt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOREWORD</head><p>This appendix provides technical details about our monocular quasi-dense 3D object tracking network, our training procedures for different datasets, and more qualitative and quantitative results. Section A offers frame-and objectbased statistical summaries of our proposed simulation dataset. Section B describes our training procedure, and network setting of each dataset. Section C illustrates various comparisons, including inference time, network settings and performance results, of our method on nuScenes, Waymo, GTA, and KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DATASET STATISTICS</head><p>To help understand our dataset and its difference, we show more statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Comparison.</head><p>Table 11 demonstrates a detailed comparison with related datasets, including detection, tracking, and driving benchmarks. KITTI-D [3] and KAIST [94] are mainly detection datasets, while KITTI-T [3], MOT15 [95], MOT16 [2], and UA-DETRAC [1] are primarily 2D tracking benchmarks. The common drawback could be the limited scale, which cannot meet the growing demand for training data. Compared to the related synthetic dataset, Virtual KITTI [32] and VIPER [66]</p><p>, we additionally provide finegrained attributes of object instances, such as color, model, maker attributes of a vehicle, motion and control signals, which leaves the space for the imitation learning system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A utonomous driving motivates much of contemporary visual deep learning research. However, many commercially successful approaches to autonomous driving control rely on a wide array of views and sensors, reconstructing 3D point clouds of the surroundings before inferring object instance trajectories in 3D. In contrast, human observers have no difficulty perceiving the 3D world in space and time from simple sequences of 2D images rather than 3D point clouds, even though human stereo vision only reaches several meters. Recent progress in monocular object detection and scene segmentation offers the promise to make low-cost mobility widely available. In this paper, we focus on monocular 3D detection and tracking so that this low-cost system can be robust and reason in 3D even without 3D sensors.</p><p>Monocular 3D detection and tracking are inherently intertwined. 3D detection is challenging by itself in the absence of depth measurements or strong priors given a single image. However, current 3D object detectors based on deep learning show promise in capturing geometric priors. This detection basis makes short-term 3D tracking more robust and makes long-term tracking possible. At the same time, 3D tracking information across multiple frames can assist 3D detection in future frames. Note that 3D tracking in the world coordinate can be achieved by projecting object 3D extents in each frame to the world coordinate so that a still object stays still although it moves in 2D observations. This 3D transformation makes both 3D detection and tracking robust as ego-motion of the sensor platform is factored out.</p><p>We propose an online 3D tracking framework to detect [R|t]</p><p>[R|t] <ref type="figure">Fig. 1</ref>: Monocular quasi-dense detection and tracking in 3D. Our dynamic 3D tracking pipeline predicts 3D bounding box association of observed target from quasidense object proposals in image sequences captured by a monocular camera with an ego-motion sensor. and track 3D objects in world coordinates from a series of monocular images. <ref type="figure">Figure 1</ref> provides an overview of our 3D detection and tracking task. After generating object proposals from a monocular image, we estimate their 3D properties, i.e., 3D bounding box center, depth, dimension, and orientation with a multi-head network. Our model also learns an instance descriptor represented as a vector in a latent space via quasi-dense similarity learning. The quasi-dense mechanism leverages densely populated object proposals, instead of scarce ground truth bounding boxes, to learn instance similarity mappings on a high-dimensional arXiv:2103.07351v1 [cs.CV] 12 Mar 2021 latent space and decide if two object detections are the same object. We utilize the instance feature embedding, estimated 3D information, and object motion from lifted 3D object instance poses to associate instances over time. Notably, we leverage novel motion-aware association and depthordering matching algorithms to overcome the occlusion and reappearance problems in tracking. Finally, our model captures the movement of instances in a world coordinate system and updates their 3D poses using velocity-based LSTM (VeloLSTM) to estimate motion along a trajectory, integrating single-frame observations associated with an object instance over time.</p><p>Like any deep network, our model is data-hungry. The more data we feed it, the better it performs. However, early public datasets are either limited to static scenes <ref type="bibr" target="#b0">[1]</ref>, lack the required ground truth trajectories <ref type="bibr" target="#b2">[2]</ref>, or are too small to train contemporary deep models <ref type="bibr" target="#b3">[3]</ref>. To bridge this gap, we resort to realistic video games. We use a new pipeline to collect large-scale 3D trajectories from a realistic synthetic driving environment, augmented with dynamic meta-data associated with each observed scene and object instance. Thanks to the hype of autonomous driving, more and more large-scale datasets, and benchmarks have surged onto the desk. However, most of the datasets, collected in days to months and with costly human resources to reach a reasonable scale for a data-hungry network, are licensed for only non-commercial use. Our 3D vehicle tracking simulation dataset democratizes the autonomous driving field that our simulation dataset is open to both the research community and industrial side. As a result, we believe the presented synthetic driving dataset would foster the community in discovering the power of a deep network as an entry point, especially in combination with sim-to-real approaches, for the large dataset and benchmarks.</p><p>In summary, we propose a new pipeline while revising each module building upon the findings of our initial work <ref type="bibr" target="#b4">[4]</ref>. The key contributions and extended results are highlighted below.</p><p>Quasi-Dense 3D Object Tracking Pipeline. We integrate Quasi-Dense Similarity Learning <ref type="bibr" target="#b5">[5]</ref> into our tracking framework. It learns to match objects from dense object proposals, instead of only a few annotated foreground regions. Our quasi-dense 3D tracking pipeline outperforms the camerabased state of the art by near 500% while bridging the gap to LiDAR-based methods on the nuScenes 3D tracking benchmark. Please refer to Section 3.3 for the method details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Confidence, Motion-based Similarity and VeloLSTM.</head><p>In addition to the 2D tracking upgrade, we revised our detection and tracking method from all aspects, i.e., detector, tracker, and motion model. We introduce a 3D confidence in Section 3.4, aggregating 2D bounding box confidence and depth confidence, that benefits the bounding box filtering. In Section 3.5, we improve our centroid-based data association scheme with a motion-based similarity so that the tracker is encouraged to match reappeared candidates in trajectories. Lastly, in Section 3.6, we enhance our VeloLSTM module to model not only object velocity, but heading angle and dimension. With all the new techniques, our 3D tracking method has improved the rank on the KITTI 2D tracking benchmarks outperforming existing online approaches.</p><p>Detailed Experiments on large-scale Datasets. Apart from the KITTI and our simulated dataset, we applied our extended model to experiment on urban-driving situations with recently collected large-scale datasets, i.e., nuScenes and Waymo Open. On nuScenes, we focus on detailed module design comparison and hyper-parameter tuning, while on Waymo Open, we discuss the advantages and limitations of a monocular-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>2D Object Detection. Object detection reaped many of the benefits from the success of convolutional representation. There are two mainstream deep detection frameworks: 1) two-step detectors: R-CNN <ref type="bibr" target="#b6">[6]</ref>, Fast R-CNN <ref type="bibr" target="#b7">[7]</ref>, and Faster R-CNN <ref type="bibr" target="#b8">[8]</ref>. 2) one-step detectors: SSD <ref type="bibr" target="#b9">[9]</ref>, YOLO9000 <ref type="bibr" target="#b10">[10]</ref> and RetinaNet <ref type="bibr" target="#b11">[11]</ref>. While these methods rely on so-called anchor boxes to predict offsets for bounding box estimation which are usually derived from dataset depended statistics, more recent works study anchor-free object detection by predicting keypoints such as centers or corners <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. We utilize Faster R-CNN, one of the most popular object detectors, as our detection model basis. <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref> has focused on 3D object detection rather than detecting objects on the image plane since understanding the environment in 3D is of fundamental importance for many vision applications such as robotic navigation and autonomous driving. Over the last years, these methods predominantly regressed the 3D bounding box parameters directly from the image domain using 2D CNNs <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>. More recent works <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref> propose to transform the data representation to detect objects by utilizing recent advances in point cloud based 3D object detection <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> where methods showed impressive performance on point clouds retrieved from LiDAR sensors. Other works leverage shape prior cues to enhance the performance of image-based 3D object detection <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>. In contrast, our work extends beyond the single-frame domain, leveraging temporal information to enhance monocular 3D detection by utilizing instancelevel uncertainty estimation in combination with our LSTMbased motion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-based 3D Object Detection. A large body of research</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Object Tracking.</head><p>Object tracking in the image-domain has been explored extensively in the last decade <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. Early methods <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> track objects based on correlation filters. Recent ConvNet-based methods typically leverage pre-trained object recognition networks. Some generic object trackers are trained entirely online, starting from the first frame of a given video <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. While object tracking has been tackled by many different paradigms <ref type="bibr" target="#b37">[37]</ref>, including tracking by object verification <ref type="bibr" target="#b38">[38]</ref>, tracking by correlation <ref type="bibr" target="#b39">[39]</ref> and tracking by detection <ref type="bibr" target="#b40">[40]</ref>, we build upon on the recent successes of tracking by detection methods <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>. These are typically distinguished by their data association mechanism, for which many different algorithms have been explored, such as network flow <ref type="bibr" target="#b44">[44]</ref>, conditional random fields <ref type="bibr" target="#b45">[45]</ref>, multihypothesis tracking <ref type="bibr" target="#b46">[46]</ref> and quadratic pseudo boolean optimization <ref type="bibr" target="#b47">[47]</ref>. These association mechanisms are fueled by different appearance and location based cues such as consistency of 2D and 3D motion <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b48">[48]</ref> as well as visual appearance similarity <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b49">[49]</ref>. Our method combines the strengths of different similarity cues for object tracking.</p><p>3D Object Tracking. The previously discussed methods for object tracking in the image-domain usually only take 2D visual features into consideration, where the search space is restricted near the original position of the object. This works well for a static observer, but fails in a dynamic 3D environment. Therefore, various research works have proposed to further leverage 3D information to narrow down the search space and stabilize the trajectory of target objects <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>. Scheidegger et al. <ref type="bibr" target="#b48">[48]</ref> estimate 3D object positions from single images and add a 3D Kalman filter on the 3D positions to get more consistent localization results and thus improve association. Another approach by Luiten et al. <ref type="bibr" target="#b51">[51]</ref> proposes to use dynamic 3D reconstruction for tracklet association in 3D space to improve long-term tracking. Osep et al. <ref type="bibr" target="#b53">[53]</ref> and Li et al. <ref type="bibr" target="#b52">[52]</ref> study the extension of this paradigm to 3D bounding box tracking using 3D information obtained from stereo cameras. Recent work by Weng et al. <ref type="bibr" target="#b54">[54]</ref> proposes specific evaluation metrics for this task and provides a baseline relying on 3D detections from LiDAR. Our work is in line with the recent developments in this field and aims to improve data association by leveraging 3D information, but goes beyond this by integrating both visual appearance and 3D localization information. Moreover, we utilize only a monocular camera and GPS information to track objects in 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Detection and Tracking.</head><p>While accurate object detection is a crucial component for tracking, i.e. to track an object throughout a video we must first detect it in every frame it is present, information from tracking can also provide strong priors for object detection. Therefore, many works have studied ways of integrating techniques from object detection with tracking algorithms. Feichtenhofer et al. <ref type="bibr" target="#b40">[40]</ref> compute correlation maps between a pair of frames which are subsequently used to predict the 2D bounding box deformation of each instance between the two frames. In <ref type="bibr" target="#b41">[41]</ref>, the authors utilize a single-frame Faster-RCNN model to predict the 2D bounding box deformation from the second stage refinement module alone. Recently, Lu et al. <ref type="bibr" target="#b55">[55]</ref> extend RetinaNet <ref type="bibr" target="#b11">[11]</ref> to learn track-level instance embeddings via a triplet loss in a joint detection and tracking model. In contrast to the popular triplet loss training scheme, Pang et al. <ref type="bibr" target="#b5">[5]</ref> propose to exploit given object proposals within the Faster-RCNN framework to learn instance embedding similarity using densely-connected contrastive pairs and show impressive improvement to instance embedding quality. Our work leverages these findings within our 3D detection and tracking framework, extending the work of Pang et al. beyond the image domain. In the 3D tracking domain, the paradigm of joint detection and tracking also caught researcher's attention recently, with Yin et al. <ref type="bibr" target="#b56">[56]</ref> combining 3D LiDAR detection with the center-based association paradigm from CenterTrack <ref type="bibr" target="#b57">[57]</ref> to perform joint 3D detection and tracking. However, because the depth can be perceived directly in LiDAR data, the task is much easier. Our work in contrast is able to perform joint 3D detection and tracking solely from 2D image information, estimating the 3D properties of the visible objects from the image information as well as associating these objects over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autonomous Driving Datasets.</head><p>Driving datasets have comprised some of the most popular benchmarks for computer vision algorithms in the last decade. Benchmarks like KITTI <ref type="bibr" target="#b3">[3]</ref>, UA-DETRAC <ref type="bibr" target="#b0">[1]</ref>, Cityscapes <ref type="bibr" target="#b58">[58]</ref> and Oxford RobotCar <ref type="bibr" target="#b59">[59]</ref> provide well annotated ground truth for visual odometry, stereo reconstruction, optical flow, scene flow, object detection and tracking as well as semantic segmentation. However, due to the high effort that the annotation of these datasets requires, these benchmarks have been limited in scale. In recent years, the topic of autonomous driving catched on more and more in the industry, providing the resources for new, large-scale driving benchmarks for computer vision, providing annotations for 3D computer vision tasks like 3D object detection and tracking at an unprecedented scale. Therefore, benchmarks like BDD100K <ref type="bibr" target="#b60">[60]</ref>, NuScenes <ref type="bibr" target="#b61">[61]</ref>, Argoverse <ref type="bibr" target="#b62">[62]</ref> and Waymo Open <ref type="bibr" target="#b63">[63]</ref> have attracted a lot of attention by the research community. Still, accurate 3D annotations are challenging to obtain and expensive to measure with 3D sensors like Li-DAR. To overcome this difficulty, there has been significant work on virtual driving datasets: virtual KITTI <ref type="bibr" target="#b32">[32]</ref>, SYN-THIA <ref type="bibr" target="#b64">[64]</ref>, GTA5 <ref type="bibr" target="#b65">[65]</ref>, VIPER <ref type="bibr" target="#b66">[66]</ref>, CARLA <ref type="bibr" target="#b67">[67]</ref>, and Free Supervision from Video Games (FSV) <ref type="bibr" target="#b68">[68]</ref>. These datasets have the potential to lower the costs of training accurate deep learning models for applications like autonomous driving tremendously, thus opening up the capabilities of these to a broader audience. The closest dataset to ours is VIPER <ref type="bibr" target="#b66">[66]</ref>, which provides a suite of videos and annotations for various computer vision problems while we focus on object tracking. We extend FSV <ref type="bibr" target="#b68">[68]</ref> to include object tracking in both 2D and 3D, as well as fine-grained object attributes, control signals from driver actions.</p><p>In the next section, we describe how to generate 3D object trajectories from 2D dash-cam videos. Considering the practical requirement of autonomous driving, we primarily focus on online tracking systems, where only the current and constant number of past frames are accessible to a tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JOINT 3D DETECTION AND TRACKING</head><p>Our goal is to jointly track objects across frames and infer the full 3D information of all tracks from a single monocular video stream and a GPS sensor. The 3D information includes the position, orientation, and dimensions of each object instance. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our system. Images are first passed through a backbone network and Region Proposal Network (RPN) to generate 2D object proposals (Section 3.2). These 2D proposals are then fed into two lightweight multi-head networks to infer per-instance similarity feature embedding (Section 3.3) and 3D information (Section 3.4). Using both feature embedding and 3D information to generate similarity metrics between all trajectories and detected proposals, we leverage estimated 3D object instance of current trajectories to track them through time (Section 3.5). We introduce motion-aware data association and depth-ordering matching to solve the occlusion problem in tracking. Finally, we refine the 3D location of objects  Our online approach processes monocular frames to estimate and track regions of interest (RoIs) in 3D (a). For each RoI, we learn the 3D layout (i.e., depth, orientation, dimensions, a projection of 3D center) estimation and instance-level feature embedding (b). With the 3D layout, our VeloLSTM helps to predict object states, and our 3D tracker produces robust linking across frames leveraging motionaware association and depth-ordering matching (c). VeloLSTM further refines the 3D estimation by fusing object motion features of the previous frames (d).</p><p>using the motion model through the newly matched trajectory (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We phrase the 3D tracking problem as a supervised learning problem. The goal is to find N trajectories T = {τ 1 , . . . , τ N } that match the ground truth trajectories in a video. Each trajectory τ i a,b links to a sequence of detected object states s</p><formula xml:id="formula_0">(i) a , s (i) a+1 , . . . , s (i) b</formula><p>starting at the first visible frame a and ending at the last visible frame b. The state of an object i at frame a is given by s i a = [P, O, D, F, ∆P ] ∈ R 10 where P defines the 3D world location (x, y, z) of the object center, O for object orientation θ, D for dimensions (l, w, h), F for appearance feature f app , and ∆P stands for its velocity (ẋ,ẏ,ż). The 3D object states in the world coordinates enable the use of our depth-ordering matching and motionaware association.</p><p>On a moving platform with a pose [R|t], the mounted camera captures objects found in the 3D world. We describe an object in the form of a 3D bounding box X ∈ R 3×8 in the world coordinates to avoid the changing reference axes introduced by ego-motion. Each 3D bounding box X can be viewed as the composition of object location P , dimensions D, and orientation O. The location P can be projected onto the camera plane using camera pose [R|t] as a point P cam = (x cam , y cam , d) where d is the depth from camera. The P cam can be further projected to image plane using camera intrinsics K as the projection of the 3D bounding box center C = (u c , v c ). The intrinsic parameter K can be obtained from camera calibration. Therefore, we can "lift" an object center C in the image plane with a depth d using camera parameters M = K[R|t] to a object location P in the world. Given the mapping from 3D points in the 3D world and 2D points in the image is known, we enclose a 2D bounding box B = (u min , v min , u max , v max ) over the projected 3D bounding box M × X as the 2D annotation for object detectors. The extrinsic parameter [R|t] can be retrieved from a commonly equipped GPS or IMU sensor. The recorded extrinsic parameter will be used later in the 3D tracking phase to cancel out the ego-motion of the moving platform.</p><p>The whole system is powered by an end-to-end multihead convolutional network trained on a considerable amount of ground truth supervision. Next, we discuss each component in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate Box Detection</head><p>We employ Faster R-CNN <ref type="bibr" target="#b8">[8]</ref> trained on our dataset to provide object candidates in the form of bounding boxes. As a two-stage detector, Faster R-CNN first generates regions of interest (RoIs) using a multiple-scale Region Proposal Network (RPN) and then utilizes a region convolutional neural network (R-CNN) to classify the RoI as well as to refine its localization. The multiple-scale RPN generates RoIs from the image feature and obtains its regional feature maps at different levels as the input of our multi-head detector, 3D estimator, and quasi-dense feature extractor networks. RoI align <ref type="bibr" target="#b69">[69]</ref> is used instead of RoI pool to obtain the regional representation, which reduces the misalignment of two-step quantization. Each object proposal <ref type="figure" target="#fig_0">(Figure 2</ref>(a)) corresponds to a 2D bounding box B as well as an estimated projection of the 3D bounding box centerĈ. The proposals are used to locate the candidate objects and extract their appearance features. However, the center of the objects' 3D bounding box usually does not project directly to the center of its 2D counterparts. Hence, we also provide an estimation of the 3D bounding box center for better accuracy.</p><p>Projection of 3D bounding box center. To estimate the 3D layout from single images more accurately, we extend the regression process to predict a projected 2D point of the 3D bounding box center from an RoI aligned feature F using Smooth L1 loss <ref type="bibr" target="#b70">[70]</ref>. Estimating a projection of the 3D center is crucial since a small gap in the image coordinate can cause a tremendous shift in 3D. With the extended multi-head detector, the model simultaneously regresses a bounding box B and an estimated centerĈ from an object proposal. We discuss how an estimated centerĈ is used in Section 3.4 for lifting predicted 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quasi-dense Similarity Learning</head><p>Data association, one challenging problem in the multiple object tracking field, requires distinguishable feature embeddings to match detections and tracklets <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>. We leverage quasi-dense proposals to train a discriminative feature space using contrastive loss. <ref type="figure">Figure 3</ref> depicts the difference of a quasi-dense approach comparing to conventional sparse ground-truth based approaches. Unlike sparse feature learning, which is commonly used in multiple object tracking that learns only from ground-truth bounding boxes, quasi-dense similarity learning utilizes all object proposals to discriminate positive proposals from negative ones. Unlike dense approaches, which is commonly used in single object tracking that compare tracklets with the whole image, our approach focuses on potential regions of interest to reduce the computational redundancy.</p><p>To obtain the proposal-to-proposal quasi-dense instance embedding correspondences, we establish pair-wise instance similarity loss between two neighboring frames, namely, a key frame and a reference frame. The multiplescale RPN generates RoIs from the two images and obtains their regional feature maps at different levels. We assign RoIs with IoU scores to an object over 0.7 as positive proposals, and those lower than 0.3 as negative proposals. Using the regional feature maps, we propose a quasi-dense embedding head to extract feature embeddings from positive and negative proposals. Adapting contrastive characteristics to the tracking problem, we defined region proposal pairs from two frames matched to the same object identity as positive pairs, and those matched to different identities are negative ones.</p><p>Given a key frame at time a, we sample a reference frame within a temporal interval n, where n ∈ [−3, 3] throughout all the experiments. For each target proposal s a , we optimize its feature embedding F sa using cross entropy loss combining a non-parametric softmax activation <ref type="bibr" target="#b73">[73]</ref>.</p><formula xml:id="formula_1">L embed = − log exp(F sa · F + sa+n ) exp(F sa · F + sa+n ) + F − s a+n exp(F sa · F − sa+n ) ,<label>(1)</label></formula><p>where feature embedding F sa of the target proposal is learned to associate its positive referenced embedding F + sa+n , and distinguish all its negative referenced ones F − sa+n . However, the formula above considers only one positive object proposal. To fully leverage the quasi-dense characteristics, we can balance the number of positive and negative examples by comparing the target proposal to all positive proposals.</p><p>Since we accumulate the whole positive targets, Equation 1 can be reformulated as follows</p><formula xml:id="formula_2">L embed = log[1 + F + s a+n F − s a+n exp(F sa · F − sa+n − F sa · F + sa+n )]<label>(2)</label></formula><p>The loss term L embed of the above formulation minimizes the cosine distance of the target proposal to all positive referenced examples while maximizing the cosine distance to all negative examples. By balancing positive and negative examples, we encourage the network to learn an embedding space that discriminates between instances most effectively while being invariant to input perturbations like change in viewpoint or lighting. In addition, we apply an auxiliary loss encouraging the learning of cosine similarity of target proposal F sa and references F sa+n</p><formula xml:id="formula_3">L aux = ( F sa · F sa+n ||F sa || · ||F sa+n || − 1(s a , s a+n )) 2<label>(3)</label></formula><p>where 1(s a , s a+n ) is the matching pair indicator function producing 1 if s a and s a+n match to the same object and 0 otherwise. We define the full quasi-dense instance similarity loss as</p><formula xml:id="formula_4">L similarity = λ embed L embed + L aux<label>(4)</label></formula><p>In short, the embedding head optimizes the quasi-dense feature embedding pairs to discriminate different instances using instance similarity loss L similarity . Next, we discuss how we estimate the 3D extent of an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">3D Bounding Box Estimation</head><p>We estimate complete 3D bounding box information (Figure 2(b)) from an RoI in the image via a feature representation of the pixels in the 2D bounding box. The RoI feature vector F is extracted from a deep convolutional backbone network. Each of the 3D information is estimated by passing the RoI features through a 2-layer convolutional sub-network with shared parameters and subsequently a 4-layer 3x3 convolution sub-network, which extends the stacked linear layers design of Mousavian et al. <ref type="bibr" target="#b17">[17]</ref>. We estimate all necessary parameters for inferring 3D bounding boxes from images, whereas Mousavian et al. <ref type="bibr" target="#b17">[17]</ref> focus on object orientation and dimensions from 2D boxes. Besides, our approach integrates 2D detection with 3D estimation, 3D tracking and motion refinement, while Mousavian et al. <ref type="bibr" target="#b17">[17]</ref> crops the input image with pre-computed boxes.</p><p>This network is trained using ground truth depth d, 3D bounding box center projection C, dimensions D, and orientation O. We explain how we estimate each component in detail in the following paragraphs.</p><p>3D World Location. Unlike previous approaches <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> that lift objects in camera coordinates, we infer 3D location P in the world coordinates from monocular images. The network learns to regress a logarithmic depth value log(d) scaled with a constant r with Smooth L1 loss <ref type="bibr" target="#b70">[70]</ref>. Compared with the characteristics of inverse depth value encoding 1/d in <ref type="bibr" target="#b4">[4]</ref> which compresses the target range ([5, 100]) drastically to ([0.80, 0.99]), our scaled logarithmic target provides a more realistic scaling inside our target  <ref type="figure">Fig. 3</ref>: The illustration of the quasi-dense similarity learning. We leverage quasi-dense object proposals (all the circles), instead of traditional sparse ground truth (solid circles), to train a discriminative feature space by comparing the region proposal pairs between the key frame and the reference frame. The quasi-dense instance similarity loss pulls the feature embedding of different object away from its paired target proposal and draws the embedding of same object pairs together in a high dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quasi-Dense Instance Similarity Loss</head><p>range of the encoded depth value ([1. <ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b4">4]</ref>). The scaled logarithmic approach remedies the curse of regression that small encoded depth value change results in a large depth estimation variation. A projected 3D location P is calculated using an estimated 2D projection of the 3D object center C as well as the depth d and camera transformation M. Initial Projection of 3D Bounding Box Center. The 3D object location in the current camera coordinate system can be obtained by estimating the depth of the detected 2D bounding box B as well as the 3D object center projection C and the camera calibration. The 3D world position P can be inferred by transforming the 3D object location based on the observer's pose. We find that accurately estimating the 3D object center and its projection is critical for accurate 3D bounding box localization. However, the projection of the 3D center can be far away from the 2D bounding box center. Naturally, a center shift from the 2D box center since the 2D bounding box encloses the 3D object from the observer's perspective. After occlusion and truncation, the estimated 2D bounding box can hardly cover the correct dimension with only the visible area of the objects. In contrast, the 3D bounding box is defined by the full physical dimensions of an object. The projected 3D center can lie even outside the detected 2D boxes. For instance, the 3D bounding box of a truncated object can be out of the camera view. These situations are illustrated in <ref type="figure">Figure 4</ref>.</p><p>Single-frame 3D Confidence. Given the high variance in estimating the object-observer relationship in the real world, we find a single-frame depth estimation is prone to predict an erroneous guess if the target object is only partially observable or in unforeseen situations. While the uncertainty of the model estimation has been well-studied <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b75">[75]</ref>, we aim to solve the uncertainty problem in a spatialtemporal approach, focusing on the confidence of the 3D Visible Invisible 2D Center 3D re-projection 2D projection 3D Center <ref type="figure">Fig. 4</ref>: 3D projected centers are perferred over 2D bounding boxes. A visible 2D center projection point may wrongly locate the object away from the ground plane in the 3D coordinates and would inevitably suffer from more severe scenarios, occluded or truncated. The center shift causes object misalignment for GT and predicted tracks and harms 3D IoU AP performance. estimation. During training, the network is supervised by the target confidence score c depth = exp(−|d − d|/r), which we formulated as an exponential of negative depth distance of prediction and ground truth scaled by a constant r. The learned confidenceĉ depth is used during inference as a 3D objectness score in the detection phase, and as a weighting score in the tracking phase. The confidence helps to handle the ambiguity of the single-frame depth estimation by balancing single-frame depth estimation and multiple-frame motion model location prediction.</p><p>Object Orientation. Given the image coordinate distancê u =û c − w img /2 from an object centerû c to the horizontal center of an image w img /2 and the focal length f , the object rotation θ cam in the camera coordinates transformed from an observed vehicle heading θ obs with simple geometry, θ cam = [θ obs + arctan(û/f )] mod 2π. Later we can restore the global rotation θ in the world coordinates using egovehicle heading obtained in R. Following Mousavian et al. <ref type="bibr" target="#b17">[17]</ref> for θ obs estimation, we first classify the angle into two bins and then regress the residual relative to the bin center using Smooth L1 loss <ref type="bibr" target="#b70">[70]</ref>.</p><p>Object Dimension. In driving scenarios, the high variance of the distribution of the dimensions of different categories of vehicles (e.g., car, bus) results in difficulty classifying various vehicles using unimodal object proposals. Therefore, we regress the dimensions (l,ŵ,ĥ) to the ground truth dimensions (l, w, h) using Smooth L1 loss <ref type="bibr" target="#b70">[70]</ref>.</p><p>To sum up, estimating an object's 3D properties provides us with an observation of its location P with orientation O, dimensions D and 2D projection of its 3D center C. Next, we discuss how we associate the estimation across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data Association and Tracking</head><p>Given a set of candidate detections S a = {s 1 a , . . . , s M a } and a set of tracks T a = {τ J , . . . , τ K } at frame a where 1 ≤ J ≤ K ≤ N from N trajectories, our goal is to associate each track τ with a candidate detection s, spawn new tracks, or end a track <ref type="figure" target="#fig_0">(Figure 2</ref>(c)) in an online fashion.</p><p>We solve the data association problem by using a weighted bipartite matching algorithm. Affinities between tracks and new detections are calculated from three criteria, (1) the deep representation similarity A deep (τ a , s a ) of the appearance embeddings f app between an accumulated feature F τa from existing tracks and a learned feature F sa from object detections; (2) the overlap of bounding boxes A iou (τ a , s a ) between the projection of a 3D bounding box in current trajectories forward in time B τa and one of the bounding box candidates B sa ; and (3) the motion similarity A motion (τ a , s a ) of a pseudo object motion vector V sa = − −−− → P τa P sa and the accumulated motion vector V τa from current trajectories. Each trajectory is projected forward in time using the estimated velocity of an object and camera ego-motion. Here, we assume that ego-motion is given by a sensor, like GPS, an accelerometer, gyro, or IMU.</p><p>We define an affinity matrix A(T a , S a ) ∈ R ||Ta||·||Sa|| between the information of existing tracks T a and candidate detections S a as a joint probability of appearance, location, and velocity correlation.</p><formula xml:id="formula_5">A deep (τ a , s a ) = σ x (F τa F T sa ) + σ y (F τa F T sa ) 2 (5) A iou (τ a , s a ) = exp( −|B τa − B sa | c ),<label>(6)</label></formula><formula xml:id="formula_6">A motion (τ a , s a ) = exp( −|V τa − V sa | c ))<label>(7)</label></formula><p>where σ x , σ y are sigmoid functions applied along x, y direction. The affinity matrix, A(T a , S a ), aggregates the semantic, spatial and temporal similarities. Given the tracklets and detections, we sort them into a list by depth order. For each detection of interest (DoI), we calculate similarities of the object state between DoI and each tracklet. The location of the object in the world coordinates naturally provides higher probabilities to tracklets near the DoI.</p><formula xml:id="formula_7">A(τ a , s a ) = w deep A deep (τ a , s a ) + (1 − w deep )A motion (τ a , s a ) · A iou (τ a , s a )<label>(8)</label></formula><p>The weight w deep balances appearance similarity and a hybrid of motion and 3D location overlap. We utilize a mixture of discriminative quasi-dense feature embeddings, bounding box overlaps, and location reasoning as similarity measures across frames, similar to the design of POI <ref type="bibr" target="#b76">[76]</ref>. We solve data association greedily instead of using Kuhn-Munkres algorithm <ref type="bibr" target="#b77">[77]</ref> in Mono3DT <ref type="bibr" target="#b4">[4]</ref>.</p><p>Comparing to 2D tracking, 3D-oriented tracking is more robust to ego-motion, visual occlusion, overlaps, and reappearances. When a target is temporarily occluded, the corresponding 3D motion estimator can roll-out for a period of time and relocate the 2D location at each new point in time via the camera coordinate transformation.</p><p>Data Association Scheme. Similar to previous methods <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b80">[80]</ref>, we model the lifespan of a tracker using four major subspaces in MDP state space: {birth, tracked, lost, death}. For each new set of detections, the tracker is updated (tracked) using pairs with the highest affinity score (Equation 8). Each unmatched detection spawns a new tracklet (birth); however, an unmatched tracklet is not immediately terminated (death), as tracklets can naturally disappear (lost) into an occluded region and reappear later. We address the dynamic object inter-occlusion problem by introducing a motion vector propagation. A lost tracklet will not update its feature representation until it reappears, but we still predict its 3D location using the estimated motion. We continue to predict the 3D location of unmatched tracklets until they disappear from our tracking range (e.g. 0.15m to 100m) or die out after their lifespan (e.g. 10 time-steps). With the sweeping scheme above, we keep disappeared object tracks inside the tracker memory to be able to recover from object occlusions while still keeping the computational requirements for storing all object tracks at a feasible level.</p><p>Depth-Ordering Matching. We introduce instance depth ordering for assigning a detection to neighbor tracklets, which models the strong prior of relative depth ordering found in human perception. Extending the formulation in Hu et al. <ref type="bibr" target="#b4">[4]</ref>, we consider probabilities of matching each detection of interest (DoI) to potential tracklets using centroid distance of their object state s a . Additionally, we introduce object orientation and dimensions to discover different neighbor objects with similar centroid distances. From the view of each DoI, we compare the pairwise distance of object state s a with all the tracklets τ a in the world coordinates. We directly enforce a smooth distance transition of an exponential function to replace the discrete depth order relationship. To cancel out the ordering ambiguity of a distant tracklet, we leverage the nature of negative exponential to cut off the matching probability of a distant tracklet smoothly. So Equation 6 becomes</p><formula xml:id="formula_8">A iou (τ a , s a ) = exp( −|τ a − s a | r ),<label>(9)</label></formula><p>where the exponential function with a constant r captures the concept that distant objects contribute less to the affinities. It naturally provides higher probabilities of linking similar neighbor tracklets than those far away. In this way, we solve the data association problem of moving objects with the help of 3D trajectories in world coordinates. <ref type="figure" target="#fig_1">Figure 5</ref> depicts the pipeline of depth ordering.</p><p>Motion-aware Data Association. Assuming a linear movement of an object, we expect a lost object to reappear at a predictable location based on its velocity and the previously seen locations. Equation 7 encodes both the motion magnitude ||V || and heading angle ∠V into the similarity. Specifically, A motion considers accumulated motion vectors V τa from tracklet trajectories and pseudo motion vectors V sa from single-frame depth estimation. Since the network can not estimate a velocity directly from a single frame, we leverage the bipartite affinity that aggregates the pseudo motion vectors − −−− → P τa P sa from all P τa to P sa . However, a motion-only similarity in Equation 7 does not distinguish similar motion vectors in different locations. We propose a new A motion (τ a , s a ) that counts in the motion vectors as well as the centroid location of the objects.</p><formula xml:id="formula_9">A motion (τ a , s a ) = w cos A centroid + (1 − w cos ) A pseudo<label>(10)</label></formula><p>which composed of the following terms</p><formula xml:id="formula_10">A centroid = exp( −|P τa − P sa | r ) A pseudo = exp( −|V τa − V sa | r ) w cos = ∠(V τa , V sa )<label>(11)</label></formula><p>where w cos is the normalized cosine similarity of two motion vectors ∠(V τa , V sa ) ∈ [0, 1]. The w cos favors centroid distance if two motion vectors within a hemisphere of the same direction and weights more on pseudo motion otherwise. Therefore, we can find the most aligned motion vector that assigns the detected object s a to the target tracklet τ a . <ref type="figure" target="#fig_2">Figure 6</ref> illustrates how our motion-aware data association scheme works. In the next subsection, we show how a motion model helps refine the estimated 3D object state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Motion Model Refinement</head><p>For any new tracklet, the detection network is trained to predict the object state s by leveraging RoI features. For any previously tracked object, the motion model network learns a mixture of multi-view monocular 3D estimates by merging the object state from previous visible frames and the current frame. Given the learned motion and updated object state, the motion model network predicts the next possible object state to assist the data association procedure. The update and predict cycles refine the final object state in tandem. VeloLSTM: Deep Motion Estimation and Update. To exploit the temporal consistency of objects, we associate the information across frames using two LSTMs for object state prediction and updating <ref type="figure" target="#fig_0">(Figure 2(c)</ref>). We use two-layer LSTMs with 128-dim hidden state size to keep track of a 3D object state in the world coordinates s Prediction LSTM (P-LSTM) models the dynamic object state s a in 3D world coordinates based on the previous refined object state s a−1 with predicted object velocitẏ s a = {∆P, ∆O, ∆D} from previously updated velocitieṡ s a−n:a−1 . We encode previous n = 5 frames of object velocity into a 64-dim velocity feature to model object motion and acceleration from the trajectory. P-LSTM predicts the next possible object states a = s a−1 +ṡ a .</p><p>Updating LSTM (U-LSTM) balances current observed object stateŝ a from 3D estimation module and previously predicted states a−1 considering current 3D confidence of depth estimationĉ depth . We encode observed, predicted object states into 64-dim state features and 3D confidence scores into a 64-dim confidence feature. By concatenating observation, prediction state features and confidence feature together, we obtained a 192-dim feature embedding as the input of U-LSTM. Given the fusion information, U-LSTM refines the object states a and updates velocitiesṡ a−n:a−1 .</p><p>Modeling motion in 3D world coordinates naturally cancels out adverse effects of ego-motion, allowing our model to handle missed and occluded objects. The LSTMs keep updating the predicted object states a−1 while assuming a linear velocity model if there is no matched bounding box. Therefore, we model 3D motion <ref type="figure" target="#fig_0">(Figure 2(d)</ref>) in world coordinates allowing occluded tracklets to move along motionplausible paths while managing the birth and death of moving objects.</p><p>Both LSTM modules are trained with predicted object statess a , refined object states s a from detector prediction, and ground truth object state s a trajectories. The refinement loss L refine (s a , s a ) and prediction loss L predict (s a , s a ) aim to reduce the distance of estimated and ground-truth object state using Smooth L1 loss <ref type="bibr" target="#b70">[70]</ref>. The linear motion loss L linear (y a−n:a ) = a−n:a |(y a − y a−1 ) − (y a−1 − y a−2 )|/∆t focuses on the smooth transition over time ∆t of object state refinement L linear (s a−n:a ) and prediction L linear (s a−n:a ).</p><p>Conclusively, our pipeline consists of a single-frame monocular 3D object detection model for object-level pose inference and recurrent neural networks for inter-frame object association and matching. We extend the region processing to include 3D estimation by employing multihead modules for each object instance. We introduce a motion-aware association to solve the inter-object occlusion problem. For tracklet matching, depth ordering lowers the mismatch rate by filtering out distant candidates from a target. The LSTM motion estimator updates the velocity and states of each object independent of camera movement or interactions with other objects. The final pipeline produces accurate and smooth object trajectories in the 3D world coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D VEHICLE TRACKING SIMULATION DATASET</head><p>It is laborious and expensive to annotate a large-scale 3D bounding box image dataset even in the presence of LiDAR data, although it is much easier to label 2D bounding boxes on tens of thousands of videos <ref type="bibr" target="#b60">[60]</ref>. Therefore, no such dataset collected from real sensors is available to the research community. To resolve the data problem, we turn to driving simulation to obtain accurate 3D bounding box annotations at no cost of human efforts. Our data collection and annotation pipeline extend the previous works like VIPER <ref type="bibr" target="#b66">[66]</ref> and FSV <ref type="bibr" target="#b68">[68]</ref>, especially in terms of linking identities across frames.</p><p>Our simulation is based on Grand Theft Auto V, a modern game that simulates a functioning city and its surroundings in a photo-realistic three-dimensional world. To associate object instances across frames, we utilize in-game API to capture global instance id and corresponding 3D annotations directly. In contrast, VIPER leverages a weighted matching algorithm based on a heuristic distance function, which can lead to inconsistencies. It should be noted that our pipeline is real-time, providing the potential of largescale data collection, while VIPER requires expensive offline processings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Statistics.</head><p>Compared to the others, our dataset has more diversity regarding instance scales ( <ref type="figure" target="#fig_4">Figure 7a</ref>) and closer instance distribution to real scenes <ref type="figure" target="#fig_4">(Figure 7b</ref>). To help understand our dataset and its difference, we show more statistics in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate our 3D detection and tracking pipeline on the real-world driving scenes, i.e., Waymo Open dataset, nuScenes Tracking, KITTI MOT benchmarks, and our largescale simulation dataset featuring a wide variety of road conditions in a diverse virtual environment. To follow the  <ref type="bibr" target="#b3">[3]</ref>, VKITTI <ref type="bibr" target="#b32">[32]</ref>, VIPER <ref type="bibr" target="#b66">[66]</ref>, and Cityscapes <ref type="bibr" target="#b58">[58]</ref> submission policy on the above public testing benchmarks, we only evaluate the final performance of our best result on the public testing benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Details</head><p>GTA 3D Vehicle Tracking dataset recorded raw data at 12 FPS, which is helpful for temporal aggregation. With the goal of autonomous driving in mind, we focus on vehicles closer than 150m, and also filtered out the bounding boxes whose areas are smaller than 256 pixels. The dataset is then split into train, validation and test set with a ratio 10 : Cross Camera Aggregation. For both nuScenes and Waymo, we align all camera results together in an online fashion because their official evaluations only support the LiDARbased perspective. The cross-camera aggregation criteria focus on removing duplicated objects across different cameras whose 3D euclidean distance is within 2 meters. For an object with smaller width or height, such as a pedestrian, the threshold is set to 1 meter. Once duplicates are found, we keep the bounding box with a higher 3D score and discard the lower ones to handle the overlapping problem.</p><p>On the nuScenes dataset, we found using only camera data achieving competitive results compared to LiDAR baselines since all 6 cameras can be stitched together to reconstruct the 360-degree scene. On the other hand, the five camera setting on Waymo Open Dataset lacks information at the rear camera and covers only 5/8 azimuth angle of the horizon. But objects are annotated in LiDAR sweeps. The limitation above refrains our method from reaching meaningful evaluation scores on Waymo Open 3D detection or tracking benchmark. Therefore, we conduct a view angle ablation study in <ref type="table" target="#tab_9">Table 7</ref> to validate the power of our datadriven method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and Evaluation</head><p>Network Specification. Our 2D detection and 3D center estimation are based (Section 3.2) on Faster RCNN <ref type="bibr" target="#b8">[8]</ref> with image classification pre-trained weights on ImageNet <ref type="bibr" target="#b81">[81]</ref> from TorchVision <ref type="bibr" target="#b82">[82]</ref>. For the KITTI dataset with a smaller amount of data, we choose a light-weight yet powerful DLA-34-up <ref type="bibr" target="#b83">[83]</ref> architecture, while for the large driving datasets, e.g., nuScenes, GTA, Waymo, we resort to a 101layer ResNet <ref type="bibr" target="#b84">[84]</ref>. The anchors of the RPN in Faster R-CNN span 5 scales and 5 ratios, in order to detect small objects at distance as well as larger, close objects. We use a maximum of 2000 candidate objects before NMS. The 3D estimation network (Section 3.4) estimate 3D bounding box parameters from features extracted from RoI align <ref type="bibr" target="#b69">[69]</ref>. For the quasidense similarity learning (Section 3.3), we use λ embed = 0.25 throughout the experiments. The LSTM motion module (Section 3.6) is trained for 100 epochs on 10 sample frames per object trajectory with 128 sequences per batch for largescale datasets and 8 for KITTI.</p><p>Training Procedure. The general training procedure is conducted for 24 epochs using SGD <ref type="bibr" target="#b85">[85]</ref> with a momentum of 0.9 and a weight decay of 10 −4 . The learning rate linearly increases from 5 × 10 −4 to 5 × 10 −3 over the initial 1000 warm-up training steps. We use a batch size of 8 images per GPU and train on 4 GPUs, resulting in an effective mini-batch size of 32. We apply image horizontal flipping as data augmentation in the training phase, and there is no augmentation performed for test/validation phase. We optimize our model on each dataset with different training procedures according to its amount of data and the GPU memory limit. The image resolution is resized and padded to fit the minimum down sample scale 32. We use 1600 × 900 resolution for nuScenes, 1280 × 854 for Waymo, 1485 × 448 for KITTI, and 1920 × 1080 for GTA. Following CenterTrack <ref type="bibr" target="#b57">[57]</ref>, we fine-tune our KITTI model from a GTA 3D detection model. Given the multiple cameras setup in nuScenes and Waymo, we treat each camera video as a different sequence during both training and inference. Since nuScenes doesn't provide 2D annotations, we project the annotated 3D bounding boxes to the image plane to train the 2D branch of our model. On the Waymo Open dataset, we use only the 3D LiDAR annotations and project them to the image coordinates as our 2D annotations since the 2D and 3D bounding boxes are annotated independently. For more details on training strategies, please refer to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Detection Evaluation.</head><p>We evaluate the performance of 3D detection with refined object state estimation of different tracking methods. For KITTI and GTA, we use the formal evaluation metrics of detection 3D mAP from KITTI <ref type="bibr" target="#b3">[3]</ref> in the 3D Object Detection Evaluation. To inspect the precisionrecall curve deeper, we also use detection 3D mAP from COCO <ref type="bibr" target="#b86">[86]</ref> with a range of IoU thresholds from 50 to 95 at a step of 5. As stated in <ref type="bibr" target="#b19">[19]</ref>, we use 40 recall positions in 3D object detection evaluation instead of the 11 recall positions proposed in the original Pascal VOC benchmark <ref type="bibr" target="#b87">[87]</ref>. For nuScenes, we use the benchmark evaluation metrics, called nuScenes detection score (NDS), which is the weighted sum of mean Average Precision (mAP) and several True Positive (TP) metrics. For Waymo, we also use the 3D detection benchmark evaluation metrics where their primary metric is mean Average Precision weighted by Heading (mAPH) with different 3D IoU thresholds for each class.</p><p>Multiple Object Tracking Evaluation. For KITTI and GTA, we use the official KITTI 2D tracking benchmark metrics following CLEAR <ref type="bibr" target="#b88">[88]</ref> and Li et al. <ref type="bibr" target="#b89">[89]</ref>, including Multiple Object Tracking Accuracy (MOTA), Multiple Object Tracking Precision (MOTP), Miss-Match (MM), False Positive (FP), False Negative (FN), Mostly-Tracked (MT), Partly-Tracked (PT), and Mostly-Lost (ML), etc. MOTA indicates the tracking performance of a tracker in dealing with three common errors, i.e., false positives, missed targets and identity switches.</p><formula xml:id="formula_11">M OT A = 1 − IDS + F P + F N GT<label>(12)</label></formula><p>where GT stands for number of ground-truth positives. For nuScenes, we follow the 3D tracking benchmark evaluation metrics using Average Multi-Object Tracking Accuracy (AMOTA) as the main metric. They average the MOTA metric over different recall thresholds</p><formula xml:id="formula_12">AM OT A = 1 n − 1 r∈{ 1 n−1 , 2 n−1 ,...,1} M OT A r , M OT A r = max(0, 1 − α IDS r + F P r + F N r − (1 − r)GT rGT )<label>(13)</label></formula><p>with recall threshold r is calculated at each of the interpolation n-points for n = 40, tolerance ratio α aims to retain the non-zero values of MOTA r . We report α = 1.0, denoted as AMOTA@1, for the official benchmark and α = 0.2, denoted as AMOTA@0.2, for backward comparison with previous arts. Instead of using 3D IoU threshold, the nuScenes tracking benchmark choose a centroid distance threshold of 2 meters on the ground plane as the matching criterion. For Waymo, we report the official 3D tracking evaluation metrics, which follow the metrics of CLEAR <ref type="bibr" target="#b88">[88]</ref> with different 3D IoU thresholds for each class. However, the limitations above (Section 5.1) refrain our method from reaching meaningful evaluation scores on benchmarks using 3D IoU based metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Evaluation Metrics</head><p>The analysis from <ref type="bibr" target="#b61">[61]</ref> has shown that objects, i.e., pedestrians, bicycles, are hard for camera-based methods to have meaningful IoU scores, while those objects are difficult to be detected in LiDAR. Similarly, we observe from quantitative results on Waymo that the IoU threshold at 0.7 prohibits the camera-based method from evaluating meaningful tracking performance. However, the score does not cohere to our qualitative results. We argue that high IoU thresholds become impractical with uncertain object state s a measurements as the metric does not reward a tracker for associating all matches right if it makes a moderate error on the object state.</p><p>Therefore, we report extended 3D tracking evaluation <ref type="bibr" target="#b62">[62]</ref> for a better understanding of the association measure in the 3D extent. The MOTA 3D is based on the object centroid distance, denoted as MOTP C , from tracked objects to the ground truth within a detection association range, i.e., 2 meters. MOTP O calculates orientation angle difference to the vertical axis, and MOTP I stands for the amodal shape error, using 1 − IoU with aligned orientation and centroid.</p><p>We conduct ablation studies in <ref type="table" target="#tab_10">Table 8</ref> with detailed metrics on translation, scale, orientation as accompanying metrics to validate the power of our data-driven method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiments</head><p>In the following, we discuss the results of our ablation experiments on the design choices of each component (3D box estimation, data association, motion modeling) in our quasi-dense 3D tracking pipeline. We report the 3D tracking performance comparison on nuScenes validation set for dropping sub-affinity scores in <ref type="table" target="#tab_3">Table 1a</ref>, using full images in <ref type="table" target="#tab_3">Table 1b</ref>, and swapping different modules in <ref type="table" target="#tab_4">Table 2</ref>. We also investigate the importance of using a 3D object center using the KITTI sub-val set in <ref type="table" target="#tab_5">Table 3</ref>. <ref type="table" target="#tab_3">Table 1a</ref>, we observe that adding a deep feature affinity (A deep ) distinguishes two near-overlapping objects and thus increasing AMOTA@1 with an observable margin. Adding 3D object state affinity (A iou ) also contributes to AMOTA@1 effectively. Lastly, we found in <ref type="table" target="#tab_3">Table 1b</ref> that applying our model on full frames during inference helps the motion model estimate an objects' velocity more accurately. Motion-based methods benefit from using a higher frame rate setting, full frames, with a (0.2421 − 0.2329)/0.2421 = 3.8% improvement on LSTM and 0.6% on KF3D.  <ref type="table" target="#tab_4">Table 2</ref>, as overlapping threshold helps identify falsely rejected objects and increases overall AMOTA@1 scores by a relative (0.2306 − 0.1381)/0.2306 = 40.1% AMOTA@1. With motion-aware association, denoted motion in <ref type="table" target="#tab_4">Table 2</ref>, our full model filtering out objects with wrong motion velocity reaches 0.2306 AMOTA@1 comparing to 0.2234 using only cosine angle difference module or 0.2300 using centroid distance module. Importance of 3D center projection estimation. We estimate the 3D location of a bounding box by predicting the projection of its center and depth, while Mousavian et al. <ref type="bibr" target="#b17">[17]</ref> uses the center of detected 2D boxes directly. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison of these two methods on the KITTI dataset. The result indicates that the correct 3D projections provide a higher tracking capacity for a motion module to associate candidates and significantly reduce the centroid misalignment (MOTP C ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of each sub-affinity matrix. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Motion Modeling Comparison.</head><p>For motion-based models, update-and-predict cycles help single-frame 3D estimates. To examine the effectiveness of our VeloLSTM, we compare variants of motion models to pure detection in refining the object locations along their trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pure Detection (Detection).</head><p>A single-frame monocular 3D estimate provides a noisy measurement of an object learned from the data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dummy Motion Model (Momentum).</head><p>Comparing with the non-linear motion model, we experimented with a dummy motion model that directly updated the object state with a fixed momentum on observed measurements and predicted the next possible state with zero motion. By applying motion momentum, we expect the dummy model to output linearly smoothed trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kalman Filter 3D baseline (KF3D).</head><p>We take a Kalman filterbased motion model as our baseline which models {x, y, z, θ, l, w, h, ∆x, ∆y, ∆z} in 3D world coordinates. The well-tuned Kalman filter <ref type="bibr" target="#b90">[90]</ref> models a sequence of observed measurements with Gaussian noise assumptions to find a prediction location that minimize a mean of square error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Motion Estimation and Update (VeloLSTM).</head><p>As mentioned in Section 3.6, we propose a VeloLSTM network to model the vehicle motion. To analyze the effectiveness of our deep motion model, we compare our LSTM model with traditional 3D Kalman filter (KF3D) and single-frame 3D estimation (Detect) using 3D detection and tracking evaluation metrics from nuScenes. <ref type="table" target="#tab_6">Table 4</ref> shows that KF3D provides a small improvement in NDS via trajectory smoothing within prediction and observation. On the other hand, our VeloLSTM module provides a learned estimation based on past n = 5 velocity predictions and current frame observation, which may compensate for the observation error.</p><p>Our VeloLSTM module achieves the highest AMOTA@1, AMOTP, and NDS, among the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Real-world Evaluation.</head><p>Besides evaluating on large-scale synthetic data, we utilize nuScenes <ref type="bibr" target="#b61">[61]</ref>, Waymo Open <ref type="bibr" target="#b63">[63]</ref> and KITTI <ref type="bibr" target="#b3">[3]</ref> benchmarks to compare our model abilities with other state of the arts.</p><p>Major results are listed in <ref type="table" target="#tab_7">Table 5 for nuScenes dataset and  Table 6a</ref> and <ref type="table" target="#tab_8">Table 6b</ref> for Waymo Open dataset. Qualitative results on all three datasets are shown in <ref type="figure" target="#fig_5">Figure 8</ref>.  <ref type="bibr" target="#b91">[91]</ref> to generate 3D detection. The LiDAR-based baselines <ref type="bibr" target="#b61">[61]</ref> uses state-of-the-art LiDARbased detectors <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b91">[91]</ref> to estimate accurate bounding boxes and feed into a Kalman-Filter-based 3D tracker, AB3DMOT <ref type="bibr" target="#b54">[54]</ref>. Our method takes only one image for 3D object detection and tracking and leverages our deep motion model to refine object trajectories compared to the prior arts. The nuScenes tracking dataset linearly interpolates GT tracks to avoid track fragments from LiDAR point filtering and removes GT objects without LiDAR points. Both invisible objects with annotation and visible objects without annotation prohibit the camera-based methods from optimizing bounding box estimation. Nevertheless, our quasi-dense 3D tracking approach reaches 0.217 AMOTA with near five times tracking accuracy of the best vision-only submission among all published methods. Waymo Open benchmark. We provide a strong baseline being the first entry in camera-only modality for both 3D tracking and detection challenges shown in <ref type="table" target="#tab_8">Table 6a</ref> and 6b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluation Metrics.</head><p>We observe from <ref type="table" target="#tab_8">Table 6a</ref> that the IoU threshold at 0.7 prohibits the camera-based method from evaluating meaningful tracking performance. However, the score does not  Our VeloLSTM continues to predict the 3D object state of unmatched tracklets after they disappear. With motion-based data association, our tracker successfully recovers tracked vehicle (cyan-colored box) from object occlusions after the vehicle re-appeared. We show predicted 3D bounding boxes and trajectories colored with tracking IDs.  cohere to our qualitative results in <ref type="figure" target="#fig_5">Figure 8</ref>. Therefore, we conduct ablation studies to validate the power of our datadriven method. We first compare 3D detection and tracking results on different extent of observable area, i.e., for the annotations from the whole field of view, denoted LiDARbased GT, and ones from full camera field of view, denoted Camera-based GT. The LiDAR-based GT setting constantly yields inferior performance with approximate a 5/8 to the Camera-based GT ones in different IoU thresholds in <ref type="table" target="#tab_9">Table 7</ref>.</p><p>The difference matches the ratio of an observable area by cameras and area by a LiDAR. We turn to centroid-based metrics to validate our pipeline's performance since the IoU-based metrics set up an unreachable bar for the camera-based method. With centroid-based metrics, the tracking scores of our final model on nuScenes and Waymo Open datasets list in Table 8. The similarity in MOTA and MOTP performance suggests that our pipeline has enough capacity to handle different datasets with minimal change of training procedure, underlining the robustness of our tracking pipeline.</p><p>From <ref type="table" target="#tab_10">Table 8</ref> and <ref type="table" target="#tab_9">Table 7</ref>, our method acts comparable in IoU-based metrics with a threshold at 0.3 to centroid-based metrics with a threshold at 2 meters on the camera-based GT setting. We argue that the IoU threshold at 0.7 is overly biased toward a precise 3D object attributes estimation, like depth estimation, 3D center, orientation, 3D center or dimensions. A trajectory with perfect matching but a moderate error in one of the object attributes, e.g., 1m off in depth, is not counted as a true positive. However, it is treated equally as those with a completely wrong estimation e.g., 10m away. Besides, the experiment in <ref type="table" target="#tab_9">Table 8 and Table 7</ref> show an interesting discrepancy in reported performance by different types of evaluation metrics, which supports analysis in <ref type="bibr" target="#b61">[61]</ref> to choose centroid-based evaluation metrics as a fair comparison to both camera-based and LiDARbased methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Amount of Data Matters.</head><p>We train the depth estimation module with 1%, 10%, and 100% training data on the GTA dataset. The results show how we can benefit from more data in <ref type="table" target="#tab_11">Table 9</ref>, where a consistent trend of performance improvement emerges as the number of data increases. The trend of our results with a different amount of training data indicates that large-scale 3D annotation is helpful, especially with the ground truth of distant and small objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Comparison of matching algorithms.</head><p>A tracker solves the assignment problem often using combinatorial optimization algorithms, e.g., Hungarian algorithm. However, from the comparison in <ref type="table" target="#tab_3">Table 10</ref>, we found the well-trained instance embeddings and 3D object states are robust in matching possible pairs without needing an optimizing algorithm. Besides, our method benefits from the lower computational complexity compared to using polynomial-time optimization algorithms. Therefore, we choose to greedily match bipartite quasi-dense pairs of detections and tracklets throughout the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose an online 3D object detection and tracking framework, combining quasi-dense similarity learning and 3D instance dynamics, to track moving objects in a 3D world. Our updated pipeline consists of four parts: a singleframe monocular 3D object inference model, cross-frames contrastive feature learning network, multimodal affinity matching scheme for inter-frame object association, and an LSTM-based motion model for refining the 3D extents from location trajectories. Besides, we introduce a 3D detection confidence to provide a balancing cue of single-frame depth estimation and multi-frame motion model refinement. We present 3D bounding boxes depth-ordering matching for robust instance association and utilize motion-based 3D trajectory prediction for re-identification of occluded vehicles. We design an object movement learning module, termed VeloLSTM, to update each object's location independent of camera movement. Moreover, our ablation study and experiments show that our quasi-dense 3D tracking pipeline takes advantage of dynamic 3D trajectories and offers robust data association on urban-driving situations. On the Waymo Open benchmark, we established the first strong baseline in camera modality with positive scores in both the 3D detection and tracking challenge. Our quasi-dense 3D tracking pipeline outperforms the camera-based state of the art by near 500% while bridging the gap to LiDAR-based methods on nuScenes 3D tracking benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Instances in Each Category.</head><p>The vehicle diversity is also very large in the GTA world, featuring 15 finegrained subcategories. We analyzed the distribution of the 15 subcategories in <ref type="figure" target="#fig_7">Figure 10a</ref>. Besides instance categories, we also show the distribution of occluded ( <ref type="figure" target="#fig_7">Figure 10c</ref>) and truncated <ref type="figure" target="#fig_7">(Figure 10b</ref>) instances to support the problem of partially visible in the 3D coordinates. Weather and Time of Day. <ref type="figure" target="#fig_8">Figure 11</ref> shows the distribution of weather, hours of our dataset. It features a full weather cycle and time of a day in a diverse virtual environment. By collecting various weather cycles <ref type="figure" target="#fig_8">(Figure 11a</ref>), our model learns to track with a higher understanding of environments. With different times of a day <ref type="figure" target="#fig_8">(Figure 11b</ref>), the network handles changeable perceptual variation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B TRAINING DETAILS</head><p>We optimize our model on each dataset with different training procedures according to its amount of data and the limit of the GPU memory.</p><p>Training Procedure. For nuScenes, we train our model on the training set with all 10 object classes and evaluate the performance on the validation set with 10 object classes for detection task and 7 object classes for tracking task. We trained the model on 8 GPUs with a total batch size of 32 for 24 epochs. The learning rate was linearly increased from 1 × 10 −3 , to 1 × 10 −2 over the initial 1000 warm-up training steps decreased by 0.5 after 16 and 22 epochs. For Waymo, we trained the model on 8 GPUs with a total batch size of 16 for 24 epochs. The learning rate was linearly increased from 5 × 10 −4 , to 5 × 10 −3 over the initial 1000 warm-up training steps. Also, due to the large amount of the training data, we decrease the learning rate by 0.5 after 8, 12, 16, 20, and 22 epochs.</p><p>For GTA, we trained the model on 8 GPUs with a total batch size of 24 for 24 epochs. The learning rate was linearly increased from 1 × 10 −3 , to 1 × 10 −2 over the initial 1000 warm-up training steps. Due to the large amount of the training data, we decreased the learning rate by 0.5 after 8, 12, 16, 20, and 22 epochs.</p><p>For KITTI, we fine-tuned the model from a GTA 3D detection model on 4 GPUs with a total batch size of 8 for 24 epochs. The learning rate was linearly increased from 5 × 10 −4 , to 5 × 10 −3 over the initial 1000 warm-up training steps and decreased by 0.5 after 16 and 22 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C EXPERIMENTS</head><p>Inference Time. The average inference time is 123.3 + 2.3 + 12.3 + 1.9 + 22.2 = 162.0 ms with 11095 frames on a single RTX 2080Ti GPU (see <ref type="table" target="#tab_3">Table 12</ref> for details). Note that the KITTI benchmark focuses only on the non-detection part inference time (22.2 ms).  11: Comparison to related dataset for detection and tracking (Upper half: real-world, Lower half: synthetic). We only count the size and types of annotations for training and validation (D=detection, T=tracking, C=car, P=pedestrian). To our knowledge, our dataset is the largest 3D tracking benchmark for dynamic scene understanding, with control signals of driving, sub-categories of object.  Data Association Weights. We use different weights of appearance during our experiment, 3D IoU overlap, and motion overlap for corresponding methods. We select weight ratios based on the results of our validation set. For deep appearance only methods, we give 100% weighting to w deep . For 3D related methods, we set w deep with 0.5 to balance appearance and 3D extents.</p><p>Tracking Performance on nuScenes dataset. We report the full table of nuScenes tracking benchmark in <ref type="table" target="#tab_3">Table 13</ref>.</p><p>Tracking Performance on KITTI dataset. As mentioned in the main paper, we resort to KITTI <ref type="bibr" target="#b3">[3]</ref> tracking benchmarks to compare our model abilities in the real-world scenario. We have improved our previous results <ref type="bibr" target="#b4">[4]</ref> to third place with five ranks improvement and have increased from 84.52</p><p>to 86.41 in MOTA. Our quasi-dense 3D tracking method ranked in the top tier among all the published methods upon the time of submission. Please note that we focus on tracking module improvements and not over-design the detector-specific architecture to fit on a small dataset. Results are listed in <ref type="table" target="#tab_3">Table 14</ref>.</p><p>Qualitative results. We show our evaluation results in <ref type="figure">Figure 13</ref> on the test set of nuScenes dataset and <ref type="figure">Figure 14</ref> on the test set of Waymo Open Dataset. The trajectories in <ref type="figure">Figure 13</ref> demonstrate our long-term tracking ability in the nuScenes dataset. The results of Waymo show that our proposed 3D tracking pipeline establishes a strong baseline using monocular images. The solid rectangular in bird's eye view stands for the predicted vehicle. The 3D bounding boxes and the rectangular are colored with their tracking ID. The figures are best viewed in color.</p><p>Evaluation Video. We have uploaded a showcase video that demonstrates video inputs with estimated 3D bounding boxes in the camera view and tracked trajectories in bird's eye view on nuScenes, KITTI, and Waymo Open dataset. Please refer to the showcase video for more qualitative examples.  <ref type="bibr" target="#b61">[61]</ref>. We report the average AMOTA@1 and AMOTP over 7 categories on the benchmark. Camera-based and published methods are shown. Our quasi-dense 3D tracking achieves the top ranking with about 5 times the 3D tracking performance of the best camera-based submission, CenterTrack Vision <ref type="bibr" target="#b57">[57]</ref> (score underlined), at the time of submission. We mark the best result in bold and runner-up in italic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Published Modality AMOTA@1.0 ↑ AMOTP ↓   <ref type="bibr" target="#b4">[4]</ref>, we have greatly increased the ranking position by leveraging the quasi-dense contrastive feature in the data association part. However, the goal objective of our training aims for 3D object tracking, and we are not optimizing the network for the 2D tracking task. Only published methods are reported. We mark the best result in bold and runner-up in italic.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our monocular quasi-dense 3D tracking framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FrameFig. 5 :</head><label>5</label><figDesc>x,y,z,Θ,l,w,h] Illustration of depth-ordering matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Demonstration of motion-aware data association. The yellow tracklet is visible all the time, while the red tracklet is occluded by the blue tracklet at frame T −1. Given pseudo motion vectors (dot arrows from a tracklet to all detection candidates) and the temporal-aggregated motion vector (dashed arrows) by the tracklet, our motion-based data association scheme is encouraged match reappeared candidates in trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>a</head><label></label><figDesc>= {P, O, D} = {x, y, z, θ, l, w, h} from the 64-dim output feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Statistical summary of our dataset in comparison of KITTI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results on testing set of nuScenes (First row), Waymo Open (Second row) datasets. Our proposed quasi-dense 3D tracking pipeline estimates accurate 3D extent and robustly associates tracking trajectories from a monocular image. We show predicted 3D bounding boxes and trajectories colored with tracking IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative results on testing set of KITTI datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>The statistics of object in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>The statistics of scene in our dataset. Examples of Our Dataset. Figure 12 shows some visual examples in different time, weather and location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 :</head><label>12</label><figDesc>Examples of our GTA dataset. We collected a diverse set of driving scenes in different weather, location and time of a day.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Qualitative results on the testing set of nuScenes dataset. Our QD-3DT robustly tracks all observed objects and locates them in 3D. We show predicted 3D bounding boxes and trajectories colored with tracking IDs. Better visualization with color. Qualitative results on the testing set of Waymo Open dataset. Our QD-3DT accurately tracks all observed objects and locates them in 3D. We show predicted 3D bounding boxes and trajectories colored with tracking IDs. Better visualization with color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>T Frame Input Image Quasi-Dense Region Proposals Multi-head 3D Estimation Deep VeloLSTM Data Association Multi-frame Refinement</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Tracklets</cell><cell></cell></row><row><cell></cell><cell>Predict</cell><cell></cell></row><row><cell></cell><cell>LSTM</cell><cell>Update</cell></row><row><cell>T-2</cell><cell>Detected objects</cell><cell>LSTM</cell></row><row><cell></cell><cell></cell><cell>Tracklets</cell></row><row><cell></cell><cell>Predict</cell><cell></cell></row><row><cell>T-1</cell><cell>LSTM Detected objects</cell><cell>Update LSTM</cell></row><row><cell></cell><cell></cell><cell>Tracklets</cell></row><row><cell></cell><cell>Predict</cell><cell></cell></row><row><cell></cell><cell>LSTM</cell><cell>Update</cell></row><row><cell></cell><cell>Detected objects</cell><cell>LSTM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>LiDAR, and 4 radars. It provides 3D annotations for LiDAR data with 10 object classes for detection task, and 7 object classes for tracking task. There are 700 training sequences, 150 validation sequences and 150 test sequences in the nuScenes dataset. Every sequence collects images at 12 FPS, denoted as full frames, and only those sampled keyframes, annotated at 2 FPS, are used for evaluation. segments of 20s each, 798 segments for training, 202 segments for validation, and 150 segments for testing. Every segment contains about 198 frames per camera. Waymo dataset provides 3 types of annotations: 2D annotations, 3D LiDAR annotations, and projected 3D LiDAR annotations. The annotated object classes are vehicles, pedestrians, cyclists, and signs. However, signs are not used in the 3D detection and 3D tracking benchmark.</figDesc><table><row><cell>1 : 4.</cell></row><row><cell>KITTI MOT benchmark [3] provides real-world driving</cell></row><row><cell>scenarios. Since the KITTI benchmark is relatively small-</cell></row><row><cell>scale, we additionally train on the whole KITTI detection</cell></row><row><cell>training set to enhance our 3D estimation module in the</cell></row><row><cell>connection of lifting 3D bounding boxes from a single</cell></row><row><cell>image. We train a full model with the whole detection and</cell></row><row><cell>tracking training set for the public benchmark submission.</cell></row><row><cell>For bounding box center comparison, we train on the whole</cell></row><row><cell>detection training set and half of the tracking training set,</cell></row><row><cell>and then we evaluate the performance difference on the</cell></row><row><cell>other half of the tracking training set.</cell></row><row><cell>nuScenes dataset [61] is another real-world benchmark con-</cell></row><row><cell>taining street scenes in Boston and Singapore captured from</cell></row><row><cell>a moving vehicle equipped with different sensors, i.e., 6</cell></row><row><cell>cameras, 1</cell></row></table><note>Each sequence consists of about 40 keyframes per camera. We apply our motion-based pipeline on the frames in a higher frame rate (full frames) and produce refined tracking results on a keyframe basis. Waymo Open Dataset [63] collects images at 10Hz from 5 different directions with partially overlapped regions: front, front left, front right, side left, and side right. There are 1, 150</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 : Ablation study of tracking performance with different methods on nuScenes validation set.</head><label>1</label><figDesc>Results in (a) suggest that the deep feature affinity matrix contributes to data association. From (b), we observe a robust trend that a motion model with a higher frame rate image sequence encourages tracking performance.(a) Ablation study of dropping affinity matrix.</figDesc><table><row><cell cols="4">Method Adeep Aiou Adepthĉdepth AMOTA@1 ↑ AMOTP ↓</cell></row><row><cell>-</cell><cell></cell><cell>0.1779</cell><cell>1.596</cell></row><row><cell></cell><cell>-</cell><cell>0.2013</cell><cell>1.540</cell></row><row><cell>KF3D</cell><cell>-</cell><cell>0.2213</cell><cell>1.535</cell></row><row><cell></cell><cell>-</cell><cell>0.2301</cell><cell>1.535</cell></row><row><cell></cell><cell></cell><cell>0.2306</cell><cell>1.535</cell></row><row><cell></cell><cell cols="2">(b) Ablation study of full frames.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">full frames AMOTA@1 ↑ AMOTP ↓</cell></row><row><cell>KF3D</cell><cell>-</cell><cell>0.2306 0.2321</cell><cell>1.535 1.530</cell></row><row><cell>VeloLSTM</cell><cell>-</cell><cell>0.2329 0.2421</cell><cell>1.528 1.518</cell></row></table><note>Importance of sub-affinity module design. Comparing boxes IoU sub-affinity (A iou ), adding depth-order matching, denoted 3D in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 : Tracking performance comparison with different designs of affinity matrix on nuScenes validation set.</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Results suggest that using 3D IoU and motion-based sub-</cell></row><row><cell cols="4">affinity matrix yielding the best validation performance of</cell></row><row><cell cols="2">our final pipeline.</cell><cell></cell><cell></cell></row><row><cell cols="2">A iou A depth</cell><cell cols="2">AMOTA@1 ↑ AMOTP ↓</cell></row><row><cell>2D</cell><cell></cell><cell>0.1381</cell><cell>1.674</cell></row><row><cell>BEV</cell><cell>motion</cell><cell>0.1929</cell><cell>1.599</cell></row><row><cell>3D</cell><cell></cell><cell>0.2306</cell><cell>1.535</cell></row><row><cell></cell><cell>cosine</cell><cell>0.2234</cell><cell>1.534</cell></row><row><cell>3D</cell><cell>centroid</cell><cell>0.2300</cell><cell>1.536</cell></row><row><cell></cell><cell>motion</cell><cell>0.2306</cell><cell>1.535</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 : Importance of using projection of 3D bounding box center estimation on KITTI sub-validation set.</head><label>3</label><figDesc>We evaluate our KF3D model using different center inputs C on the Car category to reveal the importance of estimating the projection of a 3D center. The increase of MOTA and higher 3D IoU AP with COCO (50 : 5 : 95) suggest that the projection of a 3D center benefits our tracking pipeline over the 2D center.</figDesc><table><row><cell cols="2">Method Range</cell><cell cols="4">MOTA ↑ MOTP C ↓ Difficulty AP COCO bev ↑ AP COCO 3d</cell><cell>↑</cell></row><row><cell></cell><cell>0-30m</cell><cell>77.28</cell><cell>0.56 Easy</cell><cell>24.91</cell><cell cols="2">15.11</cell></row><row><cell>2D Cen</cell><cell>0-50m</cell><cell>70.78</cell><cell>0.60 Medium</cell><cell>23.53</cell><cell cols="2">15.87</cell></row><row><cell></cell><cell>0-100m</cell><cell>65.15</cell><cell>0.62 Hard</cell><cell>21.99</cell><cell cols="2">14.99</cell></row><row><cell></cell><cell>0-30m</cell><cell>79.50</cell><cell>0.44 Easy</cell><cell>41.71</cell><cell cols="2">36.74</cell></row><row><cell>3D Cen</cell><cell>0-50m</cell><cell>72.53</cell><cell>0.51 Medium</cell><cell>33.73</cell><cell cols="2">29.30</cell></row><row><cell></cell><cell>0-100m</cell><cell>66.77</cell><cell>0.54 Hard</cell><cell>31.05</cell><cell cols="2">26.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 : Comparison of location refinement with dif- ferent motion model on nuScenes validation set.</head><label>4</label><figDesc>Result suggests the VeloLSTM achieves the highest performance among the other methods. Besides, employing a motion model benefits 3D detection and tracking. We use full frames and final affinity matrices.</figDesc><table><row><cell>Method</cell><cell cols="3">NDS AMOTA@1 ↑ AMOTP ↓</cell></row><row><cell>Detection</cell><cell>0.3622</cell><cell>0.222</cell><cell>1.538</cell></row><row><cell cols="2">Momentum 0.3620</cell><cell>0.227</cell><cell>1.532</cell></row><row><cell>KF3D</cell><cell>0.3634</cell><cell>0.232</cell><cell>1.530</cell></row><row><cell>VeloLSTM</cell><cell>0.3666</cell><cell>0.242</cell><cell>1.518</cell></row><row><cell cols="4">nuScenes tracking challenge. Our monocular 3D track-</cell></row><row><cell cols="4">ing method outperforms all the published methods with</cell></row><row><cell cols="4">a large margin. Center-Track-Vision [57] uses two con-</cell></row><row><cell cols="4">secutive frames to generate inter-frame motion for object</cell></row><row><cell cols="4">detection and 3D tracking. Center-Track-Open [57] fuses</cell></row><row><cell cols="4">LiDAR information in the Center-Track-Vision pipeline</cell></row><row><cell cols="2">with Megvii-detector</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 : Tracking performance on the testing set of nuScenes tracking benchmark [61]. We</head><label>5</label><figDesc></figDesc><table><row><cell>report the average</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 : 3D detection and tracking performance in Vehi- cle Level 2 difficulty on the testing set of Waymo dataset.</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">Our pipeline serves as a strong baseline with non-zero</cell></row><row><cell cols="4">scores using only camera modality in the 3D detection and</cell></row><row><cell cols="4">tracking challenge on LiDAR-dominated benchmark.</cell></row><row><cell cols="3">(a) 3D Tracking Performance</cell><cell></cell></row><row><cell>Method</cell><cell>Modality</cell><cell cols="2">MOTA ↑ MOTP ↓</cell></row><row><cell>HorizonMOT3D [92]</cell><cell>Lidar+Camera</cell><cell>0.6407</cell><cell>0.1577</cell></row><row><cell>CenterPoint [56]</cell><cell>Lidar</cell><cell>0.5938</cell><cell>0.1637</cell></row><row><cell cols="2">pillars kf baseline [63] Lidar</cell><cell>0.4008</cell><cell>0.1856</cell></row><row><cell>QD3DT (Ours)</cell><cell>Camera</cell><cell>0.0001</cell><cell>0.0658</cell></row><row><cell cols="3">(b) 3D Detection Performance.</cell><cell></cell></row><row><cell>Method</cell><cell>Modality</cell><cell cols="2">mAPH ↑ mAP ↑</cell></row><row><cell cols="2">HorizonMOT3D [92] Lidar+Camera</cell><cell>0.7783</cell><cell>0.7823</cell></row><row><cell>PV-RCNN [93]</cell><cell>Lidar</cell><cell>0.7323</cell><cell>0.7369</cell></row><row><cell>CenterPoint [56]</cell><cell>Lidar</cell><cell>0.7299</cell><cell>0.7342</cell></row><row><cell>QD3DT (Ours)</cell><cell>Camera</cell><cell>0.0233</cell><cell>0.0242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>3D</figDesc><table><row><cell cols="5">detection and tracking performance in Ve-</cell></row><row><cell cols="5">hicle Level 2 difficulty on the validation set of Waymo</cell></row><row><cell cols="5">dataset. We compare 3D detection and tracking results on</cell></row><row><cell cols="5">different extent of the observable area. LiDAR-based GT</cell></row><row><cell cols="5">covers a 360-degree area, while camera-based GT filters</cell></row><row><cell cols="5">out annotations outside of camera view. MOTA and mAPH</cell></row><row><cell cols="5">increase greatly while lowering IoU thresholds slightly.</cell></row><row><cell>Modality</cell><cell>Range</cell><cell cols="3">IoU MOTA ↑ mAPH ↑</cell></row><row><cell></cell><cell></cell><cell>0.3</cell><cell>0.1183</cell><cell>0.2242</cell></row><row><cell>LiDAR-based GT</cell><cell>Overall</cell><cell>0.5</cell><cell>0.0195</cell><cell>0.1161</cell></row><row><cell></cell><cell></cell><cell>0.7</cell><cell>1.8e-06</cell><cell>0.0286</cell></row><row><cell></cell><cell></cell><cell>0.3</cell><cell>0.1867</cell><cell>0.3401</cell></row><row><cell cols="2">Camera-based GT Overall</cell><cell>0.5</cell><cell>0.0308</cell><cell>0.1743</cell></row><row><cell></cell><cell></cell><cell>0.7</cell><cell>2.8e-06</cell><cell>0.0408</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 : Centroid-based evaluation across datasets. This</head><label>8</label><figDesc>experiment uses our final model and the full frames setting for nuScenes. We evaluate the results in different datasets to reveal the importance of choosing evaluation metrics. The similarity of MOTA and MOTP performance suggests that our pipeline has been well trained and performs equally well on the Waymo Open dataset. MOTP C ↓ MOTP O ↓ MOTP I ↓</figDesc><table><row><cell cols="3">Method MOTA ↑ Waymo Range 0-30m 57.33 0-50m 36.87</cell><cell>0.61 0.74</cell><cell>14.66 16.10</cell><cell>0.18 0.18</cell></row><row><cell></cell><cell>0-100m</cell><cell>18.66</cell><cell>0.79</cell><cell>17.63</cell><cell>0.19</cell></row><row><cell></cell><cell>0-30m</cell><cell>44.47</cell><cell>0.64</cell><cell>14.18</cell><cell>0.19</cell></row><row><cell>nuScenes</cell><cell>0-50m</cell><cell>25.93</cell><cell>0.73</cell><cell>14.84</cell><cell>0.19</cell></row><row><cell></cell><cell>0-100m</cell><cell>12.14</cell><cell>0.75</cell><cell>15.05</cell><cell>0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 : Performance of 3D estimation on object detec- tion IoU mAP.</head><label>9</label><figDesc>The evaluation demonstrates the effectiveness of our model from each separate metric. With different amounts of training data in our GTA dataset, the results suggest that large data capacity benefits the performance of a data-hungry network.</figDesc><table><row><cell cols="2">Dataset Amount</cell><cell>AP 70 bev</cell><cell cols="2">Medium AP 70 3d AP 70 bbox</cell><cell>AP 70 aos</cell></row><row><cell></cell><cell>1%</cell><cell>4.10</cell><cell>1.31</cell><cell>78.18</cell><cell>75.59</cell></row><row><cell>GTA</cell><cell cols="2">10% 14.61</cell><cell>8.45</cell><cell>91.66</cell><cell>90.83</cell></row><row><cell></cell><cell cols="2">100% 21.61</cell><cell>15.63</cell><cell>94.62</cell><cell>94.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10 : Comparison of the different matching algo- rithms on nuScenes validation set.</head><label>10</label><figDesc>We use only keyframes and all affinity matrices as our setting. We found that using greedy matching yields similar results to the Hungarian matching but with less computation complexity on welltrained quasi-dense embedding pairs.</figDesc><table><row><cell cols="3">Matching Algorithm AMOTA@1 ↑ AMOTA@0.2 ↑</cell></row><row><cell>Hungarian [77]</cell><cell>0.230</cell><cell>0.3479</cell></row><row><cell>Greedy</cell><cell>0.230</cell><cell>0.3479</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 12 : Inference Time (second) of our proposed frame- work on KITTI tracking benchmark.</head><label>12</label><figDesc>We recorded the wall clock of the execution time consumption of each module, which might have slight overhead, e.g., functions with result dumping. Bold fonts are the summation of each sub-module and misc. time. Note that elapsed time for object detection is not included in the specified runtime of the KITTI benchmark.</figDesc><table><row><cell>Phase</cell><cell>Second</cell></row><row><cell>Detection</cell><cell>0.1256</cell></row><row><cell>2D Box, 3D Estimation</cell><cell>0.1233</cell></row><row><cell>Contrastive Feature</cell><cell>0.0023</cell></row><row><cell>3D transform</cell><cell>0.0142</cell></row><row><cell>3D Lifting (img to world)</cell><cell>0.0123</cell></row><row><cell cols="2">Reprojection (world to img) 0.0019</cell></row><row><cell>Tracking</cell><cell>0.0222</cell></row><row><cell>LSTM Predict</cell><cell>0.0076</cell></row><row><cell>Greedy Matching</cell><cell>0.0019</cell></row><row><cell>LSTM Update</cell><cell>0.0009</cell></row><row><cell>Misc.</cell><cell>0.0118</cell></row><row><cell>Total</cell><cell>0.1620</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 13 : Tracking performance on the testing set of nuScenes tracking benchmark</head><label>13</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 14 :</head><label>14</label><figDesc></figDesc><table /><note>Tracking performance on the testing set of KITTI tracking benchmark. Improving from mono3DT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>LiDAR Stereo MOTA ↑ MOTP ↑ MT ↑ ML ↓ FP ↓ FN ↓</figDesc><table><row><cell>SRK ODESA [42]</cell><cell>90.03</cell><cell>84.32 82.62</cell><cell>2.31</cell><cell>451 2887</cell></row><row><cell>CenterTrack [57]</cell><cell>89.44</cell><cell>85.05 82.31</cell><cell>2.31</cell><cell>849 2666</cell></row><row><cell>Ours</cell><cell>86.41</cell><cell>85.82 75.38</cell><cell>2.46</cell><cell>804 3761</cell></row><row><cell>Quasi-Dense [5]</cell><cell>85.76</cell><cell>85.01 69.08</cell><cell>3.08</cell><cell>517 4288</cell></row><row><cell>JRMOT [96]</cell><cell>85.70</cell><cell>85.48 71.85</cell><cell>4.00</cell><cell>772 4049</cell></row><row><cell>MASS [97]</cell><cell>85.04</cell><cell>85.53 74.31</cell><cell>2.77</cell><cell>742 4101</cell></row><row><cell>MOSTFusion [49]</cell><cell>84.83</cell><cell>85.21 73.08</cell><cell>2.77</cell><cell>681 4260</cell></row><row><cell>mmMOT [43]</cell><cell>84.77</cell><cell>85.21 73.23</cell><cell>2.77</cell><cell>711 4243</cell></row><row><cell>Mono3DT [4]</cell><cell>84.52</cell><cell>85.64 73.38</cell><cell>2.77</cell><cell>705 4242</cell></row><row><cell>BeyondPixels [50]</cell><cell>84.24</cell><cell>85.73 73.23</cell><cell>2.77</cell><cell>705 4247</cell></row><row><cell>AB3DMOT [54]</cell><cell>83.84</cell><cell cols="3">85.24 66.92 11.38 1059 4491</cell></row><row><cell>PMBM [48]</cell><cell>80.39</cell><cell>81.26 62.77</cell><cell cols="2">6.15 1007 5616</cell></row><row><cell>extraCK [98]</cell><cell>79.99</cell><cell>82.46 62.15</cell><cell>5.54</cell><cell>642 5896</cell></row><row><cell>MCMOT-CPD [99]</cell><cell>78.90</cell><cell cols="2">82.13 52.31 11.69</cell><cell>316 6713</cell></row><row><cell>NOMT [100]</cell><cell>78.15</cell><cell cols="3">79.46 57.23 13.23 1061 6421</cell></row><row><cell>MDP [79]</cell><cell>76.59</cell><cell cols="2">82.10 52.15 13.38</cell><cell>606 7315</cell></row><row><cell>DSM [101]</cell><cell>76.15</cell><cell>83.42 60.00</cell><cell>8.31</cell><cell>578 7328</cell></row><row><cell>SCEA [102]</cell><cell>75.58</cell><cell cols="3">79.39 53.08 11.54 1306 6989</cell></row><row><cell>CIWT [53]</cell><cell>75.39</cell><cell cols="2">79.25 49.85 10.31</cell><cell>954 7345</cell></row><row><cell>NOMT-HM [100]</cell><cell>75.20</cell><cell cols="3">80.02 50.00 13.54 1143 7280</cell></row><row><cell>mbodSSP [103]</cell><cell>72.69</cell><cell>78.75 48.77</cell><cell cols="2">8.77 1918 7360</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ua-detrac: A new benchmark and protocol for multi-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
		<idno>ArXiv:1511.04136</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>ArXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quasidense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno>ArXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">M3D-RPN: Monocular 3D region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-LiDAR from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate monocular 3D object detection via color-embedded 3D reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-level 3D object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep MANTA: A coarse-to-fine many-task network for joint 2D and 3D vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointRCNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive appearance modeling for video tracking: Survey and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno>Arxiv:1409.7618</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mykheievskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borysenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Porokhonskyy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust multi-modality multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mono-camera 3d multi-object tracking using deep learning detections and pmbm filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7942" to="7951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Track to reconstruct and reconstruct to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>RA-L</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3d multi-object tracking: A baseline and new evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<idno>ArXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>ArXiv:1912.04838</idno>
		<title level="m">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Free supervision from video games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>ArXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), Virtual</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Poi: multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>JIVP)</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of basic Engineering</title>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Class-balanced grouping and sampling for point cloud 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv:1908.09492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<title level="m">1st place solutions for waymo open dataset challenges -2d and 3d tracking</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Openpcdet: An open-source toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integrated Computer-Aided Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>ArXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Jrmot: A realtime 3d multi-object tracker and a new large-scale dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Multiple object tracking with attention to appearance, structure, motion and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A lightweight online multiple object vehicle tracking method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gündüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acarman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCV Workshops)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">End-to-end learning of multi-sensor 3d tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Online multi-object tracking via structural constraint event aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Followme: Efficient online min-cost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
