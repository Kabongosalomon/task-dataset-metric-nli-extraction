<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid semi-Markov CRF for Neural Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-10">10 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Xiu</forename><surname>Ye</surname></persName>
							<email>zxye@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid semi-Markov CRF for Neural Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-10">10 May 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for neural sequence labeling in natural language processing. Based on conventional conditional random fields (CRFs), SCRFs have been designed for the tasks of assigning labels to segments by extracting features from and describing transitions between segments instead of words. In this paper, we improve the existing SCRF methods by employing word-level and segment-level information simultaneously. First, word-level labels are utilized to derive the segment scores in SCRFs. Second, a CRF output layer and an SCRF output layer are integrated into an unified neural network and trained jointly. Experimental results on CoNLL 2003 named entity recognition (NER) shared task show that our model achieves state-of-the-art performance when no external knowledge is used 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling, such as part-of-speech (POS) tagging, chunking, and named entity recognition (NER), is a category of fundamental tasks in natural language processing (NLP). Conditional random fields (CRFs) <ref type="bibr" target="#b4">(Lafferty et al., 2001)</ref>, as probabilistic undirected graphical models, have been widely applied to the sequence labeling tasks considering that they are able to describe the dependencies between adjacent word-level labels and to avoid illegal label combination (e.g., I-ORG can't follow B-LOC in the NER tasks using the BIOES tagging scheme). Original CRFs utilize hand-crafted features which increases the difficulty of performance tuning and domain adaptation. In recent years, neural networks with distributed word representations (i.e., word embeddings) <ref type="bibr" target="#b8">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b9">Pennington et al., 2014)</ref> have been introduced to calculate word scores automatically for CRFs <ref type="bibr" target="#b1">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b2">Huang et al., 2015)</ref>.</p><p>On the other hand, semi-Markov conditional random fields (SCRFs) <ref type="bibr" target="#b15">(Sarawagi and Cohen, 2005)</ref> have been proposed for the tasks of assigning labels to the segments of input sequences, e.g., NER. Different from CRFs, SCRFs adopt segments instead of words as the basic units for feature extraction and transition modeling. The word-level transitions within a segment are usually ignored. Some variations of SCRFs have also been studied. For example, Andrew (2006) extracted segment-level features by combining hand-crafted CRF features and modeled the Markov property between words instead of segments in SCRFs.</p><p>With the development of deep learning, some models of combining neural networks and SCRFs have also been studied. <ref type="bibr" target="#b18">Zhuo et al. (2016)</ref> and <ref type="bibr" target="#b3">Kong et al. (2015)</ref> employed gated recursive convolutional neural networks (grConvs) and segmental recurrent neural networks (SRNNs) to calculate segment scores for SCRFs respectively.</p><p>All these existing neural sequence labeling methods using SCRFs only adopted segment-level labels for score calculation and model training. In this paper, we suppose that word-level labels can also contribute to the building of SCRFs and thus design a hybrid SCRF (HSCRF) architecture for neural sequence labeling. In an HSCRF, word-level labels are utilized to derive the segment scores. Further, a CRF output layer and an HSCRF output layer are integrated into a unified neural network and trained jointly. We evaluate our model on <ref type="bibr">CoNLL 2003 English NER task (Sang and</ref><ref type="bibr" target="#b14">Meulder, 2003)</ref> and achieve state-of-the-art performance when no external knowledge is used.</p><p>In summary, the contributions of this paper are: (1) we propose the HSCRF architecture which employs both word-level and segment-level labels for segment score calculation. (2) we propose a joint CRF-HSCRF training framework and a naive joint decoding algorithm for neural sequence labeling.</p><p>(3) we achieve state-of-the-art performance in CoNLL 2003 NER shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hybrid semi-Markov CRFs</head><p>Let s = {s 1 , s 2 , ..., s p } denote the segmentation of an input sentence x = {x 1 , ..., x n } and w = {w 1 , ..., w n } denote the sequence of word representations of x derived by a neural network as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Each segment s i = (b i , e i , l i ), 0 ≤ i ≤ p, is a triplet of a begin word index b i , an end word index e i and a segment-level label l i , where b 1 = 1, e p = |x|, b i+1 = e i + 1, 0 ≤ e i − b i &lt; L, and L is the upperbound of the length of s i . Correspondingly, let y = {y 1 , ..., y n } denote the word-level labels of x. For example, if a sentence x in NER task is "Barack Hussein Obama and Natasha Obama", we have the corresponding s = ((1, 3, P ER), (4, 4, O), (5, 6, P ER)) and y = (B-PER, I-PER, E-PER, O, B-PER, E-PER).</p><p>Similar to conventional SCRFs <ref type="bibr" target="#b15">(Sarawagi and Cohen, 2005)</ref>, the probability of a segmentationŝ in an HSCRF is defined as</p><formula xml:id="formula_0">p(ŝ|w) = score(ŝ, w) s ′ ∈S score(s ′ , w) ,<label>(1)</label></formula><p>where S contains all possible segmentations and</p><formula xml:id="formula_1">score(s, w) = |s| i=1 ψ(l i−1 , l i , w, b i , e i ).<label>(2)</label></formula><formula xml:id="formula_2">Here, ψ(l i−1 , l i , w, b i , e i ) = exp{m i + b l i−1 ,l i }, where m i = ϕ h (l i , w, b i , e i )</formula><p>is the segment score and b i,j is the segment-level transition parameter from class i to class j. Different from existing methods of utilizing SCRFs in neural sequence labeling <ref type="bibr" target="#b18">(Zhuo et al., 2016;</ref><ref type="bibr" target="#b3">Kong et al., 2015)</ref> , the segment score in an HSCRF is calculated using word-level labels as</p><formula xml:id="formula_3">m i = e i k=b i ϕ c (y k , w ′ k ) = e i k=b i a ⊤ y k w ′ k ,<label>(3)</label></formula><p>where w ′ k is the feature vector of the k-th word, ϕ c (y k , w ′ k ) calculates the score of the k-th word being classified into word-level class y k , and a y k is a weight parameter vector corresponding to class y k . For each word, w ′ k is composed of word representation w k and another two segment-level descriptions, i.e., (1) w e i − w b i which is derived based on the assumption that word representations in the same segment (e.g., "Barack Obama") are closer to each other than otherwise (e.g., "Obama is"), and <ref type="formula" target="#formula_1">(2)</ref> </p><formula xml:id="formula_4">φ(k − b i + 1) which is the embedding vector of the word index in a segment. Finally, we have w ′ k = [w k ; w e i − w b i ; φ(k − b i + 1)], where b i ≤ k ≤ e i and [; ;</formula><p>] is a vector concatenation operation.</p><p>The training and decoding criteria of conventional SCRFs <ref type="bibr" target="#b15">(Sarawagi and Cohen, 2005)</ref> are followed. The negative log-likelihood (NLL), i.e., −logp(ŝ|w), is minimized to estimate the parameters of the HSCRF layer and the lower neural network layers that derive word representations. For decoding, the Viterbi algorithm is employed to obtain the optimal segmentation as s * = argmax</p><formula xml:id="formula_5">s ′ ∈S logp(s ′ |m),<label>(4)</label></formula><p>where S contains all legitimate segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Jointly training and decoding using CRFs and HSCRFs</head><p>To further investigate the effects of word-level labels on the training of SCRFs, we integrate a CRF output layer and a HSCRF output layer into an unified neural network and train them jointly. These two output layers share the same sequence of word representations w which are extracted by lower neural network layers. Given both word-level and segment-level ground truth labels of training sentences, the model parameters are optimized by minimizing the summation of the loss functions of the CRF layer and the HSCRF layer with equal weights. At decoding time, two label sequences, i.e., s c and s h , for an input sentence can be obtained using the CRF output layer and the HSCRF output layer respectively. A naive joint decoding algorithm is also designed to make a selection between them. Assume the NLLs of measuring s c and s h using the CRF and HSCRF layers are N LL c and N LL h respectively. Then, we exchange the models and measure the NLLs of s c and s h by HSCRF and CRF and obtain another two values N LL c by h and N LL h by c . We just naively assign the summation of N LL c and N LL c by h to s c , and the summation of N LL h and N LL h by c to s h . Finally, we choose the one between s c and s h with lower NLL sum as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluated our model on the <ref type="bibr">CoNLL 2003 English NER dataset (Sang and</ref><ref type="bibr" target="#b14">Meulder, 2003)</ref>. This dataset contained four labels of named entities (PER, LOC, ORG and MISC) and label O for others. The existing separation of training, development and test sets was followed in our experiments. We adopted the same word-level tagging scheme as the one used in <ref type="bibr" target="#b6">Liu et al. (2018)</ref> (e.g., BIOES instead of BIO). For better computation efficiency, the max segment length L introduced in Section 2.1 was set to 6, which pruned less than 0.5% training sentences for building SCRFs and had no effect on the development and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the GloVe <ref type="bibr" target="#b9">(Pennington et al., 2014)</ref> word embedding and the character encoding vector of each word in the input sentence were concatenated and fed into a bi-directional LSTM to obtain the sequence of word representations w. Two character encoding models, LM-BLSTM <ref type="bibr" target="#b6">(Liu et al., 2018)</ref> and CNN-BLSTM <ref type="bibr" target="#b7">(Ma and Hovy, 2016)</ref>, were adopted in our experiments. Regarding with the top classification layer, we compared our proposed HSCRF with conventional word-level CRF and grSemi-CRF (GSCRF) <ref type="bibr" target="#b18">(Zhuo et al., 2016)</ref>, which was an SCRF using only segment-level information. The descriptions of the models built in our experiments are summarized in <ref type="table" target="#tab_0">Table 1.</ref> For a fair comparison, we implemented all models in the same framework using PyTorch library 2 . The hyper-parameters of the models are shown in <ref type="table" target="#tab_1">Table 2</ref> and they were selected according to the two baseline methods without fine-tuning. Each model in <ref type="table" target="#tab_0">Table 1</ref> was estimated 10 times and its mean and standard deviation of F1 score were reported considering the influence of randomness and the weak correlation between development set and test set in this task <ref type="bibr" target="#b13">(Reimers and Gurevych, 2017)</ref>. <ref type="table" target="#tab_0">Table 1</ref> lists the F1 score results of all built models on CoNLL 2003 NER task. Comparing model 3 with model 1/2 and model 9 with model 7/8, we can see that HSCRF performed better than CRF and GSCRF. The superiorities were significant since the p-values of t-test were smaller than 0.01. This implies the benefits of utilizing word-level labels when deriving segment scores in SCRFs. Comparing model 1 with model 4, 3 with 5, 7 with 10, and 9 with 11, we can see that the jointly training method introduced in Section 2.2 improved the performance of CRF and HSCRF significantly (p &lt; 0.01 in all these four pairs). This may be attributed to that jointly training generates better word representations that can be shared by both CRF and HSCRF decoding layers. Finally, comparing model 6 with model 4/5 and model 12 with model 10/11, we can see the effectiveness of the jointly decoding algorithm introduced in Section 2.2 on improving F1 scores (p &lt; 0.01 in all these four pairs). The LM-BLSTM-JNT model with jointly decoding achieved the highest F1 score among all these built models. <ref type="table" target="#tab_3">Table 3</ref> shows some recent results 3 on the CoNLL 2003 English NER task. For the convenience of comparison, we also listed the maximum F1 scores among 10 repetitions when building our models. The maximum F1 score of our re-implemented CNN-BLSTM-CRF model was slightly worse than the one originally reported in  Ma and Hovy (2016), but it was similar to the one reported in <ref type="bibr" target="#b13">Reimers and Gurevych (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with existing work</head><p>In the NER models listed in <ref type="table" target="#tab_3">Table 3</ref>, <ref type="bibr" target="#b18">Zhuo et al. (2016)</ref> employed some manual features and calculated segment scores by grConv for SCRF. <ref type="bibr" target="#b5">Lample et al. (2016)</ref> and <ref type="bibr" target="#b7">Ma and Hovy (2016)</ref> constructed character-level encodings using BLSTM and CNN respectively, and concatenated them with word embeddings. Then, the same BLSTM-CRF architecture was adopted in both models. <ref type="bibr" target="#b12">Rei (2017)</ref> fed word embeddings into LSTM to obtain the word representations for CRF decoding and to predict the next word simultaneously. Similarly, <ref type="bibr" target="#b6">Liu et al. (2018)</ref>   characters into LSTM to predict the next character and to get the character-level encoding for each word.</p><p>Some of the models listed in <ref type="table" target="#tab_3">Table 3</ref> utilized external knowledge beside CoNLL 2003 training set and pre-trained word embeddings. <ref type="bibr">Luo et al. (2015)</ref> proposed JERL model, which was trained on both NER and entity linking tasks simultaneously. <ref type="bibr" target="#b1">Chiu and Nichols (2016)</ref> employed lexicon features from DBpedia <ref type="bibr" target="#b0">(Auer et al., 2007)</ref>. <ref type="bibr" target="#b16">Tran et al. (2017)</ref> and <ref type="bibr" target="#b10">Peters et al. (2017)</ref> utilized pre-trained language models from large corpus to model word representations. <ref type="bibr" target="#b17">Yang et al. (2017)</ref> utilized transfer learning to obtain shared information from other tasks, such as chunking and POS  tagging, for word representations.</p><p>From <ref type="table" target="#tab_3">Table 3</ref>, we can see that our CNN-BLSTM-JNT and LM-BLSTM-JNT models with jointly decoding both achieved state-of-the-art F1 scores among all models without using external knowledge. The maximum F1 score achieved by the LM-BLSTM-JNT model was 91.53%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis</head><p>To better understand the effectiveness of wordlevel and segment-level labels on the NER task, we evaluated the performance of models 7, 8, 9 and 12 in <ref type="table" target="#tab_3">Table 3</ref> for entities with different lengths. The mean F1 scores of 10 training repetitions are reported in <ref type="table" target="#tab_5">Table 4</ref>. Comparing model 7 with model 8, we can see that GSCRF achieved better performance than CRF for long entities (with more than 4 words) but worse for short entities (with less than 3 words). Comparing model 7 with model 9, we can find that HSCRF outperformed CRF for recognizing long entities and meanwhile achieved comparable performance with CRF for short entities.</p><p>One possible explanation is that word-level labels may supervise models to learn word-level descriptions which tend to benefit the recognition of short entities. On the other hand, segmentlevel labels may guide models to capture the descriptions of combining words for whole entities which help to recognize long entities. By utilizing both labels, the LM-BLSTM-HSCRF model can achieve better overall performance of recognizing entities with different lengths. Furthermore, the LM-BLSTM-JNT(JNT) model which adopted jointly training and decoding achieved the best performance among all models shown in <ref type="table" target="#tab_5">Table 4</ref> for all entity lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper proposes a hybrid semi-Markov conditional random field (HSCRF) architecture for neural sequence labeling, in which word-level labels are utilized to derive the segment scores in SCRFs.</p><p>Further, the methods of training and decoding CRF and HSCRF output layers jointly are also presented. Experimental results on CoNLL 2003 English NER task demonstrated the effectiveness of the proposed HSCRF model which achieved state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The diagram of a neural network with an HSCRF output layer for sequence labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model descriptions and their performance on CoNLL 2003 NER task.</figDesc><table><row><cell>No.</cell><cell cols="2">Model Name</cell><cell>Word Representation</cell><cell>Top Layer</cell><cell cols="2">Decoding Layer F1 Score (±std)</cell></row><row><cell>1</cell><cell cols="2">CNN-BLSTM-CRF</cell><cell>CNN-BLSTM</cell><cell>CRF</cell><cell>CRF</cell><cell>90.92 ± 0.08</cell></row><row><cell>2</cell><cell cols="2">CNN-BLSTM-GSCRF</cell><cell>CNN-BLSTM</cell><cell>GSCRF</cell><cell>GSCRF</cell><cell>90.96 ± 0.12</cell></row><row><cell>3</cell><cell cols="2">CNN-BLSTM-HSCRF</cell><cell>CNN-BLSTM</cell><cell>HSCRF</cell><cell>HSCRF</cell><cell>91.10 ± 0.12</cell></row><row><cell>4</cell><cell cols="2">CNN-BLSTM-JNT(CRF)</cell><cell>CNN-BLSTM</cell><cell>CRF+HSCRF</cell><cell>CRF</cell><cell>91.08 ± 0.12</cell></row><row><cell>5</cell><cell cols="2">CNN-BLSTM-JNT(HSCRF)</cell><cell>CNN-BLSTM</cell><cell>CRF+HSCRF</cell><cell>HSCRF</cell><cell>91.20 ± 0.10</cell></row><row><cell>6</cell><cell cols="2">CNN-BLSTM-JNT(JNT)</cell><cell>CNN-BLSTM</cell><cell>CRF+HSCRF</cell><cell>CRF+HSCRF</cell><cell>91.26 ± 0.10</cell></row><row><cell>7</cell><cell cols="2">LM-BLSTM-CRF</cell><cell>LM-BLSTM</cell><cell>CRF</cell><cell>CRF</cell><cell>91.17 ± 0.11</cell></row><row><cell>8</cell><cell cols="2">LM-BLSTM-GSCRF</cell><cell>LM-BLSTM</cell><cell>GSCRF</cell><cell>GSCRF</cell><cell>91.06 ± 0.05</cell></row><row><cell>9</cell><cell cols="2">LM-BLSTM-HSCRF</cell><cell>LM-BLSTM</cell><cell>HSCRF</cell><cell>HSCRF</cell><cell>91.27 ± 0.08</cell></row><row><cell>10</cell><cell cols="2">LM-BLSTM-JNT(CRF)</cell><cell>LM-BLSTM</cell><cell>CRF+HSCRF</cell><cell>CRF</cell><cell>91.24 ± 0.07</cell></row><row><cell>11</cell><cell cols="2">LM-BLSTM-JNT(HSCRF)</cell><cell>LM-BLSTM</cell><cell>CRF+HSCRF</cell><cell>HSCRF</cell><cell>91.34 ± 0.10</cell></row><row><cell>12</cell><cell cols="2">LM-BLSTM-JNT(JNT)</cell><cell>LM-BLSTM</cell><cell>CRF+HSCRF</cell><cell>CRF+HSCRF</cell><cell>91.38 ± 0.10</cell></row><row><cell></cell><cell>Component</cell><cell>Parameter</cell><cell>Value</cell><cell></cell><cell></cell></row><row><cell></cell><cell>word-level embedding  † ‡</cell><cell>dimension</cell><cell>100</cell><cell></cell><cell></cell></row><row><cell></cell><cell>character-level embedding  † ‡</cell><cell>dimension</cell><cell>30</cell><cell></cell><cell></cell></row><row><cell cols="2">character-level LSTM  †</cell><cell>depth hidden size</cell><cell>1 300</cell><cell></cell><cell></cell></row><row><cell cols="2">highway network  †</cell><cell>layer</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell cols="2">word-level BLSTM  †</cell><cell>depth hidden size</cell><cell>1 300</cell><cell></cell><cell></cell></row><row><cell cols="2">word-level BLSTM  ‡</cell><cell>depth hidden size</cell><cell>1 200</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN  ‡</cell><cell>window size filter number</cell><cell>3 30</cell><cell></cell><cell></cell></row><row><cell></cell><cell>φ(·)  † ‡</cell><cell>dimension</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>dropout  † ‡</cell><cell>dropout rate</cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>learning rate</cell><cell>0.01</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>batch size</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>optimization  † ‡</cell><cell>strategy</cell><cell>SGD</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>gradient clip</cell><cell>5.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>decay rate</cell><cell>1/(1+0.05t)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters of the models built in our experiments, where † indicates the ones when using LM-BLSTM for deriving word representations and ‡ indicates the ones when using CNN-BLSTM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with existing work on CoNLL 2003 NER task. The models labelled with</figDesc><table /><note>* utilized external knowledge beside CoNLL 2003 training set and pre-trained word embeddings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CRF 91.68 91.88 82.64 75.81 73.68 72.73 91.17 8 LM-BLSTM-GSCRF 91.57 91.68 83.61 74.32 76.64 73.64 91.06 9 LM-BLSTM-HSCRF 91.65 91.84 82.97 76.20 78.95 74.55 91.27 12 LM-BLSTM-JNT(JNT) 91.73 92.03 83.78 77.27 79.66 76.55 91.38</figDesc><table><row><cell>No.</cell><cell>Model Name</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>Entity Length 4</cell><cell>5</cell><cell>≥ 6</cell><cell>all</cell></row><row><cell>7</cell><cell>LM-BLSTM-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Model performance on CoNLL 2003 NER task for entities with different lengths.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code of our models is available at http://github.com/ZhixiuYe/HSCRF-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://pytorch.org/ 3 It should be noticed that the results of<ref type="bibr" target="#b6">Liu et al. (2018)</ref> were inconsistent with the original ones reported in their paper. According to its first author's GitHub page (https://github.com/LiyuanLucasLiu/LM-LSTM-CRF), the originally reported results had errors due to some bugs. Here, we report the results after the bugs got fixed.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06018</idno>
		<title level="m">Segmental recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Zaiqing Nie</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<idno type="DOI">10.18653/v1/P17-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-Markov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Named entity recognition with stack residual LSTM and trainable bias decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Jimeno</forename><surname>Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="566" to="575" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06345</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segment-level sequence modeling using gated recursive semi-Markov</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1413" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
