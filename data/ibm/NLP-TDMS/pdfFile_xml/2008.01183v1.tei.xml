<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Semantic Segmentation via Sub-category Exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">eBay Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">eBay Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Merced</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-Supervised Semantic Segmentation via Sub-category Exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing weakly-supervised semantic segmentation methods using image-level annotations typically rely on initial responses to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts, due to the fact that the network does not need the entire object for optimizing the objective function. To enforce the network to pay attention to other parts of an object, we propose a simple yet effective approach that introduces a self-supervised task by exploiting the sub-category information. Specifically, we perform clustering on image features to generate pseudo sub-categories labels within each annotated parent class, and construct a sub-category objective to assign the network to a more challenging task. By iteratively clustering image features, the training process does not limit itself to the most discriminative object parts, hence improving the quality of the response maps. We conduct extensive analysis to validate the proposed method and show that our approach performs favorably against the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of semantic segmentation is to assign a semantic category to each pixel in the image. It has been one of the most important tasks in computer vision that enjoys a wide range of applications such as image editing and scene understanding. Recently, deep convolutional neural network (CNN) based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref> have been developed for semantic segmentation and achieved significant progress. However, such approaches rely on learning supervised models that require pixel-wise annotations, which take extensive effort and time. To reduce the effort in annotating pixel-wise ground truth labels, numerous weakly-supervised methods are proposed using various types of labels such as image-level <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, videolevel <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35]</ref>, bounding box <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, point-level <ref type="bibr" target="#b1">[2]</ref>, and scribble-based <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref> labels. In this work, we focus on using image-level labels which can be obtained effortlessly, <ref type="figure">Figure 1</ref>: Existing weakly-supervised semantic segmentation methods based on image-level supervisions usually apply the class activation map (CAM) to obtain the response map as the initial prediction. However, this response map can only highlight the discriminative parts of the object (top). We propose a self-supervised task via sub-category exploration to enforce the classification network learn better response maps (bottom). yet a more challenging case under the weakly-supervised setting.</p><p>Existing algorithms mainly consist of three sequential steps to perform weakly-supervised training on the imagelevel label: 1) predict an initial category-wise response map to localize the object, 2) refine the initial response as the pseudo ground truth, and 3) train the segmentation network based on pseudo labels. Although promising results have been achieved by recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>, most of them focus on improving the second and the third steps. Therefore, these approaches may suffer from inaccurate predictions generated in the first step, i.e., initial response. Here, we aim to improve the performance of initial predictions which will benefit succeeding steps.</p><p>In order to predict the initial response map for each category, numerous approaches based on the class activation map (CAM) model <ref type="bibr" target="#b45">[46]</ref> have been developed. Essentially, these methods train a classification network and use its learned weights in the classifier as the cues to compute weighted sums of feature maps, which can be treated as the response map. However, such response maps may only focus on a portion of the object, instead of localizing the entire object (see top of <ref type="figure">Figure 1</ref>). One explanation is that the objective of the classifier does not need to "see" the entire object for optimizing the loss function. This impairs the classifier's ability to locate the objects.</p><p>At the core of our technique is to impose a more challenging task to the network for learning better representations, while not jeopardizing the original objective. To this end, we propose a simple yet effective method by introducing a self-supervised task that discovers sub-categories in an unsupervised manner, as illustrated at the bottom of <ref type="figure">Figure  1</ref>. Specifically, our task consists of two steps: 1) perform clustering on image features extracted from the classification network for each annotated parent class (e.g., 20 parent classes on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref>), and 2) use the clustering assignment for each image as the pseudo label to optimize the sub-category objective.</p><p>On one hand, the parent classifier establishes a feature space through supervised training as the guidance for unsupervised sub-category clustering. On the other hand, the sub-category objective provides additional gradients to enhance feature representations and leverage the sub-space of the original feature space to obtain better results. As such, the classification model takes a more challenging task and is not limited to the easier objective of learning only the parent classifier. Moreover, to ensure better convergence in practice, we iteratively alter the two steps of feature clustering and pseudo training the sub-category objective.</p><p>We conduct extensive experiments on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref> to demonstrate the effectiveness of our method, with regard to generating better initial response maps to localize objects. As a result, our approach leads to favorable performance for the final semantic segmentation results against state-of-the-art weakly-supervised approaches. Furthermore, we provide extensive ablation studies and analysis to validate the robustness of our method. Interestingly, we notice that the network is able to differentiate sub-categories with respect to their object size/type, context, and coexistence with other categories. The main contributions of this work are summarized as follows:</p><p>• We propose a simple yet effective method via a selfsupervised task to enhance feature representations in the classification network. This improves the initial class activation maps for weakly-supervised semantic segmentation as well.</p><p>• We explore the idea of sub-category discovery via iteratively performing unsupervised clustering and pseudo training on the sub-category objective in a selfsupervised fashion.</p><p>• We present extensive study and analysis to show the efficacy of the proposed method, which significantly improves the quality of initial response maps and leads to better semantic segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Within the context of this work, we discuss methods for weakly-supervised semantic segmentation (WSSS) using image-level labels, including approaches that focus on initial prediction and refinement for generating pseudo ground truths. In addition, algorithms that are relevant to unsupervised representation learning are discussed in this section.</p><p>Initial Prediction for WSSS. Initial cues are essential for segmentation task since it can provide reliable priors to generate segmentation maps. The class activation map <ref type="bibr" target="#b45">[46]</ref> is a widely used technique for localizing the object. It can highlight class-specific regions that are served as the initial cues. However, since the CAM model is trained by a classification task, it tends to activate to the small discriminative part of the object, leading to incomplete initial masks.</p><p>Several methods have been developed to alleviate this problem. Numerous approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref> deliberately hide or erase the region of an object, forcing models to seek more diverse parts. However, those methods either hide fixed-size patches randomly or require repetitive model training and response aggregation steps. A number of variants <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b24">25]</ref> have been proposed to extend the initial response via an adversarial erasing strategy in an end-to-end training manner, yet such strategies may gradually expand their attention to non-object regions, leading to inaccurate attention maps. Recently, the SeeNet approach <ref type="bibr" target="#b16">[17]</ref> applies selferasing strategies to encourage networks to use both object and background cues, which prevent the attention from including more background regions. Instead of using the erasing scheme, the FickleNet method <ref type="bibr" target="#b23">[24]</ref> introduces stochastic feature selection to obtain diverse combinations of locations on feature maps. By aggregating the localization maps, they acquire the initial cue that contains a larger region of the object.</p><p>Different from the methods that mitigate the problem by discovering complementary regions via iterative erasing steps or consolidating attention maps, our proposed approach aims at enforcing the network to learn harder on a more challenging task via self-supervised sub-category exploration, thereby enhancing feature representations and improving the response map.</p><p>Response Refinement for WSSS. Numerous approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> are proposed to refine the initial cue via expanding the region of attention map. The SEC method <ref type="bibr" target="#b21">[22]</ref> proposes a loss function that constrains both global weighted rank pooling and low-level boundary to <ref type="figure">Figure 2</ref>: Proposed framework for generating the class activation map. Given input images I, we first feed them into a feature extractor E to obtain their features f . Then, we adopt unsupervised clustering on f and obtain sub-category pseudo labels Y s for each image. Next, we train the classification network to jointly optimize the parent classifier H p with ground truth labels Y p for parent classes and the sub-category classifier H s using the sub-category pseudo labels obtained in the clustering stage. By iteratively performing unsupervised clustering on image features and pseudo training the classification module, we use the jointly optimized classification network to produce the final activation map M .</p><p>expand the localization map. To improve the network training, the MCOF scheme <ref type="bibr" target="#b37">[38]</ref> uses a bottom-up and top-down framework which alternatively expands object regions and optimize the segmentation network, while the MDC method <ref type="bibr" target="#b39">[40]</ref> expands the seeds by employing multiple branches of convolutional layers with different dilation rates. Moreover, the DSRG approach <ref type="bibr" target="#b17">[18]</ref> refines initial localization maps by applying a seeded region growing method during the training of the segmentation network. Other approaches are developed via affinity learning. For instance, the Affini-tyNet <ref type="bibr" target="#b0">[1]</ref> considers pixel-wise affinity to propagate local responses to nearby areas, while <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> explore cross-image relationships to obtain complementary information that can infer the predictions.</p><p>Nevertheless, initial seeds are still obtained from the CAM method. If these seeds only come from the discriminative parts of objects, it is difficult to expand regions into non-discriminative parts. Moreover, if the initial prediction produces wrong attention regions, applying the refinement step would cover even more inaccurate regions. In this paper, we focus on improving the initial prediction, which leads to more accurate object localization and benefits the refinement step.</p><p>Unsupervised Representation Learning. Unsupervised learning has been widely studied in the computer vision community. One advantage is to learn better representations of images and apply learned features on any specific domain or dataset where annotations are not always available. Selfsupervised learning <ref type="bibr" target="#b8">[9]</ref> utilizes a pretext task to replace the labels annotated by humans with "pseudo-labels" directly computed from the raw input data. A number of methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> are developed but require expert knowledge to carefully design a pretext task that may lead to good transferable features. To reduce the domain knowledge requirement, Coates and Ng <ref type="bibr" target="#b6">[7]</ref> validate that feature-learning systems with K-means can be a scalable unsupervised learning module that can train a model of the unlabeled data for extracting meaningful features. Furthermore, a recent approach <ref type="bibr" target="#b2">[3]</ref> employs a clustering framework to extract useful visual features by alternating between clustering the image descriptors and updating the weights of the CNN by predicting the cluster assignments, in order to learn deep representations specific to domains where annotations are scarce. In this work, we propose to learn a self-supervised method that explores the sub-category in the classification network, i.e., using unsupervised signal to enhance feature representations while improving initial response maps for weakly-supervised semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly-supervised Semantic Segmentation</head><p>In this section, we describe our framework for weaklysupervised semantic segmentation, including details of how we explore sub-categories to improve initial response maps and generate final semantic segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm Overview</head><p>To obtain the initial response, we follow the common practice of training a classification network and utilize the CAM method <ref type="bibr" target="#b45">[46]</ref> to obtain our baseline model. The CAM method typically only activates on discriminative object parts, which are not sufficient for the image classification task. To address this issue, We propose to integrate a more challenging task into the objective: self-supervised subcategory discovery, in order to enforce the network to learn from more object parts.</p><p>Firstly, for each annotated parent class, we determine K sub-categories by applying K-means clustering on image features. With the clustering results, we then assign each image with a pseudo label, which is identified as the index of the sub-category. Finally, we construct a sub-category objective to jointly train the classification network. By iteratively updating the feature extractor, two classifiers, and sub-category pseudo labels, the enhanced features representations lead to better classification, and thereby gradually produce response maps that attain to more complete regions of the objects. The overall process is illustrated in <ref type="figure">Figure 2</ref>. Then, we use the method in <ref type="bibr" target="#b0">[1]</ref> to expand response maps, which are used as pseudo ground truths to train the segmentation network. Also note that, our method focuses on the initial prediction, so it is not limited to certain region expansion or segmentation training methods.</p><p>Preliminaries: Initial Response via CAM. We adopt the CAM to generate the initial response using a typical classification network, whose architecture consists of convolutional layers as the feature extractor E, followed by global average pooling (GAP) and one fully-connected layer H p as the output classifier. Given an input image I, the network is trained with image-level labels Y p using a multi-label classification loss L p , following <ref type="bibr" target="#b45">[46]</ref>. After training, the activation map M for each category c can be obtained via directly applying classifier H p on the feature maps f = E(I):</p><formula xml:id="formula_0">M c (x, y) = θ c p f (x, y),<label>(1)</label></formula><p>where θ c p is the classifier weight for the category c, and f (x, y) is the feature at pixel (x, y). The response map is further normalized by the maximum value in M c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sub-category Exploration</head><p>The activation map for each image using (1) provides typically highlights only the discriminative object parts. However, from the perspective of a classifier, discovering the most discriminative part of the object is already sufficient for optimizing the loss function L p in classification. As the learning objective is based on the classification scores, it is inevitable for the CAM model to generate incomplete attention maps. To address this issue, we integrate a self-supervised scheme to enhance feature representations f while improving the response maps via exploring the subcategory information, in which f appears to be an important cue to compute the activation map via (1).</p><p>Sub-Category Objective. To assign a more challenging problem to the classification model, we introduce a task to discover sub-categories in an unsupervised manner. For each parent class p c , we define K sub-categories s k c , where k = {1, 2, ..., K}. For each image I with the parent label Y c p in {0, 1} c , the corresponding sub-category label for the category c is denoted as Y c,k s in {0, 1} k . We also note that, if the label of one parent class does not exist (i.e., Y c p = 0), the labels of all sub-categories would be also 0, i.e., Y c,k s = 0, k = {1, 2, ..., K}. Our objective is to learn a sub-category classifier H s parameterized with θ s , while sharing the same feature extractor E with H p . Similar to the parent classification loss L p , we adopt the standard multilabel classification loss L s with a larger and fine-grained label space Y s .</p><p>Sub-category Discovery. As there is no ground truth label for sub-category to directly optimize the above subcategory objective L s , we generate pseudo labels via unsupervised clustering. Specifically, we perform clustering for each parent class on image features extracted from the feature extractor E. The clustering objective for each class c can be written as:</p><formula xml:id="formula_1">min D∈R d×k 1 N c N c i=1 min Y c s ||f − T Y c s || 2 2 , s.t., Y c s 1 k = 1,<label>(2)</label></formula><p>where T is a D × K centroid matrix, N c is the number of images containing the class c, and f = E(I) ∈ R D is the extracted feature. We use the clustering assignment Y c s for each image as the sub-category pseudo label to optimize L s . Joint Training. After obtaining sub-category pseudo labels Y s from the above clustering process, we jointly optimize the feature representations f = E(I) and two classifiers, i.e., H p and H s :</p><formula xml:id="formula_2">min θp,θs 1 N N i=1 L p (H p (f i ), Y p ) + λL s (H s (f i ), Y s ), (3)</formula><p>where N is the total number of images and λ is weight to balance two loss functions. With this method, the parent classification learns a feature space through supervised training via L p , while the sub-category objective L s explores the feature sub-space and provides additional gradients to enhance feature representations f , which is used to compute CAM via <ref type="bibr" target="#b0">(1)</ref>.</p><p>Iterative Optimization. The proposed unsupervised clustering scheme in (2) relies on the feature f to discover sub-category pseudo labels. As such, the learned features via only the objective L p could be less discriminative for the clustering purpose. To mitigate this issue, we adopt an iterative training method by alternatively updating <ref type="formula" target="#formula_1">(2)</ref>   <ref type="formula" target="#formula_0">(1)</ref> process to generate better pseudo ground truths, which are then used to learn better feature representations in network training. The overall optimization for generating final class activation maps is summarized in Algorithm 1.</p><formula xml:id="formula_3">Optimize {E, H p } with Y p via L p while Training do Extract features via f = E(I) for c ← 1 to C do Generate pseudo labels Y c s with f via (2) Optimize {E, H p , H s } with {Y p , Y s } via (3) Compute M c via</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>In this section, we describe implementation details of the proposed framework and the following procedures to produce final semantic segmentation results. All the source code and trained models are available at https: //github.com/Juliachang/SC-CAM.</p><p>Classification Network. In this work, the ResNet-38 architecture <ref type="bibr" target="#b40">[41]</ref> is used for the CAM model, and the training procedure is similar to that in <ref type="bibr" target="#b0">[1]</ref>. The network consists of 38 convolution layers with wide channels, followed by a 3 × 3 convolution layer with 512 channels for better adaptation to the classification task, a global average pooling layer for feature aggregation, and two fully-connected layers for image and sub-category classification, respectively. The model is pre-trained on the ImageNet <ref type="bibr" target="#b9">[10]</ref> and is then finetuned on the PASCAL VOC 2012 dataset. We use the typical techniques based on the horizontal flip, random cropping, and color jittering operations to augment the training data set. We also randomly scale input images to impose scale invariance in the network.</p><p>We implement the proposed framework with PyTorch and train on a single Titan X GPU with 12 GB memory. To train the classification network, we use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with initial learning rate of 1e-3 and the weight decay of 5e-4. In practice, we use λ = 5 and K = 10 in all the experiments unless specified otherwise. For iterative training, we empirically find that the model converges after training for 3 rounds. In the experimental section, we show studies for the choice of K and iterative training results. Semantic Segmentation Generation. Based on the response map generated by our method as in Algorithm 1, we adopt the random walk method via affinity <ref type="bibr" target="#b0">[1]</ref> to refine the map as pixel-wise pseudo ground truths for semantic segmentation. In addition, as a common practice, we use dense conditional random fields (CRF) <ref type="bibr" target="#b22">[23]</ref> to further refine the response to obtain better object boundaries. To train the segmentation network, we utilize the Deeplab-v2 framework <ref type="bibr" target="#b4">[5]</ref> with the ResNet-101 architecture <ref type="bibr" target="#b14">[15]</ref> as the backbone model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first present the main results and analysis of the initial response generated by our method. Second, we show the final semantic segmentation performance on the PASCAL VOC dataset <ref type="bibr" target="#b10">[11]</ref> against the state-of-theart approaches. More results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluated Dataset and Metric</head><p>We evaluate the proposed approach on the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b10">[11]</ref> which contains 21 categories, including one background class. Each image contains one or multiple object classes. Following previous weakly-supervised semantic segmentation methods, we use augmented 10,528 training images present in <ref type="bibr" target="#b13">[14]</ref> along with their image-level labels to train the network. To evaluate the training set, we use the set without augmentation which has 1,464 examples. We adopt 1,449 images in the validation set and 1,456 images in the test set to compare our results with other methods. For all experiments, the mean Intersection-over-Union (mIoU) ratio is used as the evaluation metric. The results for the test set are obtained from the official PASCAL VOC evaluation website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Improvement on Initial Response</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we show the mean IoU of the segments computed using the CAM on both the training and validation sets. We present results after applying the refinement step to the activation map, i.e., CAM + random walk (CAM + RW). <ref type="table" target="#tab_1">Table 1</ref> shows that our approach significantly improves the IoU over AffinityNet <ref type="bibr" target="#b0">[1]</ref> by almost 3% using <ref type="figure">Figure 3</ref>: Sample results of initial responses. Our method often generates the response map that covers larger region of the object (i.e., attention on the body of the animal), while the response map produced by CAM <ref type="bibr" target="#b45">[46]</ref> tends to highlight small discriminative parts. <ref type="figure">Figure 4</ref>: Ablation study for K. We show that the proposed method performs robustly with respect to K and is consistently better than the original CAM that did not apply clustering to discover sub-categories. We mark the value of mIoU of the original CAM at K = 1 and the improved mIoUs are presented. CAM and more than 4% for CAM+RW. The improved initial response maps facilitate the downstream task in generating pixel-wise pseudo ground truths for training the semantic segmentation model.</p><p>In <ref type="figure">Figure 3</ref>, we show comparisons of generated CAMs by the conventional classification loss L p <ref type="bibr" target="#b45">[46]</ref> and the proposed method via sub-category discovery summarized in Algorithm 1. Visual results show that our method is able to localize more complete object regions, while the original CAM only focuses on discriminative object parts. We also note that this is essentially critical for the refinement stage that takes the response map as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>To demonstrate how our method helps improve feature representations and allow the network pay more attention to other object parts via exploiting the sub-category information, we present extensive analysis in this section. Here, all the experimental results are based on the PASCAL VOC Effect of Sub-Category Number K. We first study how the sub-category number K affect the performance of the proposed method. In <ref type="figure">Figure 4</ref>, we use K = {5, 10, 20, 50}, and show that the proposed method performs robustly with respect to K (within a wide range) and consistently better than the original CAM method (i.e., K = 1). The results also validate the necessity and importance of using more sub-categories (i.e., K &gt; 1) to generating better response maps. Considering the efficiency and accuracy, we use K = 10 for each parent class in all the experiments. As a future work, it is of great interest to develop an adaptive method to determine the sub-category number <ref type="bibr" target="#b32">[33]</ref>, which can reduce the redundant sub-categories and make the approach more efficient.</p><p>Iterative Improvement. To demonstrate the effectiveness of our iterative training process, we show the gradual improvement on the segment quality in <ref type="table" target="#tab_2">Table 2</ref>. We present the results of mIoU and F-Score that accounts for both the recall and precision measurements, in which they are important cues to validate whether the activation map is able to cover object parts. Compared to the results in round #0, which is the original CAM, our method gradually improves both metrics as training more rounds.  : Visualizations of weights based on the t-SNE method that illustrates the relationships on semantic-level between parent classifier and the person sub-category classifier. We show that one person sub-category is usually close to one parent class, as they often co-appear in the same image, as shown in example images on two sides.</p><p>Clustering Results. Since the ground truth labels are not available for sub-categories, we present visualizations of clustering results in <ref type="figure" target="#fig_0">Figure 5</ref> to measure the quality, in which each parent class shows 3 example clusters. Our method is able to cluster objects based on their size (Aeroplane, Bird, Cow), context (Aeroplane, Bird, Person), type (Boat, Bird), pose (Cow), and interaction with other categories (Person). For instance, persons with different categories, e.g., horse, motobike, and boat, are clustered into different groups. This visually validates that our learned feature representations are enhanced via the sub-category objective in an unsupervised manner. More visual comparisons are presented in the supplementary material.</p><p>Weight Visualization. In order to understand how our learning mechanism improves the clustering quality, we visualize the distribution of the classifier weights, i.e., θ p and θ s , via t-SNE <ref type="bibr" target="#b35">[36]</ref>. As such, we are able to find the relationship between the parent classifier H p and the sub-category module H s . <ref type="figure" target="#fig_1">Figure 6</ref> shows the visualization of weights, in which we take the sub-categories of person (denoted as yellow cross symbols) as the example, since the person category has more interactions with other parent classes (denoted as solid circles). It illustrates that one person subcategory is often close to one parent class, e.g., sub-category person and parent class bike, which makes sense as those two categories usually co-appear in the same image (see example images in <ref type="figure" target="#fig_1">Figure 6</ref> on two sides).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic Segmentation Performance</head><p>After generating the pseudo ground truths as the results in <ref type="table" target="#tab_1">Table 1</ref> (i.e., CAM + RW), we use them to train the semantic segmentation network. We first compare our method with recent work using the ResNet-101 backbone or other similarly powerful ones in <ref type="table" target="#tab_4">Table 4</ref>. On both validation and testing sets, the proposed algorithm performs favorably against the state-of-the-art approaches. We also note that, most methods focus on improving the refinement stage or network training, while ours improves the initial step to generate better object response maps.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show detailed results for each category on the validation set. We compare two groups of results    <ref type="bibr" target="#b3">[4]</ref> ResNet-101 60.8 61.9 DSRG CVPR'18 <ref type="bibr" target="#b17">[18]</ref> ResNet-101 61.4 63.2 AffinityNet CVPR'18 <ref type="bibr" target="#b0">[1]</ref> Wide ResNet-38 61.7 63.7 SeeNet NIPS'18 <ref type="bibr" target="#b16">[17]</ref> ResNet-101 63.  <ref type="bibr" target="#b12">[13]</ref> ResNet-101 63.6 64.5 OAA ICCV'19 <ref type="bibr" target="#b18">[19]</ref> ResNet-101 63.9 65.6 CIAN CVPR'19 <ref type="bibr" target="#b11">[12]</ref> ResNet-101 64.1 64.7 FickleNet CVPR'19 <ref type="bibr" target="#b23">[24]</ref> ResNet-101 64.9 65.3 Ours ResNet101 66.1 65.9 with (bottom) or without (top) applying the CRF <ref type="bibr" target="#b22">[23]</ref> refinement to the final segmentation outputs. Compared to the recent FickleNet <ref type="bibr" target="#b23">[24]</ref> method that also focuses on improving the initial response map, the proposed algorithm performs favorably for the segmentation task in terms of the mean IoU. We also note that, our results without applying CRF (mIoU as 64.8%) already achieves similar perfor-mance compared with the FickleNet (mIoU as 64.9%). In <ref type="figure" target="#fig_2">Figure 7</ref>, we present some examples of the final semantic segmentation results, and show that our results are close to the ground truth segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a simple yet effective approach to improve the class activation maps by introducing a selfsupervised task to discover sub-categories in an unsupervised manner. Without bells and whistles, our approach performs favorably against existing weakly-supervised semantic segmentation methods. Specifically, we develop an iterative learning scheme by running clustering on image features for each parent class and train the classification network on sub-category objectives. Unlike other existing schemes that aggregate multiple response maps, our approach generates better initial predictions without introducing extra complexity or inference time to the model. We conduct extensive experimental analysis to demonstrate the effectiveness of our approach via exploiting the subcategory information. Finally, we show that our algorithm produces better activation maps, thereby improving the final semantic segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Clustering results of the last round model (#3). We show 3 clusters for each parent class and demonstrate that our learned features are able to cluster objects based on their size (Aeroplane, Bird, Cow), context (Aeroplane, Bird, Person), type (Boat, Bird), pose (Cow), and interaction with other categories (Person).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6</head><label>6</label><figDesc>Figure 6: Visualizations of weights based on the t-SNE method that illustrates the relationships on semantic-level between parent classifier and the person sub-category classifier. We show that one person sub-category is usually close to one parent class, as they often co-appear in the same image, as shown in example images on two sides.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on the PASCAL VOC 2012 validation set. (a) Input images. (b) Ground truth. (c) Our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and (3). Therefore, features f are first enhanced through the sub-category objective, and in turn facilitate the clustering Algorithm 1 Learning Sub-category Discovery for CAM Input: Image I; Parent Label Y p ; Category Number C; Sub-category Number K Output: Class Activation Map M c Model: Feature extractor E; Parent Classifier (H p ; θ p ); Sub-category Classifier (H s ; θ s )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison in mIoU (%) for evaluating activation maps on the PASCAL VOC training and validation sets.</figDesc><table><row><cell></cell><cell cols="2">Training Set</cell><cell cols="2">Validation Set</cell></row><row><cell>Method</cell><cell cols="4">CAM CAM+RW CAM CAM+RW</cell></row><row><cell>AffinityNet [1]</cell><cell>48.0</cell><cell>58.1</cell><cell>46.8</cell><cell>57.0</cell></row><row><cell>Ours</cell><cell>50.9</cell><cell>63.4</cell><cell>49.6</cell><cell>61.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Segmentation quality of the initial response at different rounds of training on the PASCAL VOC 2012 validation set. We show there is a gradual improvement on both mIoU and F-Score metrics.</figDesc><table><row><cell>Round</cell><cell cols="2">mIoU (%) ↑ F-Score ↑</cell></row><row><cell>#0 (CAM)</cell><cell>46.8</cell><cell>65.1</cell></row><row><cell>#1</cell><cell>48.0</cell><cell>65.6</cell></row><row><cell>#2</cell><cell>48.7</cell><cell>66.6</cell></row><row><cell>#3</cell><cell>49.6</cell><cell>67.0</cell></row><row><cell>validation set.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation performance on the PASCAL VOC 2012 validation set. Bottom group contains results with CRF refinement, while the top group is without CRF. Note that 11/20 classes obtain improvements using our approach w/ CRF. The best three results are in red, green and blue, respectively.Zeng  et al. [43] 90.0 77.4 37.5 80.7 61.6 67.9 81.8 69.0 83.7 13.6 79.4 23.3 78.0 75.3 71.4 68.1 35.2 78.2 32.5 75.5 48.0 63.3 FickleNet [24] 89.5 76.6 32.6 74.6 51.5 71.1 83.4 74.4 83.6 24.1 73.4 47.4 78.2 74.0 68.8 73.2 47.8 79.9 37.0 57.3 64.6 64.9 Ours (w/ CRF) 88.8 51.6 30.3 82.9 53.0 75.8 88.6 74.8 86.6 32.4 79.9 53.8 82.3 78.5 70.4 71.2 40.2 78.3 42.9 66.8 58.8 66.1</figDesc><table><row><cell>Method</cell><cell>bkg aero bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv mIoU</cell></row><row><cell cols="2">AffinityNet [1] 88.2 68.2 30.6 81.1 49.6 61.0 77.8 66.1 75.1 29.0 66.0 40.2 80.4 62.0 70.4 73.7 42.5 70.7 42.6 68.1 51.6 61.7</cell></row><row><cell cols="2">Ours (w/o CRF) 88.1 49.6 30.0 79.8 51.9 74.6 87.7 73.7 85.1 31.0 77.6 53.2 80.3 76.3 69.6 69.7 40.7 75.7 42.6 66.1 58.2 64.8</cell></row><row><cell>MCOF [38]</cell><cell>87.0 78.4 29.4 68.0 44.0 67.3 80.3 74.1 82.2 21.1 70.7 28.2 73.2 71.5 67.2 53.0 47.7 74.5 32.4 71.0 45.8 60.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of weakly-supervised semantic segmentation methods on the PASCAL VOC 2012 val and test sets. In addition, we present methods that aim to improve the initial response with in the "Init. Res." column.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Init. Res. Val Test</cell></row><row><cell>MCOF CVPR'18 [38]</cell><cell>ResNet-101</cell><cell>60.3 61.2</cell></row><row><cell>DCSP BMVC'17</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported in part by the NSF CAREER Grant #1149783, and gifts from eBay and Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vostr: Video object segmentation via transferable representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning feature representations with k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning classification with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2010/workshop/index.html" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Helhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">An</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local convolutional features with unsupervised training for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattis</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient parameter-free clustering using first neighbor relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic co-segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning randomwalk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly-supervised video scene co-parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
