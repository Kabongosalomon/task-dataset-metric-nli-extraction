<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Shenoy</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sardana</surname></persName>
							<email>asardana@nvidia.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Birla Inst. of Technology and Science</orgName>
								<orgName type="institution">Pilani Pilani</orgName>
								<address>
									<region>RA</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Graphics Bengaluru</orgName>
								<address>
									<region>KA</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment Analysis and Emotion Detection in conversation is key in several real-world applications, with an increase in modalities available aiding a better understanding of the underlying emotions. Multi-modal Emotion Detection and Sentiment Analysis can be particularly useful, as applications will be able to use specific subsets of available modalities, as per the available data. Current systems dealing with Multi-modal functionality fail to leverage and capture -the context of the conversation through all modalities, the dependency between the listener(s) and speaker emotional states, and the relevance and relationship between the available modalities. In this paper, we propose an end to end RNN architecture that attempts to take into account all the mentioned drawbacks. Our proposed model, at the time of writing, out-performs the state of the art on a benchmark dataset on a variety of accuracy and regression metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal Emotion Detection and Sentiment Analysis in conversation is gathering a lot of attention recently considering its potential use cases owing to the rapid growth of online social media platforms such as YouTube, Facebook, Instagram, Twitter etc. <ref type="bibr" target="#b14">, Poria et al., 2016</ref><ref type="bibr" target="#b22">, Zadeh et al., 2016b</ref>, especially knowing that information obtained from any combination of more than one of the available modalities (e.g. text, audio, video) can be used to produce meaningful results.</p><p>The current state of the art systems on multimodal emotion detection and sentiment analysis do not treat the modalities in accordance to the information they are capable of holding (e.g. textual information is significantly more likely to hold * * The following work was pursued when author was an intern at NVIDIA Graphics, Bengaluru contextual information then audio or video features are), lack an adequate fusion mechanism, and fail to effectively capture the context of a conversation in a multi-modal setting. In addition to the lack of proper usage of the available modalities, models also fail to effectively capture the flow of a conversation, the separation between speaker and listener states, and the emotional effect a speakers utterance has on the listener (s) in dyadic conversations.</p><p>Our proposed model Multilogue-Net, attempts to embed basic domain knowledge and takes insight from <ref type="bibr" target="#b15">Poria et al. (2019)</ref>, assuming that the sentiment or emotion governing a particular utterance predominantly depends on 4 factors interlocutor state, interlocutor intent, the preceding and future emotions, and the context of the conversation. Interlocutor intent amongst the mentioned is particularly difficult to model due to its dependency of prior knowledge about the speaker, but modelling the other 3 separately, yet in an interrelated manner was theorized to produce meaningful results if managed to be captured effectively. The key intention was to attempt to simulate the setting in which an utterance is said, and use the actual utterance at that point to be able to gain better insights regarding emotion and sentiment of that utterance. The model uses information from all modalities learning multiple state vectors (representing interlocutor state) for a given utterance, followed by a pairwise attention mechanism inspired by <ref type="bibr" target="#b5">Ghosal et al. (2018)</ref>, attempting to better capture the relationship between all pairs of the available modalities.</p><p>The model uses two gated recurrent units (GRU) <ref type="bibr" target="#b1">(Chung et al., 2014)</ref> for each modality for modelling interlocutor state and emotion. Along with these GRU's, the model also uses an interconnected context network, consisting of the same number of GRU's as the number of available modalities, to model a different learned context representation for each modality. The incoming utterance representations and the historical GRU outputs are used at every timestamp to be able to arrive at a prediction for that timestamp.</p><p>The model produces m different representations at every timestamp (Where m is the number of modalities), where each representation is the emotional state at that timestamp as conveyed by each of the modalities. These m representations are used by the fusion mechanism to incorporate information from each of the m representations to be able to arrive at the final prediction for that timestamp. We understand that the usage of the pairwise attention mechanism, along with the Emotion GRU are what make the model flexible across tasks.</p><p>The usage of only the text representation as input to the context GRUs has been observed to be key to the results, as the context of the conversation would be better captured by textual information then it would have with audio or video information. We believe that Multilogue-net performs better than the current state of the art <ref type="bibr" target="#b5">(Ghosal et al., 2018)</ref> on multi-modal datasets because of better context representation leveraging all available modalities. <ref type="bibr">1</ref> The remaining sections of the paper are arranged as follows: Section 2 discusses related work; Section 3 discusses the model in detail; Section 4 provides experimental results, dataset details, and analysis; Section 5 contains our ablation studies and its implications; and finally Section 6 speaks on potential future work, and concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-modal Emotion recognition and Sentiment Analysis has always attracted attention in multiple fields such as natural language processing, psychology, cognitive science, and so on <ref type="bibr" target="#b12">(Picard, 2010)</ref>. Previous works have been done studying factors of variation that have a more direct correlation with emotion, such as <ref type="bibr" target="#b3">Ekman et al. (1992)</ref>, who found correlation between emotion and facial cues, and a lot of studies extensively focus on emotions and their relationship with one another such as Plutchiks wheel of emotions, which defines eight primary emotion types, each of which has a multitude of emotions as sub-types.</p><p>Early work done to leverage multi-modal information for emotion recognition includes works such as <ref type="bibr" target="#b2">Datcu and Rothkrantz (2012)</ref>, who fused 1 A basic model and training implementation of Multilogue-Net can be found at https://github.com/ amanshenoy/multilogue-net. acoustic information with visual cues for emotion recognition and <ref type="bibr" target="#b4">Eyben et al. (2010)</ref>, who used contextual information for emotion recognition in multi-modal settings. More recently, deep recurrent neural networks have been used to be able make the best of the learned representations of the modalities available to be able to give very effective and accurate emotion and sentiment predictions.  successfully used RNN-based deep networks for multi-modal emotion recognition, which was followed by multiple other works <ref type="bibr">Zadeh et al., 2018a;</ref><ref type="bibr" target="#b20">Zadeh et al., 2018c)</ref> giving results far better than what was seen before. Recent works also include works such as , who used memory networks for emotion recognition in dyadic conversations, where two distinct memory networks enabled interspeaker interaction.</p><p>Some works such as DialogueRNN <ref type="bibr" target="#b10">(Majumder et al., 2018)</ref>, though focused on emotion recognition and sentiment analysis using a single modality (text), works very well in a multi-modal setting by just replacing the text representation with a concatenated vector of all the modality representations. DialogueRNN effectively leveraged the separation between the speakers by maintaining two independent gated recurrent units to keep track of the interlocutor states, also effectively capturing context in the conversation, yielding state-of-theart performance on uni-modal data. Even though DialogueRNN was able to give reasonably good results on multi-modal data, the lack of an adequate fusion mechanism and the lack of focus on a multi-modal representation held its multi-modal performance back.</p><p>Apart from the kind of works shown before, where a methodology or a model was proposed, works such as <ref type="bibr" target="#b15">Poria et al. (2019)</ref> spoke extensively about the research challenges and advancements in emotion detection in conversation and gave a comprehensive overview of the problem. Most recently <ref type="bibr" target="#b5">Ghosal et al. (2018)</ref> introduced the idea of learning the relationship between pairs of all available modalities using pairwise attention, in a multimodal setting, where similar attributes learned by multiple modalities are emphasized and differences between the modality representations are diminished. Pairwise attention proved to be incredibly effective yielding state-of-the-art performance on multi-modal data with just simple representations for each modality.</p><p>3 Proposed Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let there be a P number of participants p 1 , p 2 , ..., p P in the conversation. The problem is defined such that for every utterance u 1 , u 2 , ..., u N uttered by any participant(s), a sentiment score is allotted along with a predicted emotion label (one of happy, sad, angry, surprise, disgust, and fear). Each utterance corresponds to a particular participant of the conversation, allowing this formulation of the problem to also capture the average sentiment of a participant in the conversation. Predictions over utterances also avoid problems such as classification during long moments of silence when predictions are made for a fixed time interval, and is also mostly common practice.</p><p>For every utterance u t (p), where p is the party who uttered the utterance, there exist three independent representations , t t ∈ R Dt , a t ∈ R Da , and v t ∈ R Dv , and are obtained using the feature extractors further explained in section 4.2.</p><p>This gives us our overall formulation of the problem, which is to be able to learn a function which would take as input three independent representations of a particular utterance, information regarding the previous emotional state of the participant, and a representation of the current context of the conversation -to be able to map to an output prediction of a sentiment score and emotion label.</p><p>Details regarding how these representations are updated and how the output is generated using these inputs are described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Details</head><p>Modelling was done under the underlying assumption that the sentiment or emotion of an utterance predominantly depends on four factors as mentioned before:</p><p>• Interlocutor State</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Interlocutor Intent</head><p>• Context of the conversation until that point</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Previous interlocutor states and emotions of a particular participant in the conversation</head><p>The proposed model attempts to model three out of the mentioned four explicitly, and assume that interlocutor intent will be modelled implicitly during model training. Interlocutor state is modelled using a state GRU (will be referred to as sGRU ), A context GRU is used to keep track of the context of the conversation (cGRU ), and an emotion GRU (eGRU ) is used to keep track of the emotional state of that particular participant. Finally, a pairwise attention mechanism, which uses the emotion representation of all modalities at a particular timestamp is used to leverage the important modalities and relevant combination of the modalities for emotion or sentiment prediction at that timestamp. Every utterance has three independent feature representations (text, audio, and video features), t t ∈ R Dt , a t ∈ R Da , and v t ∈ R Dv . Each of these feature representations are treated and operated on independently until the pairwise attention mechanism. The model consists of two GRUs (state GRU, and emotion GRU) for every modality and participant, and a context GRU for each modality common to all participants in the conversation (If p is the number of participants and m is the number of modalities, the model would have a total of 2mp + m GRUs). The inputs at the current timestamp and the previous state, context, and emotion representations are operated on to be able to arrive at the prediction at that timestamp. <ref type="figure" target="#fig_0">Figure 1</ref> describes the updates at a particular timestamp and the role of each GRU is further explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Context GRU (cGRU )</head><p>The Context GRU (cGRU ) for each modality aims to capture the context of the conversation by jointly encoding the utterance representation of that modality (at timestamp t in the given diagram) (t t ∈ R Dt , a t ∈ R Da , or v t ∈ R Dv ) and the previous timestamp speaker state GRU output of that modality. This accounts for inter-speaker and inter-utterance dependencies to produce an effective context rep- </p><formula xml:id="formula_0">resentation. The current utterance t t , a t , or v t , changes the state of that speaker from (s t t , s a t , s v t ) to (s t t+1 , s a t+1 , s v t+1 ). To capture this change in context we use GRU cell cGRU having output size D c , using t t , a t , or v t and (s t t , s a t , s v t ) as: c t t+1 = cGRU (c t t , (t t ⊕ s t t )) (1) c a t+1 = cGRU (c a t , (a t ⊕ s a t )) (2) c v t+1 = cGRU (c v t , (v t ⊕ s v t ))<label>(3)</label></formula><p>Where D c is the size of the context vectors c t t+1 , c a t+1 , and c v t+1 .D t , D a , and D v are the sizes of utterance representations of text, audio, and video respectively.⊕ represents the concatenation operation, D s is the size of all the state vectors s t t+1 , s a t+1 , and s v t+1 ; and all GRU weight and biases shapes are such that they produce the expected shape of outputs taking the given shape of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">State GRU (sGRU )</head><p>The network keeps track of the participants involved in a conversation by employing a p * m number of (sGRU )'s, where p is the number participants in the conversation and m is the number of available modalities.The sGRU associated with a participant outputs fixed size vectors which serve as an encoding to represent the interlocutor state, and are directly used for both emotion and sentiment prediction, and updating the context vectors.</p><p>All the state vectors are initialized to null at the first timestamp. For a timestamp t, the state vector of participant p and modality m ∈ {t, a, v} is updated using the input feature representation of that modality and simple attention over all the context vectors until that timestamp. The simple attention mechanism over all the context vectors is described by the following equations:</p><formula xml:id="formula_1">α = sof tmax(m T t W α [c m 1 , c m 2 , ..., c m t ])<label>(4)</label></formula><formula xml:id="formula_2">att t = α[c m 1 , c m s t t+1 = sGRU (s t t , (t t ⊕ att t t+1 )) (6) s a t+1 = sGRU (s a t , (a t ⊕ att a t+1 ))<label>(7)</label></formula><p>s</p><formula xml:id="formula_3">v t+1 = sGRU (s v t , (v t ⊕ att v t+1 ))<label>(8)</label></formula><p>Where D s is the size of all the state vectors s t t+1 , s a t+1 , and s v t+1 .D t , D a , D v are the sizes of utterance representations of text, audio, and video respectively.⊕ represents concatenation operation, and all GRU weights shapes are such that they produce the expected shape of outputs taking the given shape of inputs.</p><p>The intended purpose of using this as the input to sGRU t,a,v is to model the dependency of the speaker state on the context of the conversation as understood by the utterances until that point, along with the utterance representation at that point. The output of the sGRU for modality m and timestamp t serves as an encoding of the speaker state as conveyed by modality m, at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Emotion GRU (eGRU )</head><p>The emotion GRU serves as the decoder for the encoding produced by the state GRU. The emotion GRU uses the previous timestamp eGRU output, and the encoding provided by sGRU to produce an emotion or sentiment representation which is further used by the pairwise attention mechanism to be able to produce the relevant output for prediction. At timestamp (t + 1) the emotion vectors are updated as:</p><formula xml:id="formula_4">e t t+1 = eGRU (e t t , s t t+1 )<label>(9)</label></formula><p>e a t+1 = eGRU (e a t , s a t+1 )</p><formula xml:id="formula_5">e v t+1 = eGRU (e v t , s v t+1 )<label>(10)</label></formula><p>Where D e is the size of all the emotion vectors e t t+1 , e a t+1 , and e v t+1 .D t , D a , andD v are the sizes of utterance representations of text, audio, and video respectively.D e is the size of the state vectors s t t+1 , s a t+1 , and s v t+1 ; and all GRU weights shapes are such that they produce the expected shape of outputs taking the given shape of inputs.</p><p>The emotion GRU acts as a decoder to the encoding produced by the associated state GRU, producing a vector which can be used for both sentiment and emotion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Pairwise Attention Mechanism</head><p>The emotion GRU for each timestamp will produce an m number of vectors (where m is the number of modalities available). Pairwise attention is then used over these m vectors to produce the final prediction output. In particular pairwise attention is calculated over the following pairs in our case (e v , e t ), (e t , e a ), and (e a , e v ). Pairwise attention for pair (e v , e t ) would be calculated as follows: <ref type="figure">Figure 3</ref>: Pairwise attention mechanism used as the fusion mechanism followed by the final prediction layer</p><formula xml:id="formula_7">B 1 = e v .(e t ) T , B 2 = e t .(e v ) T<label>(12)</label></formula><formula xml:id="formula_8">N 1 = sof tmax(B 1 ), N 2 = sof tmax(B 2 ) (13) O 1 = N 1 .e t , O 2 = N 2 .e v<label>(14)</label></formula><formula xml:id="formula_9">A 1 = O 1 e v , A 2 = O 2 e t<label>(15)</label></formula><p>pairwise(e v , e t ) = A 1 ⊕ A 2</p><p>Where B 1 , B 2 ∈ R De×De ; N 1 , N 2 ∈ R De×De ; A 1 , A 2 ∈ R De×De ; and pairwise(e v , e t ) ∈ R De×2De ; represents element-wise product; and ⊕ represents concatenation.</p><p>A complete analysis on the pairwise attention mechanism has been done by <ref type="bibr" target="#b5">Ghosal et al. (2018)</ref>, where the role of each one of the intermediate variables has been described. These equations <ref type="bibr">(12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">16</ref>) calculate m C 2 pairwise fusion representations, which are further concatenated to make the final prediction as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Final Predictions</head><p>The prediction layer varies based on whether a sentiment or emotion prediction is expected. For sentiment prediction first all three pairs of pairwise attention i.e. pairwise(e v , e t ), pairwise(e a , e t ), and pairwise(e v , e a ) at that timestamp are concatenated along with the emotion GRU outputs at that timestamp (e t t , e a t , and e v t ) and the concatenated layer is passed through a fully connected layer followed by a sof tmax or tanh layer based on the nature of the expected prediction. For sentiment prediction between -1 and +1 at timestamp t the output layer would equate as follows:</p><formula xml:id="formula_11">pw = pw(e v , e t ) ⊕ pw(e a , e t ) ⊕ pw(e v , e a ) (17) L t = pw ⊕ e t t ⊕ e a t ⊕ e v t (18) pred sentiment(t) = tanh(W L L t )<label>(19)</label></formula><p>Where pairwise(e v , e t ) has been represented as pw(e v , e t ); and W L ∈ R 9De×1 .</p><p>For emotion prediction we use a fully connected layer along with a final sof tmax layer to calculate 6 emotion class probabilities from L t .</p><formula xml:id="formula_12">l t = ReLU (W l L t + b l )<label>(20)</label></formula><formula xml:id="formula_13">P t = sof tmax(W smax l t + b smax ) (21) pred emotion(t) = argmax(P t )<label>(22)</label></formula><formula xml:id="formula_14">Where W l ∈ R D l ×9De ; b l =∈ R D l ; W smax ∈ R c×D l ; b smax ∈ R c and P t ∈ R c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Training</head><p>Fairly standard practices have been employed for the training of the model. Categorical cross-entropy has been used along with L2-regularization as the loss function during training for emotion prediction, to maximize likelihood over each of the classes.</p><p>Mean Square Error (MSE) along with L2 regularization has been employed as loss function during training for sentiment regression. The usage of a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">CMU-MOSEI</head><p>In CMU-MOSEI dataset labels are in a continuous range of -3 to +3 and are accompanied by an emotion label being one of six emotions. However, in this work we also project the instances of CMU-MOSEI in a two-class classification setup with values ≥ 0 signifies positive sentiments and values &lt; 0 signify negative sentiments. We have called this A2 accuracy (accuracy with 2 classes). Along with this we have also shown results for continuous range prediction between -3 and +3, and emotion prediction with the 6 emotion labels for each utterance in CMU-MOSEI. We have used A2 as a metric to be consistent with the previous published works on CMU-MOSEI dataset <ref type="bibr" target="#b5">(Ghosal et al., 2018;</ref><ref type="bibr" target="#b19">Zadeh et al., 2018b)</ref>. CMU-MOSEI has further been used for other comprehensive experiments due to its large sizer and easier feature extraction 4.2 Uni-modal Feature Extraction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CMU-MOSEI</head><p>We use the CMU-Multi-modal Data SDK <ref type="bibr" target="#b19">(Zadeh et al., 2018b)</ref> for feature extraction. For MOSEI dataset, sentiment label-level features were provided where text features used were GloVe embeddings <ref type="bibr" target="#b11">(Pennington et al., 2014)</ref>, visual features extracted by Facet <ref type="bibr" target="#b16">(Stckli et al., 2017</ref>) &amp; acoustic features by OpenSMILE <ref type="bibr" target="#b4">(Eyben et al., 2010)</ref>. Thereafter, we compute the average of sentiment label-level features in an utterance to obtain the utterance-level features. For each sentiment labellevel feature, the dimension of the feature vector is set to 300 (text), 35 (visual) &amp; 384 (acoustic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">CMU-MOSI</head><p>In contrast, for MOSI dataset we use utterance level features provided in . These utterance-level features represent the outputs of a convolutional neural network <ref type="bibr" target="#b8">(Karpathy et al., 2014)</ref>, 3D convolutional neural network <ref type="bibr" target="#b7">(Ji et al., 2010)</ref> &amp; openSMILE <ref type="bibr" target="#b4">(Eyben et al., 2010)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We evaluate our proposed approach on CMU-MOSI (test-set) on accuracy and F1 score, and CMU-MOSEI (dev-set) on accuracy, F1 score, mean absolute error (M AE), pearson score (r), and accuracy's on the emotion labels. Due to the lack of speaker information in CMU-MOSI we were not able to use the CMU-Multi-modal Data SDK for sentiment label extraction, to be able to evaluate our approach on CMU-MOSI on mean absolute error and Pearson score.</p><p>Results have also been reported for usage of two of the three available modalities. Uni-modal performance has not been reported as the focus of the paper is the effective usage of multi-modal data. In a uni-modal setting the model would not be using the fusion mechanism and the output would be equivalent to having a few dense layers after the emotion GRU to directly output the final prediction. F1 scores have not been mentioned by most previous models being used for comparison, but have been reported for Multilogue-Net for additional comparison to any future models using CMU-MOSI dataset. <ref type="table">Table 1</ref> shows the performance of Multilogue-  Net on CMU-MOSI dataset, comparing to the current state of the art <ref type="bibr" target="#b5">(Ghosal et al., 2018)</ref>, previous state-of-the-art , and Dia-logueRNN <ref type="bibr" target="#b10">(Majumder et al., 2018</ref>) (Multi-modal performance of DialogueRNN has not been reported by <ref type="bibr" target="#b10">Majumder et al. (2018)</ref>, and we have run these experiments additionally for a better comparative study, where concatenating the input representations has been used as a fusion mechanism). Our model consistently outperforms the previous state-of-the-art but performs better only on one of the subsets of the modalities when compared to the current state-of-the-art.</p><p>In comparison to MMMU-BA our model also lacks in Multi-modal performance. We theorize that the model performance is lacking because of the low number of training examples (CMU-MOSI consists only of 93 conversations out of which 62 were used for training), in contrast to our model which has a high capacity (Relative to models being compared with). Since Multilogue-Net learns a lot of intermediate representations in order to make a prediction, it would need a larger dataset with more variability to be able to learn meaningful representations. The proposition that performance lacks due to a lack of training examples is backed by the results on CMU-MOSEI (demonstrated in a comparative setting in <ref type="table" target="#tab_2">Table 2</ref> and 3) where the model consistently outperforms the current stateof-the-art on most metrics.</p><p>On CMU-MOSEI, our model seems to perform very consistently on both sentiment and emotion labels. The model outperforms the current state of the art on all but one metric (both classification and accuracy) on sentiment labels in the tri-modal setting. Multilogue-Net also outperforms the current state of the art on the emotion labels by a considerable margin (This is also attributed to the fact that not a lot of models have presented results on these labels).</p><p>Similar observations are made in both datasets, where the tri-modal metrics show the best performance, and audio + video show the worst relative performance (suggesting the importance of text in a multi-modal setting). Textual information seems to be the guiding factor for multi-modal performance, with video and audio features simply acting as a push to the uni-modal performance on text.</p><p>We theorize that the performance of Multilogue-Net is majorly attributed to its increased capacity as compared to previous models. Effective usage of this increased capacity, using representations inspired from a basic understanding of conversation, along with a larger dataset for training have been key in achieving the improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies and Analysis</head><p>Until now, some architectural considerations, such as the use of eGRU and the fusion mechanism, have been briefly explained but not empirically justified. This section aims to get empirical evidence regarding the effectiveness of these modules. Since our model completely hinges around the usage of the context and state GRU's, our ablation studies and analysis have focused on the fusion mechanism and emotion GRU (eGRU ) only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fusion Mechanism</head><p>The effectiveness of the fusion mechanism can be very easily examined by observing the results of the model on both tasks − Sentiment Regression and Emotion Recognition, with and without the fusion mechanism. <ref type="table">Table 4</ref> shows these results on CMU-MOSEI modality subsets.</p><p>The bi-modal results in table 4 involve evaluating the pairwise attention module only once (Since there is only one pair available), directly followed by the prediction layer. The tri-modal case on the other hand involves evaluating the pairwise attention module thrice (Once for each pair). In general, the number of times this module will have to be evaluated for m modalities is m C 2 , which raises  <ref type="table">Table 4</ref>: Multilogue-Net performance on CMU-MOSEI with and without the fusion mechanism -for 'without' fusion we have concatenated all the representations and directly passed them to the prediction layer. a fair concern regarding the trade-off between the additional computational cost and performance.</p><p>We empirically observe that the additional computational cost can be considered negligible in context of the increased performance, largely attributing to the non-parametric nature of the fusion mechanism and the relatively small number of additional parameters in the prediction layer (6D e for the sentiment regression; 36D e for emotion recognition).</p><p>The fusion mechanism seems to clearly be beneficial in all of the reported cases apart from video + audio, implying that the fusion mechanism is useful only in the cases the text representation is used. This further strengthens our claim that the text representation guides tri-modal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Emotion GRU (eGRU )</head><p>Unlike as done with the fusion mechanism, the effectiveness of the eGRU cannot be examined by evaluating metrics with and without it. Removing the Emotion GRU would clearly be detrimental to the results, and would not convey the intention of having it.</p><p>The primary intention of having the eGRU can be considered to be maintaining consistency between tasks. To better understand what this means table 5 quantitatively demonstrates this effect. The model was trained separately for Emotion Detection and Sentiment Regression tasks. After both the models were trained satisfactorily, a particular sample from the test set (test sample 6) was inferred on. We then retrieved the intermediate text repre-  sentations (e t 4 , c t 4 , and s t 4 ; superscript t indicating text modality) at a particular timestamp (t = 4) for both models on that sample. The Euclidean Distance between these two sets of representations (one for each task) was evaluated and have been shown in table 5, where we can clearly observe that the euclidean distance between the emotion representations is much larger as compared to the state and context representations.</p><p>This shows that for both tasks, interlocutor state and context representations are relatively similar to each other, whereas the emotion state representation is more varied and task dependant. This not only allows us to use the same cGRU and sGRU weights across tasks, but would also allow us to train for multiple tasks in parallel using a different eGRU for each task -giving us consistent and accurate predictions across multiple tasks. Analysis of such a network, and whether training for multiple tasks in parallel aids one another, has not been covered in this paper and is left to our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented an RNN architecture for multi-modal sentiment analysis and emotion detection in conversation. In contrast to the current state-of-the-art models, our model focuses on effectively capturing the context of a conversation and treats each modality independently, taking into account the information a particular modality is capable of holding. Our model consistently performs well on benchmark datasets such as CMU-MOSI and CMU-MOSEI in any multi-modal setting.</p><p>The model can be further extended to have better feature extractors, and increase both the number of modalities and the number of participants in the conversation. Due to the lack of availability of datasets consisting of these extensions with emotion or sentiment labels, we have left this to our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Description of all the state updates at timestamp t for a single participant p 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>State updates and final prediction output in a conversation between two participants p 1 and p 2 , where the updates of each participant at a timestamp is as given infigure 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>:</cell><cell>Multilogue-Net performance on CMU-</cell></row><row><cell cols="2">MOSEI Sentiment Labels compared to previous state-</cell></row><row><cell cols="2">of-the-art models on regression and accuracy Metrics.</cell></row><row><cell cols="2">All metrics apart from MAE represents higher values</cell></row><row><cell cols="2">for better results, MAE represents lower values for bet-</cell></row><row><cell>ter results.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>72.8 69.1 76.6 62.0 89.9 66.3 66.3 60.4 66.9 53.7 85.5 Multilogue-Net 83.1 80.9 90.3 87.3 89.7 87.0 70.0 68.4 76.1 74.5 87.4 84.0</figDesc><table><row><cell></cell><cell></cell><cell cols="7">MOSEI Emotions (Text + Video + Audio)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Emotion</cell><cell cols="2">Anger</cell><cell cols="2">Disgust</cell><cell>Fear</cell><cell></cell><cell cols="2">Happy</cell><cell>Sad</cell><cell></cell><cell cols="2">Surprise</cell></row><row><cell>Metric</cell><cell>WA</cell><cell>F1</cell><cell>WA</cell><cell>F1</cell><cell>WA</cell><cell>F1</cell><cell>WA</cell><cell>F1</cell><cell>WA</cell><cell>F1</cell><cell>WA</cell><cell>F1</cell></row><row><cell>Graph-MFN</cell><cell>62.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Multilogue-Net performance on MOSEI Emotion Labels compared with that of Graph-MFN on weighted accuracy and F1 score. MOSEI Emotion label results were presented by only one model, and comprehensive results have not been published for the same.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Euclidean Distance between the same representations for Sentiment Regression as compared to Emotion Detection. (Distances have been converted to units for convenience and easier comparison)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">, ..., c m t ] T(5)Wherem T t ∈ {t T t , a T t , v T t }, W α ∈ R Dt,a,v×Dc , α T ∈ R (t−1), and att t ∈ R Dc . In equation 4, we calculate attention scores over all previous context representations of all previous utterances, highlighting the relative importance of all the previous context vectors to m t . A softmax layer is applied to amplify this relative importance, and finally equation 5 the final output of attention over context att t is calculated by pooling the previous context vectors with α.We then employ sGRU t,a,v to update s t,a,v t to s t,a,v t+1 on the basis of incoming utterance representations for each modality m T t ∈ {t T t , a T t , v T t } and the context representations att t t , att a t , and att v t using GRU cells sGRU t t , sGRU a t , and sGRU v t , each of output size D s .</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltruaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic audiovisual data fusion for automatic emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Rothkrantz</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118910566.ch16</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial expressions of emotion: An old controversy and new findings: Discussion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.1992.0008</idno>
	</analytic>
	<monogr>
		<title level="j">Royal Society of London Philosophical Transactions Series B</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">opensmile -the munich versatile and fast opensource audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjrn</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874246</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextual inter-modal attention for multi-modal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chauhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3454" to="3466" />
		</imprint>
	</monogr>
	<note>Soujanya Poria, and Pushpak Bhattacharyya Asif Ekbal</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1193</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Yu</forename></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.59</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.223</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoper</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Affective computing: From laughter to ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<idno type="DOI">10.1109/T-AFFC.2010.10</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facial expression analysis with affdex and facet: A validation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><surname>Stckli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schulte-Mecklenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Borer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Samson</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-017-0996-1</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Liang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1208</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI Conference on Artificial Intelligence</title>
		<meeting>the 2018 AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Systems</title>
		<imprint>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
