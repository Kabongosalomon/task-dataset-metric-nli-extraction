<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Adaptation of Convolutional Neural Networks for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
							<email>voigtlaender@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group Visual Computing Institute</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group Visual Computing Institute</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Adaptation of Convolutional Neural Networks for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>VOIGTLAENDER, LEIBE: ONLINE ADAPTATION FOR VIDEO OBJECT SEGMENTATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the task of semi-supervised video object segmentation, i.e. segmenting the pixels belonging to an object in a video using the ground truth pixel mask for the first frame. We build on the recently introduced one-shot video object segmentation (OSVOS) approach which uses a pretrained network and fine-tunes it on the first frame. While achieving impressive performance, at test time OSVOS uses the fine-tuned network in unchanged form and is not able to adapt to large changes in object appearance. To overcome this limitation, we propose Online Adaptive Video Object Segmentation (OnAVOS) which updates the network online using training examples selected based on the confidence of the network and the spatial configuration. Additionally, we add a pretraining step based on objectness, which is learned on PASCAL. Our experiments show that both extensions are highly effective and improve the state of the art on DAVIS to an intersection-over-union score of 85.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object tracking is a fundamental problem in computer vision with many applications including video editing, autonomous cars, and robotics. Recently, there has been a trend to move from bounding box level to pixel level tracking, mainly driven by the availability of new datasets, in particular DAVIS <ref type="bibr" target="#b33">[34]</ref>. In our work, we focus on semi-supervised video object segmentation (VOS), i.e. the task of segmenting the pixels belonging to a generic object in the video using the ground truth pixel mask of the first frame.</p><p>Recently, deep learning based approaches, which often utilize large classification datasets for pretraining, have shown extremely good performance for VOS <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> and the related tasks of single-object tracking <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> and background modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In particular, the one-shot video object segmentation (OSVOS) approach introduced by Caelles et al. <ref type="bibr" target="#b6">[7]</ref>, has shown very promising results for VOS. This approach fine-tunes a pretrained convolutional neural network on the first frame of the target video. However, since at test time OSVOS only learns from the first frame of the sequence, it is not able to adapt to large changes in appearance, which might for example be caused by drastic changes in viewpoint.</p><p>While online adaptation has been used with success for bounding box level tracking (e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>), its use for VOS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref> has received less attention, especially in the context of deep learning. We thus propose Online Adaptive Video Object Published as a conference paper at BMVC 2017. It can be seen that after online adaptation, the network can deal better with changes in viewpoint (left) and new objects appearing in the scene (the car in the right sequence).</p><p>Segmentation (OnAVOS), which updates a convolutional neural network based on onlineselected training examples. In order to avoid drift, we carefully select training examples by choosing pixels for which the network is very certain that they belong to the object of interest as positive examples, and pixels which are far away from the last assumed pixel mask as negative examples (see <ref type="figure" target="#fig_0">Fig. 1</ref>, second row). We further show that naively performing online updates on every frame quickly leads to drift, which manifests in strongly degraded performance. As a countermeasure, we propose to mix in the first frame (for which the ground truth pixel mask is known) as additional training example during online updates. Our contributions are the following: We introduce OnAVOS, which uses online updates to adapt to changes in appearance. Furthermore, we adopt a more recent network architecture and an additional objectness pretraining step <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and demonstrate their effectiveness for the semi-supervised setup. We further show that OnAVOS significantly improves the state of the art on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Object Segmentation. A common approach of many classical video object segmentation (VOS) methods is to reduce the granularity of the input space, e.g. by using superpixels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, patches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>, or object proposals <ref type="bibr" target="#b32">[33]</ref>. While these methods significantly reduce the complexity of subsequent optimization steps, they can introduce unrecoverable errors early in the pipeline. The obtained intermediate representations (or directly the pixels <ref type="bibr" target="#b29">[30]</ref>) are then used for either a global optimization over the whole video <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, over parts of it <ref type="bibr" target="#b14">[15]</ref>, or using only the current and the preceding frame <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Recently, neural network based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> including OSVOS <ref type="bibr" target="#b6">[7]</ref> have become the state of the art for VOS. Since OnAVOS is built on top of OSVOS, we include a detailed description in Section 3. While OSVOS handles every video frame in isolation, we expect that incorporating temporal context should be helpful. As a step in this direction, Perazzi et al. <ref type="bibr" target="#b34">[35]</ref> propose the MaskTrack method, in which the estimated segmentation mask from the last frame is used as an additional input channel to the neural network, enabling it to use temporal context. Jampani et al. <ref type="bibr" target="#b21">[22]</ref> propose a video propagation network (VPN) which applies learned bilateral filtering operations to propagate information across video frames. Furthermore, optical flow has been used as an additional temporal cue in conjunction with deep learning in the semi-supervised <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> and unsupervised setting <ref type="bibr" target="#b39">[40]</ref>, in which the ground truth for the first frame is not available. In our work, we focus on including context information implicitly by adapting the network online, i.e. we store temporal context information in the adapted weights of the network.</p><p>Recently, Jain et al. <ref type="bibr" target="#b20">[21]</ref> proposed to train a convolutional neural network for pixel objectness, i.e. for deciding for each pixel whether it belongs to an object-like region. In another paper, Jain et al. <ref type="bibr" target="#b19">[20]</ref> showed that using pixel objectness is helpful in the unsupervised VOS setting. We adopt pixel objectness as a pretraining step for the semi-supervised setting based on the one-shot approach.</p><p>The current best result on DAVIS is obtained by LucidTracker from Khoreva et al. <ref type="bibr" target="#b23">[24]</ref>, which extends MaskTrack by an elaborate data augmentation method, which creates a large number of training examples from the first annotated frames and reduces the dependence on large datasets for pretraining. Our experiments show that our approach achieves better performance using only conventional data augmentation methods.</p><p>Online Adaptation. For bounding box level tracking, Kalal et al. <ref type="bibr" target="#b22">[23]</ref> introduced the Tracking-Learning-Detection (TLD) framework, which tries to detect errors of the used object detector and to update the detector online to avoid these errors in the future. Grabner and Bischof <ref type="bibr" target="#b13">[14]</ref> used an online version of AdaBoost <ref type="bibr" target="#b12">[13]</ref> for multiple computer vision tasks including tracking. Nam and Han <ref type="bibr" target="#b30">[31]</ref> proposed a Multi-Domain Network (MDNet) for bounding box level tracking. MDNet trains a separate domain-specific output layer for each training sequence and at test time initializes a new output layer, which is updated online together with two fully-connected layers. To this end, training examples are randomly sampled close to the current assumed object position, and are used as either positive or negative targets, based on their classification scores. This scheme of sampling training examples online has some similarities to our approach. However, our method works on the pixel level instead of the bounding box level and, in order to avoid drift, we take special care to only select training examples online for which we are very certain that they are positive or negative examples. For VOS, online adaptation is less well explored; mainly classical methods like online-updated color and/or shape models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32]</ref> and online random forests <ref type="bibr" target="#b9">[10]</ref> have been proposed.</p><p>Fully Convolutional Networks for Semantic Segmentation. Fully Convolutional Networks (FCNs) for semantic segmentation have been introduced by Long et al. <ref type="bibr" target="#b28">[29]</ref>. The main idea is to repurpose a network initially designed for classification for semantic segmentation by replacing the fully-connected layers with 1 × 1 convolutions, and by introducing skip connections which help capture higher resolution details. Variants of this approach have since been widely adopted for semantic segmentation with great success (e.g. ResNets by He et al. <ref type="bibr" target="#b16">[17]</ref>).</p><p>Recently, Wu et al. <ref type="bibr" target="#b44">[45]</ref> introduced a ResNet variant with fewer but wider layers than the original ResNet architectures <ref type="bibr" target="#b16">[17]</ref> and a simple approach for segmentation, which avoids some of the subsampling steps by replacing them by dilated convolutions <ref type="bibr" target="#b46">[47]</ref> and which does not use any skip connections. Despite the simplicity of their architecture for segmentation, they obtained outstanding results across multiple classification and semantic segmentation datasets, which motivates us to adopt their architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objectness Network Domain Specific Objectness Network</head><p>Test Network Online Adapted Test Network (pretrained on PASCAL) (pretrained on DAVIS) (fine-tuned on first frame) (fine-tuned online) <ref type="figure">Figure 2</ref>: The pipeline of OnAVOS. Starting from pretrained weights, the network is first pretrained for objectness on PASCAL (a). Afterwards we pretrain on DAVIS to incorporate domain specific information (b). During test time, we fine-tune on the first frame, to obtain the test network (c). On the following frames, the network is then fine-tuned online to adapt to the changes in appearance (d).</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">One-Shot Video Object Segmentation</head><p>OnAVOS (see <ref type="figure">Fig. 2</ref> for an overview) builds upon the recently introduced one-shot video object segmentation (OSVOS) approach <ref type="bibr" target="#b6">[7]</ref>, but introduces pretraining for pixel objectness <ref type="bibr" target="#b20">[21]</ref> as a new component, adopts a more recent network architecture, and incorporates a novel online adaptation scheme, which is described in detail in Section 4.</p><p>Base Network. The first step of OnAVOS is to pretrain a base network on large datasets (e.g. ImageNet <ref type="bibr" target="#b8">[9]</ref> for image classification) in order to learn a powerful representation of objects, which can later be used as a starting point for the video object segmentation (VOS) task.</p><p>Objectness Network. In a second step, the network is further pretrained for pixel objectness <ref type="bibr" target="#b20">[21]</ref> using a binary cross-entropy loss. In order to obtain targets for foreground and background, we use the PASCAL <ref type="bibr" target="#b10">[11]</ref> dataset and map all 20 annotated classes to foreground and all other image regions are treated as background. As demonstrated by Jain et al. <ref type="bibr" target="#b19">[20]</ref>, the resulting objectness network alone already performs well on DAVIS, but here we use objectness only as a pretraining step.</p><p>Domain Specific Objectness Network. The objectness network was trained on the PASCAL dataset. However, the target dataset on which the VOS should be performed may exhibit different characteristics, e.g. a higher resolution and less noise in the case of DAVIS. Hence, we fine-tune the objectness network using the DAVIS training data and obtain a domain specific objectness network. The DAVIS annotations do not directly correspond to objectness, as usually only one object out of possibly multiple is annotated. However, we argue that the learned task here is still similar to general objectness, since in most sequences of DAVIS the number of visible objects is relatively low and the object of interest is usually relatively large and salient. Note that OSVOS trained the base network directly on DAVIS without objectness pretraining on PASCAL. Our experiments show that both steps are complementary.</p><p>Test Network. After the preceding pretraining steps, the network has learned a domain specific notion of objectness, but during test time, it does not know yet which of the possibly multiple objects of the target sequence it should segment. Hence, we fine-tune the pretrained network on the ground truth mask of the first frame, which provides it with the Algorithm 1 Online Adaptive Video Object Segmentation (OnAVOS) Input: Objectness network N , positive threshold α, distance threshold d, total online steps n online , current frame steps n curr 1: Fine-tune N for 50 steps on f rame(1) 2: lastmask ← ground_truth(1) 3: for t = 2 . . . T do 4:</p><p>lastmask ← erosion(lastmask) 5:</p><p>dtrans f orm ← distance_trans f orm(lastmask) 6:</p><p>negatives ← dtrans f orm &gt; d 7:</p><p>posteriors ← f orward(N , f rame(t)) 8:</p><p>positives ← (posteriors &gt; α) \ negatives</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>if lastmask = / 0 then 10:</p><p>interleaved: 11:</p><p>Fine-tune N for n curr steps on f rame(t) using positives and negatives 12:</p><p>Fine-tune N for n online − n curr steps on f rame(1) using ground_truth(1) 13:</p><p>end if 14:</p><p>posteriors ← f orward(N , f rame(t)) 15:</p><p>lastmask ← (posteriors &gt; 0.5) \ negatives 16:</p><p>Output lastmask for frame t 17: end for identity and specific appearance of the object of interest and allows it to learn to ignore the background. This one-shot step has been shown to be very effective for VOS <ref type="bibr" target="#b6">[7]</ref>, which we also confirm in our experiments. However, the first frame does not provide enough information for the network to adapt to drastic changes in appearance or viewpoint. In these cases, our online adaptation approach (see Section 4) is needed.</p><p>Network Architecture. While OSVOS used a variant of the well-known VGG network <ref type="bibr" target="#b38">[39]</ref>, we choose to adopt a more recent network architecture which incorporates residual connections. In particular, we adopt model A from Wu et al. <ref type="bibr" target="#b44">[45]</ref>, which is a very wide ResNet <ref type="bibr" target="#b16">[17]</ref> variant with 38 hidden layers and roughly 124 million parameters. The approach for segmentation is very simple, as no upsampling mechanism or skip connections are used. Instead, downsampling by a factor of two using strided convolutions is performed only three times. This leads to a loss of resolution by a factor of eight in each dimension, following which the receptive field is increased using dilated convolutions <ref type="bibr" target="#b46">[47]</ref> at no additional loss of resolution. Despite its simplicity, this architecture has shown excellent results both for classification (ImageNet) and segmentation (PASCAL) tasks <ref type="bibr" target="#b44">[45]</ref>. When applying it for segmentation, we bilinearly upsample the pixelwise posterior probabilities to the initial resolution before thresholding with 0.5.</p><p>We use the weights provided by Wu et al. <ref type="bibr" target="#b44">[45]</ref>, which were obtained by pretraining on ImageNet <ref type="bibr" target="#b8">[9]</ref>, Microsoft COCO <ref type="bibr" target="#b27">[28]</ref>, and PASCAL <ref type="bibr" target="#b10">[11]</ref>, as a very strong initialization for the base network. We then replace the output layer with a two-class softmax. As loss function, we use the bootstrapped cross-entropy loss function <ref type="bibr" target="#b45">[46]</ref>, which takes the average over the cross-entropy loss values only over a fraction of the hardest pixels, i.e. pixels which are predicted worst by the network, instead of all pixels. This loss function has been shown to work well for unbalanced class distributions, which also commonly occur for VOS due to the dominant background class. In all our experiments, we use a fraction of 25% of the hardest pixels and optimize this loss using the Adam optimizer <ref type="bibr" target="#b24">[25]</ref>. In our evaluations, we separate the effect of the network architecture from the effect of the algorithmic improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Online Adaptation</head><p>Since the appearance of the object of interest changes over time and new background objects can appear, we introduce an online adaptation scheme to adapt to these changes (see Algorithm 1). New objects entering the scene are especially problematic when pretrain-ing for objectness, since they were never used as negative training examples and are thus assigned a high probability (see <ref type="figure" target="#fig_0">Fig. 1 (right)</ref> for an example).</p><p>The basic idea of our online adaptation scheme is to use pixels with very confident predictions as training examples. We select the pixels for which the predicted foreground probability exceeds a certain threshold α as positive examples. One could argue that using these pixels as positive examples is useless, since the network already gives very confident predictions for them. However, it is important that the adaptation retains a memory of the positive class in order to create a counterweight to the many negative examples being added. In our experiments, leaving out this step resulted in holes in the foreground mask.</p><p>We initially selected negative training examples in the same way, i.e. using pixels with a very low foreground probability. However, this led to degraded performance, probably, because during large appearance changes, false negative pixels will be selected as negative training examples, effectively destroying all chances to adapt to these changes. We thus select negative training examples in a different way, based on the assumption that the movement between two frames is small. The idea is to select all pixels which are very far away from the last predicted object mask. In order to deal with noise, the last mask can first be shrunk by an erosion operation. For our experiments, we use a square structural element with size 15, but we found that the exact value of this parameter is not critical. Afterwards, we compute a distance transform, which for each pixel provides the Euclidean distance to the closest foreground pixel of the mask. Finally, we apply a threshold d and treat all pixels with a distance larger than d as negative examples.</p><p>Pixels which are neither marked as positive nor as negative examples are assigned a "don't care" label and are ignored during the online updates. We can now fine-tune the network on the current frame, since every pixel has a label for training. However, in practice, we found that naively fine-tuning using the obtained training examples quickly leads to drift. To circumvent this problem, we propose to mix in the first frame as additional training examples during the online updates, since for the first frame the ground truth is available. We found that in order to obtain good results, the first frame should be sampled more often than the current frame, i.e. during online adaptation we perform a total of n online update steps per frame, of which only n curr are performed on the current frame, and the rest is performed on the first frame. Additionally, we reduce the weight of the loss for the current frame by a factor β (e.g. β ≈ 0.05). A value of 0.05 might seem surprisingly small, but one has to keep in mind that the first frame is used very often for updates, quickly leading to smaller gradients, while the current frame is only selected a few times.</p><p>During online adaptation, the negative training examples are selected based on the mask of the preceding frame. Hence, it can happen that a pixel is selected as a negative example and that it is predicted as foreground at the same time. We call such pixels hard negatives. A common case in which hard negatives occur is when a previously unseen object enters the scene far away from the object of interest (see <ref type="figure" target="#fig_0">Fig. 1 (right)</ref>), which will then usually be detected as foreground by the network. We found it helpful to remove hard negatives from the foreground mask which is used in the next frame to determine negative training examples. This step allows selecting the hard negatives in the next frame again as negative examples. Additionally, we tried to adapt the network more strongly to hard negatives by increasing the number of update steps and/or the loss scale for the current frame in the presence of hard negatives. However, this did not improve the results further.</p><p>In addition to the previously described steps, we propose a simple heuristic which makes our method more robust against difficulties like occlusion: If (after the optional erosion) nothing is left of the last assumed foreground mask, we assume that the object of interest is lost and do not apply any online updates until the network again finds a nonempty foreground mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets. For objectness pretraining (cf . Section 3), we used the 1,464 training images of the PASCAL VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref> plus the additional annotations provided by Hariharan et al. <ref type="bibr" target="#b15">[16]</ref>, leading to a total of 10,582 training images with 20 classes, which we all mapped to a single foreground class. For video object segmentation (VOS), we conducted most experiments on the recently introduced DAVIS dataset <ref type="bibr" target="#b33">[34]</ref>, which consists of 50 short full-HD video sequences, from which 30 are taken for training and 20 for validation. Consistent with most prior work, we conduct all experiments on the subsampled version with a resolution of 854 × 480 pixels. In order to show that our method generalizes, we also conducted experiments on the YouTube-Objects <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> dataset for VOS, consisting of 126 sequences.</p><p>Experimental Setup. We pretrain on PASCAL and DAVIS, for 10 epochs each. For the baseline one-shot approach, we found 50 update steps on the first frame with a learning rate of 3 · 10 −6 to work well. For simplicity, we used a mini-batch size of only one image. Since DAVIS only has a training and a validation set, we tuned all hyperparameters on the training set of 30 sequences using three-fold cross validation, i.e. 20 training sequences are used for training and 10 for validation for each fold. As is standard practice, we augmented the training data by random flipping, scaling with a factor uniformly sampled from [0.7, 1.3], and gamma augmentations <ref type="bibr" target="#b35">[36]</ref>.</p><p>For evaluation, we used the Jaccard index, i.e. the mean intersection-over-union (mIoU) between the predicted foreground masks and the ground truth masks. Results for additional evaluation measures suggested by Perazzi et al. <ref type="bibr" target="#b33">[34]</ref> are shown in the supplementary material. We noticed that, especially for fine-tuning on the first frame, the random augmentations introduce non-negligible variations in the results. Hence, for these experiments, we conducted three runs and report mean and standard deviation values. All experiments were performed with our TensorFlow <ref type="bibr" target="#b0">[1]</ref> based implementation, which we will make available together with pretrained models at https://www.vision.rwth-aachen.de/ software/OnAVOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Systems</head><p>Effect of Pretraining Steps. Starting from the base network (cf . Section 3) our full baseline system (i.e. without adaptation) includes a first pretraining step on PASCAL for objectness, then on the training sequences of DAVIS, and finally a one-shot fine-tuning on the first frame. Each of these three steps can be enabled or disabled individually. <ref type="table">Table 1</ref> shows the results on DAVIS for all resulting combinations. As can be seen, each of these steps is useful since removing any step always deteriorates the results.</p><p>The base network was trained for a different task than binary segmentation and thus a new output layer needs to be learned at the same time as fine-tuning the rest of the network. Without pretraining on either PASCAL or DAVIS, the randomly initialized output layer is learned only from the first frame of the target sequence, which leads to a largely degraded performance of only 65.2% mIoU. However, when either PASCAL or DAVIS is used for pretraining, the result is greatly improved to 77.6% mIoU and 78.0% mIoU, respectively.  <ref type="table">Table 1</ref>: Effect of (pre-)training steps on the DAVIS validation set. As can be seen, each of the three training steps are useful. The objectness pretraining step on PASCAL significantly improves the results.</p><p>While both results are very similar, it can be seen that PASCAL and DAVIS do provide complementary information, since using both datasets together further improves the result to 80.3%. We argue that the relatively large PASCAL dataset is useful for learning general objectness, while the limited amount of DAVIS data is useful to adapt to the characteristics (e.g. relatively high image quality) of the data of DAVIS, which provides an advantage for evaluating on DAVIS sequences.</p><p>Interestingly, even without looking at the segmentation mask of the first frame, i.e. in the unsupervised setup, we already obtain a result of 72.7% mIoU; slightly better than the current best unsupervised method FusionSeg <ref type="bibr" target="#b19">[20]</ref>, which obtains 70.7% mIoU on the DAVIS validation set 1 using objectness and optical flow as an additional cue.</p><p>Comparison to OSVOS. Without including their boundary snapping post-processing step, OSVOS achieves a result of 77.4% mIoU on DAVIS. Our system without objectness pretraining on PASCAL is directly comparable to this result and achieves 78.0% mIoU. We attribute this moderate improvement to the more recent network architecture which we adopted. Including PASCAL for objectness pretraining improves this result by further 2.3% to 80.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Online Adaptation</head><p>Hyperparameter Study. As described in Section 4, OnAVOS involves relatively many hyperparameters. After some coarse manual tuning on the DAVIS training set, we found α = 0.97, β = 0.05, d = 220, n online = 15, n curr = 3 to work well. While the initial 50 update steps on the first frame are performed with a learning rate of 3 · 10 −6 , it proved useful to use a different learning rate λ = 10 −5 for the online updates on the current and the first frame. Starting from these values as the operating point, we conducted a more detailed study by changing one hyperparameter at a time, while keeping the others constant. We found that OnAVOS is not very sensitive to the choice of most hyperparameters and each configuration we tried performed better than the non-adapted baseline and we achieved only small improvements compared to the operating point (detailed plots are shown in the supplementary material). To avoid overfitting to the small DAVIS training set, we kept the values from the operating point for all further experiments.   Ablation Study. <ref type="table" target="#tab_2">Table 2</ref> shows the results of the proposed online adaptation scheme and multiple variants, where parts of the algorithm are disabled, on the DAVIS validation set. Using the full method, we obtain an mIoU score of 82.8%. When disabling all adaptation steps, the performance significantly degrades to 80.3%, which demonstrates the effectiveness of the online adaptation method. The table further shows that negative training examples are more important than positive ones. If we do not mix in the first frame during online updates, the result is significantly degraded to 69.1% due to drift.</p><p>Timing Information. For the initial fine-tuning stage on the first frame, we used 50 update steps. Including the time for the forward pass for all further frames, this leads to a total runtime of around 90 seconds per sequence (corresponding to roughly 1.3 seconds per frame) of the DAVIS validation set using an NVIDIA Titan X (Pascal) GPU. When using online adaptation with n online = 15, the runtime increases to around 15 minutes per sequence (corresponding to roughly 13 seconds per frame). However, our hyperparameter analysis revealed that this runtime can be significantly decreased by reducing n online without much loss of accuracy. Note that for best results, OSVOS used a higher number of update steps on the first frame and needs about 10 minutes per sequence (corresponding to roughly 9 seconds per frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to State of the Art</head><p>Current state of the art methods use post-processing steps such as boundary snapping <ref type="bibr" target="#b6">[7]</ref>, or conditional random field (CRF) smoothing <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> to improve the contours. In order to compare with them, we included per-frame post-processing using DenseCRF <ref type="bibr" target="#b25">[26]</ref>. This might be especially useful since our network only provides one output for each 8 × 8 pixel block. Additionally, we added data augmentations during test time. To this end, we created 10 variants of each test image by random flipping, zooming, and gamma augmentations, and averaged the posterior probabilities over all 10 images. In order to demonstrate the generalization ability of OnAVOS and since there is no separate training set for YouTube-Objects, we conducted our experiments on this dataset using the same hyperparameter values as for DAVIS, including the CRF parameters. Additionally, we omitted the pretraining step on DAVIS. Note that for YouTube-Objects, the evaluation protocols in prior publications sometimes differed by not including frames in which the object of interest is not present <ref type="bibr" target="#b23">[24]</ref>. Here, we report results following the DAVIS evaluation protocol, i.e. including these frames, consistent with Khoreva et al. <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows the effect of our post-processing steps and compares our results on DAVIS and YouTube-Objects to other methods. Note that the effect of the test time augmentations is stronger when combined with online adaptation. We argue that this is because in this case, the augmentations do not only directly improve the end result as a postprocessing step, but they also deliver better adaptation targets. On DAVIS, we achieve an mIoU of 85.7% which is, to the best of our knowledge significantly higher than any previously published result. Compared to OSVOS, this is an improvement of almost 6%. On YouTube-Objects, we achieve an mIoU of 77.4%, which is also a significant improvement over the second best result obtained by LucidTracker with 76.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have proposed OnAVOS, which builds on the OSVOS approach. We have demonstrated that the inclusion of an objectness pretraining step and our online adaptation scheme for semi-supervised video object segmentation are highly effective. We have further shown that our online adaptation scheme is robust against choices of hyperparameters and generalizes to another dataset. We expect that, in the future, more methods will adopt adaptation schemes which make them more robust against large changes in appearance. For future work, we plan to explicitly incorporate temporal context information into our method.   <ref type="table" target="#tab_7">Table 5</ref> shows a more detailed evaluation on the DAVIS validation set using the evaluation measures suggested by Perazzi et al. <ref type="bibr" target="#b33">[34]</ref>. The measures used here are the Jaccard index J , defined as the mean intersection-over-union (mIoU) between the predicted foreground masks and the ground truth masks; the contour accuracy measure F, which measures how well the segmentation boundaries agree; and the temporal stability measure T , which measures the consistency of the predicted masks over time. For more details of these measures, we refer the interested reader to Perazzi et al. <ref type="bibr" target="#b33">[34]</ref>. Note that the results for additional measures for LucidTracker <ref type="bibr" target="#b23">[24]</ref> are missing since they are only reported averaged over all 50 sequences of DAVIS and not on the validation set. The table shows that each evaluation measure is significantly improved by the proposed online adaptation scheme. OnAVOS obtains the best mean results for all three measures. It is surprising that our result for the temporal stability T is better than the result by MaskTrack <ref type="bibr" target="#b34">[35]</ref>, although in contrast to our method, they explicitly incorporate temporal context by propagating masks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A More Comprehensive Comparison to Other Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Evaluation Measures for DAVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Per-Sequence Results for DAVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameter Study on DAVIS</head><p>As described in the main paper, we found α = 0.97, β = 0.05, d = 220, n online = 15, n curr = 3, λ = 10 −5 and 15 for the erosion size to work well on DAVIS. Starting from these values as the operating point, we conducted a more detailed hyperparameter study by changing one hyperparameter at a time, while keeping all others constant (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>The plots show that the performance of OnAVOS is in general very stable with respect to the choice of most of its hyperparameters and for every configuration we tried, the result was better than the un-adapted baseline (the dashed line in the plots). The single most important hyperparameter is the online learning rate λ , which is common for deep learning approaches. The online loss scale β and the positive threshold α have a moderate influence on performance, while changing the distance threshold d and the number of steps n online and n curr in a reasonable range only leads to minor changes in accuracy. For the erosion The dashed line marks the un-adapted baseline. The plots show that overall our method is very robust against the exact choice of hyperparameters, except for the online learning rate λ . The standard deviations estimated by three runs are shown as error bars. In some cases, including the operating point, the estimated standard deviation is so small that it is hardly visible. size, the optimum is achieved at 1, i.e. when no erosion is applied. This result suggests that the erosion operation is not helpful for DAVIS. The plots show that there is still some potential for improving the results by further tuning the hyperparameters. However, this study was meant as a characterization of our method rather than a systematic tuning.</p><p>The generalizability and the robustness of OnAVOS with respect to the choice of hyperparameters is further confirmed by the experiments on YouTube-Objects, which used the same hyperparameter settings as on DAVIS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results on two sequences of the DAVIS validation set. The second row shows the pixels selected as positive (red) and negative (blue) training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Influence of online adaptation hyperparameters on the DAVIS training set. The blue circle marks the operating point, based on which one parameter is changed at a time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>No first frame during online adaptation 69.1 ± 0.2</figDesc><table><row><cell>Method</cell><cell>mIoU [%]</cell></row><row><cell>No adaptation</cell><cell>80.3 ± 0.4</cell></row><row><cell>Full adaptation</cell><cell>82.8 82.8 82.8 ± 0.5</cell></row><row><cell>Only negatives</cell><cell>82.4 ± 0.3</cell></row><row><cell>Only positives</cell><cell>81.6 ± 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Online adaptation ablation experiments on the DAVIS validation set. As can be seen, mixing in the first frame during online updates is essential, and negative examples are more important than positive ones.</figDesc><table><row><cell>Method</cell><cell>DAVIS mIoU [%]</cell><cell>YouTube-Objects mIoU [%]</cell></row><row><cell>OnAVOS (ours), no adaptation</cell><cell>80.3 ± 0.4</cell><cell>76.1 ± 1.3</cell></row><row><cell>+CRF</cell><cell>81.7 ± 0.5</cell><cell>76.4 ± 0.2</cell></row><row><cell>+CRF +Test time augmentations</cell><cell>81.7 ± 0.2</cell><cell>76.6 ± 0.1</cell></row><row><cell cols="2">OnAVOS (ours), online adaptation 82.8 ± 0.5</cell><cell>76.8 ± 0.1</cell></row><row><cell>+CRF</cell><cell>84.3 ± 0.5</cell><cell>77.2 ± 0.2</cell></row><row><cell>+CRF +Test time augmentations</cell><cell>85.7 85.7 85.7 ± 0.6</cell><cell>77.4 77.4 77.4 ± 0.2</cell></row><row><cell>OSVOS [7]</cell><cell>79.8</cell><cell>72.5</cell></row><row><cell>MaskTrack [35]</cell><cell>79.7</cell><cell>72.6</cell></row><row><cell>LucidTracker [24]  †</cell><cell>80.5</cell><cell>76.2</cell></row><row><cell>VPN [22]</cell><cell>75.0</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the state of the art on the DAVIS validation set and the YouTube-Objects dataset. †: Concurrent work only published on arXiv. More results are shown in the supplementary material.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 shows</head><label>4</label><figDesc>a more comprehensive comparison of our results to the results obtained by other methods.</figDesc><table><row><cell>Method</cell><cell>DAVIS mIoU [%]</cell><cell>YouTube-Objects mIoU [%]</cell></row><row><cell>OnAVOS (ours), no adaptation</cell><cell>81.7 ± 0.2</cell><cell>76.6 ± 0.1</cell></row><row><cell cols="2">OnAVOS (ours), online adaptation 85.7 85.7 85.7 ± 0.6</cell><cell>77.4 77.4 77.4 ± 0.2</cell></row><row><cell>OSVOS [7]</cell><cell>79.8</cell><cell>72.5</cell></row><row><cell>MaskTrack [35]</cell><cell>79.7</cell><cell>72.6</cell></row><row><cell>LucidTracker [24]  †</cell><cell>80.5</cell><cell>76.2</cell></row><row><cell>VPN [22]</cell><cell>75.0</cell><cell>-</cell></row><row><cell>FCP [33]</cell><cell>63.1</cell><cell>-</cell></row><row><cell>BVS [30]</cell><cell>66.5</cell><cell>59.7</cell></row><row><cell>OFL [41]</cell><cell>71.1</cell><cell>70.1</cell></row><row><cell>STV [42]</cell><cell>73.6</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison to other methods on the DAVIS validation set and the YouTube-Objects dataset. Note that MaskTrack<ref type="bibr" target="#b34">[35]</ref> and LucidTracker<ref type="bibr" target="#b23">[24]</ref> report results on DAVIS for all sequences including the training set, but here we show their results for the validation set only. †: Concurrent work only published on arXiv.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 shows</head><label>6</label><figDesc>mIoU results for each of the 20 sequences of the DAVIS validation set. On 18 out of 20 sequences, OnAVOS obtains either the best or the second best result.</figDesc><table><row><cell></cell><cell>Measure</cell><cell cols="2">OnAVOS (ours) Un-adapted Adapted</cell><cell cols="2">OSVOS [7] MaskTrack [35]</cell><cell>LucidTracker [24]</cell></row><row><cell></cell><cell>mean ↑</cell><cell>81.7 ± 0.2</cell><cell>85.7 85.7 85.7 ± 0.6</cell><cell>79.8</cell><cell>79.7</cell><cell>80.5</cell></row><row><cell>J</cell><cell>recall ↑</cell><cell>92.2 ± 0.6</cell><cell>95.4 95.4 95.4 ± 0.8</cell><cell>93.6</cell><cell>93.1</cell><cell>-</cell></row><row><cell></cell><cell>decay ↓</cell><cell>11.9 ± 0.3</cell><cell>7.1 7.1 7.1 ± 1.7</cell><cell>14.9</cell><cell>8.9</cell><cell>-</cell></row><row><cell></cell><cell>mean ↑</cell><cell>81.1 ± 0.2</cell><cell>84.2 84.2 84.2 ± 0.8</cell><cell>80.6</cell><cell>75.4</cell><cell>-</cell></row><row><cell>F</cell><cell>recall ↑</cell><cell>88.2 ± 0.3</cell><cell>88.7 ± 1.3</cell><cell>92.6 92.6 92.6</cell><cell>87.1</cell><cell>-</cell></row><row><cell></cell><cell>decay ↓</cell><cell>11.2 ± 0.5</cell><cell>7.8 7.8 7.8 ± 1.8</cell><cell>15.0</cell><cell>9.0</cell><cell>-</cell></row><row><cell>T</cell><cell>mean ↓</cell><cell>27.3 ± 2.2</cell><cell>18.5 18.5 18.5 ± 0.1</cell><cell>37.6</cell><cell>21.8</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Additional evaluation measures on the DAVIS validation set. Best and second best results are highlighted with bold and italic fonts, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method, mIoU [%]</cell><cell></cell></row><row><cell>Sequence</cell><cell cols="2">OnAVOS (ours) Un-adapted Adapted</cell><cell cols="2">OSVOS [7] MaskTrack [35]</cell><cell>LucidTracker [24]</cell></row><row><cell>blackswan</cell><cell>96.1 ± 0.1</cell><cell>96.2 96.2 96.2 ± 0.1</cell><cell>94.2</cell><cell>90.3</cell><cell>95.0</cell></row><row><cell>bmx-trees</cell><cell>48.2 ± 0.8</cell><cell>57.0 ± 1.0</cell><cell>55.5</cell><cell>57.5 57.5 57.5</cell><cell>55.0</cell></row><row><cell>breakdance</cell><cell>62.6 ± 4.2</cell><cell>73.6 ± 3.8</cell><cell>70.8</cell><cell>76.1</cell><cell>87.2 87.2 87.2</cell></row><row><cell>camel</cell><cell>84.6 ± 0.1</cell><cell>85.5 ± 0.1</cell><cell>85.1</cell><cell>80.1</cell><cell>94.3 94.3 94.3</cell></row><row><cell>car-roundabout</cell><cell>86.5 ± 0.2</cell><cell>97.5 97.5 97.5 ± 0.0</cell><cell>95.3</cell><cell>96.0</cell><cell>96.0</cell></row><row><cell>car-shadow</cell><cell>94.1 ± 0.1</cell><cell>96.8 96.8 96.8 ± 0.1</cell><cell>93.7</cell><cell>93.5</cell><cell>90.3</cell></row><row><cell>cows</cell><cell>95.4 95.4 95.4 ± 0.0</cell><cell>95.4 95.4 95.4 ± 0.0</cell><cell>94.6</cell><cell>88.2</cell><cell>93.1</cell></row><row><cell>dance-twirl</cell><cell>78.4 ± 0.7</cell><cell>85.6 ± 1.0</cell><cell>67.0</cell><cell>84.4</cell><cell>88.6 88.6 88.6</cell></row><row><cell>dog</cell><cell>95.6 95.6 95.6 ± 0.1</cell><cell>95.6 95.6 95.6 ± 0.1</cell><cell>90.7</cell><cell>90.8</cell><cell>95.0</cell></row><row><cell>drift-chicane</cell><cell>87.4 ± 0.5</cell><cell>89.2 89.2 89.2 ± 0.2</cell><cell>83.5</cell><cell>86.2</cell><cell>1.4</cell></row><row><cell>drift-straight</cell><cell>81.3 ± 5.6</cell><cell>93.7 93.7 93.7 ± 0.9</cell><cell>67.6</cell><cell>56.0</cell><cell>79.9</cell></row><row><cell>goat</cell><cell>90.8 ± 0.1</cell><cell>91.4 91.4 91.4 ± 0.1</cell><cell>88.0</cell><cell>84.5</cell><cell>88.9</cell></row><row><cell>horsejump-high</cell><cell>89.3 ± 0.3</cell><cell>90.1 90.1 90.1 ± 0.0</cell><cell>78.0</cell><cell>81.8</cell><cell>87.1</cell></row><row><cell>kite-surf</cell><cell>70.1 70.1 70.1 ± 1.0</cell><cell>69.1 ± 0.1</cell><cell>68.6</cell><cell>60.0</cell><cell>64.6</cell></row><row><cell>libby</cell><cell>87.1 ± 1.0</cell><cell>88.6 88.6 88.6 ± 0.1</cell><cell>80.8</cell><cell>77.5</cell><cell>85.5</cell></row><row><cell>motocross-jump</cell><cell>89.7 89.7 89.7 ± 0.2</cell><cell>70.4 ± 11.9</cell><cell>81.6</cell><cell>68.3</cell><cell>75.1</cell></row><row><cell>paragliding-launch</cell><cell>64.6 64.6 64.6 ± 0.1</cell><cell>64.3 ± 0.1</cell><cell>62.5</cell><cell>62.1</cell><cell>63.7</cell></row><row><cell>parkour</cell><cell>92.4 ± 0.2</cell><cell>93.6 93.6 93.6 ± 0.0</cell><cell>85.6</cell><cell>88.2</cell><cell>93.2</cell></row><row><cell>scooter-black</cell><cell>64.8 ± 7.1</cell><cell>91.3 91.3 91.3 ± 0.1</cell><cell>71.1</cell><cell>82.4</cell><cell>86.5</cell></row><row><cell>soapbox</cell><cell>74.0 ± 4.6</cell><cell>89.8 ± 1.2</cell><cell>81.2</cell><cell>89.9</cell><cell>90.5 90.5 90.5</cell></row><row><cell>mean</cell><cell>81.7 ± 0.2</cell><cell>85.7 85.7 85.7 ± 0.6</cell><cell>79.8</cell><cell>79.7</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Per-sequence results on the DAVIS validation set. Best and second best results are highlighted with bold and italic fonts, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">VOIGTLAENDER, LEIBE: ONLINE ADAPTATION FOR VIDEO OBJECT SEGMENTATION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In FusionSeg<ref type="bibr" target="#b19">[20]</ref>, the result for all sequences including the training set is reported, but here we calculated the average only over the validation sequences for better comparability</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The work in this paper is funded by the EU project STRANDS (ICT-2011-600623) and the ERC Starting Grant project CV-SUPER (ERC-2012-StG-307432). We would like to thank István Sárándi, Jörg Stückler, Lucas Beyer, and Aljoša Ošep for helpful discussions and proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01731</idno>
		<title level="m">A deep convolutional neural network for background subtraction</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video snapcut: Robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<idno>70:1-70:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic color flow: A motion-adaptive color model for object segmentation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep background subtraction with scenespecific convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Systems, Signals and Image Proc</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online learning for fast segmentation of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zografos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zisserman. The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Comput. Learning Theory</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05349</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Pixel objectness. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeptrack: Learning discriminative feature representations online for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating tracking with fine object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Papoutsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="771" to="785" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wenguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shenjian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08634</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-Y.</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interactive deep learning method for segmenting moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
