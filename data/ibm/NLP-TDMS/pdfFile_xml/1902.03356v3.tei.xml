<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Curvature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
							<email>eunbyung@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
							<email>joliva@cs.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Curvature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the modelagnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model's parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task specific techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.</p><p>Given the sensitivity to the inner-loop optimization algorithm, second order optimization methods (or preconditioning the gradients) are worth considering. They have been extensively studied and have shown their practical benefits in terms of faster convergence rates <ref type="bibr" target="#b30">[31]</ref>, an important aspect of few-shot learning. In addition, the problems of computational and spatial complexity for training deep networks can be effectively handled thanks to recent approximation techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>. Nevertheless, there are issues with using second order methods in its current form as an inner loop optimizer in the meta-learning framework. First, they do not usually consider generalization performance. They compute local curvatures with training losses and move along the curvatures as far as possible. It can be very harmful, especially in the few-shot learning setup, because it can overfit easily and quickly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite huge progress in artificial intelligence, the ability to quickly learn from few examples is still far short of that of a human. We are capable of utilizing prior knowledge from past experiences to efficiently learn new concepts or skills. With the goal of building machines with this capability, learning-to-learn or meta-learning has begun to emerge with promising results.</p><p>One notable example is model-agnostic meta-learning (MAML) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>, which has shown its effectiveness on various few-shot learning tasks. It formalizes learning-to-learn as meta objective function and optimizes it with respect to a model's initial parameters. Through the meta-training procedure, the resulting model's initial parameters become a very good prior representation and the model can quickly adapt to new tasks or skills through one or more gradient steps with a few examples. Although this end-to-end approach, using standard gradient descent as the inner optimization algorithm, was theoretically shown to approximate any learning algorithm <ref type="bibr" target="#b9">[10]</ref>, recent studies indicate that the choice of the inner-loop optimization algorithm affects performance. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In this work, we propose to learn a curvature for better generalization and faster model adaptation in the meta-learning framework, we call meta-curvature. The key intuition behind MAML is that there are some representations are broadly applicable to all tasks. In the same spirit, we hypothesize that there are some curvatures that are broadly applicable to many tasks. Curvatures are determined by the model's parameters, network architectures, loss functions, and training data. Assuming new tasks are distributed from the similar distribution as meta-training distribution, there may exist common curvatures that can be obtained through meta-training procedure. The resulting meta-curvatures, coupled with the simultaneously meta-trained model's initial parameters, will transform the gradients such that the updated model has better performance on new tasks with fewer gradient steps. In order to efficiently capture the dependencies between all gradient coordinates for large networks, we design a multilinear mapping consisting of a series of tensor-products to transform the gradients. It also considers layer specific structures, e.g. convolutional layers, to effectively reflects our inductive bias. In addition, meta-curvature can be easily implemented (simply transform the gradients right before passing through the optimizers) and can be plugged into existing meta-learning frameworks like MAML without additional, burdensome higher-order gradients.</p><p>We demonstrate the effectiveness of our proposed method on the few-shot learning tasks done by <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9]</ref>. We evaluated our methods on few-shot regression and few-shot classification tasks over Omniglot <ref type="bibr" target="#b18">[19]</ref>, miniImagenet <ref type="bibr" target="#b46">[47]</ref>, and tieredImagnet <ref type="bibr" target="#b34">[35]</ref> datasets. Experimental results show significant improvements on other MAML variants on all few-shot learning tasks. In addition, MC's simple gradient transformation outperformed other more complicated state-of-the-art methods that include additional bells and whistles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tensor Algebra</head><p>We review basics of tensor algebra that will be used to formalize the proposed method. We refer the reader to <ref type="bibr" target="#b16">[17]</ref> for a more comprehensive review. Throughout the paper, tensors are defined as multidimensional arrays and denoted by calligraphic letters, e.g. N th-order tensor, X ∈ R I1×I2×···×I N . Matrices are second-order tensors and denoted by boldface uppercase, e.g. X ∈ R I1×I2 .</p><p>Fibers: Fibers are a higher-order generalization of matrix rows and columns. A matrix column is a mode-1 fiber and a matrix row is a mode-2 fiber. The mode-1 fibers of a third order tensor X are denoted as X :,j,k , where a colon is used to denote all elements of a mode.</p><p>Tensor unfolding: Also known as flattening (reshaping) or matricization, is the operation of arranging the elements of an higher-order tensors into a matrix. The mode-n unfolding of a N th-order tensor X ∈ R I1×I2×···×I N , arranges the mode-n fibers to be the columns of the matrix, denoted by X [n] ∈ R In×I M , where I M = k =n I k . The elements of the tensor, X i1,i2,...,i N are mapped to</p><formula xml:id="formula_0">X [n]in,j , where j = 1 + N k =n,k=1 (i k − 1)J k , with J k = k−1 m=1,m =n I m .</formula><p>n-mode product: It defines the product between tensors and matrices. The n-mode product of a tensor X ∈ R I1×I2×···×I N with a matrix M ∈ R J×In is denoted by X × n M and computed as</p><formula xml:id="formula_1">(X × n M) i1,...,in−1,j,in+1,...,i N = In in=1 X i1,i2,...,i N M j,in .<label>(1)</label></formula><p>More concisely, it can be written as (X × n M) [n] = MX [n] ∈ R I1×···×In−1×J×In+1×···×I N . Despite cumbersome notation, it is simply n-mode unfolding (reshaping) followed by matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model-Agnostic Meta-Learning (MAML)</head><p>MAML aims to find a transferable initialization (a prior representation) of any model such that the model can adapt quickly from the initialization and produce good generalization performance on new tasks. The meta-objective is defined as validation performance after one or few step gradient updates from the model's initial parameters. By using gradient descent algorithms to optimize the meta-objective, its training algorithm usually takes the form of nested gradient updates: inner updates for model adaptation to a task and outer-updates for the model's initialization parameters. Formally,</p><formula xml:id="formula_2">min θ E τi [L τi val θ − α∇L τi tr (θ) inner udpate ],<label>(2)</label></formula><p>where L τi val (·) denotes a loss function for a validation set of a task τ i , and L τi tr (·) for a training set, or L tr (·) for brevity. The inner update is defined as a standard gradient descent with fixed learning rate α. For conciseness, we assume as single adaptation step, but it can be easily extended to more steps. For more details, we refer to <ref type="bibr" target="#b8">[9]</ref>. Several variations of inner update rules were suggested. Meta-SGD <ref type="bibr" target="#b21">[22]</ref> suggested coordinate-wise learning rates, θ − α • ∇L tr , where α is the learnable parameters and • is element wise product. Recently, <ref type="bibr" target="#b3">[4]</ref> proposed a learnable learning rate per each layers for more flexible model adaptation. To alleviate computational complexity, <ref type="bibr" target="#b29">[30]</ref> suggested an algorithm that do not require higher order gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Second order optimization</head><p>The biggest motivation of second order methods is that first-order optimization such as standard gradient descent performs poorly if the Hessian of a loss function is ill-conditioned, e.g. a long narrow valley loss surface. There are a plethora of works that try to accelerate gradient descent by considering local curvatures. Most notably, the update rules of Newton's method can be written as θ − αH −1 ∇L tr , with Hessian matrix H and a step size α <ref type="bibr" target="#b30">[31]</ref>. Every step, it minimizes a local quadratic approximation of a loss function, and the local curvature is encoded in the Hessian matrix. Another promising approach, especially in neural network literature, is natural gradient descent <ref type="bibr" target="#b1">[2]</ref>. It finds a steepest descent direction in distribution space rather than parameter space by measuring KL-divergence as a distance metric. Similar to Newton's method, it preconditions the gradient with the Fisher information matrix and a common update rule is θ − αF −1 ∇L tr . In order to mitigate computational and spatial issues for large scale problems, several approximation techniques has been proposed, such as online update methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref>, Kronecker-factored approximations <ref type="bibr" target="#b23">[24]</ref>, and diagonal approximations of second order matrices <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Meta-Curvature</head><p>We propose to learn a curvature along with the model's initial parameters simultaneously via the meta-learning process. The goal is that the meta-learned curvature works collaboratively with the meta-learned model's initial parameters to produce good generalization performance on new tasks with fewer gradient steps. In this work, we focus on learning a meta-curvature and its efficient forms to scale large networks. We follow the meta-training algorithms suggested in <ref type="bibr" target="#b8">[9]</ref> and the proposed method can be easily plugged in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>We begin with the hypothesis that there are broadly applicable curvatures to many tasks. In training a neural network with a loss function, local curvatures are determined by the model's parameters, the network architecture, the loss function, and training data. Since new tasks are sampled from the same or similar distributions and all other factors are fixed, it is intuitive idea that there may exist some curvatures found via meta-training that can be effectively applied to the new tasks. Throughout the meta-training, we can observe how the gradients affect the validation performance and use those experiences to learn how to transform or correct the gradient from the new task.</p><p>We take a learning approach because existing curvature estimations do not consider generalization performance, e.g. Hessian and the Fisher-information matrix. The local curvatures are approximated with only current training data and loss functions. Therefore, these methods may end up converging fast to a poor local minimum. This is especially true when we have few training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>First, we present a simple and efficient form of the meta-curvature computation through the lens of tensor algebra. Then, we present a matrix-vector product view to provide intuitive idea of the connection to the second order matrices. Lastly, we discuss the relationships to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tensor product view</head><p>We consider neural networks as our models. With a slight abuse of notation, let the model's parameters W l ∈ R C l out ×C l in ×d l and its gradients of loss function G l ∈ R C l out ×C l in ×d l , at each layers l. To avoid <ref type="figure">Figure 1</ref>: An example of meta-curvature computational illustration with G ∈ R 2×3×d . Top: tensor algebra view, Bottom: matrix-vector product view. cluttered notation, we will omit the superscript l. We choose superscripts and dimensions with 2D convolutional layers in mind, but the method can be easily extended to higher dimension convolutional layers or other layers that consists of higher dimension parameters. C out , C in , and d are the number of output channels, the number of input channels, and the filter size respectively. d is height × width in convolutional layers and 1 in fully connected layers. We also define meta-curvature matrices,</p><formula xml:id="formula_3">M o ∈ R Cout×Cout , M i ∈ R Cin×Cin , and M f ∈ R d×d</formula><p>. Now a meta-curvature function takes a multidimensional tensor as an input and has all meta-curvature matrices as learnable parameters:</p><formula xml:id="formula_4">MC(G) = G × 3 M f × 2 M i × 1 M o .<label>(3)</label></formula><p>Figure 1 (top) shows an example of computational illustration with an input tensor G ∈ R 2×3×d . First, it performs linear transformations for all 3-mode fibers of G. In other words, M f captures the parameter dependencies between the elements within a 3-mode fiber, e.g. all gradient elements in a channel of a convolutional filter. Secondly, the 2-mode product models the dependencies between 3mode fibers computed from the previous stage. All 3-mode fibers are updated by linear combinations of other 3-mode fibers belonging to the same output channel (linear combinations of 3-mode fibers in a convolutional filter). Finally, the 1-mode product is performed in order to model the dependencies between the gradients of all convolutional filters. Similarly, the gradients of all convolutional filters are updated by linear combinations of gradients of other convolutional filters.</p><p>A useful property of n-mode products is the fact that the order of the multiplications is irrelevant for distinct modes in a series of multiplications. For example,</p><formula xml:id="formula_5">G × 3 M f × 2 M i × 1 M o = G × 1 M o × 2 M i × 3 M f .</formula><p>Thus, the proposed method indeed examines the dependencies of the elements in the gradient all together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Matrix-vector product view</head><p>We can also view the proposed meta-curvature computation as a matrix-vector product analogous to that from other second order methods. Note that this is for the purpose of intuitive illustration and we cannot compute or maintain this large matrices for large deep networks. We can expand the meta-curvature matrices as follows.</p><formula xml:id="formula_6">M o = M o ⊗ I Cin ⊗ I d , M i = I Cout ⊗ M i ⊗ I d , M f = I Cout ⊗ I Cin ⊗ M f ,<label>(4)</label></formula><p>where ⊗ is the Kronecker product, I k is k dimensional identity matrix, and the three expanded matrices are all same size M o , M i , M f ∈ R CoutCind×CoutCind . Now we can transform the gradients with the meta-curvature as vec(MC(G)) = M mc vec(G),</p><p>where </p><formula xml:id="formula_8">M mc = M o M i M f . The expanded matrices satisfy commutative property, e.g. M o M i M f = M f M i M o ,</formula><formula xml:id="formula_9">= M o ⊗ M i ⊗ M f , but this is non- commutative, M o ⊗ M i ⊗ M f = M f ⊗ M i ⊗ M o .</formula><p>Figure 1 (bottom) shows a computational illustration. M f vec(G), which is equivalent computation to G × 3 M f , can be interpreted as a giant matrix-vector multiplication with block diagonal matrix, where each block shares same meta-curvature matrix M f . It resembles the block diagonal approximation strategies in some second-order methods for training deep networks, but as we are interested in learning meta-curvature matrices, no approximation is involved. And matrix-vector product with M o and M i are used to capture inter-parameter dependencies and are computationally equivalent to 2-mode and 3-mode products of Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relationship to other methods</head><p>Tucker decomposition <ref type="bibr" target="#b16">[17]</ref> decomposes a tensor into low rank cores with projection factors and aims to closely reconstruct the original tensor. We maintain full rank gradient tensors, however, and our main goal is to transform the gradients for better generalization. <ref type="bibr" target="#b17">[18]</ref> proposed to learn the projection factors in Tucker decomposition for fully connected layers in deep networks. Again, their goal was to find the low rank approximations of fully connected layers for saving computational and spatial cost.</p><p>Kronecker-factored Approximate Curvature (K-FAC) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref> approximates the Fisher matrix by the Kronecker product, e.g. F ≈ A ⊗ G, where A is computed from the activation of input units and G is computed from the gradient of output units. Its main goal is to approximate the Fisher such that matrix vector products between its inversion and the gradient can be computed efficiently. However, we found that maintaining A ∈ R Cind×Cind was quite expensive both computationally and spatially even for smaller networks. In addition, when we applied this factorization scheme to meta-curvature, it tends to easily overfit to meta-training set. On the contrary, we maintain two separated matrices, M i ∈ R Cin×Cin and M f ∈ R d×d , which allows us to avoid overfitting and heavy computation. More importantly, we learn meta-curvature matrices to improve generalization instead of directly computing them from the activation and the gradient of training loss. Also, we do not require expensive matrix inversions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Meta-training</head><p>We follow a typical meta-training algorithm and initialize all meta-curvature matrices as identity matrices so that the gradients do not change at the beginning. We used the ADAM <ref type="bibr" target="#b15">[16]</ref> optimizer for the outer loop optimization and update the model's initial parameters and meta-curvatures simultaneously. We provide the details of algorithm in appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>In this section, we will explore how a meta-trained matrix M mc , or M for brevity, can operate for better generalization. Let us take the gradient of meta-objective w.r.t M for a task τ i . With the inner update rule θ τi (M) = θ − αM∇ θ L τi tr (θ), and by applying chain rule,</p><formula xml:id="formula_10">∇ M L τi val (θ τi (M)) = −α∇ θ τ i L τi val (θ τi )∇ θ L τi tr (θ) ,<label>(6)</label></formula><p>where θ τi is the parameter for the task τ i after the inner update. It is the outer product between the gradients of validation loss and training loss. Note that there is a significant connection to the Fisher information matrix. For a task τ i , if we define the loss function as negative log likelihood, e.g. a supervised classification task L τi (θ) = E (x,y)∼p(τi) [− log θ p(y|x)], then the empirical Fisher can be defined as</p><formula xml:id="formula_11">F = E (x,y)∼p(τi) [∇ θ log θ p(y|x)∇ θ log θ p(y|x) ].</formula><p>There are three clear distinctions. First, the training and validation sets are treated separately in the meta-gradient ∇ M L τi val , while the empirical Fisher is computed with only training set (validation set is not available during training). Secondly, the gradient of the validation set is evaluated at new parameters θ τi after the inner update in the meta-gradient. Finally, the Fisher is positive semi-definite by construction, but it is not the case for the meta-gradient. This is an attractive property since it guarantees that the transformed gradient is always a descent direction. However, we mainly care about generalization performance in this work. Hence, we rather not force this property in this work, but leave it for future work. Now let us consider what the meta-gradient can do for good generalization performance. Given a fixed point θ and a meta training set T = {τ i }, standard gradient descent from an initialization M, gives the following update.</p><formula xml:id="formula_12">M T = M − β |T | i=1 ∇ M L τi val (θ τi (M)) = M + αβ |T | i=1 ∇ θ L τi val (θ τi (M))∇ θ L τi tr (θ) ,<label>(7)</label></formula><p>where α and β are fixed inner/outer learning rates respectively. Here, we assume a standard gradient descent for simplicity. But the argument extends to other advanced gradient algorithms, such as momentum and ADAM.</p><p>We apply M T to the gradients of a new task, giving the transformed gradients</p><formula xml:id="formula_13">M T ∇ θ L τnew tr (θ) = M + αβ |T | i=1 ∇ θ L τi val (θ τi )∇ θ L τi tr (θ) ∇ θ L τnew tr (θ) (8) = M∇ θ L τnew tr (θ) + β |T | i=1 ∇ θ L τi tr (θ) ∇ θ L τnew tr (θ) α∇ θ L τi val (θ τi ) (9) = M∇ θ L τnew tr (θ) + β |T | i=1 ∇ θ L τi tr (θ) ∇ θ L τnew tr (θ) A. Gradient similarity α∇ θ L τi val (θ) + O(α 2 ) B. Taylor expansion .<label>(10)</label></formula><p>Given M = I, the second term in the R.H.S. of Eq. 10 can represent the final gradient direction for the new task. For Eq. 10, we used the Taylor expansion of vector-valued function,</p><formula xml:id="formula_14">∇ θ L τi val (θ τi ) ≈ ∇ θ L τi val (θ) + ∇ 2 θ L τi val (θ)(θ − αM∇ θ L τi tr (θ) − θ)</formula><p>. The term A of Eq. 10 is the inner product between the gradients of meta-training losses and new test losses. We can simply interpret this as how similar the gradient directions between two different tasks. This has been explicitly used in continual learning or multi-task learning setup to consider task similarity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. When we have a loss function in the form of finite sums, this term can be also interpreted as a kernel similarity between the respective sets of gradients (see Eq. 4 of <ref type="bibr" target="#b27">[28]</ref>).</p><p>With the first term in B of Eq. 10, we compute a linear combination of the gradients of validation losses from the meta-training set. Its weighting factors are computed based on the similarities between the tasks from the meta-training set and the new task as explained above. Therefore, we essentially perform a soft nearest neighbor voting to find the direction among the validation gradients from the meta-training set. Given the new task, the gradient may lead the model to overfit (or underfit). However, the proposed method will extract the knowledge from the past experiences and find the gradients that gave us good validation performance during the meta-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Meta-learning: Model-agnostic meta-learning (MAML) highlighted the importance of the model's initial parameters for better generalization <ref type="bibr" target="#b9">[10]</ref> and there have been many extensions to improve the framework, e.g. for continuous adaptation <ref type="bibr" target="#b0">[1]</ref>, better credit assignment <ref type="bibr" target="#b36">[37]</ref>, and robustness <ref type="bibr" target="#b14">[15]</ref>. In this work, we improve the inner update optimizers by learning a curvature for better generalization and fast model adaptation. Meta-SGD <ref type="bibr" target="#b21">[22]</ref> suggests to learn coordinate-wise learning rates. We can interpret it as an diagonal approximation to meta-curvature in a similar vein to recent adaptive learning rates methods, such as <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>, performing diagonal approximations of second-order matrices. Recently, <ref type="bibr" target="#b3">[4]</ref> suggested to learn layer-wise learning rates through the meta-training. However, both methods do not consider the dependencies between the parameters, which was crucial to provide more robust meta-training process and faster convergence. <ref type="bibr" target="#b20">[21]</ref> also attempted to transform the gradients. They used simple binary mask applied to the gradient update to determine which parameters are to be updated while we introduce dense learnable tensors to model second-order dependencies with a series of tensor products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot classification:</head><p>As a good test bed to evaluate few-shot learning, huge progress has been made in the few-shot classification task. Triggered by <ref type="bibr" target="#b46">[47]</ref>, many recent studies have focused on discovering effective inductive bias on classification task. For example, network architectures that perform nearest neighbor search <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b42">43]</ref> were suggested. Some improved the performance by modeling the interactions or correlation between training examples <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref>. In order to overcome the nature of few-shot learning, the generative models have been suggested to augment the training data <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref> or generate model parameters for the specified task <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. The state-of-the-art results are achieved by additionally training 64-way classification task for pretraining <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32]</ref>  with larger ResNet models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref>. In this work, our focus is to improve the model-agnostic few-shot learner that is broadly applicable to other tasks, e.g. reinforcement learning setup.</p><p>Learning optimizers: Our proposed method may fall within the learning optimizer category <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b24">25]</ref>. They also take as input the gradient and transform it via a neural network to achieve better convergence behavior. However, their main focus is to capture the training dynamics of individual gradient coordinates <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3]</ref> or to obtain a generic optimizer that is broadly applicable for different datasets and architectures <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref>. On the other hand, we meta-learn a curvature coupled with the model's initialization parameters. We focus on a fast adaptation scenario requiring a small number of gradient steps. Therefore, our method does not consider a history of the gradients, which enables us to avoid considering a complex recurrent architecture. Finally, our approach is well connected to existing second order methods while learned optimizers are not easily interpretable since the gradient passes through nonlinear and multilayer recurrent neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the proposed method on a synthetic data few-shot regression task few-shot image classification tasks with Omniglot and MiniImagenet datasets. We test two versions of the metacurvature. The first one, named as MC1, we fixed the M o = I Eq. 4. The second one, named as MC2, we learn all three meta-curvature matrices. We also report results on few-shot reinforcement learning in appendices. To begin with, we perform a simple regression problem following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. During the meta-training process, sinusoidal functions are sampled, where the amplitude and phase are varied within [0.1, 5.0] and [0, π] respectively. The network architecture and all hyperparameters are same as <ref type="bibr" target="#b8">[9]</ref> and we only introduce the suggested meta-curvature. We reported the mean squared error with 95% confidence interval after one gradient step in <ref type="figure" target="#fig_2">Figure 5</ref>. The details are provided in appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Few-shot regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Few-shot classification on Omniglot</head><p>The Omniglot dataset consists of handwritten characters from 50 different languages and 1632 different characters. It has been widely used to evaluate few-shot classification performance. We follow the experimental protocol in <ref type="bibr" target="#b8">[9]</ref> and all hyperparameters and network architecture are same as <ref type="bibr" target="#b8">[9]</ref>. Further experimental details are provided in appendices. Except 5-shot 5-way setting, our simple 4 layers CNN with meta-curvatures outperform all MAML variants and also achieved state-of-theart results without additional specialized architectures, such as attention module (SNAIL <ref type="bibr" target="#b26">[27]</ref>) or relational module (GNN <ref type="bibr" target="#b11">[12]</ref>). We provide the training curves in <ref type="figure" target="#fig_0">Figure 2</ref> and our methods converge much faster and achieve higher accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Few-shot classification on miniImagenet and tieredImagenet</head><p>Datasets: The miniImagenet dataset was proposed by <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34]</ref> and it consists of 100 subclasses out of 1000 classes in the original dataset (64 training classes, 12 validation classes, 24 test classes). The tieredImagenet dataset <ref type="bibr" target="#b34">[35]</ref> is a larger subset, composed of 608 classes and reduce the semantic similarity between train/val/test splits by considering high-level categories.</p><p>baseline CNNs: We used 4 layers convolutional neural network with the batch normalization followed by a fully connected layer for the final classification. In order to increase the capacity of the network, we increased the filter size up to 128. We found that the model with the larger filter seriously overfit (also reported in <ref type="bibr" target="#b8">[9]</ref>). To avoid overfitting, we applied data augmentation techniques suggested in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. For a fair comparison to <ref type="bibr" target="#b3">[4]</ref>, we also reported the results of model ensemble. Throughout the meta-training, we saved the model regularly and picked 3 models that have the best accuracy on the meta-validation dataset. We re-implemented all three baselines and performed the experiments with the same settings. We provide further the details in the appendices. <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="table" target="#tab_3">Table 3</ref> shows the results of baseline CNNs experiments on miniImagenet. MC1 and MC2 outperformed all other baselines for all different experiment settings. Not only does MC reach a higher accuracy at convergence, but also showed a much faster convergence rates for meta-training. Our methods share the same benefits as second order methods although we do not approximate any Hessian or Fisher matrices. Unlike other MAML variants, which required an extensive hyperparameter search, our methods are very robust to hyperparameter settings. Usually, MC2 outperforms MC1 because the more fine-grained meta-curvature enable us to effectively increase the model's capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WRN-28-10 features and MLP:</head><p>To the best of our knowledge, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref> are current state-of-the-art methods that use a pretrained WRN-28-10 [50] network (trained with 64-way classification task on entire meta-training set) as a feature extractor network. We evaluated our methods on this setting by adding one hidden layer MLP followed by a softmax classifier and our method again improved MAML variants by a large margin. Despite our best attempts, we could not find a good hyperparameters to train original MAML in this setting. Although our main goal is to push how much a simple gradient transformation in the inner loop optimization can improve general and broadly applicable MAML frameworks, our methods outperformed the recent methods that used various task specific techniques, e.g. task dependent weight generating methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref> and relational networks <ref type="bibr" target="#b38">[39]</ref>. Our methods also outperformed the very latest state of the art results <ref type="bibr" target="#b19">[20]</ref> that used extensive data-augmentation, regularization, and 15-shot meta-training schemes with different backbone networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose to meta-learn the curvature for faster adaptation and better generalization. The suggested method significantly improved the performance upon previous MAML variants and outperformed the recent state of the art methods. It also leads to faster convergence during meta-training. We present an analysis about generalization performance and connect to existing second order methods, which would provide useful insights for further research. </p><formula xml:id="formula_15">θ τ i = θ − αMmc∇L τ i tr (θ) {Assuming one gradient step} end for θ ← ADAM θ, β, ∇ θ τ i L τ i val (θ τ i ) Mo ← ADAM Mo, β, ∇ Mo τ i L τ i val (θ τ i ) Mi ← ADAM Mi, β, ∇ M i τ i L τ i val (θ τ i ) M f ← ADAM M f , β, ∇ M f τ i L τ i val (θ τ i ) end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Few-shot regression</head><p>Experimental setup We used the same experimental setups in <ref type="bibr" target="#b8">[9]</ref>. During training and testing, the amplitude and the phase vary within [0.1, 5.0] and [0, π] respectively, and data points are sampled from uniform distribution <ref type="bibr">[−5, 5]</ref>. We used one gradient step with the fixed learning rate 0.01 and Adam was used for meta-training with the outer loop learning rate 0.001. We used the same network architecture, which has two 40 dimension fully connected layers with ReLU activation. We sampled 25 tasks for every iterations and trained 70000 iterations. We reported the performance from the trained model that had the minimum loss value. <ref type="bibr" target="#b8">[9]</ref> reported the MSE for 5-shot setting, and we could reproduced the results. <ref type="bibr" target="#b21">[22]</ref> has slightly different settings, so the MSE are not directly comparable to theirs.</p><p>Qualitative results: We provide qualitative results of few-shot regression task on sinusoidal functions in <ref type="figure">Figure 3</ref>. The star shape markers are the few data points for training, and we draw the curves based on each methods, MAML, Meta-SGD, and the proposed MC2. The left column is 5-shot and the right column is 10-shot experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Few-shot classification on Omniglot dataset</head><p>We used the same experimental setups in <ref type="bibr" target="#b8">[9]</ref>. Out of 1623 characters, we used 1100 characters for training, 100 characters for validation, and remaining 423 characters for testing. The network architecture is 4 convolutional layers with 64 filters and 1 fully connected layer for the final classification. We only used one inner gradient step with 0.4 learning rate for all meta-curvature experiments for training and testing. The batch size was set to 32 (5-way) and 16 (20-way), and outer loop learning rate is 0.001 and we trained 60000 iterations. The inner/outer learning rates are β = 0.001, α = 0.01. We apply dropout rate 0.2 in the final linear layer for only MC1 and MC2 (other methods did perform worse with dropout). For cutout data augmentation, we cut out 36 × 36 random crops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 WRN-28-10 features and MLP</head><p>We used the WRN-28-10 features provided by <ref type="bibr" target="#b38">[39]</ref>. For miniImagenet, we provided the results from both center and multi-view features (average of center and corner crops). The dimension of feature was 640 and we used one hidden layer with ReLU activation function followed by a softmax classifier. We used separate meta-curvature matrices for each inner updates. The details of hyperparameters used for training MC2 is provided in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Few-shot reinforcement learning</head><p>The goal of few-shot learning in reinforcement learning (RL) is that an agent can quickly adapt to a new task with little prior experience. A distinct feature from the few-shot supervised learning task is that the RL objective is not generally differentiable. Therefore, we use policy gradient methods to estimate the gradient both for inner and outer loop gradients. In addition, policy gradient methods are generally on-policy, which means that the training data depends on the agent's initial policy. Therefore, the initial policy (with the meta-learned initial parameters) needs to explore as diverse experiences as possible to get proper feedback from a new task. We described the method and interpretation with respect to supervised classification tasks, but it can be easily modified to RL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Experimental setup</head><p>We tested our method on complex high-dimensional locomotion tasks with the MuJoCo simulator <ref type="bibr" target="#b45">[46]</ref>. Most of the settings are based on <ref type="bibr" target="#b8">[9]</ref> for fair comparison. We consider two simulated robots (HalfCheetah and Walker2d) and two types of task environments (to run in a forward/backward direction or a particular velocity). The network architecture is two hidden layers of size 100 with ReLU activations for both. We used the standard linear feature baseline estimator. We evaluated the performance after one policy gradient step with 20 trajectories. We compare against MAML-TRPO and MAML-PPO. In the original MAML, TRPO <ref type="bibr" target="#b39">[40]</ref> was used as the outer loop optimizer but we found out that using PPO <ref type="bibr" target="#b40">[41]</ref> consistently outperformed the TRPO. MAML-PPO is also computationally more efficient since MAML-TRPO requires third-order gradients (or computed by hessian-vector product instead). To the best of our knowledge, MAML-PPO has not been tested on this setup. We evaluated two variations of meta-curvature similar to the classification setup, MC1 and MC2, and used PPO as the meta-optimizer. Note that this is a preliminary result, so this is not by no means conclusive. We provide this information for the readers who might be interested in this direction. <ref type="figure">Fig. 4</ref> shows the rewards obtained after one step policy gradient update. In the HalfCheetahDir experiment, our methods outperformed both strong baselines. MC1-PPO reached the same performance of a strong baseline, MAML-PPO three times faster. In HalfCheetahVel and Walker2dDir experiments, both MC2-PPO and MAML-PPO reached nearly the same performance, but in a more sample efficient manner. For Walker2dVel, MAML-TRPO showed the fastest convergence at the earlier meta-training stage, but our meta-curvatures outperformed eventually. In this setting, most of the rewards come from the survival reward (the agent gets 1.0 reward for every step if they do not fall over). All methods were able to survive throughout the episode, but our methods run better at a given velocity. One thing we noticed that it stops obtaining more rewards and starts to degrade the performance in Walker2dDir experiment. The recently proposed approach <ref type="bibr" target="#b36">[37]</ref> may alleviate this issue through better credit assignment in the meta-gradients. Combining it would be interesting direction to be explored. <ref type="figure" target="#fig_2">Fig. 5</ref> is a visualization of meta-trained meta-curvature matrices for 5-way 1-shot classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visualization</head><p>To visualize the full matrix, M mc , we picked up the matrices from the first convolutional layer in the small model (filter size 64). Therefore with the 3 color input channels, M f ∈ R 9×9 , M i ∈ R 3×3 , M o ∈ R 64×64 , and M mc ∈ R 1728×1728 . The diagonal elements are high values, mostly &gt; 0.5. Interestingly, there are also a lot of off-diagonal elements &gt; 0.5 or &lt; −0.5. Thus, they capture the dependencies between the gradients.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Few-shot classification accuracy over training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Qualitative results of few-shot regression on sinusoidal functions. The left column -5 shot, The right column -10 shot Reinforcement learning experimental results. Y-axis: rewards after the model updates. X-axis: meta-training steps. We performed at least three runs with random seeds and the curves are averaged over them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of meta-curvature matrices. We clipped the values [−1, 1] for better visualization (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Few-shot classification results on Omniglot dataset. † denotes 3 model ensemble.</figDesc><table><row><cell></cell><cell cols="4">5-way 1-shot 5-way 5-shot 20-way 1-shot 20-way 5-shot</cell></row><row><cell>SNAIL [27]</cell><cell cols="2">99.07 ± 0.16 99.78 ± 0.09</cell><cell>97.64 ± 0.30</cell><cell>99.36 ± 0.18</cell></row><row><cell>GNN [12]</cell><cell>99.2</cell><cell>99.7</cell><cell>97.4</cell><cell>99.0</cell></row><row><cell>MAML</cell><cell>98.7 ± 0.4</cell><cell>99.9 ± 0.1</cell><cell>95.8 ± 0.3</cell><cell>98.9 ± 0.2</cell></row><row><cell>Meta-SGD</cell><cell cols="2">99.53 ± 0.26 99.93 ± 0.09</cell><cell>95.93 ± 0.38</cell><cell>98.97 ± 0.19</cell></row><row><cell>MAML++  † [4]</cell><cell>99.47</cell><cell>99.93</cell><cell>97.65 ± 0.05</cell><cell>99.33 ± 0.03</cell></row><row><cell>MC1</cell><cell cols="2">99.47 ± 0.27 99.57 ± 0.12</cell><cell>97.60 ± 0.29</cell><cell>99.23 ± 0.08</cell></row><row><cell>MC2</cell><cell cols="2">99.77 ± 0.17 99.79 ± 0.10</cell><cell>97.86 ± 0.26</cell><cell>99.24 ± 0.07</cell></row><row><cell>MC2  †</cell><cell cols="2">99.97 ± 0.06 99.89 ± 0.06</cell><cell>99.12 ± 0.16</cell><cell>99.65 ± 0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Few-shot regression results.</figDesc><table><row><cell>Method</cell><cell>5-shot</cell><cell>10-shot</cell></row><row><cell>MAML</cell><cell cols="2">0.686 ± 0.070 0.435 ± 0.039</cell></row><row><cell cols="3">Meta-SGD 0.482 ± 0.061 0.258 ± 0.026</cell></row><row><cell>LayerLR</cell><cell cols="2">0.528 ± 0.068 0.269 ± 0.027</cell></row><row><cell>MC1</cell><cell cols="2">0.426 ± 0.054 0.239 ± 0.025</cell></row><row><cell>MC2</cell><cell cols="2">0.405 ± 0.048 0.201 ± 0.020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Few-shot classification results on miniImagenet test set (5-way classification) with baseline 4 layer CNNs. * is from the original papers. † denotes 3 model ensembles. ± 0.89 48.85 ± 0.88 59.26 ± 0.72 63.92 ± 0.74 Meta-SGD 49.87 ± 0.87 48.99 ± 0.86 66.35 ± 0.72 63.84 ± 0.71 LayerLR 50.04 ± 0.87 50.55 ± 0.87 65.06 ± 0.71 66.64 ± 0.69 MC1 53.37 ± 0.88 53.74 ± 0.84 68.47 ± 0.69 68.01 ± 0.73 MC2 54.23 ± 0.88 54.08 ± 0.93 67.94 ± 0.71 67.99 ± 0.73 MC2</figDesc><table><row><cell></cell><cell cols="2">1-shot</cell><cell cols="2">5-shot</cell></row><row><cell>Inner steps</cell><cell>1 step</cell><cell>5 step</cell><cell>1 step</cell><cell>5 step</cell></row><row><cell>*MAML</cell><cell>·</cell><cell>48.7 ± 1.84</cell><cell>·</cell><cell>63.1 ± 0.92</cell></row><row><cell>*Meta-SGD</cell><cell>50.47 ± 1.87</cell><cell>·</cell><cell>64.03 ± 0.94</cell><cell>·</cell></row><row><cell cols="3">*MAML++  † 51.05 ± 0.31 52.15 ± 0.26</cell><cell>·</cell><cell>68.32 ± 0.44</cell></row><row><cell>MAML</cell><cell>46.28</cell><cell></cell><cell></cell><cell></cell></row></table><note>† 54.90 ± 0.90 55.73 ± 0.94 69.46 ± 0.70 70.33 ± 0.72</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results on miniImagenet and tieredImagenet. ‡ indicates that both meta-train and meta-validation are used during meta-training. † denotes indicates that 15-shot meta-training was used for both 1-shot and 5-shot testing. MetaOptNet<ref type="bibr" target="#b2">[3]</ref> used ResNet-12 backbone and trained end-to-end manner while we used the fixed features provided by<ref type="bibr" target="#b1">[2]</ref> (center -features from the central crop, multiview -features averaged over four corners, central crops, and horizontal mirrored). SVM ‡ †<ref type="bibr" target="#b19">[20]</ref> 64.09 ± 0.62 80.00 ± 0.45 65.81 ± 0.74 81.75 ± 0.53</figDesc><table><row><cell></cell><cell cols="2">miniImagenet</cell><cell cols="2">tieredImagenet</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>[33]  ‡</cell><cell cols="2">59.60 ± 0.41 73.74 ± 0.19</cell><cell>·</cell><cell>·</cell></row><row><cell>LEO (center)  ‡ [39]</cell><cell cols="4">61.76 ± 0.08 77.59 ± 0.12 66.33 ± 0.05 81.44 ± 0.09</cell></row><row><cell>LEO (multiview)  ‡ [39]</cell><cell cols="2">63.97 ± 0.20 79.49 ± 0.70</cell><cell>·</cell><cell>·</cell></row><row><cell>MetaOptNet-Meta-SGD (center)</cell><cell cols="4">56.58 ± 0.21 68.84 ± 0.19 59.75 ± 0.25 69.04 ± 0.22</cell></row><row><cell>MC2 (center)</cell><cell cols="4">61.22 ± 0.10 75.92 ± 0.17 66.20 ± 0.10 82.21 ± 0.08</cell></row><row><cell>MC2 (center)  ‡</cell><cell cols="4">61.85 ± 0.10 77.02 ± 0.11 67.21 ± 0.10 82.61 ± 0.08</cell></row><row><cell>MC2 (multiview)  ‡</cell><cell cols="2">64.40 ± 0.10 80.21 ± 0.10</cell><cell>·</cell><cell>·</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Few-shot regression results on sinusoidal functions. ± 0.070 0.435 ± 0.039 0.228 ± 0.024 Meta-SGD 0.482 ± 0.061 0.258 ± 0.026 0.127 ± 0.</figDesc><table><row><cell>Method</cell><cell>5-shot</cell><cell>10-shot</cell><cell>20-shot</cell></row><row><cell>MAML</cell><cell cols="3">0.686 013</cell></row><row><cell>LayerLR</cell><cell cols="3">0.528 ± 0.068 0.269 ± 0.027 0.134 ± 0.014</cell></row><row><cell>MC1</cell><cell cols="3">0.426 ± 0.054 0.239 ± 0.025 0.125 ± 0.013</cell></row><row><cell>MC2</cell><cell cols="3">0.405 ± 0.048 0.201 ± 0.020 0.112 ± 0.011</cell></row><row><cell cols="2">A Meta-training algorithm</cell><cell></cell><cell></cell></row><row><cell cols="4">Alg. 1 shows the details of the algorithm to train meta-curvature matrices and the initial model param-</cell></row><row><cell cols="4">eters. To avoid cluttered notation, we assumed the model has only one layer and it is straightforward</cell></row><row><cell>to extend to multiple layers.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Algorithm 1 Training MAML with the meta-curvature for few-shot supervised learning</cell></row><row><cell cols="2">Input: task distribution p(T ), learning rate α, β</cell><cell></cell><cell></cell></row><row><cell>Initialize Mo, Mi, M f = I</cell><cell></cell><cell></cell><cell></cell></row><row><cell>while not converged do</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Sample batch of tasks τi ∼ p(T )</cell><cell></cell><cell></cell></row><row><cell>for all τi do do</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters used for training MC2 on miniImagenet and tieredImagenet datasets with WRN-28-10 features and single hidden layer MLP. step experiments. 15 examples per class were used for evaluating the model after updates. In total, we ran 100,000 iterations for 1 step experiments and 200,000 iterations for 2 step experiments.</figDesc><table><row><cell>Hyperparameters</cell><cell></cell><cell cols="2">miniImagenet</cell><cell></cell><cell cols="2">tieredImagenet</cell></row><row><cell>Features</cell><cell cols="2">center</cell><cell cols="2">multiview</cell><cell cols="2">center</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Batch size</cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total training iterations</cell><cell></cell><cell></cell><cell cols="2">100,000</cell><cell></cell><cell></cell></row><row><cell>Learning rate (inner loop)</cell><cell></cell><cell></cell><cell cols="2">0.01</cell><cell></cell><cell></cell></row><row><cell>Learning rate (outer loop)</cell><cell cols="6">0.0005 0.0003 0.0005 0.0003 0.00075 0.0001</cell></row><row><cell>The number of inner steps</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell></row><row><cell>The number of hidden units</cell><cell>1024</cell><cell>1024</cell><cell>512</cell><cell>4096</cell><cell>4096</cell><cell>4096</cell></row><row><cell>Dropout rate over WRN-28-10 features</cell><cell>0.5</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell cols="7">D Few-shot classification on miniImagenet and tieredImagenet dataset</cell></row><row><cell>D.1 Baseline CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">For both 5-way 1-shot and 5-way 5-shot classification, we set the batch size 4 for 1 step experiments</cell></row><row><cell>and 2 for 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning Augmentation Policies from Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<title level="m">Adapting Auxiliary Losses Using Gradient Similarity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meta-Learning and Universality: Deep Representations and Gradient Descent Can Approximate Any Learning Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recasting Gradient-Based Meta-Learning as Hierarchical Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Kronecker-factored approximate Fisher matrix for convolution layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian Model-Agnostic Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tensor Decompositions and Applications. SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08308</idno>
		<title level="m">Tensor Regression Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-Learning with Differentiable Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><forename type="middle">Li</forename><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to Learn Quickly for Few Shot Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient Episodic Memory for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing Neural Networks with Kronecker-factored Approximate Curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10180</idno>
		<title level="m">Learned Optimizers That Outperform SGD On Wall-Clock And Test Loss</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Simple Neural Attentive Meta-Learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Simple Neural Attentive Meta-Learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from Distributions via Support Measure Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Dinuzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rapid Adaptation with Conditionally Shifted Neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
	</analytic>
	<monogr>
		<title level="j">On First-Order Meta-Learning Algorithms</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-Shot Image Recognition by Predicting Parameters from Activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimization As a Model For Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-Learning for Semi-Supervised Few-Shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Promp: Proximal meta-policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignasi</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamim</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Topmoumoute online natural gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simon Osindero, and Raia Hadsell. Meta-Learning with Latent Embedding Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><forename type="middle">Abbeel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
	</analytic>
	<monogr>
		<title level="j">Proximal Policy Optimization Algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Low-Shot Learning from Imaginary Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nando de Freitas, and Jascha Sohl-Dickstein. Learned Optimizers that Scale and Generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Wichrowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
