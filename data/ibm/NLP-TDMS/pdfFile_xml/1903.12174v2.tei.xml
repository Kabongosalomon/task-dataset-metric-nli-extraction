<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TensorMask: A Foundation for Dense Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TensorMask: A Foundation for Dense Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sliding-window object detectors that generate boundingbox object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense slidingwindow instance segmentation, which is surprisingly underexplored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The sliding-window paradigm-finding objects by looking in each window placed over a dense set of image locations-is one of the earliest and most successful concepts in computer vision <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> and is naturally connected to convolutional networks <ref type="bibr" target="#b19">[20]</ref>. However, while today's topperforming object detectors rely on sliding window prediction to generate initial candidate regions, a refinement stage is applied to these candidate regions to obtain more accurate predictions, as pioneered by Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> and Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> for bounding-box object detection and instance segmentation, respectively. This class of methods has dominated the COCO detection challenges <ref type="bibr" target="#b23">[24]</ref>.</p><p>Recently, bounding-box object detectors which eschew the refinement step and focus on direct sliding-window pre-  <ref type="figure">Figure 1</ref>. Selected output of TensorMask, our proposed framework for performing dense sliding-window instance segmentation. We treat dense instance segmentation as a prediction task over structured 4D tensors. In addition to obtaining competitive quantitative results, TensorMask achieves results that are qualitatively reasonable. Observe that both small and large objects are well delineated and more critically overlapping objects are properly handled. diction, as exemplified by SSD <ref type="bibr" target="#b26">[27]</ref> and RetinaNet <ref type="bibr" target="#b22">[23]</ref>, have witnessed a resurgence and shown promising results. In contrast, the field has not witnessed equivalent progress in dense sliding-window instance segmentation; there are no direct, dense approaches analogous to SSD / RetinaNet for mask prediction. Why is the dense approach thriving for box detection, yet entirely missing for instance segmentation? This is a question of fundamental scientific interest. The goal of this work is to bridge this gap and provide a foundation for exploring dense instance segmentation.</p><p>Our main insight is that the core concepts for defining dense mask representations, as well as effective realizations of these concepts in neural networks, are both lacking. Unlike bounding boxes, which have a fixed, low-dimensional representation regardless of scale, segmentation masks can benefit from richer, more structured representations. For example, each mask is itself a 2D spatial map, and masks for larger objects can benefit from the use of larger spatial maps. Developing effective representations for dense masks is a key step toward enabling dense instance segmentation.</p><p>To address this, we define a set of core concepts for representing masks with high-dimensional tensors that allows for the exploration of novel network architectures for dense mask prediction. We present and experiment with several such networks in order to demonstrate the merits of the pro-  <ref type="bibr" target="#b16">[17]</ref> with a ResNet-101-FPN backbone (on the same images as used in <ref type="figure">Fig. 6</ref> of Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>). The results are quantitatively and qualitatively similar, demonstrating that the dense sliding window paradigm can indeed be effective for the instance segmentation task. We challenge the reader to identify which results were generated by TensorMask. 1 posed representations. Our framework, called TensorMask, establishes the first dense sliding-window instance segmentation system that achieves results near to Mask R-CNN.</p><p>The central idea of the TensorMask representation is to use structured 4D tensors to represent masks over a spatial domain. This perspective stands in contrast to prior work on the related task of segmenting class-agnostic object proposals such as DeepMask <ref type="bibr" target="#b30">[31]</ref> and InstanceFCN <ref type="bibr" target="#b6">[7]</ref> that used unstructured 3D tensors, in which the mask is packed into the third 'channel' axis. The channel axis, unlike the axes representing object position, does not have a clear geometric meaning and is therefore difficult to manipulate. By using a basic channel representation, one misses an opportunity to benefit from using structural arrays to represent masks as 2D entities-analogous to the difference between MLPs and ConvNets <ref type="bibr" target="#b19">[20]</ref> for representing 2D images.</p><p>Unlike these channel-oriented approaches, we propose to leverage 4D tensors of shape (V, U, H, W ), in which both (H, W )-representing object position-and (V, U )representing relative mask position-are geometric subtensors, i.e., they have axes with well-defined units and geometric meaning w.r.t. the image. This shift in perspective from encoding masks in an unstructured channel axis to using structured geometric sub-tensors enables the definition of novel operations and network architectures. These networks can operate directly on the (V, U ) sub-tensor in geometrically meaningful ways, including coordinate transformation, up-/downscaling, and use of scale pyramids.</p><p>Enabled by the TensorMask framework, we develop a pyramid structure over a scale-indexed list of 4D tensors, which we call a tensor bipyramid. Analogous to a feature pyramid, which is a list of feature maps at multiple scales, a tensor bipyramid contains a list of 4D tensors with shapes (2 k V, 2 k U, <ref type="bibr" target="#b0">1</ref> 2 k H, 1 2 k W ), where k ≥ 0 indexes scale. This structure has a pyramidal shape in both (H, W ) and (V, U ) geometric sub-tensors, but growing in opposite directions. This natural design captures the desirable property that large objects have high-resolution masks with coarse 1 <ref type="figure" target="#fig_1">In Fig. 2</ref> spatial localization (large k) and small objects have lowresolution masks with fine spatial localization (small k).</p><p>We combine these components into a network backbone and training procedure closely following RetinaNet <ref type="bibr" target="#b22">[23]</ref> in which our dense mask predictor extends the original dense bounding box predictor. With detailed ablation experiments, we evaluate the effectiveness of the TensorMask framework and show the importance of explicitly capturing the geometric structure of this task. Finally, we show TensorMask yields similar results to its Mask R-CNN counterpart (see Figs. 1 and 2). These promising results suggest the proposed framework can help pave the way for future research on dense sliding-window instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classify mask proposals. The modern instance segmentation task was introduced by Hariharan et al. <ref type="bibr" target="#b14">[15]</ref> (before being popularized by COCO <ref type="bibr" target="#b23">[24]</ref>). In their work, the method proposed for this task involved first generating object mask proposals <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1]</ref>, then classifying these proposals <ref type="bibr" target="#b14">[15]</ref>. In earlier work, the classify-mask-proposals methodology was used for other tasks. For example, Selective Search <ref type="bibr" target="#b36">[37]</ref> and the original R-CNN <ref type="bibr" target="#b11">[12]</ref> classified mask proposals to obtain box detections and semantic segmentation results; these methods could easily be applied to instance segmentation. These early methods relied on bottom-up mask proposals computed by pre-deep-learning era methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1]</ref>; our work is more closely related to dense sliding-window methods for mask object proposals as pioneered by Deep-Mask <ref type="bibr" target="#b30">[31]</ref>. We discuss this connection shortly.</p><p>Detect then segment. The now dominant paradigm for instance segmentation involves first detecting objects with a box and then segmenting each object using the box as a guide <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref>. Perhaps the most successful instantiation of the detect-then-segment methodology is Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>, which extended the Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> detector with a simple mask predictor. Approaches that build on Mask R-CNN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref> have dominated leaderboards of recent challenges <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6]</ref>. Unlike in bounding-box detection, where sliding-window <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> and regionbased <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> methods have both thrived, in the area of instance segmentation, research on dense sliding-window methods has been missing. Our work aims to close this gap.</p><p>Label pixels then cluster. A third class of approaches to instance segmentation (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25]</ref>) builds on models developed for semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>. These approaches label each image pixel with a category and some auxiliary information that a clustering algorithm can use to group pixels into object instances. These approaches benefit from improvements on semantic segmentation and natively predict higher-resolution masks for larger objects. Compared to detect-then-segment methods, label-pixels-thencluster methods lag behind in accuracy on popular benchmarks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6]</ref>. Instead of employing fully convolutional models for dense pixel labeling, TensorMask explores the framework of building fully convolutional (i.e., dense sliding window) models for dense mask prediction, where the output at each spatial location is itself a 2D spatial map.</p><p>Dense sliding window methods. To the best of our knowledge, no prior methods exist for dense sliding-window instance segmentation. The proposed TensorMask framework is the first such approach. The closest methods are for the related task of class-agnostic mask proposal generation, specifically models such as DeepMask <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and In-stanceFCN <ref type="bibr" target="#b6">[7]</ref> which apply convolutional neural networks to generate mask proposals in a dense sliding-window manner. Like these approaches, TensorMask is a dense slidingwindow model, but it spans a more expressive design space. DeepMask and InstanceFCN can be expressed naturally as class-agnostic TensorMask models, but TensorMask enables novel architectures that perform better. Also, unlike these class-agnostic methods, TensorMask performs multiclass classification in parallel to mask prediction, and thus can be applied to the task of instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tensor Representations for Masks</head><p>The central idea of the TensorMask framework is to use structured high-dimensional tensors to represent image content (e.g., masks) in a set of densely sliding windows.</p><p>Consider a V ×U window sliding on a feature map of width W and height H. It is possible to represent all masks in all sliding window locations by a tensor of a shape (C, H, W ), where each mask is parameterized by C=V ·U pixels. This is the representation used in DeepMask <ref type="bibr" target="#b30">[31]</ref>.</p><p>The underlying spirit of this representation, however, is in fact a higher dimensional (4D) tensor with shape (V, U, H, W ). The sub-tensor (V, U ) represents a mask as a 2D spatial entity. Instead of viewing the channel dimension C as a black box into which a V ×U mask is arranged, the tensor perspective enables several important concepts for representing dense masks, discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unit of Length</head><p>The unit of length (or simply unit) of each spatial axis is a necessary concept for understanding 4D tensors in our framework. Intuitively, the unit of an axis defines the length of one pixel along it. Different axes can have different units.</p><p>The unit of the H and W axes, denoted as σ HW , can be set as the stride w.r.t. the input image (e.g., res 4 of ResNet-50 <ref type="bibr" target="#b17">[18]</ref> has σ HW =16 image pixels). Analogously, the V and U axes define another 2D spatial domain and have their own unit, denoted as σ VU . Shifting one pixel along the V or U axis corresponds to shifting σ VU pixels on the input image. The unit σ VU need not be equal to the unit σ HW , a property that our models will benefit from.</p><p>Defining units is necessary because the interpretation of the tensor shape (V, U, H, W ) is ambiguous if units are not specified. For example, (V, U ) represents a V ×U window in image pixels if σ VU =1 image pixel, but a 2V ×2U window in image pixels if σ VU =2 image pixels. The units and how they change due to up/down-scaling operations are central to multi-scale representations (more in §3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Natural Representation</head><p>With the definition of units, we can formally describe the representational meaning of a (V, U, H, W ) tensor. In our simplest definition, this tensor represents the windows sliding over (H, W ). We call this the natural representation.</p><p>Denoting α= σVU /σHW as the ratio of units, formally we have:</p><p>Natural Representation: For a 4D tensor of shape (V, U, H, W ), its value at coordinates (v, u, y, x) represents the mask value at (y + αv, x + αu) in the αV ×αU window centered at (y, x). 2</p><formula xml:id="formula_0">Here (v, u, y, x) ∈ [− V 2 , V 2 )×[− U 2 , U 2 )×[0, H)×[0, W ),</formula><p>where '×' denotes cartesian product. Conceptually, the tensor can be thought of as a continuous function in this domain. For implementation, we must instead rasterize the 4D tensor as a discrete function defined on sampled locations. We assume a sampling rate of one sample per unit, with samples located at integer coordinates (e.g., if U =3, then u∈{−1, 0, 1}). This assumption allows the same value U to represent both the length of the axis in terms of units (e.g., 3σ VU ) and also the number of discrete samples stored for the axis. This is convenient for working with tensors produced by neural networks that are discrete and have lengths. <ref type="figure" target="#fig_2">Fig. 3</ref> (left) illustrates an example when V =U =3 and α is 1. The natural representation is intuitive and easy to parse as the output of a network, but it is not the only possible representation in a deep network, as discussed next. </p><formula xml:id="formula_1">V ,Û Ĥ W H W V , U U V (y, x) (y + v, x + u) = (y − 1, x − 1) (v,û) = (−1, −1) (v, u) = (−1, −1) (ŷ −v,ŷ −û) = (ŷ + 1,x + 1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Aligned Representation</head><p>In the natural representation, a sub-tensor (V, U ) located at (y, x) represents values at offset pixels (y+αv, x+αu) instead of directly at (y, x). When using convolutions to compute features, preserving pixel-to-pixel alignment between input pixels and predicted output pixels can lead to improvements (this is similar to the motivation for RoIAlign <ref type="bibr" target="#b16">[17]</ref>). Next we describe a pixel-aligned representation for dense masks under the tensor perspective.</p><p>Formally, we define the aligned representation as:</p><p>Aligned Representation: For a 4D tensor (V ,Û ,Ĥ,Ŵ ), its value at coordinates (v,û,ŷ,x) represents the mask value at (ŷ,x) in theαV ×αÛ window centered at (ŷ −αv,x −αû).</p><p>α=σ VU /σHW is the ratio of units in the aligned representation.</p><p>Here, the sub-tensor (V ,Û ) at pixel (ŷ,x) always describes the values taken at this pixel, i.e. it is aligned. The subspace (V ,Û ) does not represent a single mask, but instead enumerates mask values in allV ·Û windows that overlap pixel (ŷ,x). <ref type="figure" target="#fig_2">Fig. 3</ref> (right) illustrates an example whenV =Û =3 (nine overlapping windows) andα is 1.</p><p>Note that we denote tensors in the aligned representation as (V ,Û ,Ĥ,Ŵ ) (and likewise for coordinates/units). This is in the spirit of 'named tensors' <ref type="bibr" target="#b34">[35]</ref> and proves useful.</p><p>Our aligned representation is related to the instancesensitive score maps proposed in InstanceFCN <ref type="bibr" target="#b6">[7]</ref>. We prove (in §A.2) that those score maps behave like our aligned representation but with nearest-neighbor interpolation on (V ,Û ), which makes them unaligned. We test this experimentally and show it degrades results severely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Coordinate Transformation</head><p>We introduce a coordinate transformation between natural and aligned representations, so they can be used interchangeably in a single network. This gives us additional flexibility in the design of novel network architectures.</p><p>For simplicity, we assume units in both representations are the same: i.e., σ HW =σ HW and σ VU =σ VU , and thus α=α (for the more general case see §A.1). Comparing the definitions of natural vs. aligned representations, we have the following two relations for x, u: x+αu=x and x=x−αû. With α=α, solving this equation forx andû gives:x=x+αu andû=u. A similar results hold for y, v. So the transformation from the aligned representation (F) to the natural representation (F) is:</p><formula xml:id="formula_2">F(v, u, y, x) =F(v, u, y + αv, x + αu).<label>(1)</label></formula><p>We call this transform align2nat. Likewise, solving this set of two relations for x and u gives the reverse transform of nat2align:</p><formula xml:id="formula_3">F(v,û,ŷ,x)=F(v,û,ŷ−αv,x−αû).</formula><p>While all the models presented in this work only use align2nat, we present both cases for completeness. Without restrictions on α, these transformations may involve indexing a tensor at a non-integer coordinate, e.g. if x+αu is not an integer. Since we only permit integer coordinates in our implementation, we adopt a simple strategy: when the op align2nat is called, we ensure that α is a positive integer. We can satisfy this constraint on α by changing units with up/down-scaling ops, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Upscaling Transformation</head><p>The aligned representation enables the use of a coarse (V ,Û ) sub-tensors to create finer (V, U ) sub-tensors, which proves quite useful. <ref type="figure">Fig. 4</ref> illustrates this transformation, which we call up align2nat and describe next.</p><p>The up align2nat op accepts a (V ,Û ,Ĥ,Ŵ ) tensor as input. The (V ,Û ) sub-tensor is λ× coarser than the desired output (so its unit is λ× bigger). It performs bilinear upsampling, up bilinear, in the (V ,Û ) domain by λ, reducing the underlying unit by λ×. Next, the align2nat op converts the output into the natural representation. The full up align2nat op is shown in <ref type="figure">Fig. 4</ref>.</p><p>As our experiments demonstrate, the up align2nat op is effective for generating high-resolution masks without inflating channel counts in preceding feature maps. This in turn enables novel architectures, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Tensor Bipyramid</head><p>In multi-scale box detection it is common practice to use a lower-resolution feature map to extract larger-scale objects <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>-this is because a sliding window of a fixed size on a lower-resolution map corresponds to a larger region in the input image. This also holds for multi-scale mask detection. However, unlike a box that is always represented by four numbers regardless of its scale, a mask's pixel size must scale with object size in order to maintain constant resolution density. Thus, instead of always using V ×U units to present masks of different scales, we propose to adapt the number of mask pixels based on the scale. <ref type="figure">Figure 4</ref>. The up align2nat op is defined as a sequence of two ops. It takes an input tensor that has a coarse, λ× lower resolution onVÛ (so the unitσVU is λ× larger). The op performs upsampling onVÛ by λ followed by align2nat, resulting in an output where σVU=σHW=s (where s is the stride).</p><formula xml:id="formula_4">up_bilinear align2nat λV , λÛ ,Ĥ,Ŵ λV, λU, H, Ŵ V ,Û ,Ĥ,Ŵ σ V U =λsσ HW =sσ HW =s σ HW =ŝ σ V U =s σ V U =s</formula><p>Consider the natural representation (V, U, H, W ) on a feature map of the finest level. Here, the (H, W ) domain has the highest resolution (smallest unit). We expect this level to handle the smallest objects, so the (V, U ) domain should have the lowest resolution. With reference to this, we build a pyramid that gradually reduces (H, W ) and increases (V, U ). Formally, we define a tensor bipyramid as:</p><p>Tensor bipyramid: A tensor bipyramid is a list of tensors of shapes:</p><formula xml:id="formula_5">(2 k V, 2 k U, 1 2 k H, 1 2 k W ), for k=0, 1, 2, . . ., with units σ k+1 VU = σ k VU and σ k+1 HW = 2σ k HW , ∀k.</formula><p>Because the units σ k VU are the same across all levels, a 2 k V ×2 k U mask has 4 k × more pixels in the input image. In the (H, W ) domain, because the units σ k HW increase with k, the number of predicted masks decreases for larger masks, as desired. Note that the total size of each level is the same (it is V ·U ·H·W ). A tensor bipyramid can be constructed using the swap align2nat operation, described next.</p><p>This swap align2nat op is composed of two steps: first, an input tensor with fine (Ĥ,Ŵ ) and coarse (V ,Û ) is upscaled to (2 k V, 2 k U, H, W ) using up align2nat. Then (H, W ) is subsampled to obtain the final shape. The combination of up align2nat and subsample, shown in <ref type="figure">Fig. 5</ref>, is called swap align2nat: the units before and after this op are swapped. For efficiency, it is not necessary to compute the intermediate tensor of shape (2 k V, 2 k U, H, W ) from up align2nat, which would be prohibitive. This is because only a small subset of values in this intermediate tensor appear in the final output after subsampling. So although <ref type="figure">Fig. 5</ref> shows the conceptual computation, in practice we implement swap align2nat as a single op that only performs the necessary computation and has complexity O(V ·U ·H·W ) regardless of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TensorMask Architecture</head><p>We now present models enabled by TensorMask representations. These models have a mask prediction head that generates masks in sliding windows and a classification head to predict object categories, analogous to the box regression and classification heads in sliding-window object detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>. Box prediction is not necessary for TensorMask models, but can easily be included.  <ref type="figure">Figure 5</ref>. The swap align2nat op is defined by two ops. It upscales the input by up align2nat <ref type="figure">(Fig. 4)</ref>, then performs subsample on the HW domain. Note how the op swaps the units between the V U and HW domains. In practice, we implement this op in place so the complexity is independent of λ.  <ref type="figure">Figure 6</ref>. Baseline mask prediction heads: Each of the four heads shown starts from a feature map (e.g., from a level of an FPN <ref type="bibr" target="#b21">[22]</ref>) with an arbitrary channel number C. Then a 1×1 conv layer projects the features into an appropriate number of channels, which form the specified 4D tensor by reshape. The output units of these four heads are the same, and σVU=σHW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Mask Prediction Heads</head><p>Our mask prediction branch attaches to a convolutional backbone. We use FPN <ref type="bibr" target="#b21">[22]</ref>, which generates a pyramid of feature maps with sizes (C, <ref type="bibr" target="#b0">1</ref> 2 k H, 1 2 k W ) with a fixed number of channels C per level k. These maps are used as input for each prediction head: mask, class, and box. Weights for the heads are shared across levels, but not between tasks.</p><p>Output representation. We always use the natural representation ( §3.2) as the output format of the network. Any representation (natural, aligned, etc.) can be used in the intermediate layers, but it will be transformed into the natural representation for the output. This standardization decouples the loss definition from network design, making use of different representations simpler. Also, our mask output is class-agnostic, i.e., the window always predicts a single mask regardless of class; the class of the mask is predicted by the classification head. Class-agnostic mask prediction avoids multiplying the output size by the number of classes.</p><p>Baseline heads. We consider a set of four baseline heads, illustrated in <ref type="figure">Fig. 6</ref>. Each head accepts an input feature map of shape (C, H, W ) for any (H, W ). It then applies a 1×1 convolutional layer (with ReLU) with the appropriate number of output channels such that reshaping it into a 4D tensor produces the desired shape for the next layer, denoted as 'conv+reshape'. <ref type="figure">Fig. 6a and 6b</ref>    <ref type="figure">Figure 7</ref>. Conceptual comparison between: (a) a feature pyramid with any one of the baseline heads ( <ref type="figure">Fig. 6</ref>) attached, and (b) a tensor bipyramid that uses swap align2nat <ref type="figure">(Fig. 5)</ref>. A baseline head on the feature pyramid has σVU=σHW for each level, which implies that masks for large objects and small objects are predicted using the same number of pixels. On the other hand, the swap align2nat head can keep the mask resolution high (i.e., σVU is the same across levels) despite the HW resolution changes.</p><p>spectively. In both cases, we use V ·U output channels for the 1×1 conv, followed by align2nat in the latter case. <ref type="figure">Fig. 6c</ref> and 6d are upscaling heads that use the natural and aligned representations, respectively. Their 1×1 conv has λ 2 × fewer output channels than in the simple heads.</p><p>In a baseline TensorMask model, one of these four heads is selected and attached to all FPN levels. The output forms a pyramid of (V, U, 1 2 k H, 1 2 k W ), see <ref type="figure">Fig. 7a</ref>. For each head, the output sliding window always has the same unit as the feature map on which it slides: σ VU =σ HW for all levels.</p><p>Tensor bipyramid head. Unlike the baseline heads, the tensor bipyramid head ( §3.6) accepts a feature map of fine resolution (H, W ) at all levels. <ref type="figure">Fig. 8</ref> shows a minor modification of FPN to obtain these maps. For each of the resulting levels, now all (C, H, W ), we first use conv+reshape to produce the appropriate 4D tensor, then run a mask prediction head with swap align2nat, see <ref type="figure">Fig. 7b</ref>. The tensor bipyramid model is the most effective TensorMask variant explored in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>Label assignment. We use a version of the DeepMask assignment rule <ref type="bibr" target="#b30">[31]</ref> to label each window. A window satisfying three conditions w.r.t. a ground-truth mask m is positive:</p><p>(i) Containment: the window fully contains m and the longer side of m, in image pixels, is at least 1/2 of the longer  <ref type="figure">Figure 8</ref>. Conversion of FPN feature maps from (C, 1 2 k H, 1 2 k W ) to (C, H, W ) for use with tensor bipyramid (see <ref type="figure">Fig. 7b</ref>). For an FPN level (C, <ref type="bibr" target="#b0">1</ref> 2 k H, 1 2 k W ), we apply bilinear interpolation to upsample the feature map by a factor of 2 k . As the upscaling can be large, we add the finest level feature map to all levels (including the finest level itself), followed by one 3×3 conv with ReLU. side of the window, that is, max(U ·σ VU , V ·σ VU ). <ref type="bibr" target="#b2">3</ref> (ii) Centrality: the center of m's bounding box is within one unit (σ VU ) of the window center in 2 distance.</p><p>(iii) Uniqueness: there is no other mask m =m that satisfies the other two conditions.</p><p>If m satisfies these three conditions, then the window is labeled as a positive example whose ground-truth mask, object category, and box are given by m. Otherwise, the window is labeled as a negative example.</p><p>In contrast to the IoU-based assignment rules for boxes in sliding-window detectors (e.g., RPN <ref type="bibr" target="#b33">[34]</ref>, SSD <ref type="bibr" target="#b26">[27]</ref>, RetinaNet <ref type="bibr" target="#b22">[23]</ref>), our rules are mask-driven. Experiments show that our rules work well even when using only 1 or 2 window sizes with a single aspect ratio of 1:1, versus, e.g., RetinaNet's 9 anchors of multiple scales and aspect ratios.</p><p>Loss. For the mask prediction head, we adopt a per-pixel binary classification loss. In our setting, the ground-truth mask inside a sliding window often has a wide margin, resulting in an imbalance between foreground vs. background pixels. To address this imbalance, we set the weights for foreground pixels to 1.5 in the binary cross-entropy loss. The mask loss of a window is averaged over all pixels in the window (note that in a tensor bipyramid the window size varies across levels), and the total mask loss is averaged over all positive windows (negative windows do not contribute to the mask loss).</p><p>For the classification head, we again adopt FL * with γ=3 and α=0.3. For box regression, we use a parameter-free 1 loss. The total loss is a weighted sum of all task losses. Implementation details. Our FPN implementation closely follows <ref type="bibr" target="#b22">[23]</ref>; each FPN level is output by four 3×3 conv layers of C channels with ReLU (instead of one conv in the original FPN <ref type="bibr" target="#b21">[22]</ref>). As with the heads, weights are shared across levels, but not between tasks. In addition, we found that averaging (instead of summing <ref type="bibr" target="#b21">[22]</ref>) the top-down and lateral connections in FPN improved training stability. We use FPN levels 2 through 7 (k=0, . . . , 5) with C=128 channels for the four conv layers in the mask and box branches, and C=256 (the same as RetinaNet <ref type="bibr" target="#b22">[23]</ref>) for the classification branch. Unless noted, we use ResNet-50 <ref type="bibr" target="#b17">[18]</ref>. For training, all models are initialized from ImageNet pre-trained weights. We use scale jitter where the shorter image side is randomly sampled from [640, 800] pixels <ref type="bibr" target="#b15">[16]</ref>. Following SSD <ref type="bibr" target="#b26">[27]</ref> and YOLO <ref type="bibr" target="#b32">[33]</ref>, which train models longer (∼65 and 160 epochs) than <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>, we adopt the '6×' schedule <ref type="bibr" target="#b15">[16]</ref> (∼72 epochs), which improves results. The minibatch size is 16 images in 8 GPUs. The base learning rate is 0.02, with linear warm-up <ref type="bibr" target="#b13">[14]</ref> of 1k iterations. Other hyper-parameters are kept the same as <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference</head><p>Inference is similar to dense sliding-window object detectors. We use a single scale of 800 pixels for the shorter image side. Our model outputs a mask prediction, a class score, and a predicted box for each sliding window. Nonmaximum suppression (NMS) is applied to the top-scoring predictions using box IoU on the regressed boxes, following the settings in <ref type="bibr" target="#b21">[22]</ref>. To convert predicted soft masks to binary masks at the original image resolution, we use the same method and hyper-parameters as Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We report results on COCO instance segmentation <ref type="bibr" target="#b23">[24]</ref>. All models are trained on the ∼118k train2017 images and tested on the 5k val2017 images. Final results are on test-dev. We use COCO mask average precision (denoted by AP). When reporting box AP, we denote it as AP bb .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">TensorMask Representations</head><p>First we explore various tensor representations for masks using V =U =15 and a ResNet-50-FPN backbone. We report quantitative results in Tab. 2 and show qualitative comparisons in <ref type="figure" target="#fig_1">Figs. 2 and 9</ref>.</p><p>Simple heads. Tab. 1 compares natural vs. aligned representations with simple heads <ref type="figure">(Fig. 6a vs. 6b</ref>). Both representations perform similarly, with a marginal gap of 0.4 AP. The simple natural head can be thought of as a class-specific variant of DeepMask <ref type="bibr" target="#b30">[31]</ref> with an FPN backbone <ref type="bibr" target="#b21">[22]</ref> and focal loss <ref type="bibr" target="#b22">[23]</ref>. As we aim to use lower-resolution intermediate representations, we explore upscaling heads next.</p><p>Upscaling heads. Tab. 2a compares natural vs. aligned representations with upscaling heads <ref type="figure">(Fig. 6c vs. 6d)</ref>. The output size is fixed at V ×U =15×15. Given an upscaling factor λ, the conv in <ref type="figure">Fig. 6 has 1</ref> λ 2 V U channels, e.g., 9 channels with λ=5 (vs. 225 channels if no upscaling). The difference in accuracy is big for large λ: the aligned variant improves AP +9.2 over the natural head (48% relative) when λ=5.</p><p>The visual difference is clear in <ref type="figure" target="#fig_4">Fig. 9a</ref> (natural) vs. 9c (aligned). The upscale aligned head still produces sharp masks with large λ. This is critical for the tensor bipyramid, where we have an output of 2 k V ×2 k U , which is achieved with a large upscaling factor of λ=2 k (e.g., 32); see <ref type="figure">Fig. 5</ref>.</p><p>Interpolation. The tensor view reveals the (V ,Û ) subtensor as a 2D spatial entity that can be manipulated. Tab. 2b compares the upscale aligned head with bilinear (default) vs. nearest-neighbor interpolation on (V ,Û ). We refer to this latter variant as unaligned since quantization breaks pixel-to-pixel alignment. The unaligned variant is related to InstanceFCN <ref type="bibr" target="#b6">[7]</ref> (see §A.2).</p><p>We observe in Tab. 2b that bilinear interpolation yields solid improvements over nearest-neighbor interpolation, especially if λ is large (∆AP=3.1). These interpolation methods lead to striking visual differences when objects overlap: see <ref type="figure" target="#fig_4">Fig. 9b</ref> (unaligned) vs. 9c (aligned).</p><p>Tensor bipyramid. Replacing the best feature pyramid model with a tensor bipyramid yields a large 5.1 AP improvement (Tab. 2c). Here, the mask size is V ×U =15×15 on level k=0, and is 32V ×32U =480×480 for k=5; see <ref type="figure">Fig. 7b</ref>. The higher resolution masks predicted for large objects (e.g., at k=5) have clear benefit: AP L jumps by head λ AP AP 50 AP 75 ∆ aligned -natural natural 1. <ref type="bibr" target="#b4">5</ref> 28 28.4 51. <ref type="bibr" target="#b7">8</ref> 28.6 (a) Upscaling heads: natural vs. aligned heads <ref type="figure">(Fig. 6c vs. 6d)</ref>. The V ×U =15×15 output is upscaled by λ×: conv+reshape uses 1 λ 2 V U output channels as input. The aligned representation has a large gain over its natural counterpart when λ is large. 28.4 51. <ref type="bibr" target="#b7">8</ref> 28.6 (b) Upscaling: bilinear vs. nearest-neighbor interpolation for the aligned head <ref type="figure">(Fig. 6d)</ref>. The output has V ×U =15×15. With nearest-neighbor interpolation, the aligned upscaling head is similar to the InstanceFCN <ref type="bibr" target="#b6">[7]</ref> head. Bilinear interpolation shows a large gain when λ is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>head</head><p>AP AP 50 AP 75 AP S AP M AP L feature pyramid, best 28.9 52. <ref type="bibr" target="#b4">5</ref>    7.7 points. This improvement does not come at the cost of denser windows as the k=5 output is at ( H 32 , W 32 ) resolution. Again, we note that it is intractable to have, e.g., a 480 2channel conv. The upscaling aligned head with bilinear interpolation is key to making tensor bipyramid possible.</p><p>Multiple window sizes. Thus far we have used a single window size (per-level) for all models, that is, V ×U =15×15. Analogous to the concept of anchors in RPN <ref type="bibr" target="#b33">[34]</ref> that are also used in current detectors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>, we extend our method to multiple window sizes. We set V ×U ∈{15×15, 11×11}, leading to two heads per level. Tab. 2d shows the benefit of having two window sizes: it increases AP by 1.2 points. More window sizes and aspect ratios are possible, suggesting room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Mask R-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tab. 3 summarizes the best TensorMask model on</head><p>test-dev and compares it to the current dominant approach for COCO instance segmentation: Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. We use the Detectron <ref type="bibr" target="#b12">[13]</ref> code to reflect improvements since <ref type="bibr" target="#b16">[17]</ref> was published. We modify it to match our implementation details (FPN average fusion, 1k warm-up, and 1 box loss). Tab. 3 row 1 &amp; 2 verify that these subtleties have a negligible effect. Then we use training-time scale augmentation and a longer schedule <ref type="bibr" target="#b15">[16]</ref>, which yields an ∼2 AP increase (Tab. Conclusion. TensorMask is a dense sliding-window instance segmentation framework that, for the first time, achieves results close to the well-developed Mask R-CNN framework-both qualitatively and quantitatively. It establishes a conceptually complementary direction for instance segmentation research. We hope our work will create new opportunities and make both directions thrive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Generalized Coordinate Transformation</head><p>In Sec. 3.4 we have assumed σ HW =σ HW and σ VU =σ VU . Here we relax this condition and only assume σ HW =σ HW . Again, we still have the following two relations for x, u: x+αu=x and x=x−αû. Solving forx andû gives: x=x+αu andû= α α u. Then align2nat is:</p><formula xml:id="formula_6">F(v, u, y, x) =F( α α v, α α u, y+αv, x+αu).<label>(2)</label></formula><p>More generally, consider arbitrary units σ HW ,σ HW , σ VU , andσ VU . Then the relations between the natural and aligned representation can be rewritten as:</p><formula xml:id="formula_7">x·σ HW +u·σ VU =x·σ HW x·σ HW =x·σ HW −û·σ VU<label>(3)</label></formula><p>Note that these relations only hold in the image pixel domain (hence the usage of all units). Solving forx,û gives:</p><formula xml:id="formula_8">x = σHŴ σHW x+ σVÛ σHW û u = σVÛ σVU u<label>(4)</label></formula><p>And the align2nat transform becomes: </p><formula xml:id="formula_9">F (v,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Aligned Representation and InstanceFCN</head><p>We prove that the InstanceFCN <ref type="bibr" target="#b6">[7]</ref> output behaves as an upscaling aligned head with nearest-neighbor interpolation.</p><p>In <ref type="bibr" target="#b6">[7]</ref>, each output mask has V ×U pixels that are divided into K×K bins. A mask pixel is read from the channel corresponding to the pixel's bin. In our notation, <ref type="bibr" target="#b6">[7]</ref> predicts G which is related to the natural representation F by:</p><formula xml:id="formula_10">F(v, u, y, x) = G([ K V v], [ K U u], y + v, x + u),<label>(6)</label></formula><p>where [·] is a rounding operation and the integers [ K V v] and [ K U u] index a bin. Now, define a new functionF by:</p><formula xml:id="formula_11">F(v, u, y+v, x+u) G([ K V v], [ K U u], y+v, x+u),<label>(7)</label></formula><p>and new coordinates:x=x+u andũ=u (likewise for v and y). ThenF can be written as:</p><formula xml:id="formula_12">F(ṽ,ũ,ỹ,x) G([ K Vṽ ], [ K Uũ ],ỹ,x).<label>(8)</label></formula><p>Eqn.(8) says thatF is the nearest-neighbor interpolation of G on (Ṽ ,Ũ ). Eqn. <ref type="bibr" target="#b6">(7)</ref>, <ref type="bibr" target="#b5">(6)</ref>, and the new coordinates show that F is computed fromF by the align2nat transform with α=1. Thus, InstanceFCN masks can be constructed in the TensorMask framework by predicting G, performing nearest-neighbor interpolation of G on (Ṽ ,Ũ ) to getF, and then using align2nat to compute natural masks F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Object Detection Results</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Mask-Only TensorMask</head><p>One intriguing property of TensorMask is that masks are not dependent on boxes. This not only opens up new model designs that are mask-specific, but also allows us to investigate whether box predictions improve masks in a multi-task setting. Here, we conduct experiments without the use of a box head. Note that although we predict masks densely, we still need to perform NMS for post-processing. If regressed boxes are absent, we simply use the bounding boxes of the masks as a substitute (and also to report box AP).</p><p>Tab. 5 gives the results. We observe a slight degradation switching from the default setting which uses original boxes (row 1) for NMS to using mask bounding boxes (row 2). After accounting for this, TensorMask without a box head (row 3) has nearly equal mask AP to the mask+box variant (row 2). These results indicate that the role of the box head is auxiliary in our system, in contrast to Mask R-CNN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Qualitative Comparisons and Calibration</head><p>We show more results in Figs. 10 and 11. For these, and all visualizations in the main text, we display all detections that have a calibrated score ≥0.6. We use a simple calibration that maps uncalibrated detector scores to precision values: for each model and for each category, we compute its precision-recall (PR) curve on val2017. As a PR curve is parameterized by score, we can map an uncalibrated score for the detector-category pair to its corresponding precision value. Score-to-precision calibration enables a fair visual comparison between methods using a fixed threshold.  <ref type="figure" target="#fig_7">Fig. 11</ref>). These models use a ResNet-101-FPN backbone and obtain 38.3 and 37.1 AP, on test-dev, respectively. Visually, TensorMask gives sharper masks compared to Mask R-CNN although its AP is 1 point lower. Best viewed in a digital format with zoom. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example results of TensorMask and Mask R-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Left: Natural representation. The (V, U ) sub-tensor at a pixel represents a window centered at this pixel. Right: Aligned representation. The (V ,Û ) sub-tensor at a pixel represents the values at this pixel in each of the windows overlapping it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>σ</head><label></label><figDesc>HW = 4s σ V U = 4s σ V U = s σ HW = s σ V U = s σ HW = s σ HW = 4s σ V U = s σ HW = 2s σ V U = s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Baseline upscaling heads (λ=5). Top: the natural upscaling head (a) produces coarse masks, and is ineffective for large λ. Left: for simple scenes, the unaligned head (b) and aligned head (c) (which use nearest-neighbor and bilinear interpolation, respectively), behave similarly. Right: for overlapping objects the difference is striking: the unaligned head creates severe artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3 row 3) and establishes a fair and solid baseline for comparison. The best TensorMask in Tab. 2d achieves 35.4 mask AP on test-dev (Tab. 3 row 4), close to Mask R-CNN counterpart's 36.8. With ResNet-101, TensorMask achieves 37.1 mask AP with a 1.2 AP gap behind Mask R-CNN. These results demonstrate that dense sliding-window methods can close the gap to 'detect-then-segment' systems ( §2). Qualitative results are shown in Figs. 2, 10, and 11.We report box AP of TensorMask in §A.3. Moreover, compared to Mask R-CNN, one intriguing property of TensorMask is that masks are independent from boxes. In fact, we find joint training of box and mask only gives marginal gain over mask-only training, see §A.4.Speed-wise, the best R-101-FPN TensorMask runs at 0.38s/im on a V100 GPU (all post-processing included), vs. Mask R-CNN's 0.09s/im. Predicting masks in dense sliding windows (&gt;100k) results in a computation overhead, vs. Mask R-CNN's sparse prediction on ≤100 final boxes. Accelerations are possible but outside the scope of this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>More results of Mask R-CNN [17] (top row per set) and TensorMask (bottom row per set) on the last 65 val2017 images (continued in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>More results of Mask R-CNN [17] (top row per set) and TensorMask (bottom row per set) continued from Fig. 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>V U =λsσ HW =s σ HW =s σ HW =λs σ V U =s σ V U =s</figDesc><table><row><cell>V ,Û ,Ĥ,Ŵ</cell><cell cols="2">λV, λU, H, W</cell><cell>λV, λU,</cell><cell>1 λ</cell><cell>H,</cell><cell>1 λ</cell><cell>W</cell></row><row><cell cols="2">up_align2nat</cell><cell cols="6">subsamplê</cell></row></table><note>σ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>are simple heads that use natural and aligned representations, re-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>any baseline head</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C,</cell><cell>1 4</cell><cell>H,</cell><cell>1 4</cell><cell>W</cell><cell>V, U,</cell><cell>1 4</cell><cell cols="2">H,</cell><cell cols="2">1 4</cell><cell>W</cell></row><row><cell>C,</cell><cell>1 2</cell><cell>H,</cell><cell>1 2</cell><cell>W</cell><cell>V, U,</cell><cell>1 2</cell><cell cols="2">H,</cell><cell cols="2">1 2</cell><cell>W</cell></row><row><cell cols="5">C, H, W</cell><cell cols="6">V, U, H, W</cell></row><row><cell></cell><cell></cell><cell cols="5">swap_align2nat head</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">C, H, W</cell><cell cols="2">4V, 4U,</cell><cell>1 4</cell><cell cols="2">H,</cell><cell>1 4</cell><cell>W</cell></row><row><cell cols="5">C, H, W</cell><cell cols="2">2V, 2U,</cell><cell>1 2</cell><cell cols="2">H,</cell><cell>1 2</cell><cell>W</cell></row><row><cell cols="5">C, H, W</cell><cell cols="6">V, U, H, W</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) tensor bipyramid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Simple heads: natural vs. aligned (Fig. 6avs. 6b) with V ×U =15×15 perform comparably if upscaling is not used.</figDesc><table><row><cell>head</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>natural</cell><cell>28.5</cell><cell>52.2</cell><cell>28.6</cell><cell>14.4</cell><cell>30.2</cell><cell>40.1</cell></row><row><cell>aligned</cell><cell>28.9</cell><cell>52.5</cell><cell>29.3</cell><cell>14.6</cell><cell>30.8</cell><cell>40.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The tensor bipyramid substantially improves results compared to the best baseline head (Tab. 2a, row 2) on a feature pyramid(Fig. 7a).V ×UAP AP 50 AP 75 AP S AP M AP L Window sizes: extending from one V ×U window size (per level) to two increases all AP metrics. Both rows use the tensor bipyramid.</figDesc><table><row><cell>29.3 34.0 55.2 35.8 +5.1 +2.7 +6.5 +0.7 +5.5 +7.7 14.6 30.8 40.7 15.3 36.3 48.4 (c) 15×15 tensor bipyramid ∆ 15×15, 11×11 35.2 56.4 37.0 34.0 55.2 35.8 ∆ +1.2 +1.2 +1.2 +2.1 +1.1 +1.3 15.3 36.3 48.4 17.4 37.4 49.7 (d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 .</head><label>2</label><figDesc>Ablations on TensorMask representations on COCO val2017. All variants use ResNet-50-FPN and a 72 epoch schedule.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>aug epochs</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>Mask R-CNN [13]</cell><cell>R-50-FPN</cell><cell>24</cell><cell>34.9</cell><cell>57.2</cell><cell>36.9</cell><cell>15.4</cell><cell>36.6</cell><cell>50.8</cell></row><row><cell>Mask R-CNN, ours</cell><cell>R-50-FPN</cell><cell>24</cell><cell>34.9</cell><cell>56.8</cell><cell>36.8</cell><cell>15.1</cell><cell>36.7</cell><cell>50.6</cell></row><row><cell>Mask R-CNN, ours</cell><cell>R-50-FPN</cell><cell>72</cell><cell>36.8</cell><cell>59.2</cell><cell>39.3</cell><cell>17.1</cell><cell>38.7</cell><cell>52.1</cell></row><row><cell>TensorMask</cell><cell>R-50-FPN</cell><cell>72</cell><cell>35.4</cell><cell>57.2</cell><cell>37.3</cell><cell>16.3</cell><cell>36.8</cell><cell>49.3</cell></row><row><cell cols="2">Mask R-CNN, ours R-101-FPN</cell><cell>72</cell><cell>38.3</cell><cell>61.2</cell><cell>40.8</cell><cell>18.2</cell><cell>40.6</cell><cell>54.1</cell></row><row><cell>TensorMask</cell><cell>R-101-FPN</cell><cell>72</cell><cell>37.1</cell><cell>59.3</cell><cell>39.4</cell><cell>17.4</cell><cell>39.1</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>Comparison with Mask R-CNN for instance segmentation on COCO test-dev.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 .</head><label>4</label><figDesc>Tab. 4 we show the associated bounding-box (bb) object detection results. Overall, TensorMask has a comparable box AP with Mask R-CNN and outperforms RetinaNet. Object detection box AP on COCO test-dev. All models use ResNet-50-FPN. 'TensorMask, box-only' is our model without the mask head: it resembles RetinaNet but with the maskdriven assignment rule and only 2 window sizes instead of 9<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>method</cell><cell>aug epochs</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell></row><row><cell>RetinaNet, ours</cell><cell>24</cell><cell>37.1</cell><cell>55.0</cell><cell>39.9</cell></row><row><cell>RetinaNet, ours</cell><cell>72</cell><cell>39.3</cell><cell>57.2</cell><cell>42.4</cell></row><row><cell>Faster R-CNN, ours</cell><cell>72</cell><cell>40.6</cell><cell>61.4</cell><cell>44.2</cell></row><row><cell>Mask R-CNN, ours</cell><cell>72</cell><cell>41.7</cell><cell>62.5</cell><cell>45.7</cell></row><row><cell>TensorMask, box-only</cell><cell>72</cell><cell>40.8</cell><cell>60.4</cell><cell>43.9</cell></row><row><cell>TensorMask</cell><cell>72</cell><cell>41.6</cell><cell>61.0</cell><cell>45.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5 .</head><label>5</label><figDesc>Multi-task benefits of box training for mask prediction on COCO val2017 with our final ResNet-50-FPN model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Derivation: on the input image pixels, the center of a sliding window is (y·σ HW , x·σ HW ), and a pixel located w.r.t. this window is at (y·σ HW + v·σ VU , x·σ HW +u·σ VU ). Projecting to the HW domain (i.e., normalizing by the unit σ HW ) gives us (y, x) and (y + αv, x + αu).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A fallback is used to increase small object recall: masks smaller than the minimum assignable size are assigned to windows of the smallest size.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07518</idno>
		<title level="m">Hybrid task cascade for instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pietro Perona, and Serge Belongie. Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Rethinking imagenet pre-training</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Instancecut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGN: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MegDet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tensor considered harmful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. on Vision, Image, and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koen Ea Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Soumith Chintala, and Piotr Dollár. A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
