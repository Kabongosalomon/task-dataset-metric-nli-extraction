<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
							<email>jialianw@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Horizon Robotics</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
						</author>
						<title level="a" type="main">Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Confer-ence on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Confer-ence on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA 2020; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413970</idno>
					<note>ACM Reference Format: ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Object detection KEYWORDS object detection</term>
					<term>instance segmentation</term>
					<term>large vocabulary</term>
					<term>long- tailed data distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the previous success of object analysis, detecting and segmenting a large number of object categories with a long-tailed data distribution remains a challenging problem and is less investigated. For a large-vocabulary classifier, the chance of obtaining noisy logits is much higher, which can easily lead to a wrong recognition. In this paper, we exploit prior knowledge of the relations among object categories to cluster fine-grained classes into coarser parent classes, and construct a classification tree that is responsible for parsing an object instance into a fine-grained category via its parent class. In the classification tree, as the number of parent class nodes are significantly less, their logits are less noisy and can be utilized to suppress the wrong/noisy logits existed in the fine-grained class nodes. As the way to construct the parent class is not unique, we further build multiple trees to form a classification forest where each tree contributes its vote to the fine-grained classification. To alleviate the imbalanced learning caused by the long-tail phenomena, we propose a simple yet effective resampling method, NMS Resampling, to re-balance the data distribution. Our method, termed as Forest R-CNN, can serve as a plug-and-play module being applied to most object recognition models for recognizing more than 1000 categories. Extensive experiments are performed on the large vocabulary dataset LVIS. Compared with the Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance with 11.5% and 3.9% AP improvements on the rare categories and overall categories, respectively. Moreover, we achieve state-of-the-art results on the LVIS dataset. Code is available at https:// github.com/ JialianW/ Forest_RCNN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Class</head><p>LVIS <ref type="bibr" target="#b14">[15]</ref> val set tested by Mask R-CNN with ResNet-50-FPN where the noisy logit is defined in Eq. 1. It is seen that the large vocabulary brings a higher chance of noisy logits which can easily lead to a wrong recognition. In the worst case, an object can have 1 and − 1 noisy logits for ground truth class and negative classes, respectively, where is the total number of classes. (b) Probability density distribution of confidence scores on the LVIS v0.5 val set. With our proposed classification forest, the confidence scores of correctly classified samples are improved (top) and those of wrongly classified samples are suppressed (bottom). The top figure is measured by considering the background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the renaissance of deep convolutional neural networks (CNNs), recent years have witnessed great progress in object recognition, including object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> and instance segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref>. Object recognition plays a central role in visual learning and has been widely applied to various applications, e.g., person retrieval <ref type="bibr" target="#b44">[45]</ref>, human-object interaction <ref type="bibr" target="#b39">[40]</ref>, and visual reasoning <ref type="bibr" target="#b41">[42]</ref>.</p><p>Most great works on object recognition have been thoroughly investigated in the few category regime, e.g., PASCAL VOC <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr">(20 classes)</ref> and COCO <ref type="bibr" target="#b24">[25]</ref> (80 classes). However, practical applications are urging the need for recognizing a large number of categories with a long-tailed data distribution, which is a great challenge for most existing methods. Generally, the challenge boils down to two aspects: (i) As the number of categories grows, the chance of obtaining noisy logits in a classifier becomes higher (i.e., inaccurate classifier predictions on either the ground truth class or other negative classes) as shown in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>. This increases the difficulty for generating a correct category label or a high confidence score on the ground truth category, therefore easily leading to a wrong recognition. (ii) The long-tail phenomena inherently occur in a large vocabulary scenario and cause extreme imbalanced data distribution, where few classes (a.k.a. head class) appear very often yet most of other classes (a.k.a. tail class) rarely appear. Due to the imbalanced distribution, most tail classes are overwhelmed by head classes in training, making it difficult to well learn effective classifier and feature representations especially for tail classes.</p><p>In this paper, we propose a novel classification forest together with a simple yet effective data resampling method, striving to alleviate the above problems (i) and (ii), respectively. For (i), we exploit prior knowledge of the relations among categories to cluster thousands of fine-grained classes into tens of parent classes, and construct a classification tree that is responsible for parsing an object into a fine-grained node via its parent node. Since the number of parent classes is significantly less, the parent classifier within the tree obtains fewer noisy logits. We then utilize the parent class probabilities estimated from the parent classifier to suppress the noisy logits produced by the fine-grained classifier. Moreover, in terms of different types of prior knowledge, we construct multiple classification trees to form a classification forest, where each tree will contribute its vote to the fine-grained classification. To illustrate the idea of the classification tree, we show an example in <ref type="figure" target="#fig_2">Fig. 2</ref> where the fine-grained classifier wrongly predicts a "toy" as a "sedan" with ( ) = 0.8 while the parent class probability of "sedan" is ( ℎ ) = 0.05. In the classification tree, the noisy logit of class "sedan" will be calibrated and suppressed by ( ℎ ). By suppressing the noisy logits, our method is able to improve the confidence scores of ground truth class and suppress those of other negative classes as shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>. For (ii), we propose a data resampling method, named as NMS Resampling, to adaptively adjust the Non-maximum Suppression (NMS) threshold for different categories based on their category frequency in training. The NMS Resampling can re-balance the data distribution by preserving more training proposal candidates from tail classes while suppressing those from head classes in the bounding box NMS procedure.</p><p>Recent attempts <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref> also strive to recognize objects under the setting of large vocabulary. For example, <ref type="bibr" target="#b37">[38]</ref> studies the influence of loss functions on head classes and tail classes, and propose an equalization loss to reduce the negative influence of loss gradients on tail classes. Note that <ref type="bibr" target="#b37">[38]</ref> is complementary to our method and can be employed together for gaining performance. Other works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> also utilize a hierarchical tree structure for better classification, which however may still easily yield a wrong recognition due to inaccurate parent class nodes generated by a single tree. Our method, instead, exploits different types of prior knowledge to build a forest, in which classification is achieved in the form of plurality vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>In this work, we propose a novel classification forest that incorporates relations among fine-grained categories via different prior knowledge. When dealing with large vocabulary object recognition, our classification forest can better perform via suppressing the noisy logits existed in the fine-grained classifier. In addition, the NMS Resampling method is proposed for re-balancing the data distribution of a long-tailed dataset during training. Our method, termed as Forest R-CNN, is exhaustively evaluated on the large vocabulary object recognition dataset LVIS <ref type="bibr" target="#b14">[15]</ref> which contains more than 1000 categories. The Forest R-CNN achieves significant AP gains of 11.5% and 3.9% on the rare categories and overall categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Object Detection and Instance Segmentation. Object detection, which plays an important role in various vision applications, has been actively investigated in past decades <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. With deep learning, two mainstream frameworks, i.e., single-stage detector <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> and two-stage detector <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>, have dramatically improved both accuracy and efficiency. Instance segmentation can be viewed as an extension of object detection, where each object instance is bounded by a precise mask instead of a rough bounding box. As a pioneer work, Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> achieves instance segmentation by building an extra segmentation head upon the two-stage object detector Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>. On the basis of the two-stage fashion, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> further propose several framework modifications so as to yield better performance. On the other hand, single-stage approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>, aiming at a faster inference speed, adopt a more straightforward way to directly generate instance masks from the whole feature maps. Different from the previous works that mostly perform in the few category regime, our method focuses more on object detection and instance segmentation in a large vocabulary with long-tailed data distribution.</p><p>Large-Vocabulary and Long-Tailed Visual Recognition. To be applied in the natural world, a visual recognition model is expected to deal with a large number of categories with the long-tail phenomena. One prominent method for long-tailed visual recognition is data re-balancing, which is usually divided into two groups: resampling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> and re-weighting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. Re-sampling schemes typically oversample the data from minority classes while undersample those from frequent classes in training. Re-weighting schemes, instead, assign larger loss weights for the samples from minority classes in training. In addition to data re-balancing, many great works have been made using different ways, e.g., model finetuning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>, metric learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47]</ref>, meta-learning <ref type="bibr" target="#b27">[28]</ref>, and knowledge transfer learning <ref type="bibr" target="#b47">[48]</ref>. Recently, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> also strive to solve the problem of object recognition in a large vocabulary. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36</ref>] exploit a single hierarchical tree structure to aid the fine-grained classification. Different from the above approaches, we propose a novel classification forest aiming at reducing the noisy logits existed in the fine-grained classifier to improve classification capability in a large vocabulary. Besides, in contrast to most re-sampling methods that resample data on the image level, our proposed NMS Resampling scheme re-balances data on the instance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FOREST R-CNN 3.1 Problem Formulation</head><p>Given a static image I ∈ R × ×3 , an object recognition model is required to perform object detection and instance segmentation simultaneously. Thus, each recognized object instance is associated with four predictions:</p><formula xml:id="formula_0">b ∈ R 4 , m ∈ [0, 1] × , ∈ R 1 , ∈ R 1 ,</formula><p>which are bounding box, segmentation mask, category label, and confidence score, respectively. In this paper, we implement our method based on the baseline model Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. Let us denote by f img = N feat (I) ∈ R × × the feature maps extracted from input image I, where N feat , , and are the backbone network, feature stride, and feature channels, respectively. A region proposal network (RPN) <ref type="bibr" target="#b32">[33]</ref> is built upon f img to generate a set of proposal candidate boxes {b ∈ R 4 }. For each proposal candidate, its corresponding proposal features</p><formula xml:id="formula_1">f roi ∈ R 7×7× are ob- tained by f roi = (f img , b ),</formula><p>where is the RoI Align operation <ref type="bibr" target="#b16">[17]</ref>. On the basis of f roi , three head networks, N cls , N box , and N mask are employed to generate , = N cls (f roi ), b = N box (f roi ), and m = N mask (f roi ), in which N cls and N box share part of network parameters. N cls is critical for an object recognition model, since classification result is the premise for evaluating boxes and masks. Let { } =1 be the category set of a given dataset, where is the number of total classes and is the -th fine-grained class. ( ) indicates the classifier inferred probability that a given object belongs to class , and it is calculated by the softmax function ( ) = =1 , where = and is predicted by the -th neuron in the final layer of N cls . In this paper, we name as the logit of class (exponential version) and call as a noisy logit when:</p><formula xml:id="formula_2">       =1 &lt; 1 − , if = =1 &gt; , if ≠ ,<label>(1)</label></formula><p>where is the ground truth class and 0 &lt; , &lt; 1 are two small constants. In <ref type="bibr" target="#b16">[17]</ref>, the confidence score of class and the category label of a given object are obtained by:</p><formula xml:id="formula_3">= ( ), = argmax , = 1, 2, ..., .<label>(2)</label></formula><p>Generally speaking, to yield a correct label and a high confidence score on , Eq. 2 is satisfactory in the few category regime. However, in a large vocabulary scenario, e.g., &gt; 1000, there exist many more noisy logits within { } =1 as shown in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>, making it difficult to generate a correct label or a high confidence score on using Eq. 2. As shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, with Eq. 2 the noisy logits result in many samples correctly classified with low confidence scores or misclassified with high confidence scores (blue curve). Moreover, the long-tail phenomena that inherently occur in a large vocabulary make it hard for a model to well learn effective classifier and feature representations against tail classes. To alleviate the above problems, we propose a classification forest ( § 3.2 and § 3.3) for enhancing the capability of classifying a large number of categories and an NMS Resampling ( § 3.4) for re-balancing the long-tailed data distribution, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification Tree</head><p>Tree Structure. In real-world scenes, we are aware of prior knowledge of the relationships among object categories. For example, a "school bus" and a "sedan" have the same parent class "vehicle" when considering lexical relation, while "steering wheel" and "basketball" have the same parent class "circularity" when considering geometrical relation. Accordingly, for each type of prior knowledge, all fine-grained classes can be clustered into parent classes, where each parent class is on behalf of a characteristic of its children classes. Note that may vary when we consider different types of prior knowledge. Based on the hierarchical affiliation obtained by prior knowledge, we can build up a tree structure which is a basic component of the classification forest. As shown in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, the proposed classification tree has 3 levels: the first level is a single root node; the second level consists of parent class nodes { } =1 ; the last level comprises leaf nodes which represent the fine-grained classes. Each node (excluding the root node) is associated with its corresponding logit = as the node value. We add additional fully connected layers within N cls to generate the parent class logit . During training, each level is regarded as an individual classifier and supervised by the softmax function with cross-entropy loss for learning . Since is the classifier logit, it essentially reflects the likelihood that an object belongs to class . Namely, the higher is, the more confidently the given object belongs to class (vice versa). Also, in the proposed tree structure, we define the path score as the product of the values of nodes through which the path passes (excluding the root node). For an instance, the path score from the root node to 3 is</p><formula xml:id="formula_4">→ 3 = 3 × 1 .</formula><p>Preliminary Model. In Eq. 2, both category label and confidence score are determined by the logits of fine-grained classifier only. Different from Eq. 2, our method takes into account the parent classes   in order to reduce the negative influence of the noisy logits produced by the fine-grained classifier. To this end, a straightforward way is to combine ( ) with the corresponding ( ) as:</p><formula xml:id="formula_5">= ( ) × ( ), ∈ ℎ( ), (3a) = argmax , = 1, 2, ..., ,<label>(3b)</label></formula><p>where ∈ ℎ( ) indicates the fine-grained class that is a child of parent class . In Eq. 3, to parse an object into a fine-grained category, both the fine-grained classifier and parent classifier need to reach a consensus. As an example, the fine-grained classifier wrongly predicts a "toy" as a "sedan" with ( ) = 0.8, while the probability of the parent class of "sedan" is ( ℎ ) = 0.05. In this case, the parent classifier has a different "opinion" with the fine-grained classifier, and the final confidence score will be downgraded to 0.04. Therefore, by incorporating the parent class probability, we are able to reduce the confidence scores of those categories misclassified by the fine-grained classifier, enabling a model to be more fault-tolerant with regard to the noisy logits of the fine-grained classifier.</p><p>Confidence Score Generation. In Eq. 3a, the confidence score is generated by the product of fine-grained class probability and parent class probability, which however fails to improve the confidence score of the ground truth class. For example, given a "sedan", if ( ) = 0.7 and ( ℎ ) = 0.6, then becomes 0.42 unexpectedly. To solve this problem, we use the parent class probabilities to directly calibrate the fine-grained class logits instead of scaling the fine-grained class probabilities. Concretely, the logit/node value of is calculated by ′ = × ( ), where ∈ ℎ( ). Afterwards, similar to the original classifier, the confidence score of the fine-grained class is obtained according to the calibrated fine-grained nodes as:</p><formula xml:id="formula_6">= ( ) × =1 ( ★ ) × , ∈ ℎ( ), ∈ ℎ( ★ ),<label>(4a)</label></formula><formula xml:id="formula_7">= ′ =1 ′ .<label>(4b)</label></formula><p>Compared to the logits of fine-grained classifier</p><formula xml:id="formula_8">{ } =1 , { ′ } =1</formula><p>are calibrated by the parent class probabilities which can effectively reduce the noisy logits (as evidenced in <ref type="figure">Fig. 7)</ref>. In contrast to Eq. 3a, Eq. 4 suppresses the noisy logits on not only negative classes but also the ground truth class. That is to say, we not only reduce the confidence scores of negative classes but also further improve those of the ground truth classes (see <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>). For the above example of ( ) = 0.7 and ( ℎ ) = 0.6, it indicates that account for 70% in =1 . With Eq. 4, our method will scale by ( ℎ ) = 0.6 and the other ∉ ℎ by a smaller ( ≠ ℎ ) ≤ 0.4. Accordingly, ′ will be less noisy and make up more than 70% of =1 ′ , which leads to a new higher than 0.7.</p><p>Label Inference. Based on Eq. 4 and the definition of path score, the category label is inferred by:</p><formula xml:id="formula_9">= argmax = argmax ′ , = 1, 2, ..., ,</formula><p>= argmax × , ∈ ℎ( ) and = 1, 2, ..., , = argmax → , = 1, 2, ..., .</p><p>It is seen that the proposed classification tree parses an object into a fine-grained class by finding the maximum path score from the root node to leaf nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification Forest</head><p>Forest Structure. Although a classification tree ( § 3.2) can suppress the noisy logits produced by the fine-grained classifier via incorporating the parent class probabilities, false classification can still easily happen if there exist errors in the parent classifier. To alleviate this problem, we build a classification forest that consists of different classification trees, and each tree will vote for the final classification decision. As shown in <ref type="figure" target="#fig_5">Fig. 4 (a)</ref>, each tree that is based on one type of prior knowledge of category relations may have a different structure from the others, and the parent class nodes set of the -th tree is denoted by { } =1 . The leaf node set, which represents the fine-grained classes of a given dataset, is the same for all the trees. In this paper, we exploit three types of prior knowledge of the relations among fine-grained classes, i.e., lexical relation, visual relation, and geometrical relation, in order to take different aspects of object characteristics into consideration. Based on the relations, we then construct three different classification trees, respectively. Note that the proposed classification forest does not constrain the number of classification trees and we observe that in our experiments three trees are sufficient for achieving improved performance.</p><p>Confidence Score Generation. In each tree of the classification forest, the value of fine-grained class node is calibrated by its corresponding parent class probability as shown in <ref type="figure" target="#fig_5">Fig. 4 (c)</ref>. Thus, for , we have different calibrated values</p><formula xml:id="formula_11">{ ′ − } =1 , where ′ −</formula><p>implies the likelihood that the -th classification tree considers a given object to belong to . In order to take advantage of each one of { ′ − } =1 and produce a final confidence score , we take the average of ′ −1 , ..., ′ − and denote it as △ , which indicates the comprehensive likelihood that all the classification trees consider a given object to belong to . Then, similar to Eq. 4, the confidence score of is calculated according to { △ } =1 as:</p><formula xml:id="formula_12">= △ =1 △ .<label>(6)</label></formula><p>Compared to ′ − , △ is calibrated by multiple parent classifiers.</p><p>Since every tree is dedicated to generating confidence score and the likelihood that multiple different parent classifiers yield errors simultaneously is low, the classification forest is robuster than a single classification tree as shown in Tab.3.</p><p>Label Inference. In § 3.2, the category label is inferred by finding the maximum path score from the root node to leaf nodes. By contrast, to integrate the decisions made by all the trees, we consider a leaf node as the predicted category if the sum of each path score from the root node to of all the trees is maximum. Formally, the category label is inferred as:</p><formula xml:id="formula_13">= argmax ∑︁ =1 → , = 1, 2, ..., ,<label>(7)</label></formula><p>where → denotes the path score from the root node to in the -th tree. In essence, Eq. 7 is a plurality vote of all the classification trees, which is generally considered to be better than using a single classification tree as in Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NMS Resampling</head><p>The long-tail phenomena inherently occur in a large vocabulary dataset and the real visual world, in which few classes appear very often but most other classes rarely appear. Such imbalanced data distribution introduces great challenges for learning effective classifier and feature representations against tail classes. To re-balance the long-tailed data distribution, we propose a simple yet effective resampling method, termed as NMS Resampling, by adaptively adjusting the NMS threshold during training.</p><p>As known, after generating a large amount of proposal boxes from the RPN, the NMS is applied to filter out highly overlapped proposals so as to reduce redundancy. The NMS threshold is classagnostic and set to a fixed value (e.g., 0.7) for all categories in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>, while our method adaptively adjusts the thresholds for different categories. Specifically, we strive to re-balance the data distribution by utilizing an NMS threshold that is inverse to the data amount of a certain category. That is, we set higher thresholds for tail classes but lower thresholds for head classes. Following this idea, we propose two NMS Resampling schemes to determine the thresholds for specific categories as follows:</p><p>NMS Resampling-Discrete. The LVIS <ref type="bibr" target="#b14">[15]</ref> dataset annotates each category with a category frequency indicating the number of images in which the category appears. Following the descending category frequency, in <ref type="bibr" target="#b14">[15]</ref>, all 1230 classes are uniformly divided into three groups: frequent, common, and rare. In NMS Resampling-Discrete, we employ three discrete NMS thresholds: , , and for the frequent, common, and rare classes, respectively, where &lt; &lt; . In experiments our method is not sensitive to specific values of , , and so long as , , follow ascending order NMS Resampling-Linear. We first uniformly divides three intervals with length for the frequent, common, and rare classes, respectively. Then, in each interval, we linearly assign the NMS threshold to each category as:</p><formula xml:id="formula_14">ℎ ℎ =                + × − − , if ∈ + × − − , if ∈ + × − − , if ∈ ,<label>(8)</label></formula><p>where &lt; &lt; and is the category frequency of . and are the maximum and minimum category frequencies in the class group. , , , and are respectively set to 0.65, 0.75, 0.85, and 0.1 by default in our experiments.</p><p>Given a foreground proposal box b , we first compute its corresponding NMS threshold based on the above schemes. Then, the remaining proposal boxes will be suppressed if they have overlaps with b large then the threshold, otherwise, they will be preserved for the next round of NMS procedure. We use the original NMS threshold of 0.7 for background proposals. The proposed NMS Resampling eases the problem of imbalanced data distribution by preserving more training proposal candidates from the tail classes and suppressing some of those from head classes during training. Compared with the image resampling <ref type="bibr" target="#b14">[15]</ref>, our method not only is more effective (as shown in Tab.4) but also avoids repeating training images which may cause overfitting and extra training time. In principle, the NMS Resampling is also applicable to single-stage detectors that need the NMS to reduce redundancy during training, and we leave it for future work.  <ref type="bibr" target="#b16">[17]</ref> using different backbone networks on the LVIS v0.5 val set. AP denotes the mask AP and AP denotes the box AP. The subscripts "r", "c", and "f" denote performance on the rare, common, and frequent classes, respectively. Network Architecture. The overall network architecture of the proposed Forest R-CNN is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. In the classification head N cls , we add extra fully connected layer (FC) branches for predicting the node values of parent classes compared with <ref type="bibr" target="#b16">[17]</ref>. Since the extra FC branches are inserted after the first FC of N cls whose channel dimensions are 1024, our method only introduces little additional computational overhead. Moreover, we incorporate the prior knowledge of geometrical relation into the mask head N mask . In contrast to the original class-specific mask head, we reduce the number of output channels from to , where is the number of parent classes in the geometrical tree. In inference, the mask of class is fetched from the -th channel, where ∈ ℎ(</p><p>) and is the -th parent class in the geometrical tree.</p><p>Loss Function. The overall loss function of the Forest R-CNN is defined as:</p><formula xml:id="formula_15">= + 1 − + ... + − ,<label>(9)</label></formula><p>where is the original loss of Mask R-CNN and − is the parent classification loss of the -th classification tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experiment Setup</head><p>LVIS Dataset. LVIS <ref type="bibr" target="#b14">[15]</ref> is a large vocabulary dataset for object detection and instance segmentation. There are in total 1230 and 1203 categories in LVIS v0.5 and v1.0 datasets, which follow a longtailed data distribution. All categories are divided into three groups based on the number of images that contains those categories: rare (1-10 images), common (11-100 images), and frequent (&gt;100 images). Our method is trained on the train set and evaluated on the val set. We adopt the evaluation metric AP across IoU threshold from 0.5 to 0.95 for both object detection and instance segmentation results. Our major experiments and ablation studies are performed on LVIS v0.5 dataset. We update the results of Forest R-CNN on LVIS v1.0 dataset in this arxiv V2 version, which is shown in Tab.7.</p><p>Implementation Details. The Forest R-CNN uses the same basic settings as in <ref type="bibr" target="#b14">[15]</ref>, e.g., image size, score threshold, and the number of object instances per image. Our method is trained with 24 epochs using the SGD optimizer, and the initial learning rate is set to 0.02 and decreased by a factor of 10 at the 16-th epoch and 22-th epoch, respectively. We use three relations among fine-grained categories, i.e., lexical relation, visual relation, and geometrical relation, respectively, to construct three classification trees before training. For the visual and geometrical trees, we employ K-means to cluster the visual features and ground truth binary masks of 1230 fine-grained categories into and parent classes, respectively, where the visual features are obtained by the Mask-RCNN baseline. and are set to 25 and 50, respectively. For the lexical tree, it is constructed according to a subgraph of WordNet <ref type="bibr" target="#b28">[29]</ref>, which results in = 108 parent classes. In the following experiments, our method is equipped with the NMS Resampling-Discrete and ResNet-50-FPN by default. For the NMS Resampling-Discrete, we empirically set , , to 0.7, 0.8 and 0.9 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Comparison with Baselines. As shown in Tab.1, we compare the proposed Forest R-CNN with the baseline Mask R-CNN under different backbone networks. The Forest R-CNN consistently improves AP and AP over the baseline with significant gains of 10.1%-12% and 3.3%-4.1% on rare categories and overall categories, respectively. We also separately assess the proposed classification forest ( § 3.3) and NMS Resampling ( § 3.4) in Tab.2. We see from the results that the NMS Resampling improves AP from 6.8% to 15.6% with the ResNet-50 <ref type="bibr" target="#b17">[18]</ref>, demonstrating the strong effectiveness of our re-balancing schemes for tail classes. With the classification forest, our method consistently improves performance on overall categories, which shows that the classification forest is specialized to recognize a large number of categories. Visualized samples can be found in <ref type="figure" target="#fig_10">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the Classification Forest.</head><p>To study the effectiveness of the classification forest ( § 3.3), we evaluate the average number of noisy logits per object on the LVIS v0.5 val set. As shown in <ref type="figure">Fig. 7</ref>, { △ } =1 contains fewer noisy logitis than { } =1 , which demonstrates that the classification forest can effectively suppress the noisy logits produced by the fine-grained classifier. We also investigate the probability density distribution of confidence scores as presented in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>. We clearly see from the figure that with classification forest our method can effectively improve the  confidence scores of correct classified objects and suppress those of wrongly classified objects. This can help to improve the precision/recall of a model under the same recall/precision as evidenced in <ref type="figure">Fig. 6</ref>. Besides, we evaluate the Forest R-CNN with different settings of the classification trees. As shown in Tab.3, the Forest R-CNN with a single classification tree ( § 3.2) improves around 1% AP over the baseline, and the lexical tree achieves slightly better performance. With multiple trees, our method further boosts the AP to 24.9%-25.6%, validating the effectiveness of the classification forest ( § 3.3).</p><p>Effectiveness of the NMS Resampling. We compare the image resampling method <ref type="bibr" target="#b14">[15]</ref> with the two proposed NMS Resampling schemes in Tab.4. We see from the results that both the NMS Resampling-Linear and NMS Resampling-Discrete achieve better performance than the image resampling. The reason for the performance gap is that image resampling may introduce severer overfitting by repeating the same images during training. Also, we assess       parent classes using K-means. We observe in experiments that the results are stable under different K-means initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art</head><p>LVIS v0.5. In Tab.8, we compare our method with state-of-theart methods on the LIVS v0.5 dataset. It is worth noting that the proposed Forest R-CNN achieves state-of-the-art performance under different experimental setups. Moreover, the Forest R-CNN improves 3.9%-7.0% AP over the second-best results of different setups on the rare category. It demonstrates that our method is skilled in recognizing the tail classes as well.</p><p>LVIS v1.0. We report the result of Forest R-CNN with ResNet-50-FPN on the LVIS v1.0 dataset in Tab.7. Forest R-CNN improves 4% AP and 14.2% AP on overall categories and rare categories, respectively, compared to the baseline Mask R-CNN. Moreover, Forest R-CNN achieves competitive performance compared to EQL <ref type="bibr" target="#b37">[38]</ref> and De-confound <ref type="bibr" target="#b38">[39]</ref> which are equipped with more complex network settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work presents a novel object recognition model, Forest R-CNN, which is equipped with two key components: (i) the classification forest and (ii) the NMS Resampling. The classification forest suppresses the noisy logits produced by a fine-grained classifier, enhancing the capability of classifying thousands of categories. The NMS Resampling re-balances the long-tailed data distribution by adaptively adjusting the NMS thresholds for different categories, which aids our method in recognizing more objects from tail classes. The above designs enable strong performance on detecting and segmenting a large number of object instances, outperforming stateof-the-art competitors on the LVIS dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Statistics of noisy logits of COCO val set vs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Brief illustration of the classification tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Classification tree where parent and finegrained class nodes are contained in and , respectively. entity and indicates the root node and calibrated leaf node, respectively. (b) The category label is estimated by finding the maximum path score from the root node to leaf nodes. (c) The confidence scores of fine-grained classes are produced in terms of the values of calibrated leaf nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) Classification forest with 3 trees. (b) and (c) are brief illustrations of label inference and confidence score generation using (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Network architecture of the Forest R-CNN. The dotted rectangles in the NMS Resampling denote the proposal boxes which are suppressed during the NMS process. The NMS Resampling is only used in the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 ResNet 7 3. 5</head><label>275</label><figDesc>Backbone Method AP AP 50 AP 75 AP r AP c AP f AP AP 50 AP 75 AP AP AP ResNet-50-FPN Mask R-CNN 21.7 34.7 22.8 6.8 22.6 26.4 21.8 37.1 22.5 6.5 21.6 28.0 Forest R-CNN(Ours) 25.6 40.3 27.1 18.3 26.4 27.6 25.9 42.7 27.2 16.9 26.1 29.CNN(Ours) 26.9 42.2 28.4 20.1 27.9 28.3 27.5 44.9 29.0 20.0 27.5 30.4 ResNeXt-101-32×4d-FPN Mask R-CNN 24.8 38.5 26.2 10.0 26.4 28.6 24.8 41.5 25.8 8.6 25.0 30.9 Forest R-CNN(Ours) 28.5 43.8 30.9 21.6 29.7 29.7 28.8 46.3 30.9 20.6 29.2 31.Network Architecture and Loss Function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The mask precision-recall (PR) curves w/ and w/o using the classification forest under different IoU thresholds. Statistics of the noisy logits w/ and w/o using the classification forest on the LVIS v0.5 val set. The results suggest that the proposed classification forest can effectively suppress the noisy logits from the fine-grained classifier. The NMS resampling is enabled for both baseline and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>NMS Resampling-Discrete AP AP AP r AP c AP f = 0.7, = 0.8, = 0.9 23.5 23.5 15.6 24.1 25.9 = 0.6, = 0.7, = 0.8 23.4 23.1 15.2 23.8 26.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Mask R-CNN<ref type="bibr" target="#b16">[17]</ref> vs. Forest R-CNN. Mask R-CNN exhibits more wrong classification and miss recognition. For neat visualization, we apply the NMS with threshold of 0.7 and filter out the predictions with scores lower than 0.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with the baseline Mask R-CNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the proposed NMS Resampling and classification forest. "NR" denotes the NMS Resampling.</figDesc><table><row><cell>NR Classification Forest AP AP</cell><cell cols="2">AP r AP c AP f</cell></row><row><cell>21.7 21.8</cell><cell>6.8</cell><cell>22.6 26.4</cell></row><row><cell cols="3">23.5 23.5 15.6 24.1 25.9</cell></row><row><cell cols="3">23.6 24.0 10.9 24.4 27.5</cell></row><row><cell cols="3">25.6 25.9 18.3 26.4 27.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of the Forest R-CNN using different classification trees.</figDesc><table><row><cell>Geometrical Visual Lexical AP AP</cell><cell>AP r AP c AP f</cell></row><row><cell cols="2">23.5 23.5 15.6 24.1 25.9</cell></row><row><cell cols="2">24.3 24.3 15.2 24.8 27.3</cell></row><row><cell cols="2">24.3 24.7 14.8 25.0 27.2</cell></row><row><cell cols="2">24.6 24.9 16.5 25.0 27.2</cell></row><row><cell cols="2">24.9 25.3 16.0 25.8 27.4</cell></row><row><cell cols="2">25.2 25.4 17.5 25.8 27.5</cell></row><row><cell cols="2">25.6 25.9 18.3 26.4 27.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Linear) 23.7 23.5 12.3 25.2 26.3 (NR-Discrete) 23.5 23.5 15.6 24.1 25.9</figDesc><table><row><cell></cell><cell cols="4">Performance comparison with the image resam-</cell></row><row><cell cols="5">pling [15]. "NR-Linear" denotes the NMS Resampling-</cell></row><row><cell cols="5">Linear and "IR" denotes the image resampling.</cell></row><row><cell>IR [15]</cell><cell>NR</cell><cell>AP AP</cell><cell cols="2">AP r AP c AP f</cell></row><row><cell></cell><cell></cell><cell>21.7 21.8</cell><cell>6.8</cell><cell>22.6 26.4</cell></row><row><cell></cell><cell></cell><cell cols="3">23.0 22.7 13.8 23.4 26.1</cell></row><row><cell></cell><cell>(NR-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of the NMS Resampling-Discrete with different thresholds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results of the Forest R-CNN using different number of parent classes. The visual tree is used for experiments.</figDesc><table><row><cell cols="2">Number of Parent Classes AP AP</cell><cell>AP r AP c AP f</cell></row><row><cell>=25</cell><cell cols="2">24.3 24.7 14.8 25.0 27.2</cell></row><row><cell>=50</cell><cell cols="2">24.1 24.8 15.0 24.8 26.8</cell></row><row><cell>=100</cell><cell cols="2">23.7 24.3 12.8 24.9 26.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance Comparison on the LVIS v1.0 val set. "R" denotes the ResNet with FPN, and "Cascade" denotes the Cascade R-CNN<ref type="bibr" target="#b2">[3]</ref>. Resampling with different threshold settings. As shown in Tab.5, our method is not sensitive to the specific threshold values so long as they are inverse to the data amount of categories.Number of the Parent Classes. To investigate the impact of hyper-parameter on the classification tree, we experiment the Forest R-CNN with the Visual tree in Tab.6. We see from the table that the performances of = 25 and = 50 are close and the results of = 100 get slightly worse. This suggests that it is better to set ≤ 50 when clustering ∼ 1000 fine-grained class into</figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell>AP AP AP r AP c AP f</cell></row><row><cell>Mask R-CNN</cell><cell>R-101</cell><cell>20.8 21.7 1.4 19.4 30.9</cell></row><row><cell>EQL [38]</cell><cell>R-101</cell><cell>22.9 24.2 3.7 23.6 30.7</cell></row><row><cell cols="3">Cascade Mask R-CNN Cascade R-101 22.6 25.2 2.0 22.0 32.5</cell></row><row><cell cols="3">De-confound [39] Cascade R-101 23.5 25.8 5.2 22.7 32.3</cell></row><row><cell>Mask R-CNN</cell><cell>R-50</cell><cell>19.2 20.0 0.0 17.2 29.5</cell></row><row><cell>EQL [38]</cell><cell>R-50</cell><cell>21.6 22.5 3.8 21.7 29.2</cell></row><row><cell>Forest R-CNN (Ours)</cell><cell>R-50</cell><cell>23.2 24.6 14.2 22.7 27.7</cell></row><row><cell>the proposed NMS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison with the state-of-the-art methods on the LVIS v0.5 val set. We denote "IR" as the image resampling<ref type="bibr" target="#b14">[15]</ref> and "MST" as the multi-scale training.Method Setting AP AP 50 AP 75 AP r AP c AP f AP FPN &amp; MST 28.2 44.3 29.7 20.2 29.6 29.6 21.0 36.0 42.0 28.6</figDesc><table><row><cell>AP</cell><cell>AP</cell><cell>AP</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is supported in part by the start-up funds from State University of New York at Buffalo and gift grant from Horizon Robotics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">YOLACT: Realtime instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SipMask: spatial information preservation for fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">D2Det: Towards high quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11485" to="11494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9705" to="9714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Triply supervised decoder networks for joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7392" to="7401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-level semantic networks for multi-Scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1565" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lvis: a dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Intelligent Computing (ICIC)</title>
		<meeting>International Conference on Intelligent Computing (ICIC)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00900</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning sampling distributions for efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">R-FCN-3000 at 30fps: Decoupling detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08548</idno>
		<title level="m">Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Equalization Loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10152</idno>
		<title level="m">Solov2: dynamic, faster and stronger</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual deanimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal-context enhanced detection of heavily occluded pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13430" to="13439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-Mimic learning for small-scale pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unequal-training for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7812" to="7821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">BBN: Bilateralbranch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
