<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECURRENT NEURAL NETWORKS FOR MULTIVARI- ATE TIME SERIES WITH MISSING VALUES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
							<email>zche@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
							<email>dsontag@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECURRENT NEURAL NETWORKS FOR MULTIVARI- ATE TIME SERIES WITH MISSING VALUES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multivariate time series data are ubiquitous in many practical applications ranging from health care, geoscience, astronomy, to biology and others. They often inevitably carry missing observations due to various reasons, such as medical events, saving costs, anomalies, inconvenience and so on. It has been noted that these missing values are usually informative missingness <ref type="bibr" target="#b24">(Rubin, 1976)</ref>, i.e., the missing values and patterns provide rich information about target labels in supervised learning tasks (e.g, time series classification). To illustrate this idea, we show some examples from MIMIC-III, a real world health care dataset in <ref type="figure" target="#fig_0">Figure 1</ref>. We plot the Pearson correlation coefficient between variable missing rates, which indicates how often the variable is missing in the time series, and the labels of our interests such as mortality and ICD-9 diagnoses. We observe that the missing rate is correlated with the labels, and the missing rates with low rate values are usually highly (either positive or negative) correlated with the labels. These findings demonstrate the usefulness of missingness patterns in solving a prediction task.</p><p>In the past decades, various approaches have been developed to address missing values in time series <ref type="bibr" target="#b25">(Schafer &amp; Graham, 2002)</ref>. A simple solution is to omit the missing data and to perform analysis only on the observed data. A variety of methods have been developed to fill in the missing values, such as smoothing or interpolation <ref type="bibr" target="#b14">(Kreindler &amp; Lumsden, 2012)</ref>, spectral analysis <ref type="bibr" target="#b18">(Mondal &amp; Percival, 2010)</ref>, kernel methods <ref type="bibr" target="#b22">(Rehfeld et al., 2011)</ref>, multiple imputation (White et al., 2011), Middle/right figures respectively shows the correlations between missing rate and mortality/ICD-9 diagnosis categories (x-axis, target label; y-axis, input variable; color, correlation value). Please refer to Appendix A.1 for more details.</p><p>and EM algorithm <ref type="bibr" target="#b7">(García-Laencina et al., 2010)</ref>. <ref type="bibr" target="#b25">Schafer &amp; Graham (2002)</ref> and references therein provide excellent reviews on related solutions. However, these solutions often result in a twostep process where imputations are disparate from prediction models and missing patterns are not effectively explored, thus leading to suboptimal analyses and predictions <ref type="bibr" target="#b31">(Wells et al., 2013)</ref>.</p><p>In the meantime, Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997)</ref> and Gated Recurrent Unit (GRU) , have shown to achieve the state-of-the-art results in many applications with time series or sequential data, including machine translation  and speech recognition <ref type="bibr" target="#b8">(Hinton et al., 2012)</ref>. RNNs enjoy several nice properties such as strong prediction performance as well as the ability to capture long-term temporal dependencies and variable-length observations. RNNs for missing data has been studied in earlier works <ref type="bibr" target="#b1">(Bengio &amp; Gingras, 1996;</ref><ref type="bibr" target="#b29">Tresp &amp; Briegel, 1998;</ref><ref type="bibr" target="#b19">Parveen &amp; Green, 2001)</ref> and applied for speech recognition and blood-glucose prediction. Recent works <ref type="bibr" target="#b15">(Lipton et al., 2016;</ref><ref type="bibr" target="#b4">Choi et al., 2015)</ref> tried to handle missingness in RNNs by concatenating missing entries or timestamps with the input or performing simple imputations. However, there have not been works which systematically model missing patterns into RNN for time series classification problems. Exploiting the power of RNNs along with the informativeness of missing patterns is a new promising venue to effectively model multivariate time series and is the main motivation behind our work.</p><p>In this paper, we develop a novel deep learning model based on GRU, namely GRU-D, to effectively exploit two representations of informative missingness patterns, i.e., masking and time interval. Masking informs the model which inputs are observed (or missing), while time interval encapsulates the input observation patterns. Our model captures the observations and their dependencies by applying masking and time interval (using a decay term) to the inputs and network states of GRU, and jointly train all model components using back-propagation. Thus, our model not only captures the long-term temporal dependencies of time series observations but also utilizes the missing patterns to improve the prediction results. Empirical experiments on real-world clinical datasets as well as synthetic datasets demonstrate that our proposed model outperforms strong deep learning models built on GRU with imputation as well as other strong baselines. These experiments show that our proposed method is suitable for many time series classification problems with missing data, and in particular is readily applicable to the predictive tasks in emerging health care applications. Moreover, our method provides useful insights into more general research challenges of time series analysis with missing data beyond classification tasks, including 1) a general deep learning framework to handle time series with missing data, 2) effective solutions to characterize the missing patterns of not missing-completely-at-random time series data such as modeling masking and time interval, and 3) an insightful approach to study the impact of variable missingness on the prediction labels by decay analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RNN MODELS FOR TIME SERIES WITH MISSING VARIABLES</head><p>We denote a multivariate time series with D variables of length T as X = (x 1 , x 2 , . . . , x T ) T ∈ R T ×D , where x t ∈ R D represents the t-th observations (a.k.a., measurements) of all variables and x d t denotes the measurement of d-th variable of x t . Let s t ∈ R denote the time-stamp when the t-th observation is obtained and we assume that the first observation is made at time t = 0 (s 1 = 0). A time series X could have missing values. We introduce a masking vector m t ∈ {0, 1} D to denote which variables are missing at time step t. The masking vector for x t is given by</p><formula xml:id="formula_0">m d t = 1, if x d t is observed 0, otherwise</formula><p>For each variable d, we also maintain the time interval δ d t ∈ R since its last observation as</p><formula xml:id="formula_1">δ d t =    s t − s t−1 + δ d t−1 , t &gt; 1, m d t−1 = 0 s t − s t−1 , t &gt; 1, m d t−1 = 1 0, t = 1</formula><p>An example of these notations is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In this paper, we are interested in the time series classification problem, where we predict the labels l n given the time series data D,</p><formula xml:id="formula_2">where D = {(X n , s n , M n , ∆ n , l n )} N n=1 , and X n = x (n) 1 , . . . , x (n) Tn , s n = s (n) 1 , . . . , s (n) Tn , M n = m (n) 1 , . . . , m (n) Tn , ∆ n = δ (n) 1 , . . . , δ (n)</formula><p>Tn , and l n ∈ {1, . . . , L}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRU-RNN FOR TIME SERIES CLASSIFICATION</head><p>We investigate the use of recurrent neural networks (RNN) for time-series classification, as their recursive formulation allow them to handle variable-length sequences naturally. Moreover, RNN shares the same parameters across all time steps which greatly reduces the total number of parameters we need to learn. Among different variants of the RNN, we specifically consider an RNN with gated recurrent units <ref type="bibr" target="#b5">Chung et al., 2014)</ref>, but similar discussion and convolutions are also valid for other RNN models such as LSTM <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><p>The structure of GRU is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a). GRU has a reset gate r j t and an update gate z j t for each of the hidden state h j t to control. At each time t, the update functions are shown as follows:</p><formula xml:id="formula_3">z t = σ (W z x t + U z h t−1 + b z ) r t = σ (W r x t + U r h t−1 + b r ) h t = tanh (W x t + U (r t h t−1 ) + b) h t = (1 − z t ) h t−1 + z t h t</formula><p>where matrices W z , W r , W , U z , U r , U and vectors b z , b r , b are model parameters. We use σ for element-wise sigmoid function, and for element-wise multiplication. This formulation assumes that all the variables are observed. A sigmoid or soft-max layer is then applied on the output of the GRU layer at the last time step for classification task.</p><p>Existing work on handling missing values lead to three possible solutions with no modification on GRU network structure. One straightforward approach is simply replacing each missing observation with the mean of the variable across the training examples. In the context of GRU, we have Tn t=1 m d t,n . We refer to this approach as GRU-mean.</p><formula xml:id="formula_4">x d t ← m d t x d t + (1 − m d t )x d (1) ෩ IN OUT (a) GRU</formula><p>A second approach is exploiting the temporal structure in time series. For example, we may assume any missing value is same as its last measurement and use forward imputation (GRU-forward), i.e.,</p><formula xml:id="formula_5">x d t ← m d t x d t + (1 − m d t )x d t<label>(2)</label></formula><p>where t &lt; t is the last time the d-th variable was observed.</p><p>Instead of explicitly imputing missing values, the third approach simply indicates which variables are missing and how long they have been missing as a part of input, by concatenating the measurement, masking and time interval vectors as</p><formula xml:id="formula_6">x (n) t ← x (n) t ; m (n) t ; δ (n) t (3) where x (n) t</formula><p>can be either from Equation (1) or (2). We later refer to this approach as GRU-simple.</p><p>These approaches solve the missing value issue to a certain extent, However, it is known that imputing the missing value with mean or forward imputation cannot distinguish whether missing values are imputed or truly observed. Simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values. Thus none of them fully utilize missingness in data to achieve desirable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRU-D: MODEL WITH TRAINABLE DECAYS</head><p>To fundamentally address the issue of missing values in time series, we notice two important properties of the missing values in time series, especially in health care domain: First, the value of the missing variable tend to be close to some default value if its last observation happens a long time ago. This property usually exists in health care data for human body as homeostasis mechanisms and is considered to be critical for disease diagnosis and treatment <ref type="bibr" target="#b30">(Vodovotz et al., 2013)</ref>. Second, the influence of the input variables will fade away over time if the variable has been missing for a while. For example, one medical feature in electronic health records (EHRs) is only significant in a certain temporal context <ref type="bibr" target="#b33">(Zhou &amp; Hripcsak, 2007)</ref>. Therefore we propose a GRU-based model called GRU-D, in which a decay mechanism is designed for the input variables and the hidden states to capture the aforementioned properties. We introduce decay rates in the model to control the decay mechanism by considering the following important factors. First, each input variable in health care time series has its own medical meaning and importance. The decay rates should be flexible to differ from variable to variable based on the underlying properties associated with the variables. Second, as we see lots of missing patterns are informative in prediction tasks, the decay rate should be indicative of such patterns and benefits the prediction tasks. Furthermore, since the missing patterns are unknown and possibly complex, we aim at learning decay rates from the training data rather than being fixed a priori. That is, we model a vector of decay rates γ as</p><formula xml:id="formula_7">γ t = exp {− max (0, W γ δ t + b γ )}<label>(4)</label></formula><p>where W γ and b γ are model parameters that we train jointly with all the other parameters of the GRU. We chose the exponentiated negative rectifier in order to keep each decay rate monotonically decreasing in a reasonable range between 0 and 1. Note that other formulations such as a sigmoid function can be used instead, as long as the resulting decay is monotonic and is in the same range.</p><p>Our proposed GRU-D model incorporates two different trainable decays to utilize the missingness directly with the input feature values and implicitly in the RNN states. First, for a missing variable, we use an input decay γ x to decay it over time toward the empirical mean (which we take as a default configuration), instead of using the last observation as it is. Under this assumption, the trainable decay scheme can be readily applied to the measurement vector by  be represented in decayed input values. In order to capture richer knowledge from missingness, we also have a hidden state decay γ h in GRU-D. Intuitively, this has an effect of decaying the extracted features (GRU hidden states) rather than raw input variables directly. This is implemented by decaying the previous hidden state h t−1 before computing the new hidden state h t as</p><formula xml:id="formula_8">h t−1 ← γ ht h t−1 ,<label>(6)</label></formula><p>in which case we do not constrain W γ h to be diagonal. In addition, we feed the masking vectors (m t ) directly into the model. The update functions of GRU-D are</p><formula xml:id="formula_9">z t = σ (W z x t + U z h t−1 + V z m t + b z ) r t = σ (W r x t + U r h t−1 + V r m t + b r ) h t = tanh (W x t + U (r t h t−1 ) + V m t + b) h t = (1 − z t ) h t−1 + z t h t</formula><p>where x t and h t−1 are respectively updated by Equation <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_8">(6)</ref>, and V z , V r , V are new parameters for masking vector m t .</p><p>To validate GRU-D model and demonstrate how it utilizes informative missing patterns, in <ref type="figure" target="#fig_3">Figure 4</ref>, we show the input decay (γ x ) plots and hidden decay (γ h ) histograms for all the variables for predicting mortality on PhysioNet dataset. For input decay, we notice that the decay rate is almost constant for the majority of variables. However, a few variables have large decay which means that the model relies less on the previous observations for prediction. For example, the changes in the variable values of weight, arterial pH, temperature, and respiration rate are known to impact the ICU patients health condition. The hidden decay histograms show the distribution of decay parameters related to each variable. We noticed that the parameters related to variables with smaller missing rate are more spread out. This indicates that the missingness of those variables has more impact on decaying or keeping the hidden states of the models.</p><p>Notice that the decay term can be generalized to LSTM straightforwardly. In practical applications, missing values in time series may contain useful information in a variety of ways. A better model should have the flexibility to capture different missing patterns. In order to demonstrate the capacity of our GRU-D model, we discuss some model variations in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATASET DESCRIPTIONS AND EXPERIMENTAL DESIGN</head><p>We demonstrate the performance of our proposed models on one synthetic and two real-world health-care datasets 1 and compare it to several strong machine learning and deep learning approaches in classification tasks. We evaluate our models for different settings such as early prediction and different training sizes and investigate the impact of informative missingness.</p><p>Gesture phase segmentation dataset (Gesture) This UCI dataset <ref type="bibr" target="#b16">(Madeo et al., 2013)</ref> has multivariate time series features, regularly sampled and with no missing values, for 5 different gesticulations. We extracted 378 time series and generate 4 synthetic datasets for the purpose of understanding model behaviors with different missing patterns. We treat it as multi-class classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physionet Challenge 2012 dataset (PhysioNet)</head><p>This dataset, from PhysioNet Challenge 2012 <ref type="bibr" target="#b26">(Silva et al., 2012)</ref>, is a publicly available collection of multivariate clinical time series from 8000 intensive care unit (ICU) records. Each record is a multivariate time series of roughly 48 hours and contains 33 variables such as Albumin, heart-rate, glucose etc. We used Training Set A subset in our experiments since outcomes (such as in-hospital mortality labels) are publicly available only for this subset. We conduct the following two prediction tasks on this dataset: 1) Mortality task: Predict whether the patient dies in the hospital. There are 554 patients with positive mortality label. We treat this as a binary classification problem. and 2) All 4 tasks: Predict 4 tasks: in-hospital mortality, length-of-stay less than 3 days, whether the patient had a cardiac condition, and whether the patient was recovering from surgery. We treat this as a multi-task classification problem.</p><p>MIMIC-III dataset (MIMIC-III) This public dataset <ref type="bibr" target="#b11">(Johnson et al., 2016)</ref> has deidentified clinical care data collected at Beth Israel Deaconess Medical Center from 2001 to 2012. It contains over 58,000 hospital admission records. We extracted 99 time series features from 19714 admission records for 4 modalities including input-events (fluids into patient, e.g., insulin), output-events (fluids out of the patient, e.g., urine), lab-events (lab test results, e.g., pH values) and prescription-events (drugs prescribed by doctors, e.g., aspirin). These modalities are known to be extremely useful for monitoring ICU patients. We only use the first 48 hours data after admission from each time series. We perform following two predictive tasks: 1) Mortality task: Predict whether the patient dies in the hospital after 48 hours. There are 1716 patients with positive mortality label and we perform binary classification. and 2) ICD-9 Code tasks: Predict 20 ICD-9 diagnosis categories (e.g., respiratory system diagnosis) for each admission. We treat this as a multi-task classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">METHODS AND IMPLEMENTATION DETAILS</head><p>We categorize all evaluated prediction models into three following groups:</p><p>• Non-RNN Baselines (Non-RNN): We evaluate logistic regression (LR), support vector machines (SVM) and Random Forest (RF) which are widely used in health care applications.</p><p>• RNN Baselines (RNN): We take GRU-mean, GRU-forward, GRU-simple, and LSTM-mean (LSTM model with mean-imputation on the missing measurements) as RNN baselines.</p><p>• Proposed Methods (Proposed): This is our proposed GRU-D model from Section 2.2.</p><p>Recently RNN models have been explored for modeling diseases and patient diagnosis in health care domain <ref type="bibr" target="#b15">(Lipton et al., 2016;</ref><ref type="bibr" target="#b4">Choi et al., 2015;</ref><ref type="bibr" target="#b21">Pham et al., 2016)</ref> using EHR data. These methods do not systematically handle missing values in data or are equivalent to our RNN baselines. We provide more detailed discussions and comparisons in Appendix A.2.3 and A.3.4.</p><p>The non-RNN baselines cannot handle missing data directly. We carefully design experiments for non-RNN models to capture the informative missingness as much as possible to have fair comparison with the RNN methods. Since non-RNN models only work with fixed length inputs, we regularly sample the time-series data to get a fixed length input and perform imputation to fill in the missing values. Similar to RNN baselines, we can concatenate the masking vector along with the measurements and feed it to non-RNN models. For PhysioNet dataset, we sample the time series on an hourly basis and propagate measurements forward (or backward) in time to fill gaps. For MIMIC-III dataset, we consider two hourly samples (in the first 48 hours) and do forward (or backward) imputation.</p><p>Our preliminary experiments showed 2-hourly samples obtains better performance than one-hourly samples for MIMIC-III. We report results for both concatenation of input and masking vectors (i.e., SVM/LR/RF-simple) and only input vector without masking (i.e., SVM/LR/RF-forward). We use the scikit-learn <ref type="bibr" target="#b20">(Pedregosa et al., 2011)</ref> for the non-RNN model implementation and tune the parameters by cross-validation. We choose RBF kernel for SVM since it performs better than other kernels.</p><p>For RNN models, we use a one layer RNN to model the sequence, and then apply a soft-max regressor on top of the last hidden state h T to do classification. We use 100 and 64 hidden units in GRU-mean for MIMIC-III and PhysioNet datasets, respectively. All the other RNN models were constructed to  have a comparable number of parameters. 2 For GRU-simple, we use mean imputation for input as shown in Equation <ref type="formula">(1)</ref>. Batch normalization <ref type="bibr" target="#b10">(Ioffe &amp; Szegedy, 2015)</ref> and dropout <ref type="bibr" target="#b27">(Srivastava et al., 2014)</ref> of rate 0.5 are applied to the top regressor layer. We train all the RNN models with the Adam optimization method <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2014)</ref> and use early stopping to find the best weights on the validation dataset. All the input variables are normalized to be 0 mean and 1 standard deviation. We report the results from 5-fold cross validation in terms of area under the ROC curve (AUC score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">QUANTITATIVE RESULTS</head><p>Exploiting informative missingness on synthetic dataset As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, missing patterns can be useful in solving prediction tasks. A robust model should exploit informative missingness properly and avoid inducing inexistent relations between missingness and predictions. To evaluate the impact of modeling missingness we conduct experiments on the synthetic Gesture datasets. We process the data in 4 different settings with the same missing rate but different correlations between missing rate and the label. A higher correlation implies more informative missingness. <ref type="figure" target="#fig_4">Figure 5</ref> shows the AUC score comparison of three GRU baseline models (GRU-mean, GRU-forward, GRU-simple) and the proposed GRU-D. Since GRU-mean and GRU-forward do not utilize any missingness (i.e., masking or time interval), they perform similarly across all 4 settings. GRU-simple and GRU-D benefit from utilizing the missingness, especially when the correlation is high. Our GRU-D achieves the best performance in all settings, while GRU-simple fails when the correlation is low. The results on synthetic datasets demonstrates that our proposed model can model and distinguish useful missing patterns in data properly compared with baselines.</p><p>Prediction task evaluation on real datasets We evaluate all methods in Section 3.2 on MIMIC-III and PhysioNet datasets. We noticed that dropout in the recurrent layer helps a lot for all RNN models on both of the datasets, probably because they contain more input variables and training samples than synthetic dataset. Similar to <ref type="bibr" target="#b6">Gal (2015)</ref>, we apply dropout rate of 0.3 with same dropout samples at each time step on weights W , U , V . <ref type="table" target="#tab_2">Table 2</ref> shows the prediction performance of all the models on mortality task. All models except for random forest improve their performance when they feed missingness indicators along with inputs. The proposed GRU-D achieves the best AUC score on both the datasets. We also conduct multi-task classification experiments for all 4 tasks on PhysioNet and 20 ICD-9 code tasks on MIMIC-III using all the GRU models. As shown in <ref type="table" target="#tab_1">Table 1</ref>, GRU-D performs best in terms of average AUC score across all tasks and in most of the single tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DISCUSSIONS</head><p>Online prediction in early stage Although our model is trained on the first 48 hours data and makes prediction at the last time step, it can be used directly to make predictions before it sees all the time series and can make predictions on the fly. This is very useful in applications such as health care, where early decision making is beneficial and critical for patient care. <ref type="figure">Figure 6</ref> shows the online prediction results for MIMIC-III mortality task. As we can see, AUC is around 0.7 at first 12 hours for all the GRU models and it keeps increasing when longer time series is fed into these models. GRU-D and GRU-simple, which explicitly handle missingness, perform consistently  superior compared to the other two methods. In addition, GRU-D outperforms GRU-simple when making predictions given time series of more than 24 hours, and has at least 2.5% higher AUC score after 30 hours. This indicates that GRU-D is able to capture and utilize long-range temporal missing patterns. Furthermore, GRU-D achieves similar prediction performance (i.e., same AUC) as best non-RNN baseline model with less time series data. As shown in the figure, GRU-D has same AUC performance at 36 hours as the best non-RNN baseline model (RF-simple) at 48 hours. This 12 hour improvement of GRU-D over non-RNN baseline is highly significant in hospital settings such as ICU where time-saving critical decisions demands accurate early predictions.</p><p>Model Scalability with growing data size In many practical applications, model scalability with large dataset size is very important. To evaluate the model performance with different training dataset size, we subsample three smaller datasets of 2000 and 10000 admissions from the entire MIMIC-III dataset while keeping the same mortality rate. We compare our proposed models with all GRU baselines and two most competitive non-RNN baselines (SVM-simple, RF-simple). We observe that all models can achieve improved performance given more training samples. However, the improvements of non-RNN baselines are quite limited compared to GRU models, and our GRU-D model achieves the best results on the larger datasets. These results indicate the performance gap between RNN and non-RNN baselines will continue to grow as more data become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SUMMARY</head><p>In this paper, we proposed novel GRU-based model to effectively handle missing values in multivariate time series data. Our model captures the informative missingness by incorporating masking and time interval directly inside the GRU architecture. Empirical experiments on both synthetic and real-world health care datasets showed promising results and provided insightful findings. In our future work, we will explore deep learning approaches to characterize missing-not-at-random data and we will conduct theoretical analysis to understand the behaviors of existing solutions for missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 INVESTIGATION OF RELATION BETWEEN MISSINGNESS AND LABELS</head><p>In many time series applications, the pattern of missing variables in the time series is often informative and useful for prediction tasks. Here, we empirically confirm this claim on real health care dataset by investigating the correlation between the missingness and prediction labels (mortality and ICD-9 diagnosis categories). We denote the missing rate for a variable d as p d X and calculate it by</p><formula xml:id="formula_10">p d X = 1 − 1 T T t=1 m d t .</formula><p>Note that p d X is dependent on mask vector (m d t ) and number of time steps T . For each prediction task, we compute the Pearson correlation coefficient between p d X and label across all the time series. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we observe that on MIMIC-III dataset the missing rates with low rate values are usually highly (either positive or negative) correlated with the labels. The distinct correlation between missingness and labels demonstrates usefulness of missingness patterns in solving prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 GRU-D MODEL VARIATIONS</head><p>In this section, we will discuss some variations of GRU-D model, and also compare some related RNN models which are used for time series with missing data with the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 GRU MODEL WITH DIFFERENT TRAINABLE DECAYS</head><p>The proposed GRU-D applies trainable decays on both the input and hidden state transitions in order to capture the temporal missing patterns explicitly. This decay idea can be straightforwardly generated to other parts inside the GRU models separately or jointly, given different assumptions on the impact of missingness. As comparisons, we also describe and evaluate several modifications of GRU-D model. <ref type="figure" target="#fig_6">(Figure 8(a)</ref>) and GRU-DS <ref type="figure" target="#fig_6">(Figure 8(b)</ref>) decay only the input and only the hidden state by Equation <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_8">(6)</ref>, respectively. They can be considered as two simplified models of the proposed GRU-D. GRU-DI aims at capturing direct impact of missing values in the data, while GRU-DS captures more indirect impact of missingness. Another intuition comes from this perspective: if an input variable is just missing, we should pay more attention to this missingness; however, if an variable has been missing for a long time and keeps missing, the missingness becomes less important. We can utilize this assumption by decaying the masking. This brings us the model GRU-DM shown in <ref type="figure" target="#fig_6">Figure 8(c)</ref>, where we replace the masking m d t fed into GRU-D in by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU-DI</head><formula xml:id="formula_11">m d t ← m d t + (1 − m d t )γ m d t (1 − m d t ) = m d t + (1 − m d t )γ m d t<label>(7)</label></formula><p>where the equality holds since m d t is either 0 or 1. We decay the masking for each variable independently from others by constraining W γm to be diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 GRU-IMP: GOAL-ORIENTED IMPUTATION MODEL</head><p>We may alternatively let the GRU-RNN predict the missing values in the next timestep on its own. When missing values occur only during test time, we simply train the model to predict the measurement vector of the next time step as a language model <ref type="bibr" target="#b17">(Mikolov et al., 2010)</ref> and use it to fill the missing values during test time. This is unfortunately not applicable for some time series applications such as in health care domain, which also have missing data during training.</p><p>Instead, we propose goal-oriented imputation model here called GRU-IMP, and view missing values as latent variables in a probabilistic graphical model. Given a timeseries X, we denote all the missing variables by M X and all the observed ones by O X . Then, training a time-series classifier with missing variables becomes equivalent to maximizing the marginalized log-conditional probability of a correct label l, i.e., log p(l|O X ).</p><p>The exact marginalized log-conditional probability is however intractable to compute, and we instead maximize its lowerbound:</p><formula xml:id="formula_12">log p(l|O X ) = log M X p (l|M X , O X ) p (M X |O X ) ≥ E M X ∼p(M X |O X ) log p (l|M X , O X )</formula><p>where we assume the distribution over the missing variables at each time step is only conditioned on all the previous observations:</p><formula xml:id="formula_13">p (M X |O X ) = T t=1 m d t =1 1≤d≤D p(x d t |x 1:(t−1) , m 1:(t−1) , δ 1:(t−1) )<label>(8)</label></formula><p>Although this lowerbound is still intractable to compute exactly, we can approximate it by Monte Carlo method, which amounts to sampling the missing variables at each time as the RNN reads the input sequence from the beginning to the end, such that</p><formula xml:id="formula_14">x d t ← m d t x d t + (1 − m d t )x d t (9) wherex t ∼ x d t |x 1:(t−1) , m 1:(t−1) , δ 1:(t−1) . By further assuming thatx t ∼ N µ t , σ 2 t , µ t = γ t (W x h t−1 + b x )</formula><p>and σ t = 1, we can use a reparametrization technique widely used in stochastic variational inference <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b23">Rezende et al., 2014)</ref> to estimate the gradient of the lowerbound efficiently. During the test time, we simply use the mean of the missing variable, i.e.,x t = µ t , as we have not seen any improvement from Monte Carlo approximation in our preliminary experiments. We view this approach as a goal-oriented imputation method and show its structure in <ref type="figure" target="#fig_6">Figure 8(d)</ref>. The whole model is trained to minimize the classification cross-entropy error log loss and we take the negative log likelihood of the observed values as a regularizer.  <ref type="bibr" target="#b4">Choi et al. (2015)</ref> feeds medical codes along with its time stamps into GRU model to predict the next medical event. This feeding time stamps idea is equivalent to the baseline GRU-simple without feeding the masking, which we denote as GRU-simple (interval only). <ref type="bibr" target="#b21">Pham et al. (2016)</ref> takes time stamps into LSTM model, and modify its forgetting gate by either time decay and parametric time both from time stamps. However, their non-trainable decay is not that flexible, and the parametric time also does not change RNN model structure and is similar to GRU-simple (interval only). In addition, neither of them consider missing values in time series medical records, and the time stamp input used in these two models is vector for one patient, but not matrix for each input variable of one patient as ours. <ref type="bibr" target="#b15">Lipton et al. (2016)</ref> achieves their best performance on diagnosis prediction by feeding masking with zero-filled missing values. Their model is equivalent to GRU-simple without feeding the time interval, and no model structure modification is made for further capturing and utilizing missingness. We denote their best model as GRU-simple (masking only). Conclusively, our GRU-simple baseline can be considered as a generalization from all related RNN models mentioned above.</p><formula xml:id="formula_15">= log loss + λ 1 N N n=1 1 T n Tn t=1 D d=1 m d t · log p(x d t |µ d t , σ d t ) D d=1 m d t<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SUPPLEMENTARY EXPERIMENT DETAILS</head><p>A.3.1 DATA STATSITICS For each of the three datasets used in our experiments, we list the number of samples, the number of input variables, the mean and max number of time steps for all the samples, and the mean of all the variable missing rates in <ref type="table" target="#tab_4">Table 3</ref>. In order to fairly compare the capacity of all GRU-RNN models, we build each model in proper size so they share similar number of parameters. <ref type="table" target="#tab_5">Table 4</ref> shows the statistics of all GRU-based models for on three datasets. We show the statistics for mortality prediction on the two real datasets, and it's almost the same for multi-task classifications tasks on these datasets. In addition, having comparable number of parameters also makes all the models have number of iterations and training time close in the same scale in all the experiments. GRU-mean GRU-forward GRU-simple GRU-D <ref type="figure" target="#fig_0">Figure 10</ref>: Performance for predicting all 4 tasks on PhysioNet dataset. mortality, in-hospital mortality; los&lt; 3, length-of-stay less than 3 days; surgery, whether the patient was recovering from surgery; cardiac, whether the patient had a cardiac condition; y-axis, AUC score. classification performance. We show the AUC scores for predicting 20 ICD-9 diagnosis categories on MIMIC-III dataset in <ref type="figure">Figure 9</ref>, and all 4 tasks on PhysioNet dataset in <ref type="figure" target="#fig_0">Figure 10</ref>. The proposed GRU-D achieves the best average AUC score on both datasets and wins 11 of the 20 ICD-9 prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 EMPIRICAL COMPARISON OF MODEL VARIATIONS</head><p>Finally, we test all GRU model variations mentioned in Appendix A.2 along with the proposed GRU-D. These include 1) 4 models with trainable decays (GRU-DI, GRU-DS, GRU-DM, GRU-IMP), and 2) two models simplified from GRU-simple (interval only and masking only). The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. As we can see, GRU-D performs best among these models. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Demonstrations of informative missingness on MIMIC-III dataset. Left figure shows variable missing rate (x-axis, missing rate; y-axis, input variable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of measurement vectors x t , time stamps s t , masking m t , and time interval δ t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Graphical illustrations of the original GRU (left) and the proposed GRU-D (right) models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Plots of input decay γ xt (top) and histrograms of hidden state decay γ ht (bottom) of all 33 variables in GRU-D model for predicting mortality on PhysioNet dataset. Variables in green are lab measurements; variables in red are vital signs; mr refers to missing rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Classification performance on Gesture synthetic datasets. x-axis: average Pearson correlation of variable missing rates and target label in that dataset; y-axis: AUC score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Performance for early predicting mortality on MIMIC-III dataset. x-axis, # of hours after admission; y-axis, AUC score; Dash line, RF-simple results for 48 hours. Performance for predicting mortality on subsampled MIMIC-III dataset. x-axis, subsampled dataset size; y-axis, AUC score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Graphical illustrations of variations of proposed GRU models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-axis, value of decay parameter W γ h ; y-axis, count.</figDesc><table><row><cell>mr: 0.9989 11: Glucose mr: 0.9528 32: pH mr: 0.9118 (a) x-axis, time interval δ d 27: TroponinI mr: 0.9984 28: TroponinT mr: 0.9923 19: Na mr: 0.9508 18: Mg mr: 0.9507 9: FiO2 mr: 0.883 23: RespRate mr: 0.8053 t between 0 and 24 hours; y-axis, value of decay rate γx d 3: Albumin mr: 0.9915 0: ALP mr: 0.9888 1: ALT mr: 0.9885 2: AST mr: 0.9885 5: Bilirubin mr: 0.9884 16: Lactate mr: 0.9709 12: HCO3 mr: 0.9507 4: BUN mr: 0.9496 7: Creatinine mr: 0.9493 22: Platelets mr: 0.9489 15: K mr: 0.9477 13: HCT mr: 0.9338 10: GCS mr: 0.7767 26: Temp mr: 0.6915 31: Weight mr: 0.5452 29: Urine mr: 0.5095 17: MAP mr: 0.2141 8: DiasABP mr: 0.2054 t between 0 and 1. 24: SaO2 mr: 0.9705 30: WBC mr: 0.9532 21: PaO2 mr: 0.9158 20: PaCO2 mr: 0.9157 25: SysABP mr: 0.2052 14: HR mr: 0.1984 10 1 10 2 6: Cholesterol mr: 0.9989 27: TroponinI mr: 0.9984 28: TroponinT mr: 0.9923 3: Albumin mr: 0.9915 0: ALP mr: 0.9888 1: ALT mr: 0.9885 2: AST mr: 0.9885 5: Bilirubin mr: 0.9884 16: Lactate mr: 0.9709 24: SaO2 mr: 0.9705 30: WBC mr: 0.9532 10 1 10 2 11: Glucose mr: 0.9528 19: Na mr: 0.9508 18: Mg mr: 0.9507 12: HCO3 mr: 0.9507 4: BUN mr: 0.9496 7: Creatinine mr: 0.9493 22: Platelets mr: 0.9489 15: K mr: 0.9477 13: HCT mr: 0.9338 21: PaO2 mr: 0.9158 20: PaCO2 mr: 0.9157 0.3 0.3 10 1 10 2 32: pH mr: 0.9118 0.3 0.3 9: FiO2 mr: 0.883 0.3 0.3 23: RespRate mr: 0.8053 0.3 0.3 10: GCS mr: 0.7767 0.3 0.3 26: Temp mr: 0.6915 0.3 0.3 31: Weight mr: 0.5452 0.3 0.3 29: Urine mr: 0.5095 0.3 0.3 17: MAP mr: 0.2141 0.3 0.3 8: DiasABP mr: 0.2054 0.3 0.3 25: SysABP mr: 0.2052 0.3 0.3 14: HR mr: 0.1984 x d 6: Cholesterol (b) x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model performances measured by average AUC score (mean ± std) for multi-task predictions on real datasets. Results on each class are shown in Appendix A.3.3 for reference.</figDesc><table><row><cell>Models</cell><cell>MIMIC-III ICD-9 20 tasks</cell><cell>PhysioNet All 4 tasks</cell></row><row><cell>GRU-mean</cell><cell>0.7070 ± 0.001</cell><cell>0.8099 ± 0.011</cell></row><row><cell>GRU-forward</cell><cell>0.7077 ± 0.001</cell><cell>0.8091 ± 0.008</cell></row><row><cell>GRU-simple</cell><cell>0.7105 ± 0.001</cell><cell>0.8249 ± 0.010</cell></row><row><cell>GRU-D</cell><cell cols="2">0.7123 ± 0.003 0.8370 ± 0.012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performances measured by AUC score (mean ± std) for mortality prediction.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Models</cell><cell></cell><cell></cell><cell>MIMIC-III</cell><cell>PhysioNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LR-forward</cell><cell></cell><cell>0.7589 ± 0.015</cell><cell>0.7423 ± 0.011</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SVM-forward</cell><cell>0.7908 ± 0.006</cell><cell>0.8131 ± 0.018</cell></row><row><cell></cell><cell></cell><cell cols="2">Non-RNN</cell><cell cols="2">RF-forward</cell><cell></cell><cell>0.8293 ± 0.004</cell><cell>0.8183 ± 0.015</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LR-simple</cell><cell></cell><cell>0.7715 ± 0.015</cell><cell>0.7625 ± 0.004</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SVM-simple</cell><cell></cell><cell>0.8146 ± 0.008</cell><cell>0.8277 ± 0.012</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RF-simple</cell><cell></cell><cell>0.8294 ± 0.007</cell><cell>0.8157 ± 0.013</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LSTM-mean</cell><cell></cell><cell>0.8142 ± 0.014</cell><cell>0.8025 ± 0.013</cell></row><row><cell></cell><cell></cell><cell cols="2">RNN</cell><cell cols="2">GRU-mean</cell><cell></cell><cell>0.8192 ± 0.013</cell><cell>0.8195 ± 0.004</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GRU-forward</cell><cell>0.8252 ± 0.011</cell><cell>0.8162 ± 0.014</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GRU-simple</cell><cell></cell><cell>0.8380 ± 0.008</cell><cell>0.8155 ± 0.004</cell></row><row><cell></cell><cell></cell><cell cols="4">Proposed GRU-D</cell><cell></cell><cell>0.8527 ± 0.003 0.8424 ± 0.012</cell></row><row><cell>0.87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.75</cell><cell></cell><cell></cell><cell cols="2">GRU-mean</cell><cell cols="3">GRU-forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GRU-simple</cell><cell cols="2">GRU-D</cell></row><row><cell>0.69</cell><cell></cell><cell></cell><cell cols="2">SVM-simple</cell><cell cols="2">RF-simple</cell></row><row><cell>12</cell><cell>18</cell><cell>24</cell><cell>30</cell><cell>36</cell><cell>42</cell><cell>48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>A.2.3 COMPARISONS OF RELATED RNN MODELSSeveral recent works<ref type="bibr" target="#b15">(Lipton et al., 2016;</ref><ref type="bibr" target="#b4">Choi et al., 2015;</ref><ref type="bibr" target="#b21">Pham et al., 2016)</ref> use RNNs on EHR data to model diseases and to predict patient diagnosis from health care time series data with irregular time stamps or missing values, but none of them have explicitly attempted to capture and model the missing patterns in their RNNs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics.</figDesc><table><row><cell></cell><cell cols="3">MIMIC-III PhysioNet2012 Gesture</cell></row><row><cell># of samples (N )</cell><cell>19714</cell><cell>4000</cell><cell>378</cell></row><row><cell># of variables (D)</cell><cell>99</cell><cell>33</cell><cell>23</cell></row><row><cell>Mean of # of time steps</cell><cell>35.89</cell><cell>68.91</cell><cell>21.42</cell></row><row><cell>Maximum of # of time steps</cell><cell>150</cell><cell>155</cell><cell>31</cell></row><row><cell>Mean of variable missing rate</cell><cell>0.9621</cell><cell>0.8225</cell><cell>N/A</cell></row><row><cell>A.3.2 GRU MODEL SIZE COMPARISON</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of GRU model size in our experiments. Size refers to the number of hidden states (h) in GRU .</figDesc><table><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mortality</cell><cell>los &lt; 3</cell><cell>surgery</cell><cell>cardiac</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Model performances of GRU variations measured by AUC score (mean ± std) for mortality prediction.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ← m d t x d t + (1 − m d t )γ x d t x d t + (1 − m d t )(1 − γ x d t )x d(5)where x d t is the last observation of the d-th variable (t &lt; t) andx d is the empirical mean of the d-th variable. When decaying the input variable directly, we constrain W γx to be diagonal, which effectively makes the decay rate of each variable independent from the others. Sometimes the input decay may not fully capture the missing patterns since not all missingness information can</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A summary statistics of the three datasets is shown in Appendix A.3.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Appendix A.3.2 compares all GRU models tested in the experiments in terms of model size.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 MULTI-TASK PREDICTION DETAILS</head><p>The RNN models for multi-task learning with m tasks is almost the same as that for binary classification, except that 1) the soft-max prediction layer is replaced by a fully connected layer with n sigmoid logistic functions, and 2) a data-driven prior regularizer <ref type="bibr" target="#b2">(Che et al., 2015)</ref>, parameterized by comorbidity (co-occurrence) counts in training data, is applied to the prediction layer to improve the GRU-mean GRU-forward GRU-simple GRU-D <ref type="figure">Figure 9</ref>: Performance for predicting 20 ICD-9 diagnosis categories on MIMIC-III dataset. x-axis, ICD-9 diagnosis category id; y-axis, AUC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III PhysioNet</head><p>Baselines GRU-simple (masking only) 0.8367 ± 0.009 0.8226 ± 0.010</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU-simple (interval only)</head><p>0.8266 ± 0.009 0.8125 ± 0.005 GRU-simple 0.8380 ± 0.008 0.8155 ± 0.004</p><p>Proposed GRU-DI 0.8345 ± 0.006 0.8328 ± 0.008 GRU-DS 0.8425 ± 0.006 0.8241 ± 0.009 GRU-DM 0.8342 ± 0.005 0.8248 ± 0.009 GRU-IMP 0.8248 ± 0.010 0.8231 ± 0.005 GRU-D 0.8527 ± 0.003 0.8424 ± 0.012</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for missing or asynchronous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Gingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="395" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep computational phenotyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05942</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05287</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pattern classification with missing data: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José-Luis</forename><surname>García-Laencina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aníbal R Figueiras-</forename><surname>Sancho-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database. Scientific Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aew Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The effects of the irregular sample and missing data in time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles J</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lumsden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04130</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gesture unit segmentation using support vector machines: segmenting gestures from rest positions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Renata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Clodoaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarajane</forename><forename type="middle">M</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wavelet variance analysis for gappy time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debashis</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="943" to="966" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech recognition with missing data using recurrent neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahla</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1189" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepcare: A deep dynamic memory model for predictive medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparison of correlation analysis techniques for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Marwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jobst</forename><surname>Heitzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Processes in Geophysics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Missing data: our view of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John W</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivanovitch</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galan</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<editor>CinC</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A solution for missing data in recurrent neural networks with an application to blood glucose prediction. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Briegel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="971" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A systems engineering perspective on homeostasis and disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Vodovotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Androulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in bioengineering and biotechnology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Strategies for handling missing data in electronic health record derived data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">S</forename><surname>Chagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Nowacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EGEMS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple imputation using chained equations: issues and guidance for practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ian R White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">M</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal reasoning with medical dataa review with emphasis on medical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Hripcsak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
