<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Text Recognition from Two-Dimensional Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
							<email>mhliao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengming</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
							<email>liangjiajun@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
						</author>
						<title level="a" type="main">Scene Text Recognition from Two-Dimensional Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scene text recognition has been an active research field in computer vision because it is a critical element of a lot of real-world applications, such as street sign reading in the driverless vehicle, human computer interaction, assistive technologies for the blind and guide board recognition <ref type="bibr" target="#b17">(Rong, Yi, and Tian 2016;</ref>. As compared to the maturity of document recognition, scene text recognition is still a challenging task due to large variations in text shapes, fonts, colors, backgrounds, etc.</p><p>Most of the recent works <ref type="bibr" target="#b18">(Shi, Bai, and Yao 2017;</ref><ref type="bibr" target="#b20">Shi et al. 2016;</ref><ref type="bibr" target="#b29">Zhu, Yao, and Bai 2016)</ref> convert scene text recognition into sequence recognition, which hugely simplifies the problem and leads to great performance on regular text. As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, they firstly encode the input image into a feature sequence and then apply decoders such as <ref type="bibr">RNN (Hochreiter and Schmidhuber 1997)</ref> and <ref type="bibr">CTC (Graves et al. 2006)</ref> to decode the target sequence. These methods produce good results when the text in the image is horizontal or nearly horizontal. However, different from speech, text in scene images is essentially distributed in a two-dimensional space. For example, the distribution of the characters can be scattered, in arbitrary orientations, and even in curve shapes, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In these cases, roughly encoding the images into one-dimensional sequences may lose key information or bring undesired noises. <ref type="bibr" target="#b20">(Shi et al. 2016)</ref> tried to alleviate this problem by adopting a Spatial Transform Network (STN) <ref type="bibr">(Jaderberg et al. 2015b</ref>) to rectify the shape of the text. Nevertheless, <ref type="bibr" target="#b20">(Shi et al. 2016</ref>) still used a sequencebased model, so the effect of the rectification is limited. As discussed above, the limitations of sequence-based methods are mainly caused by the difference between the one-dimensional distribution of feature sequences and the two-dimensional distribution of text in scene images. To overcome these limitations, we tackle the scene text recognition problem in a new and natural perspective. We propose to directly predict the text in a two-dimensional space instead of a one-dimensional sequence. Inspired by FCN <ref type="bibr" target="#b15">(Long, Shelhamer, and Darrell 2015)</ref>, a Character Attention Fully Convolutional Network (CA-FCN) is proposed to predict the characters at pixel level. Then the word, as well as the location of each character, can be obtained by a word formation module, as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>  compressing and slicing the features, which are widely used in the sequence-based methods, are avoided. Profiting from the higher dimensional perspective, the proposed method is much more robust than the previous sequence-based methods in terms of text shapes, background noises, and imprecise localizations from the detection stage <ref type="bibr" target="#b12">(Liao et al. 2017;</ref>. Character-level annotations are needed in our proposed method. However, the character annotations are free of labor because only public synthetic data is used in the training period, where the annotations are easy to obtain. The contributions of this paper can be summarized as follows: (1) A totally different perspective for recognizing scene text is proposed. Different from the recent works which treat the text recognition problem as a sequence recognition problem in one-dimensional space, we propose to solve the problem in two-dimensional space. (2) We devise character attention FCN for scene text recognition. To the best of our knowledge, which can deal with images of arbitrary height and width, as well as naturally recognize text in various shapes, including but not limited to oriented and curve shapes. (3) The proposed method achieves state-of-the-art performance on regular datasets and outperforms the existing methods with a large margin on irregular datasets. (4) We investigate the network's robustness to imprecise localization in the text detection phase for the first time. This problem is important in real-world applications but was previously ignored. Experiments show that the proposed method is more robust to imprecise localization (see . Ablation study).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Traditionally, scene text recognition systems firstly detect each character, using binarization or sliding-window operation, then recognize these characters as a word. Binarizationbased methods, such as Extremal Regions <ref type="bibr" target="#b17">(Novikova et al. 2012</ref>) and Niblack's adaptive binarization <ref type="bibr" target="#b2">(Bissacco et al. 2013</ref>), find character pixels after binarization. However, text in the natural scene image may have varying backgrounds, fonts, colors or uneven illumination and so on, which binarization based methods can hardly handle. Sliding window methods use multi-scale sliding window strategy to localize characters from the text image directly, such as Random Ferns <ref type="bibr" target="#b24">(Wang, Babenko, and Belongie 2011)</ref>, Integer Programming <ref type="bibr" target="#b21">(Smith, Feild, and Learned-Miller 2011)</ref> and Convolutional Neural Network (CNN) . For the word recognition stage, common methods are integrating contextual information with character classification scores, such as Pictorial Structure models, Bayesian inference, and Conditional Random Field (CRF), which are employed in <ref type="bibr" target="#b24">(Wang, Babenko, and Belongie 2011;</ref><ref type="bibr" target="#b25">Weinman, Learned-Miller, and Hanson 2009;</ref><ref type="bibr" target="#b17">Mishra, Alahari, and Jawahar 2012a;</ref><ref type="bibr" target="#b17">Mishra, Alahari, and Jawahar 2012b;</ref><ref type="bibr" target="#b19">Shi et al. 2013)</ref>.</p><p>Inspired by speech recognition, recent works designed an encoder-decoder framework, where text in images are encoded into feature sequences and then decoded as characters. With the development of the deep neural network, convolutional features are extracted at encoder stage, and then RNN or CNN network is applied to decode these features, then CTC is used to form the final word. This framework was proposed by <ref type="bibr" target="#b18">(Shi, Bai, and Yao 2017)</ref>. Later they also developed an attention-based STN for rectifying text distortion, which is useful to recognize curved scene text <ref type="bibr" target="#b20">(Shi et al. 2016</ref>). Based on this framework, subsequent works <ref type="bibr" target="#b8">(He et al. 2016;</ref><ref type="bibr" target="#b25">Wu et al. 2016;</ref><ref type="bibr" target="#b15">Liu, Chen, and Wong 2018)</ref> also focus on irregular scene text.</p><p>The encoder-decoder framework has dominated current text recognition works. Many systems based on this framework have achieved state-of-the-art performance. However, text in scene images are distributed in a two-dimensional space, which is different from speech. The encoder-decoder framework just considers them as one-dimensional sequences, bringing some problems. For example, compressing a text image into a feature sequence may lose key in-formation and add extra noise, especially when the text is curved or seriously distorted.</p><p>There are some works that tried to improve some disadvantages of the encoder-decoder framework.  found that when considering the scene text recognition problem under the attention-based encoder-decoder framework, the misalignment between the ground truth strings and the attention's output sequences of the probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process. To handle this problem, they propose a method called edit probability which considered losses including not only the probability distribution but also the possible occurrences of missing/superfluous characters.  aimed to handle oriented text and realized that it is hard for the current encoder-decoder framework to capture the deep features of the oriented text. To solve this problem, they encode the input image to four feature sequences of four directions to extract scene text features in those directions. <ref type="bibr" target="#b16">(Lyu et al. 2018)</ref> proposed an instance segmentation model for word spotting, which uses an FCN-based method in its recognition part. However, it focused on the end-to-end word spotting task and no discussion is applied to verify the recognition part.</p><p>In this paper, we consider text recognition from the twodimensional perspective and design a character attention FCN to deal with text recognition problem, which can naturally avoid those disadvantages of the encoder-decoder frameworks. For example, compressing a text image into a feature sequence may lose key information and add extra noise, especially when the text is curved or seriously distorted. The proposed method obtains high accuracy on both regular and irregular text, Meanwhile, it is also robust to imprecise localization in the text detection phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Overview</head><p>The whole architecture of our proposed method consists of two parts. The first part is a Character Attention FCN (CA-FCN) which predicts the characters at pixel level. Another part is a word formation module which groups and arranges the pixels to form the final word result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character attention FCN</head><p>The architecture of CA-FCN is basically a fully convolutional network, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We use VGG-16 as the backbone while dropping the fully connected layers and removing its pooling layers of stage-4 and stage-5. Besides, a pyramid-like structure <ref type="bibr" target="#b15">(Lin et al. 2017</ref>) is adopted to handle varying scales of characters. The final output is of shape</p><formula xml:id="formula_0">H 2 × W 2 × C,</formula><p>where H, W are the height and width of the input image and C is the number of classes including character categories and background. It can handle text of various shapes by predicting characters in a two-dimensional space.</p><p>Character attention module Attention module plays an important role in our network. Natural scene text recognition suffers from complex backgrounds, shadow, irrelevant symbols and so on. Moreover, characters in natural images are usually crowded, which can hardly be separated. To deal with those problems, inspired by , we propose a character attention module to highlight the foreground characters and weaken the background, as well as separate adjacent characters, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Attention module is appended to each output layer of VGG16. The low-level attention models mainly focus on the appearance, such as edge, color, and texture. And the high-level modules can extract more semantic information. The character attention module can be expressed as follows:</p><formula xml:id="formula_1">F o = F i ⊗ (1 + A)<label>(1)</label></formula><p>where F i and F o are the input and output feature map respectively; A indicates the attention map; ⊗ means elementwise multiplication. The attention map is generated by two convolutional layers and a two-class (characters and background) soft-max function where 0 represents background and 1 indicates characters. The attention map A is broadcast to the same shape as F i to achieve element-wise multiplication. Compared with , our character attention module uses a simpler network structure, profiting from the character supervision. The effectiveness of the character attention module is discussed in Sec. Ablation study. Deformable convolution As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, deformable convolution <ref type="bibr" target="#b5">(Dai et al. 2017</ref>) is applied in stage-4 and stage-5. The deformable convolution learns offsets of the convolution kernel, which provides more flexible receptive fields for the character prediction. The kernel size of deformable convolution is set to 3 × 3 as default. The kernel size of the convolution after the deformable convolution is set to 3 × 1. In <ref type="figure" target="#fig_2">Fig. 3</ref>, there is a toy description of normal convolution, the deformable convolution with 3 × 1 convolutional kernel, as well as their receptive fields. The image in <ref type="figure" target="#fig_2">Fig. 3</ref> is an expanded text image where more background is included in the image. Since most of the training images are cropped with tight bounding boxes, and the normal convolution contains a lot of character information due to the fixed receptive field, it tends to predict the extra background as a character. However, if deformable convolution and 3 × 1 convolution kernel are applied, with better and more flexible receptive field, the extra background can be predicted correctly. Note that the extra background is very common in real-world applications as the detection results may be inaccurate. Thus, the robustness on expanded text images is significant. The effectiveness of the deformable convolution is discussed in Sec. Ablation study by experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Label generation Let b = (x min , y min , x max , y max ) be the original bounding boxes of characters, which can be expressed as the minimum axis-aligned rectangle boxes that covers the characters. The ground truth character regions g = (x g min , y g min , x g max , y g max ) can be calculated as follows:</p><formula xml:id="formula_2">w = x max − x min h = y max − y min x g min = (x min + x max − w × r)/2 y g min = (y min + y max − h × r)/2 x g max = (x min + x max + w × r)/2 y g max = (y min + y max + h × r)/2<label>(2)</label></formula><p>where r is the shrink ratio of the character regions. We shrink the character regions because the adjacent characters tend to be overlapped without shrinking. The shrink process can reduce the difficulty of the word formation. Specifically, we set r to 0.5 and 0.25 for the attention supervision and the final output supervision respectively. Loss function The loss function is a weighted sum of the character prediction loss function L p and the character attention loss function L a :</p><formula xml:id="formula_3">L = L p + α 5 s=2 L s a<label>(3)</label></formula><p>where s indicates the index of the stages, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>; α is empirically set to 1.0. The final output of the CA-FCN is of shape H 2 × W 2 × C, where H, W are the height and width of an input image respectively. C is the number of classes including character classes and background. Assume that X i,j,c is one of the element of the output map, where i ∈ {1, ..., H 2 }, j ∈ {1, ..., W 2 }, and c ∈ {0, 1, ..., C −1}; Y i,j ∈ {0, 1, ..., C −1} indicates the corresponding class label. The prediction loss can be calculated as follows:</p><formula xml:id="formula_4">L p = − 4 H × W H/2 i=1 W/2 j=1 W i,j ( C−1 c=0 (Y i,j == c)log( e Xi,j,c C−1 k=0 e X i,j,k )),<label>(4)</label></formula><p>where W i,j is the corresponding weight of each pixel. Assume that N = H 2 × W 2 and N neg is the number of background pixels. The weight can be calculated as follows:</p><formula xml:id="formula_5">W i,j = N neg /(N − N neg ) if Y i,j &gt; 0, 1 otherwise (5)</formula><p>The character attention loss function is a binary cross entropy loss function which take all characters labels as 1, background label as 0:</p><formula xml:id="formula_6">L s a = − 4 H s × W s Hs/2 i=1 Ws/2 j=1 ( 1 c=0 (Y i,j == c)log( e Xi,j,c 1 k=0 e X i,j,k ),<label>(6)</label></formula><p>where H s and W s are the height and width of the feature map in the corresponding stage s respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word formation module</head><p>The word formation module converts the accurate, twodimensional character maps predicted by CA-FCN into character sequence. As shown in <ref type="figure">Fig. 5</ref>, we firstly transform the character prediction map into a binary map with a threshold to extract the corresponding character regions; then, we calculate the average values of each region for C classes and assign the class with the largest average value to the corresponding region; finally, the word is formed by sorting the regions from left to right. In this way, both the word and location of each character are produced. The word formation module assumes that words are roughly sorted from left to right, which may not work in certain scenarios. However, if necessary, a learnable component can be plugged into CA-FCN. The word formation module is simple yet effective, with only one hyper-parameter (the threshold to form binary map), which is set to 240/255 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>Our proposed CA-FCN is purely trained on the synthetic datasets without real-world images. The trained model, without further fine-tuning, was evaluated on 4 benchmarks including regular and irregular text datasets.</p><p>SynthText is a synthetic text dataset proposed in <ref type="bibr" target="#b7">(Gupta, Vedaldi, and Zisserman 2016)</ref>. It contains 800,000 training images which are aimed at text detection. We crop them based on their word bounding boxes. It generates about 7 million images for text recognition. These images are with character-level annotations.</p><p>IIIT5k-Words (IIIT) (Mishra, Alahari, and Jawahar 2012b) consists of 3000 test images collected from the web. It provides two lexicons for each image in the dataset, which contains 50 words and 1000 words respectively.</p><p>Street View Text (SVT) (Wang, Babenko, and Belongie 2011) comes from the Google Street View. The test set consists of 647 images. It is challenging due to its low resolution and noises. A 50-word lexicon is given for each image. ICDAR 2013 (IC13) <ref type="bibr" target="#b10">(Karatzas et al. 2013</ref>) contains 1015 images and no lexicon is provided. We remove images that contain non-alphanumeric characters or have less than three characters, following previous works.</p><p>CUTE <ref type="bibr" target="#b17">(Risnumawan et al. 2014</ref>) is a dataset consists of 288 images with a lot of curved text. It is challenging because the shapes vary hugely. No lexicon is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Training Since our network is fully convolutional, there is no restriction on the size of input images. We adopt multiscale training to make our model more robust. The input images are randomly resized to 32 × 128, 48 × 192, and 64 × 256. Besides, data augmentation is also applied in the training period, including random rotation, hue, brightness, contrast, and blur. Specifically, we randomly rotate the image with an angle in the range of [−15 • , 15 • ]. We use Adam (Kingma and Ba 2014) to optimize our training with the initial learning rate 10 −4 . The learning rate is decreased to 10 −5 and 10 −6 at epoch 3 and epoch 4. The model is totally trained for about 5 epochs. The number of character classes is set to 38, including 26 alphabet, 10 digitals, 1 special character which represents those characters out of alphabet and digitals, and 1 background.</p><p>Testing At runtime, images are resized to H t × W t , where H t is fixed to 64 and W t is calculated as follows:</p><formula xml:id="formula_7">W t = W * H t /H if W/H &gt; 4, 256 otherwise<label>(7)</label></formula><p>where H and W are the height and width of the origin images.</p><p>The speed is about 45 FPS on IC13 dataset with a batch size of 1, where the CA-FCN costs 0.018 second per image and word formation module costs 0.004 second per image on average. Higher speed can be achieved if the batch size increases. We test our method with a single Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performances on benchmarks</head><p>We evaluate our method on several benchmarks to indicate the superiority of the proposed method. Some results of IIIT and CUTE are visualized in <ref type="figure">Fig. 6</ref>. As can be seen, our proposed method can handle various shapes of text.</p><p>Quantitative results are listed in Tab. 1. Compared to previous methods, our proposed method achieves state-ofthe-art performance on most of those benchmarks. More specifically, "Ours" outperforms the previous state-of-theart by 3.7 percents on IIIT without lexicons. On irregular text dataset CUTE, 3.1 percents improvement is achieved by "Ours". Note that no extra training data for curved text is included to achieve this performance. Comparable results are also performed on other datasets, including SVT, IC13.</p><p>The training data of <ref type="bibr" target="#b3">(Cheng et al. 2017</ref>) consist of two synthetic datasets including Synth90k  and SynthText <ref type="bibr" target="#b7">(Gupta, Vedaldi, and Zisserman 2016)</ref>. The former is generated according to a large lexicon which contains the lexicon of SVT and ICDAR, while the latter uses a normal corpus, where the distribution of words are not balanced. To fairly compared with <ref type="bibr" target="#b3">(Cheng et al. 2017)</ref>, we also generate extra 4 million synthetic images using the <ref type="table">Table 1</ref>: Results across different methods and datasets. "50" and "1k" indicate the sizes of the lexicons. "0" means no lexicon. "data" indicates using extra synthetic data to fine-tune the model. <ref type="table" target="#tab_2">IIIT  SVT  IC13 CUTE  50  1k  0  50  0  0  0  (Wang, Babenko, and Belongie 2011)</ref> ---57.0 --- <ref type="bibr" target="#b17">(Mishra, Alahari, and Jawahar 2012a)</ref> 64.1 57. algorithm of SynthText with the lexicon used in Synth90k. As shown in Tab. 1, after fine-tuning with the extra data, "Ours+data" also outperforms <ref type="bibr" target="#b3">(Cheng et al. 2017</ref>) on SVT.  improves <ref type="bibr" target="#b3">(Cheng et al. 2017;</ref><ref type="bibr" target="#b20">Shi et al. 2016</ref>) by solving their misalignment problem and achieves excellent results in regular text recognition. However, it may fail in irregular text benchmarks such as CUTE due to its one-dimensional perspective. Moreover, we argue that our method can be further improved if the idea of  is well adapted to our word formulation module. Nevertheless, our method outperforms ) on most of the benchmarks in Tab. 1, especially on IIIT and CUTE. ) focuses on dealing with arbitraryoriented text by introducing four one-dimensional feature sequences with different directions adaptively. Our method is more superior in recognizing the text of irregular shapes such as curve shape. As shown in Tab. 1, our method outperforms ) on all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>Scene text recognition is usually a following step of scene text detection, whose results may be not as accurate as expected. Thus, performances of text spotting systems in realworld applications are significantly affected by the robustness of text recognition algorithms on expanded images. We conduct experiments with expanded datasets to show the effect of text bounding box variance on recognition and prove the robustness of our method.</p><p>For the datasets which have the original background, such as IC13, we expand their bounding boxes and then crop them from the original images. If no extra background is provided like IIIT, padding by repeating the border pixels is applied to these images. The expanded datasets are described below: IIIT-p Padding the images in IIIT with extra 10% height vertically and 10% width horizontally by repeating the border pixels. IIIT-r-p Separately stretching the four vertexes of the images in IIIT with a random scale up to 20% of height and width respectively; border pixels are repeated to fill the quadrilateral images; images are transformed back to axisaligned rectangles. IC13-ex Expanding the bounding boxes of the images in IC13 to expanded rectangles with extra 10% height and width before cropping. IC13-r-ex Expanding the bounding boxes of the images in IC13 randomly with a maximum 20% of width and height to form expanded quadrilaterals; The pixels in axis-aligned circumscribed rectangles of those images are cropped.</p><p>We compare our method with two representative sequence-based models including CRNN <ref type="bibr" target="#b18">(Shi, Bai, and Yao 2017)</ref> and Attention Convolutional Sequence Model (ACSM) <ref type="bibr" target="#b6">(Gao et al. 2017)</ref>. The model of CRNN is provided by its authors and the model of <ref type="bibr" target="#b6">(Gao et al. 2017</ref>) is re-implemented by ourself with the same training data as ours. Qualitative results of the three methods are visualized in <ref type="figure" target="#fig_5">Fig. 7</ref>. As can be observed, the sequence-based models usually predict extra characters if the images are expanded while CA-FCN is stable and robust.</p><p>The quantitative results are listed in Tab. 2. Compared to the sequence-based models, our proposed method is more robust among these expanding datasets. For example, on IIIT-p dataset, the gap ratio of CRNN is 6.4% while ours  is only 2.6%. Note that even though our performances on the standard datasets are higher, the gaps of ours are still much smaller than CRNN. As shown in Tab. 2, both the deformable module and the attention module can improve the performance and the former also contributes to the robustness of the model. It indicates the effectiveness of the deformable convolution and the character attention module. The possible reasons that our method is more robust than previous sequence-based models on expanded images could be: Sequence-based models are in one-dimensional perspective, which are hard to endure extra background because the background noises are easy to encode into the feature sequence. In contrast, our method predicts the characters in a two-dimensional space, where both the characters and the background are the targeted predicting objects. The extra background is less likely to mislead the prediction of the characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have presented a method called Character Attention FCN (CA-FCN) for scene text recognition, which models the problem in a two-dimensional fashion. By performing character classification at each pixel location, the algorithm can effectively recognize irregular as well as regular text instances. Experiments show that the proposed model outperforms existing methods on datasets with regular and irregular text. We also analyzed the impact of imprecise text localization to the performances of text recognition algorithms and proved that our method is much more robust. For future research, we will make the word formation module learnable and build an end-to-end text spotting system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of text recognition in one-dimensional and two-dimensional spaces. (a) shows the recognition procedures of sequence-based methods. (b) presents the proposed segmentation-based method. Different colors mean different character classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the CA-FCN. The blue feature maps in the left are inherited from the VGG-16 backbone; The yellow feature maps in the right are extra layers. H, W mean the height and width of the input image; C is the number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of our deformable convolution. (a) normal convolution; (b) deformable convolution with 3 * 1 convolution. The green boxes indicate convolutional kernels. The yellow boxes mean the regions covered by receptive fields. The receptive fields out of the image are clipped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of ground truth generation. (a) Original bounding boxes; (b) Ground truth for character attention; (c) Ground truth for character prediction, where different colors represent different character classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Illustration of the word formation module. Visualization of character prediction maps on IIIT and CUTE. The character prediction map generated by the CA-FCN is visualized with colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the character prediction maps on expanded datasets. Red: wrong results; Green: correct results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. In this way, the procedures of</figDesc><table><row><cell></cell><cell cols="2">Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Deformable Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Upsampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell cols="2">Character attention module</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A</cell></row><row><cell></cell><cell></cell><cell>stage-1</cell><cell>stage-2</cell><cell>stage-3</cell><cell>stage-4</cell><cell>stage-5</cell><cell>A</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>× ×256</cell><cell cols="2">× ×512 × ×512</cell><cell>× ×128</cell><cell>× ×128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>× ×128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>× ×128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>× ×C</cell></row><row><cell cols="2">H×W×3</cell><cell>× ×64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>× ×128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on expanded datasets. "ac": accuracy; "gap": the gap between the original dataset; "ratio" indicates the decreasing ratio compared to the accuracy on the original dataset.</figDesc><table><row><cell>Methods</cell><cell>IIIT ac</cell><cell>ac</cell><cell>IIIT-p gap</cell><cell>ratio</cell><cell>ac</cell><cell>IIIT-r-p gap</cell><cell>ratio</cell><cell>IC13 ac</cell><cell>ac</cell><cell>IC13-ex gap</cell><cell>ratio</cell><cell>ac</cell><cell>IC13-r-ex gap</cell><cell>ratio</cell></row><row><cell>CRNN</cell><cell>81.2</cell><cell>76.0</cell><cell>-5.2</cell><cell>6.4%</cell><cell>72.4</cell><cell>-8.8</cell><cell>10.8%</cell><cell>89.6</cell><cell>81.9</cell><cell>-7.7</cell><cell>8.6%</cell><cell>76.7</cell><cell>-12.9</cell><cell>14.4%</cell></row><row><cell>ACSM</cell><cell>85.4</cell><cell>79.1</cell><cell>-6.3</cell><cell>7.4%</cell><cell>74.9</cell><cell>-10.5</cell><cell>12.3%</cell><cell>88.0</cell><cell>81.2</cell><cell>-6.8</cell><cell>7.7%</cell><cell>70.0</cell><cell>-18.0</cell><cell>20.5%</cell></row><row><cell>baseline</cell><cell>90.5</cell><cell>87.0</cell><cell>-3.5</cell><cell>3.9%</cell><cell>85.7</cell><cell>-4.8</cell><cell>5.3%</cell><cell>90.5</cell><cell>83.2</cell><cell>-7.3</cell><cell>8.1%</cell><cell>82.3</cell><cell>-8.2</cell><cell>9.1%</cell></row><row><cell>baseline + attention</cell><cell>91.0</cell><cell>86.7</cell><cell>-4.3</cell><cell>4.7%</cell><cell>85.7</cell><cell>-5.3</cell><cell>5.8%</cell><cell>90.1</cell><cell>85.6</cell><cell>-4.5</cell><cell>5.0%</cell><cell>83.0</cell><cell>-7.1</cell><cell>7.9%</cell></row><row><cell>baseline + deform</cell><cell>91.4</cell><cell>87.6</cell><cell>-3.8</cell><cell>4.2%</cell><cell>86.7</cell><cell>-4.7</cell><cell>5.1%</cell><cell>91.1</cell><cell>87.4</cell><cell>-3.7</cell><cell>4.1%</cell><cell>84.2</cell><cell>-6.9</cell><cell>7.6%</cell></row><row><cell>baseline + attention + deform</cell><cell>92.0</cell><cell>89.3</cell><cell>-2.7</cell><cell>2.9%</cell><cell>87.6</cell><cell>-4.4</cell><cell>4.8%</cell><cell>91.4</cell><cell>87.2</cell><cell>-4.2</cell><cell>4.6%</cell><cell>83.8</cell><cell>-7.6</cell><cell>8.3%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [almazán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bissacco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5086" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Gordo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedaldi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jaderberg</surname></persName>
		</author>
		<idno>abs/1406.2227. [Jaderberg et al. 2015a</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR. [Jaderberg et al. 2015b] Jaderberg, M.; Simonyan, K.; Zisserman</title>
		<meeting>ICLR. [Jaderberg et al. 2015b] Jaderberg, M.; Simonyan, K.; Zisserman</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Proc. NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jaderberg</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR<address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>Proc. ECCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for OCR in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-lexicon attributeconsistent text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodríguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops<address><addrLine>Gordo, and Perronnin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
	<note>Proc. BMVC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao ;</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene text recognition using partbased tree-structured character detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enforcing similarity constraints with integer programming for better scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feild</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Learned-Miller ;</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate scene text recognition based on recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="35" to="48" />
		</imprint>
	</monogr>
	<note>and Lu</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for OCR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babenko</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene text recognition using similarity and a lexicon with sparse belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICFHR</title>
		<meeting>ICFHR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
	<note>Random projected convolutional feature for scene text recognition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded segmentation-detection networks for textbased traffic sign detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai ; Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
