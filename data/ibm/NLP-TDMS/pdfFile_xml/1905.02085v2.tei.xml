<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form Representation and Differentiable Decoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019">2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form Representation and Differentiable Decoder</title>
					</analytic>
					<monogr>
						<title level="j" type="main">JOURNAL OF IEEE TRANS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2019">2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D Hand pose estimation from a single depth image is an essential topic in computer vision and human-computer interaction. Although the rising of deep learning method boosts the accuracy a lot, the problem is still hard to solve due to the complex structure of the human hand. Existing methods with deep learning either lose spatial information of hand structure or lack a direct supervision of joint coordinates. In this paper, we propose a novel Pixel-wise Regression method, which use spatialform representation (SFR) and differentiable decoder (DD) to solve the two problems. To use our method, we build a model, in which we design a particular SFR and its correlative DD which divided the 3D joint coordinates into two parts, plane coordinates and depth coordinates and use two modules named Plane Regression (PR) and Depth Regression (DR) to deal with them respectively. We conduct an ablation experiment to show the method we proposed achieve better results than the former methods. We also make an exploration on how different training strategies influence the learned SFRs and results. The experiment on three public datasets demonstrates that our model is comparable with the existing state-of-the-art models and in one of them our model can reduce mean 3D joint error by 25%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Hand pose estimation is always an essential topic in computer vision and human-computer interaction <ref type="bibr" target="#b0">[1]</ref> since the human hand is one of the most important things to understand the intention of human, and how humans interact with another object. This field is greatly boosted by the arising of deep learning and availability of depth cameras such as Microsoft Kinect and Intel Realsense <ref type="bibr" target="#b1">[2]</ref>. However, the problem is still very hard to solve, because the complex structure of human hands, along with variety of view-point, causes severe selfocclusion in the image.</p><p>3D Hand pose estimation based on single-frame depth images mostly uses convolution neural networks (CNNs) based methods <ref type="bibr" target="#b2">[3]</ref>. The most popular method is regressionbased method, which treat hand pose estimation as an endto-end learning problem. That is, they build a gigantic neural network to regress the 3D coordinates of each joint point in an end-to-end fashion and let the neural network find out what features it should learn. The obvious defect of this regression-based method is that the output of CNNs must be flattened to go through full-connection (FC) layers to get the joint coordinates. This flatten operation completely destroy the spatial information in the original image, so it is unfavorable The authors are with the State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin 150001, China (e-mail: zfh-hit@hit.edu.cn) for recognizing such a complex structure as the hand and result in a highly nonlinear regression problem.</p><p>In order to preserve the spatial information of human hands, the most commonly used method is the detection-based method with heatmaps, which also has a lot of applications in other similar fields such as human pose estimation. The detection-based method uses fully convolution network (FCN) to maintain the spatial information, and instead of joint coordinates they design an encoded spatial-form representation (SFR) as the learning target. Then a decoder should be used to convert the encoded SFR back to joint coordinates. Heatmaps (or sometimes called believe maps <ref type="bibr" target="#b3">[4]</ref>) is the most commonly used and the most native choice for this encoded SFR. Research <ref type="bibr" target="#b2">[3]</ref> shows that detection-based method has a higher accuracy than the regression-based method.</p><p>However, heatmap is only an approximate representation for 2D coordinates in plane space, so it is not capable to deal with 3D coordinates even when the depth image is provided in 3D hand pose estimation problem. Because the depth image is only a projection of the real 3D data, there exists an offset between actual joint depth value and the value on the depth image. This offset can vary a lot among different joints and different gestures for the severe self-occlusion and heavy noise in depth image. Therefore, in order to use detectionbased method in 3D hand pose estimation, researchers either come up with additional representations along with heatmap or transform the depth image back to the 3D space then use 3D heatmap or other representations.</p><p>Although the detection-based method maintains the spatial information with SFR, it also has defects. First, as we talk above, the depth image is only a projection of the real 3D data, so it is nontrivial to define such a transformation that perfectly reconstructs the original 3D data from a single depth image. Second, the detection-based method lacks a direct supervision from the 3D coordinates. Unlike the regression-based method, in detection-based method, we supervise SFR instead of 3D coordinates. Since the way we build SFR is essentially encoding a low-dimensional data into high-dimension, the SFR must contain some redundancy. The decoder that deals with these redundancies are always non-linear and nondifferentiable in the most of time, thus has to be put outside the neural network. This nondifferentiable structure disable the direct supervision form the 3D coordinates and build a gap between what the neural network learns and what we really want. Like any complex non-linear system, there is no guarantee that a slight error in the SFR wont result in a dramatic change in 3D arXiv:1905.02085v2 [cs.CV] 27 May 2019 <ref type="figure">Fig. 1</ref>. Three types of 3D hand pose estimation method. (a) shows the regression-based method which uses CNNs to extract features and then uses FCs to regress the joint coordinates. The flatten operation between CNNs and FCs destroy the spatial information. (b) shows the detection-based method which uses CNNs to extract a designed spatial-form representation then uses a nondifferentiable decoder outside the neural netwoek to convert the SFR to the joint coordinates. Some models may perform a data transformation to convert the input depth image to another form like voxel map. (c) shows our proposed Pixel-wise Regression method which is a combination of the former two methods. We use CNNs to extract a designed spatial-form representation like the detection-based method, and enables the direct supervision of 3D coordinates like regression-based method by putting a differentiable decoder inside the neural network.</p><p>coordinates. The 3D coordinates just make the situation worse because, in plane space, we need more than one form of SFR to encode the 3D coordinates. Since the decoder is outside the neural network, we cannot directly use the 3D coordinates to supervise the learning. Without such supervision, the SFRs may not corporate with each other.</p><p>In this paper, in order to overcome the defects of existing method we state above, we integrate existing methods and propose a Pixel-wise Regression method, which uses spatialform representation to maintain the spatial information and enables the direct supervision of 3D coordinates by putting a differentiable decoder (DD) inside the neural network. To use our method, we build a particular model, in which we design a particular SFR and its correlative DD which divided the 3D coordinates into two parts, plane coordinates and depth coordinates and use two modules named Plane Regression (PR) and Depth Regression (DR) to deal with them respectively. The relationship among our method and regression-based method and detection-based method is shown in <ref type="figure">Fig. 1</ref>.</p><p>The contributions of this work are summarized as follows:</p><p>• We propose a novel method for 3D pose estimation called Pixel-wise regression, which is a combination of existing two popular methods. Our method uses SFR of 3D coordinates in plane space to maintain the spatial information, and enables the direct supervision of 3D coordinates by putting a DD inside the neural network. • We design a particular SFR and its correlative DD to build a model for 3D hand pose estimation problem from single depth image. We divided the 3D coordinates into two parts, plane coordinates and depth coordinates and use two modules named PR and DR to deal with them respectively. • We implement several baseline models and conduct an ablation experiment to show that our method can achieve better results than former methods. We also study how different training strategies influence the learned SFRs and provide meaningful insights. The experiment on three public datasets shows that our model can achieve comparable result with the state-of-the-art models. The remainder of the paper is organized as follows. In Section II, we give a brief review of the regression-based and detection-based methods and point out the main differences between our models and their models. In Section III, we formalize the problem and describe our proposed Pixel-wise Regression method. We show how we design the SFR and its correlative DD to fulfill the requirement of our method. We also demonstrate other modules we use in our model and the training details. In Section IV, we designed our ablation experiments to analyze our method and performed experiments on three challenging public datasets, MSRA, ICVL and HAND17. Experiments show that our model can achieve state-of-the-art levels on the test set. Finally, we make a conclusion and discuss future potential of our method in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will review some related works of our proposed model. Firstly, we will review the regressionbased method, which is the most popular method in hand pose estimation. Secondly, we talk about the detection-based method that, in general, make a more accurate prediction than the regression-based method but lack the direct supervision from 3D coordinates. Our Pixel-wise regression method is a combination of these two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regression-based Method</head><p>Regression-based method <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b13">[14]</ref> is widely used in hand pose estimation field. It treats hand pose estimation as an end-to-end problem and regresses the 3D coordinate of each joint directly. Some of these methods have learned the lowdimensional space or latent space representation of the hand location. For example, Oberweger, M et al. <ref type="bibr" target="#b10">[11]</ref> believe that joint vectors can be regarded as some low-dimensional spatial representations. They use a special bottleneck structure to force the network to learn such pattern, but since this low-dimensional space is originally a kind of approximate expression, its effect is not good. X. Zhou et al. <ref type="bibr" target="#b11">[12]</ref> used the joint angle as a hidden variable to learn angle information in the network and convert the angle into 3D coordinates through a predefined hand model. This model requires a predefined hand model, lacking generalization ability for new samples. Both of these models have destroyed the spatial information with a flattened representation. There are also some attempts to include spatial information in the network. Chen, Xinghao et al. <ref type="bibr" target="#b12">[13]</ref> used local feature information to correct 3D coordinates. However, because it uses local segmentation to obtain local information, the network can obtain limited information and lack the information about the overall structure of the hand. Wu, X, etc. <ref type="bibr" target="#b13">[14]</ref> adopts a scheme combined with the detection-based method, which is similar to ours, but it needs to first convert the input depth map into a three-dimensional voxel map, and our method is to obtain the 3D coordinates directly in the plane space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection-based Method</head><p>Comparing to the regression-based approach, detectionbased methods <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> hopes use an intermediate SFR to maintain the spatial inforamtions, but often requires artificially design of SFR and lack direct supervision from the 3D coordinates. Since the method of directly applying heatmap can solve the problem of 2D coordinate recognition well, the detection-based method based on heatmap is very commonly used in human pose estimation. However, the detection-based method is not easy to implement in the hand pose estimation problem, because in addition to the need to identify the 2D coordinates in the image, it is also necessary to identify the depth coordinates, which is difficult to represent with a heatmap. However, due to the high precision and intuitiveness of the detection-based method, many scholars still try to use the Detection-based method in hand pose estimation. Tompson, J et al. <ref type="bibr" target="#b15">[16]</ref> used a heatmap for 2D coordinate recognition, and then used a hand model-based iterative method PSO for postprocessing to obtain depth coordinates. Gyeongsik Moon et al. <ref type="bibr" target="#b16">[17]</ref> converted the input depth image into a three-dimensional voxel map, and established a three-dimensional heatmap in the voxel map to convert the two-dimensional problem into a three-dimensional problem, and achieved good results. But this model relies on artificially defined pre-processing to complete the transformation from depth map to 3D voxel map. Wan, C et al. <ref type="bibr" target="#b17">[18]</ref> proposed a method based on an offset vector field, which allows the neural network to learn the bias vector field, and then uses the mean-shift algorithm as decoder to convert the obtained vector field into 3D coordinates. Our design of SFRs is inspired by this work, but instead of bias vector field we use local offset map. We will compare with these models in the experiment part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>To make the problem formally and prevent confusion in the rest of the paper, the input of our model is a depth image I D ∈ R n×n , which only contains one single complete hand. That is our basic assumption. Some filter method and resize need to apply to the raw data to make such an image. We will discuss this in Section IV. And the output of our model is the normalized UVD coordinates in the image plane of each joint P j∈{1...J} j ∈ R 3 . Specifically, we separate the coordinates into two parts, P j(uv) and P j(d) . P j(uv) ∈ R 2 denote the plane coordinates in camera plane space. And P j(d) ∈ R denote depth coordinates which is the distance between the camera and joint j. Notice that, we do not use XYZ coordinates as output like some model did, because the UVD coordinates are more direct information from a depth image without a transformation influenced by the internal parameters of the camera. In this section, we will first propose the Pixelwise method, clarify the problem and why our method can help. Then we will design a particular model use Pixel-wise Regression. Specifically, we design the SFR and its correlative DD which consists of two module CR and DR. Finally, we show other techniques we use when building our model and some implement details about the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pixel-wise Regression method</head><p>The 3D hand pose estimation problem from single depth image can be formalized as a function mapping from depth image to 3D joint coordinates, i.e. φ : I D → P . The regression-based method just wants approximate this function with a single neural network. On the other hand, as we discuss above, the key motivation of detection-based method is that we can make a detour. That is instead of directly mapping to the joint coordinates we can first mapping to an encoded representation of it, i.e. ϕ : I D → L, then use a decoder to recover the joint coordinates, i.e. f : L → P . In which L denotes the encoded representation. To use the detection-based method, one must first define the encoder g : P → L. Ideally, the encoder and the decoder are reciprocal, i.e. P = f (g(P )).</p><p>The intuition here is quite simple, since the SFR and the direct supervision from 3D coordinates both benefit the result of the neural network model, we can just put the decoder inside the network and make use of both, just like what we shown in the <ref type="figure">Fig. 1</ref>. We call this type of method Pixel-wise Regression.</p><p>The essential requirement for this method is that the decoder g must be differentiable. However, the decoder of the commonly used heatmap is argmax, which is a nondifferentiable operation and also not reciprocal of the complex encoder. The truth is, due to the complex nature of the encoder, which maps low-dimensional data to high dimension, the representation must contain certain redundancy which make it hard to perfectly recover the original data with a differentiable decoder. To use the Pixel-wise Regression method, we need to build a particular model. That is we need to design an SFR with a DD which is nontrivial. The overview of our model is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Overview of our designed model. The input depth image goes through a low-level CNN to extract some low-level feature and down sampling to the representation scale. The hourglass module is used to extract the overall features of the input image. The Plane Regression module goes first after the feature extraction, predicting the plane coordinates of each joint and also passing the predicted heat map to Depth Regression module. On the other hand, the Depth Regression module, getting the heat map from Plane Regression module, predicting the depth coordinates of each joint and also output the predicted depth map. We concatenate the heatmap, depth map and representation scale image as the input of the refine stage whose structure is the same as the initial stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design of SFR and DD</head><p>As we discuss in Section I, since we are dealing with 3D coordinates in 2D plane, we need more than one form of SFR to encode the 3D coordinates. In our design, we use two SFRs, heat map and local offset depth map to encode plane coordinates and depth coordinates receptively. We assemble each SFR and its DD to module, thus we have two modules named Plane Regression and Depth Regression. The following of this part is to describe these two modules in details.</p><p>1) Plane Regression: We start from the easy part; the SFR of plane coordinates. The detailed structure of PR is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Generally, PR take a feature map F ∈ R m×m×f as input and predict two things, a heatmapH j ∈ R m×m and P j(uv) for each joint.</p><p>What we need here is an SFR for plane coordinates with a DD. And because heatmap matches our human intuition and already achieve good result in 2D problem, we want to modify it to meet our requirement. The problem is that the normal heatmap use argmax as decoder which is nondifferentiable. A simple idea is to use the center of mass (COM) of the heatmap to represent the desire plane coordinates, i.e.</p><formula xml:id="formula_0">P j(u) = i,k H j (i, k) · u(i, k)<label>(1)</label></formula><formula xml:id="formula_1">P j(v) = i,k H j (i, k) · v(i, k)<label>(2)</label></formula><p>where the u(i, j) and v(i, j) denote the normalized u, v coordinates for the pixel (i, j) .</p><p>This COM-based decoder has two clear advantages. First, it meets our requirement of being differentiable. Moreover, it is easy to make this decoder in a convolutional form, which is the most common building block in deep learning thus can accelerate computation. In the convolutional form, the above equation can be rewritten as:</p><formula xml:id="formula_2">P j(uv) = H j * C<label>(3)</label></formula><p>where C ∈ R m×m×2 is the COM convolutional kernel, defined as:</p><formula xml:id="formula_3">C(i, j, k) = u(i, j) k = 1 v(i, j) k = 2<label>(4)</label></formula><p>To complete this module, we need to define an encoder to build our desired heatmap. Although the heatmap we want is unlike the commonly used heatmap, we still want to retain its unique intuitive nature, that is, the closer the pixel to the given joint is, the larger the value it will be. We divided the encoder into two steps. In the first step, we only take the four nearby pixels into account. Notice that it is still an indeterminate system because the COM in plane space can only provide two condition and we have four variables to solve. But since it is still a probability map, we can form some constraint as H j (i, k) ≥ 0 and i,k H j (i, k) = 1. And we choose the middle value of the efficient solution set as the four corners value for our basic heatmap. Then we apply a gauss kernel with size k to the basic heatmap we get from the first step to form our desired heatmap. It is obvious that the convolution here does not change the COM for the given heatmap. The kernel size here measures how much redundancy we want to put into the heatmap. And in practice, the valid region, where the heatmap have a non-zero value, in the heatmap also measures the uncertainty that the network feels about the prediction of a certain joint, we will discuss more about this in Section IV.</p><p>We use the mean-square-error between predicted value and ground truth as loss of this module which contains two part L uv and L H which denote the coordinates loss and representation loss respectively: <ref type="figure">Fig. 4</ref>. Detail structure of Depth Regression. Input features go through a CNN network and predict the depth map for each joint. Then combining with label scale image, we recover the depth information in local joint. The mask filter values that do not on the hand. Finally, a weighted sum is conducted to convert the heatmap and depth map to the depth coordinates.</p><formula xml:id="formula_4">L uv = j P j(uv) −P j(uv) 2 2 (5) L H = i,j,k H j (i, k) −H j (i, k) 2 (6)</formula><p>2) Depth Regression: Then we deal with the depth coordinates. The detailed structure of DR is shown in <ref type="figure">Fig. 4</ref>. Generally, DR take a feature map F ∈ R m×m×f , a representation scale image I ∈ R m×m , a mask matrix M ∈ R m×m and the predicted heatmapH j={1...J} j from PR as input and predict two things, a local offset depth mapD j ∈ R m×m andP j(d) for each joint.</p><p>As we discuss in the PR part, we need an SFR of depth coordinates with a DD. We have shown in Section I that the real depth coordinates of the joints have varied offsets for the values on the depth image. Therefore, the idea here is that we can use the neural network to predict these offsets in a spatialform. Intuitively, like the heatmap, only the pixels that are close to the joints can give the information about this offset. So, we build a local offset depth map (depth map for short) to encode the depth information into plane space. The depth map is defined as:</p><formula xml:id="formula_5">D j (i, k) = P j(d) − I(i, k) · M (i, k) if H j (i, k) &gt; 0 0 otherwise (7)</formula><p>We use the heatmap we build in the PR to define the local region of the given joint to make sure that the heatmap and the depth map have same level of redundancy. And the representation scale image I is just a resized image from the input depth image L D . The reason we need this image is that since we have some down sampling process in the network, the size of the depth map is different from the input depth image. Since we use the depth offset to encode the depth coordinates, it is meaningless if there are some values not on the hand. The offset value for the background is just the exact value for the depth coordinates of the joints, which makes the problem no easier than the direct regression. Hence, we use mask matrix M which denotes which pixel is on the hand to filter the depth map. The mask matrix M is defined as:</p><formula xml:id="formula_6">M (i, k) = 1 if I(i, k) &gt; 0 0 otherwise<label>(8)</label></formula><p>After we design the SFR of depth coordinates, we also need a DD to fuse the redundant depth map to a scalar. Ideally, we can fully recover the depth coordinates by adding any two value of D and I in the local region of the given joint. So, it seems that the DD can be simply defined as averaging the recover values. However, since the depth mapD j is predicted by the neural network, the result will not be perfect. We want the depth map has some same property as heatmaps, which has more accuracy when closer to the given joint. Therefore, our decoder uses the heatmap to weight the recover values given by the neural network:</p><formula xml:id="formula_7">P j(d) = i,k M (i, k)H j (i, k) I(i, k) +D j (i, k) i,k M (i, k)H j (i, k)<label>(9)</label></formula><p>Due to the background problem we state above, we also use the mask matrix M here to ignore the pixel off the hand.</p><p>Like the PR, we use the mean-square-error as well. The loss of this module also contains two part L d and L D which denote the coordinates loss and representation loss respectively:</p><formula xml:id="formula_8">L d = j P j(d) −P j(d) 2 (10) L D = i,j,k D j (i, k) −D j (i, k) 2 (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network architecture</head><p>As shown in previous works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, the multistage network can refine the result and deal with occlusions by inferring from results from the previous stage. In our model, making a tradeoff between speed and accuracy, we use a typically two-stage network, which uses the same structure. The loss function of the multi-stage network can be defined as the sum loss of each stage:</p><formula xml:id="formula_9">L = s L (s)<label>(12)</label></formula><p>In our model, the loss function for a single stage is the sum of loss of PR and DR:</p><formula xml:id="formula_10">L (s) = L (s) iw + L (s) d + λ H L (s) H + λ D L (s) D<label>(13)</label></formula><p>The λ H and λ D are two scale factors to make sure the losses are on a similar scale. In our model, we empirically set λ H = λ D = 1.</p><p>We also use the hourglass module <ref type="bibr" target="#b19">[20]</ref> as our main feature extractor. The hourglass module, due to its unique recursive structure, can provide features extracted in multi-scales and widely used in pose estimation problems.</p><p>We have two sizes of the image in our model. Only the input depth image is in a bigger size of R m×m , and all the other inputs and representations are in the size of R n×n . We use a bigger image to contain more details in the low-level features as we have a low-level CNN block before the initial stage which extracts low-level features and down sampling it to R n×n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implement Detial</head><p>We use TensorFlow framework to build and train our network using Adam optimizer with a learning rate of 2e − 4. Since our model uses massive channel-wise operation, we use instance normalization <ref type="bibr" target="#b20">[21]</ref> to accelerate training. We use a similar data normalization method to <ref type="bibr" target="#b10">[11]</ref>, which extracts a fixed-size cube centered on the center of mass of this object from the depth image. We set the input image size m to 128, and representation size n to 64. We also implement a [-30, 30] random rotation as data augmentation. Since our model is an FCN which is translation invariant, we do not perform any random translation for data augmentation. Empirically, we set the kernel size k to 7 to get certain level of redundancy. The batch size is set to 32, and we train our model with 10 epochs. In the test time, our model can achieve about 100 FPS on a single TITAN XP GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In this section, we conduct experiments on three challenge public datasets, i.e. MSRA, ICVL and HAND17. We choose the MSRA dataset to conduct the ablation experiment because the dataset contains only full hand image that fit our assumption for the input image best. We used two metrics in all experiments. The first is the mean 3D distance error for all joint points. Another metric, which is more challenging, is that the percentage of frames in which all joint errors are below a certain threshold.</p><p>A. Datasets 1) MSRA dataset: The MSRA hand pose dataset <ref type="bibr" target="#b21">[22]</ref> contains 76500 frames from 9 different subjects captured by Intels Creative Interactive Camera. The dataset provides annotations for 21 joint points, four joints per finger, and one joint on the palm. Since the dataset itself does not divide the training set and the test set, we randomly divide the training set and the test set by a ratio of 8:1. Since the data itself is a segmented hand image, we do not conduct any new segmentation.</p><p>2) ICVL dataset: The ICVL dataset <ref type="bibr" target="#b22">[23]</ref> contains 330k frames from 10 different subjects also using Intels Creative Interactive Gesture Camera. This dataset contains annotations for 16 joint points, three for each finger and one for the palm. The dataset contains random rotations, and since we used online data augmentation, we only used 22k of original images. Since the image contains the background, we first use a boundary box built by the ground truth of joints coordinates to roughly remove the background. Then we use an empirical threshold to remove the rest.</p><p>3) HAND17 dataset: The HAND17 dataset <ref type="bibr" target="#b2">[3]</ref> is the largest scale dataset in 3D hand pose estimation field which contains 957k frames for training and 295k frames for testing. The dataset captures the depth images by the latest Intel RealSense SR300 camera and automatic annotates 21 joints using six 6D magnetic sensors and inverse kinematics. Since the images provided by the dataset has background, we use the same procedure we describe in ICVL dataset to remove them. The test set provides the boundary box for us, thus we only use the empirical threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Experiment</head><p>To demonstrate the validity of our method, we design and perform an ablation experiment. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref> we modify some structures in our model to form four different baseline models. To set up a fare comparison, we retain the feature extraction part of the model and take the extracted features as the input of the ablation experiment. Generally, we conduct this ablation experiment for three purposes.</p><p>First, we want to prove that the Pixel-wise Regression method we proposed is better than the former two methods. In order to compare with the regression-based method, we replace PR and DR with three fully connected layers and directly regress 3D coordinates from features. We call this model Direct Rregression. On the other hand, in order to compare with the detection-based method, we remove the supervision of the joint coordinates and put the decoder outside the neural network. We call this model Coordinate Unsupervised.</p><p>Second, we want to explore how different training strategies influence the learned SFRs and results. As we discuss in Section III, although we design reasonable SFRs, we control the redundancy in such representation by an empirical kernel size k. In the original design we use, we supervised the coordinates along with the representation, which can guide the representation predicted by the network converge to the representation we design. However, even if we do not supervise the representation, the model can still learn a representation of 3D coordinates due to the gradient backpropagated by the decoder. The decoder here serves not only a regression module but also a constraint that limits the search space of the representation. Because there are infinite such SFRs, the learning outcomes of the network may be different from supervised learning model. To check out the outcome of such a model and get deeper understanding of our model, we remove the supervision for the representation and call this network Representation Unsupervised.</p><p>Finally, we want to verify that the multi-stage network is able to improve the accuracy of the predictions. So, in the last comparison experiment we only keep the first stage, so we call this model One Stage. We compare these four models with our designed two-stage model which we call Two Stage here on the MSRA dataset.</p><p>The results of the ablation experiment are shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. We can see that as we estimated, Direct Regression, which  does not preserve spatial information, yields the worst results. Coordinate Unsupervised, a detection-based method, yields a better result than the regression-based method. However, since there is no direct supervision from coordinates, the two modules are learned separately and cannot perfectly cooperate, so the result is not good either. Moreover, compared with the One Stage model, it can be found that the Two Stage method does have a significant improvement in the accuracy.</p><p>For the Representation Unsupervised method, we can find that its performance is better than other methods, and yield comparable result with our original design. Since we want to study the learned representation effect by supervision, we draw the learned representation upon the input depth image, as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. The result is quite interesting. We can see that the supervised representation tends to have a low-level redundancy as we design and have same level of redundancy for different joints. The valid region of depth map is slightly larger than the heatmap, but they converge to the same joint. That is because the recovered depth values given by depth map are weighted by the heatmap which cannot be precisely predicted. Therefore, the depth map must make the prediction in a larger local region to compensate the uncertainty of the heatmap. This is also what make our Pixel-wise Regression method better than the detection-based method.</p><p>On the other hand, the unsupervised model has more  <ref type="bibr" target="#b12">[13]</ref> 8.649 DenseReg <ref type="bibr" target="#b17">[18]</ref> 7.234 3DCNN <ref type="bibr" target="#b25">[26]</ref> 9.584 SHPR-Net <ref type="bibr" target="#b6">[7]</ref> 7.756 HandPointNet <ref type="bibr" target="#b26">[27]</ref> 8.505 Point-to-Point <ref type="bibr" target="#b27">[28]</ref> 7.707 Ours 5.186 redundancy for the larger joints, i.e. the MCP joint. That is because the larger the joint the more similar the local region looks like. For instance, the MCP joint is closer to the palm and other root joint of finger, so it is always surround by the dense and similar information provided by the depth image. Thus, without the supervision, these similarities can cause confusion and result in uncertainty. On the other hand, the TIP joint, which is usually locates at the boundary of the hand has a clearer context and has lower uncertainty. The depth map predicted by the unsupervised model is quite strange with a circular form and do not focus on a particular region. That is because the valid region of depth map is defined by heatmap and mask matrix, when we do not supervise the depth map, there are zero gradients for those invalid pixels which gives the network more freedoms to choose the value for them. These freedoms loosen the local constrain when we design the depth map and may cause the model to overfit to particular patterns. Actually, the circular form of the depth map is an evidence that the network is overfit to the random rotation we perform during data processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing with State-of-the-Art</head><p>In this part, we compare our model with the state-of-the-art models. We have chosen models that are representative and have the state-of-the-art accuracy. In addition to the models we mentioned in Section II, we also include Region Ensemble network (include REN-9x6x6 <ref type="bibr" target="#b23">[24]</ref> and REN-4x6x6 <ref type="bibr" target="#b24">[25]</ref>), 3D Convolutional Neural Networks <ref type="figure" target="#fig_0">(3DCNN [26]</ref>), SHPR-Net <ref type="bibr" target="#b6">[7]</ref>, HandPointNet <ref type="bibr" target="#b26">[27]</ref>, Point-to-Point <ref type="bibr" target="#b27">[28]</ref>. We use tools provide by Chen, X <ref type="bibr" target="#b12">[13]</ref> to evaluate the result.</p><p>As shown in <ref type="figure">Fig. 8</ref> and TABLE I. in MSRA dataset, our model greatly outperforms current state-of-the-art models in all joints. For the mean error of all joints, we only have 75% of the best results present <ref type="bibr" target="#b17">[18]</ref>, which is a big improvement. In addition, as can be seen in <ref type="figure">Fig. 8</ref>. our model greatly exceeds other models with a small allowable threshold, which means that we propose that the model is very effective in highprecision recognition.</p><p>The results of ICVL dataset are shown in <ref type="figure">Fig. 9</ref> and TABLE II. As can be seen from the results, we have achieved similar results with the best model <ref type="bibr" target="#b16">[17]</ref> on the average 3D error. As can be seen from <ref type="figure">Figure 9</ref>, when the error threshold is less than 10mm, we exceed all other models, but when the threshold is greater than 30mm, our model is not as effective as other <ref type="figure">Fig. 8</ref>. The results of MSRA dataset. Left is mean error (mm) for each joint. Right is the proportion of frame that all joints error are under the given threshold. <ref type="figure">Fig. 9</ref>. The results of ICVL dataset. Left is mean error (mm) for each joint. Right is the proportion of frame that all joints error are under the given threshold. DeepModel <ref type="bibr" target="#b11">[12]</ref> 11.561 REN-4x6x6 <ref type="bibr" target="#b24">[25]</ref> 7.628 REN-9x6x6 <ref type="bibr" target="#b23">[24]</ref> 7.305 Pose-REN <ref type="bibr" target="#b12">[13]</ref> 6.791 DenseReg <ref type="bibr" target="#b17">[18]</ref> 7.239 SHPR-Net <ref type="bibr" target="#b6">[7]</ref> 7.219 V2V-PoseNet <ref type="bibr" target="#b16">[17]</ref> 6.284 Ours 6.177 models. This is because we manually segmented the image and the background of the image was not completely removed, which affects the performance of our model. The result of HAND17 dataset is shown in TABLE III. Although it is two to three millimeters worse than the best model, we still achieve comparable result with other competitors. Specially, we find that our model gets better performance on seen joints than the unseen ones. We attribute this to the cause that our design of SFR is not efficient for the self-occlusion joints which have larger variation range than the seen ones. We also show some qualitative results in these two datasets in <ref type="figure" target="#fig_5">Fig. 10</ref>. Some of these results are even better than the original annotations, which demonstrate that our model learn the essential information of the data and not just remember all the patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a Pixel-wise Regression method for 3D hand pose estimation, which use spatial-form representation and differentiable decoder to solve the losing spatial information and lacking direct supervision problems faced by existing methods. We design a particular model that use our proposed method. Specifically, we design the spatial-form representation and its correlative differentiable decoder which consists of two modules Plane regression and Depth Regression that deal with plane coordinates and depth coordinates respectively. The ablation experiment shows that our proposed method is better than the former methods. And it also shows that the supervision on the representation is vital to the performance for our model. Experiments on public datasets show that our model reach the state-of-the-art level performance.</p><p>In the future, we intend to explore more design of representation used for Pixel-wise Regression method or directly use our model in the field of human-computer interaction to control real robotics hand. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our designed model. The input depth image goes through a low-level CNN to extract some low-level feature and down sampling to the representation scale. The hourglass module is used to extract the overall features of the input image. The Plane Regression module goes first after the feature extraction, predicting the plane coordinates of each joint and also passing the predicted heat map to Depth Regression module. On the other hand, the Depth Regression module, getting the heat map from Plane Regression module, predicting the depth coordinates of each joint and also output the predicted depth map. We concatenate the heatmap, depth map and representation scale image as the input of the refine stage whose structure is the same as the initial stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Detail structure of Plane Regression. Input features go through a CNN network and predict the heatmap for each joint. Then the heatmap goes through a COM(center of mass) based convolution to convert the heatmap to 2D coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Setup of the ablation experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The result of the ablation experiment using the maximum allowed distance metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Learned equivalent representation. The top is the result of supervised model, and the bottom is the result of unsupervised method. For each part, the first row is learned depth map, and the second row is learned heatmap. From left to right is MCP, PIP, DIP, TIP joint of the index finger receptively. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results of our model. (a), (b), (c) shows the result form MSRA, ICVL, HAND17 dataset respectively. Specially, in (a) and (b) we provide ground truth in the second row for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="2">MEAN 3D ERROR ON MSRA DATASET</cell></row><row><cell>Model</cell><cell>3D error (mm)</cell></row><row><cell>REN-9x6x6 [24]</cell><cell>9.792</cell></row><row><cell>Pose-REN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="2">MEAN 3D ERROR ON ICVL DATASET</cell></row><row><cell>Model</cell><cell>3D error (mm)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">MEAN 3D ERROR ON HAND17 DATASET</cell><cell></cell></row><row><cell>Team Name</cell><cell cols="3">Average (mm) Seen (mm) Unseen (mm)</cell></row><row><cell>BUPT</cell><cell>8.39</cell><cell>6.06</cell><cell>10.33</cell></row><row><cell>SNU CVLAB</cell><cell>9.95</cell><cell>6.97</cell><cell>12.43</cell></row><row><cell>NTU</cell><cell>11.30</cell><cell>8.86</cell><cell>13.33</cell></row><row><cell>THU VCLab</cell><cell>11.70</cell><cell>9.15</cell><cell>13.83</cell></row><row><cell>NAIST RV</cell><cell>12.74</cell><cell>9.73</cell><cell>15.24</cell></row><row><cell>HuPBA</cell><cell>14.74</cell><cell>11.87</cell><cell>17.14</cell></row><row><cell>NAIST RVLab G2</cell><cell>16.61</cell><cell>13.53</cell><cell>19.18</cell></row><row><cell>Baseline</cell><cell>19.71</cell><cell>14.58</cell><cell>23.98</cell></row><row><cell>Ours</cell><cell>12.22</cell><cell>8.73</cell><cell>15.13</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An assistive decisionand-control architecture for force-sensitive hand-arm systems driven by human-machine interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haddadin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jarosiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simeral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="763" to="780" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depthbased hand pose estimation: Methods, data, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S S</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1180" to="1198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depthbased 3d hand pose estimation: From current achievements to future goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d hand poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1374" to="1387" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time 3d hand pose estimation with 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="956" to="970" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shpr-net: Deep semantic hand pose regression from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="43" to="425" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A crnn module for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurfess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware deep spatio-temporal network for hand pose estimation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep network with spherical part model for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2421" to="2427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03416</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Handmap: Robust hand pose estimation via intermediate dense guidance map supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Finnegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O&amp;apos;neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="246" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation from single depth images using multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4422" to="4436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;14 Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Region ensemble network: Towards good practices for deep 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="404" to="414" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4512" to="4516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8417" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="505" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
