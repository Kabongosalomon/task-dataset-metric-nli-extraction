<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Topological Representation for Networks via Hierarchical Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoji</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computational Intelligence University Key Laboratory of Evolving Intelligent Systems of Guangdong Province</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengbin</forename><surname>Hou</surname></persName>
							<email>chengbin.hou10@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computational Intelligence University Key Laboratory of Evolving Intelligent Systems of Guangdong Province</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
							<email>xiny@sustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of Computational Intelligence University Key Laboratory of Evolving Intelligent Systems of Guangdong Province</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Topological Representation for Networks via Hierarchical Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Networks analysis</term>
					<term>network topology</term>
					<term>represen- tation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The topological information is essential for studying the relationship between nodes in a network. Recently, Network Representation Learning (NRL), which projects a network into a low-dimensional vector space, has been shown their advantages in analyzing large-scale networks. However, most existing NRL methods are designed to preserve the local topology of a network, they fail to capture the global topology. To tackle this issue, we propose a new NRL framework, named HSRL, to help existing NRL methods capture both the local and global topological information of a network. Specifically, HSRL recursively compresses an input network into a series of smaller networks using a community-awareness compressing strategy. Then, an existing NRL method is used to learn node embeddings for each compressed network. Finally, the node embeddings of the input network are obtained by concatenating the node embeddings from all compressed networks. Empirical studies for link prediction on five real-world datasets demonstrate the advantages of HSRL over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The science of networks has been widely used to understand the behaviours of complex systems. These systems are typically described as networks, such as social networks in social media <ref type="bibr" target="#b0">[1]</ref>, bibliographic networks in academic field <ref type="bibr" target="#b1">[2]</ref>, protein-protein interaction networks in biology <ref type="bibr" target="#b2">[3]</ref>. Studying the relationship between entities in a complex system is an essential topic, which benefits a variety of applications <ref type="bibr" target="#b3">[4]</ref>. Just take a few examples, predicting potential new friendship between users in social networks <ref type="bibr" target="#b4">[5]</ref>, searching similar authors in bibliographic networks <ref type="bibr" target="#b1">[2]</ref>, recommending new movies to users in movie user-movie interest networks <ref type="bibr" target="#b5">[6]</ref>. The topologies of these networks provide insight information on the relationship between nodes. We can find out strongly connected neighborhoods of a node by exploring the local topology in a network. Meanwhile, the global topology is another significant aspect for studying the relationship between communities. As shown in <ref type="figure" target="#fig_0">Fig.1</ref>, such hierarchical topological information is helpful to learn the relationship between nodes in a network.</p><p>Many networks are large-scale in real-world scenarios, such as a Facebook social network contains billion of users <ref type="bibr" target="#b6">[7]</ref>. As a result, most traditional network analytic methods suffer from high computation and space cost <ref type="bibr" target="#b3">[4]</ref>. To tackle this issue, Network Representation Learning (NRL) has been a popular technique to analyze large-scale networks recently. In particular, NRL aims to map a network into a low-dimensional vector space, while preserving as much of the original network topological information as possible. Nodes in a network are represented as low-dimensional vectors which are used as input features for downstream network analysis algorithms.</p><p>Traditional NRL methods such as LLE <ref type="bibr" target="#b7">[8]</ref> and ISOMap <ref type="bibr" target="#b8">[9]</ref> work well on small networks, while they are infeasible to large-scale networks due to the high computational cost. Recently, some online learning methods, e.g., DeepWalk <ref type="bibr" target="#b9">[10]</ref>, node2vec <ref type="bibr" target="#b10">[11]</ref>, and LINE <ref type="bibr" target="#b11">[12]</ref>, have been proposed to learn large-scale network representation, which has been demonstrated their efficiency and effectiveness for the large-scale network analysis. However, the above NRL methods only consider the local topology of networks and fail to capture the global topological information. DeepWalk and node2vec firstly employ short random walks to explore the local neighborhoods of nodes and obtain node embeddings by the Skip-Gram model <ref type="bibr" target="#b12">[13]</ref>. LINE preserves the first-order and second-order proximities so that it can only measure the relationship between nodes at most two-hops away. These methods are efficient to capture the relationship between close nodes, however, fail to consider the case for nodes which are far away from each other. Recently, HARP <ref type="bibr" target="#b13">[14]</ref> has been proposed to overcome this issue. It recursively compresses a network into a series of small networks based on two node collapsing schemes and learns node embeddings for each compressed network by using an existing NRL method. Unfortunately, the compressed networks may not reveal the global topology of an input network, since HARP heuristically merges two closed nodes into a new node. Furthermore, when learning node embeddings on the original network, using node embeddings obtained on compressed networks as the initialization solution may mislead the optimization process to a bad local minimum. This paper presents a new NRL framework, called Hierarchical Sampling Representation Learning (HSRL), to learn node embeddings for a network with preserving both their local and global topological information. Specifically, HSRL uses a community-awareness network compressing strategy, called hierarchical sampling, to recursively compress an input network into a series of smaller networks, and then engage an existing NRL method to learn node embeddings for each compressed network. Finally, the node embeddings of the original network can be obtained by concatenating all node embeddings learned on compressed networks. Besides, we mathematically show that HSRL is able to capture the local and global topological relationship between nodes. Novel contributions of this paper include the following:</p><p>• We propose a new NRL framework called HSRL, to learn node embeddings for a network, which is able to capture both local and global topological information of the network via a community-awareness network compressing strategy. <ref type="bibr">•</ref> We mathematically show that the node embeddings obtained by HSRL explicitly embed the local and global topological information of the input network. • We demonstrate that HSRL statistically significantly outperforms DeepWalk, node2vec, LINE, and HARP on link prediction tasks on five real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Most early methods in NRL field represent an input network in the form of a matrix, e.g., adjacency matrices <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, Laplacian matrices <ref type="bibr" target="#b15">[16]</ref>, node transition probability matrices <ref type="bibr" target="#b16">[17]</ref>, and then factorize that matrix to obtain node embeddings. They are effective for small networks, but cannot scale to large-scale networks due to high computation cost.</p><p>To analyze large-scale networks, DeepWalk <ref type="bibr" target="#b9">[10]</ref> employs truncated random walks to obtain node sequences, and then learns node embeddings by feeding node sequences into Skip-Gram model <ref type="bibr" target="#b14">[15]</ref>. To generalize DeepWalk, node2vec <ref type="bibr" target="#b10">[11]</ref> provides a trade-off between breadth-first search (BFS) and depth-first search (DFS) when generating truncated random walks for a network. LINE <ref type="bibr" target="#b11">[12]</ref> intends to preserve first-order and second-order proximities, respectively, by minimizing the Kullback-Leibler divergence of two joint probability distributions for each pair nodes. These methods are scalable to large-scale networks, but fail to capture the global topological information of networks. Because random walks are only effective to explore local neighborhoods for a node, and both first-order and second-order proximities defined by LINE just measure the relationship between nodes at most two-hops away.</p><p>To investigate global topologies of a network, HARP <ref type="bibr" target="#b13">[14]</ref> recursively uses two collapsing schemes, edge collapsing and star collapsing, to compress an input network into a series of small networks. Starting from the smallest compressed network, it then recursively conducts a NRL method to learn node embeddings based on the node embeddings obtained from its previous level (if any) as the initialization. However, HARP has two weaknesses: 1) nodes that are connected but belong to different communities may be merged, which leads to that the compressed networks cannot well reveal the global topology of an input network. 2) taking the node embeddings learned on such compressed networks as initialization would mislead NRL methods to a bad local minimum. HARP could work well on node classification tasks since close nodes tend to have the same labels but may ineffective for the link prediction tasks. Because predicting the link between two nodes needs to consider both the local and global topological information of a network, such as neighborhoods they are sharing with and communities they are both involved in. This paper proposes HSRL to tackle the above issues of the existing NRL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARY AND PROBLEM DEFINITION</head><p>This section gives the notations and definitions throughout this paper.</p><p>We firstly introduce the definition of a network and related notations.</p><formula xml:id="formula_0">Definition 1. (Network) [4] A network (a.k.a. graph) is defined as G = (V, E),</formula><p>where V is a set of nodes and E is a set of edges between nodes. The edge e ∈ E between nodes u and v is represented as e = (u, v) with a weight w u,v ≥ 0.</p><formula xml:id="formula_1">Particularly, we have (u, v) ≡ (v, u) and w u,v ≡ w v,u if G is undirected; (u, v) ≡ (v, u) and w u,v ≡ w v,u , otherwise.</formula><p>In most networks, some nodes are densely connected to form a community/cluster, while nodes in different communities are sparsely connected. Detecting communities in a network is beneficial to analyze the relationship between nodes. We employ modularity as defined below to evaluate the quality of community detection. Definition 2. (Modularity) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> Modularity is a measure of the structure of networks, which measures the density of edges between the nodes within communities as compared to the edges between nodes in different communities. It is defined as below.</p><formula xml:id="formula_2">Q = 1 2m i,j w i,j − k i k j 2m δ(c i , c j )<label>(1)</label></formula><p>where w i,j is the weight of edge e i,j between nodes v i and v j ,</p><formula xml:id="formula_3">k i = i w i,j , m = 1</formula><p>Networks with high modularity have dense connections between nodes within communities but sparse connections between nodes in different communities.</p><p>We give the definition of hierarchical sampling which is used to recursively compress a network into a series of smaller networks as follows.</p><p>Definition 3. (Hierarchical Sampling) Given a network G = (V, E), hierarchical sampling compresses the original network level by level and obtains a series of compressed networks G 0 , G 1 , ..., G K , which reveals the global topological information of original network at different levels, respectively.</p><p>These compressed networks reveal the hierarchical topologies of the input network. Therefore, the node embeddings obtained in compressed networks embed the hierarchical topological information of the original network.</p><p>To learn node embeddings of a network, NRL maps the original network into a low-dimensional space and represents each node as a low-dimensional vector as formulated below. </p><formula xml:id="formula_4">v → z ∈ R d where d</formula><p>|V |, and preserving as much of the original topological information in the embedding space R d .</p><p>Finally, we present the formulation of the hierarchical network representation learning problem as following: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HSRL</head><p>In this section, we present Hierarchical Sampling Representation Learning framework which consists of two parts: 1) Hierarchical Sampling that aims to discover the hierarchical topological information of a network via a communityawareness compressing strategy; and 2) Representation Learning that aims to learn low-dimensional node embeddings while preserving the hierarchical topological information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchical Sampling</head><p>Here we present the hierarchical sampling which is intended to compress a network into a series of compressed networks according to different compressing levels. Each compressed network reveals one of the hierarchical levels of global topology of the original network.</p><p>A community is one of the significant patterns of networks. Nodes in the same community are densely connected and nodes in different communities are sparsely connected. The relationship between nodes inside a community presents the local topological information of a network, while the relationship between communities reveals its global topology. It is worth noticing that in most large-scale networks, there are several natural organization levels -communities divide themselves into sub-communities -and thus communities with different hierarchical levels reveal the hierarchical topological information of original networks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Consequently, we compress a network into a new network based on communities by taking each community as a new node in the compressed network. Based on different hierarchical levels of communities, we can obtain a series of compressed networks which reveal the hierarchical global topological information of the input network.</p><p>The quality of the partitions obtained by community detection algorithms can be measured by the modularity of the partition <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As a result, we can detect communities through optimizing the modularity of a network. As shown in <ref type="figure" target="#fig_3">Fig.2</ref>, inspired by the Louvain method <ref type="bibr" target="#b20">[21]</ref>, hierarchical sampling compresses a network into a new network by implementing two phases: modularity optimization and node aggregation.</p><p>Modularity optimization. The first phase initializes each node in a network as a community and merges two connected nodes into one community if it can improve the modularity of the network. The implementation of community amalgamation will be repeated until a local maximum of the modularity is attained.</p><p>Node aggregation. The second phase builds a new network whose nodes are the communities found in the previous phase. The weights of edges between new nodes are the sum of the weights of edges between nodes in the corresponding two communities.</p><p>As shown in Algorithm 1, by recursively repeating the above two phases, hierarchical sampling obtains a series of compressed networks which reveal hierarchical global topology of the original network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Hierarchical Sampling</head><formula xml:id="formula_5">Input: network G = (V, E), the largest # compressed levels K Output: a series of compressed networks G 0 , G 1 , ..., G K 1: G 0 ← G 2: for k ≤ K do 3: C k ← M odularityOptimization(G k ) 4: G k+1 ← N odeAggregation(C k ) 5:</formula><p>k ← k + 1 6: end for 7: return G 0 , G 1 , ..., G K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Representation Learning</head><p>This section introduces representation learning on the compressed networks obtained by the previous section and concatenating the learned embeddings into node embeddings of the original network. We further provide a mathematical proof to demonstrate that HSRL embeds both local and global topological relationship of nodes in the original network into the learned embeddings.</p><p>As shown in <ref type="figure" target="#fig_4">Fig.3</ref>, we conduct representation learning on each compressed network. It is worth noticing that any NRL method can be used for this purpose. The embeddings of nodes in each compressed network are used to generate the final node embeddings of the original network. Particularly, the embedding Z i of node v i in the original network G is the concatenation of the embeddings of hierarchical communities it involved in, as shown below.</p><formula xml:id="formula_6">Z i = [Z 0 c 0 i , Z 1 c 1 i , ..., Z K c K i ],<label>(2)</label></formula><p>where c k i is the k-th hierarchical community v i belongs to. The node embeddings learned by the above representation learning process hold the following two Lemmas. Lemma 1. Nodes within the same hierarchical communities will get similar embeddings. The more the same hierarchical communities in which nodes involved, more similar embeddings they have.</p><p>From Eq.2, it is easy to find that the above lemma holds. Lemma 1 shows that HSRL preserves the relationship between densely connected nodes in the original network. Therefore, HSRL is capable to preserve the local topological information of a network. Lemma 2. The cosine similarity between embedding Z i and embedding Z j is proportional to the sum of similarities of the embeddings between their hierarchical communities.</p><formula xml:id="formula_7">sim(Z i , Z j ) ∝ K k=0 sim(Z k c k i , Z k c k j ).<label>(3)</label></formula><p>Proof.</p><formula xml:id="formula_8">sim(Z i , Z j ) = Z i · Z j Z i Z j ∝ [Z 0 c 0 i , Z 1 c 1 i , ..., Z K c K i ] · [Z 0 c 0 j , Z 1 c 1 j , ..., Z K c K j ] = K k=0 Z k c k i · Z k c k j ∝ K k=0 sim(Z k c k i , Z k c k j ).</formula><p>From Lemma 2, we know that two nodes will obtain similar embeddings if they are involved in similar hierarchical communities no matter the distance between them in the original network. The relationship between communities in different hierarchies is embedded in the embeddings of their involved nodes. Hence, HSRL can preserve the hierarchical global topological information of a network.</p><p>Finally, HSRL is presented in Algorithm 2.</p><p>Algorithm 2 HSRL Input: network G = (V, E), compressing levels K, NRL mapping function f Output: node embeddings Z 1: G 0 , G 1 , ..., G K ← HierarchicalSampling(G, K) 2: for k ≤ K do 3:</p><formula xml:id="formula_9">Z k ← f (G k ) 4: k ← k + 1 5: end for 6: Z ← Concatenating(Z 0 , Z 1 , ..., Z K ) 7: return Z V. EXPERIMENTS</formula><p>In this section, five real-world datasets are used to evaluate the performance of HSRL on link prediction task. The source code is available at https://github.com/fuguoji/HSRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our method on various real-world datasets, including Movielens 1 , MIT <ref type="bibr" target="#b22">[23]</ref>, DBLP <ref type="bibr" target="#b1">[2]</ref>, Douban <ref type="bibr" target="#b23">[24]</ref>, and Yelp 2 . These datasets are commonly used in NRL field. The detailed statistics of datasets are shown in <ref type="table" target="#tab_1">Table I</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We compare our method with four start-of-the-art algorithms, which are introduced below.</p><p>• DeepWalk: DeepWalk is a random walk based NRL method. It conducts random walks on each node to sample node sequences from a network and uses the Skip-Gram model to learn node embeddings by treating node sequences as sentences and nodes as words. • node2vec: node2vec is a biased random walk based method that provides a trade-off between DFS and BFS when employing random walks on nodes. Then the Skip-Gram model is used to learn node embeddings based on the sampling node sequences. • LINE: LINE defines the first-order and second-order proximities to measure the similarity between nodes, and learns node embeddings by preserving the aforementioned proximities of nodes in the embedding space. • HARP: HARP recursively uses two collapsing schemes, edge collapsing and star collapsing, to compress an input network into a series of small networks. Starting from the smallest compressed network, it recursively conducts a NRL method to learns node embeddings in each network using node embeddings of the previous level network as initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Settings</head><p>Here we discuss the parameter setting for our method and baselines: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hierarchical Sampling for Networks</head><p>We firstly discuss the results of hierarchical sampling on testing networks. <ref type="figure" target="#fig_1">Fig.4</ref> presents the network compressing results by the hierarchical sampling on five datasets. As shown in <ref type="figure" target="#fig_1">Fig.4</ref>, the number of nodes and edges of compressed networks drastically decrease as the compressing process continues and finally becomes stable when the compressing level is large than 3. Therefore, in the following link prediction tasks, we set the largest number of compressing level as 3.</p><p>As shown in <ref type="figure" target="#fig_2">Fig.5</ref>, we present three networks, <ref type="figure" target="#fig_2">Fig.5(a)</ref>, <ref type="figure" target="#fig_2">Fig.5(e)</ref>, and <ref type="figure" target="#fig_2">Fig.5(i)</ref>, as the intuitive examples to illustrate how hierarchical sampling works.</p><p>The network in <ref type="figure" target="#fig_2">Fig.5(a)</ref> contains two dense communities which are merged into a new node in the following compressed networks respectively. Therefore in the compressed networks, the local topological information of original networks are preserved by considering densely connected nodes in the same community as a whole. Meanwhile, the compressed networks, <ref type="figure" target="#fig_2">Fig.5(b)</ref>, <ref type="figure" target="#fig_2">Fig.5(c)</ref>, and <ref type="figure" target="#fig_2">Fig.5(d)</ref>, reveal the hierarchical topological information of the input network. For a balanced tree network and a grid network as shown in <ref type="figure" target="#fig_2">Fig.5</ref>(e) and <ref type="figure" target="#fig_2">Fig.5</ref>(i), their hierarchical topologies can be revealed by their compressed networks as well. The results of <ref type="figure" target="#fig_2">Fig.5</ref> show that the network compressing strategy of HSRL works well on different types of networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Link Prediction</head><p>We conduct link prediction tasks to evaluate the performance of our method on five real-world datasets. Specifically, we prediction the link between nodes based on the cosine similarity of their embeddings. The evaluation metric used in this task is AUC. Higher AUC indicates the better performance of NRL methods.</p><p>We randomly split the edges of a network into 80% edges as the training set and the left 20% as the testing set. Each experiment is independently implemented for 20 times and the average performances on the testing set are reported in <ref type="table" target="#tab_1">Table  II</ref>.</p><p>We summarize the observations from  The compressed networks generated by HARP on a network could not reveal its global topologies. Hence, using node embeddings of compressed networks as initialization could mislead the NRL methods to a bad local minimum. Such an issue could occur especially when the input network is large-scale and the objective function of the NRL method is highly non-convex, e.g., LINE.</p><p>• The improvements of HSRL on DBLP, Yelp, and Duban are larger than that on Movielens and MIT. It demonstrates that HSRL works much better than baselines on large-scale networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameter Sensitivity Analysis</head><p>We conduct link prediction task on DBLP to study the parameter sensitivity of HSRL. Without loss of generality, we used DeepWalk to learn node embeddings for each compressed network. <ref type="figure">Fig.6</ref> shows using 80% edges as training set and the left as testing set, the link prediction performance (AUC) as a function of one of the chosen parameters when fixing the others.</p><p>When fixing the largest number of compressed level to 3, <ref type="figure">Fig.6(a)</ref> shows the AUC of link prediction drastically improves as the number of embedding dimension d increases and finally becomes stable when d is larger than 32. When d is small, it is inadequate to embody rich information of networks. However, when d is large enough to embody all original network information, increasing d will not improve the performance of link prediction. <ref type="figure">Fig.6(b)</ref> shows that the impact of the largest number of network compressing level K on the performance of link prediction by fixing the representation size d to 64. As we increase K, the AUC of link prediction drastically improves. It demonstrates that the hierarchical topologies help to capture the potential relationship between nodes (even they are far away from each other) in a network. When K is larger than 3, the performance of link prediction becomes stable. It is  reasonable since the DBLP network could not be compressed further after level 3 as shown in <ref type="figure" target="#fig_1">Fig.4(b)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Running Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Most conventional NRL methods aim to preserve the local topological information of a network but overlook their global topology. Recently, HARP was proposed to preserve both local and global topological information. However, it could easily get stuck at a bad local minimum due to its poor network compressing schemes. In this paper, we propose a new NRL framework, HSRL, to tackle these issues. Specifically, HSRL employs a community-awareness network compressing scheme to obtain a series of smaller networks based on an input network and conducts a NRL method to learn node embeddings for each compressed network. Finally, the node embeddings of the original network can be obtained by concatenating all node embeddings of compressed networks. Empirical studies on link prediction on various real-world networks demonstrate HSRL significantly outperforms the state-of-the-art algorithms.</p><p>Our future work includes combining HSRL with deep learning-based methods, such as DNGR <ref type="bibr" target="#b24">[25]</ref>, SDNE <ref type="bibr" target="#b25">[26]</ref>, and GCN <ref type="bibr" target="#b26">[27]</ref>. It is also very interesting to extend HSRL to learn node embeddings of more complex networks which may be more common in real-world applications, e.g., heterogeneous networks, attributed networks, and dynamic networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of hierarchical view of network topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 .</head><label>4</label><figDesc>(Network Representation Learning) [4] Given a network G = (V, E), network representation learning aims to learn a mapping function f :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 5 .</head><label>5</label><figDesc>(Hierarchical Network Representation Learning) Given a series of compressed networks G 0 , G 1 , ..., G K of original network G = (V, E) and a network representation learning mapping function f , hierarchical network representation learning learns the node embeddings for each compressed network by Z k ← f (G k ), 0 ≤ k ≤ K, and finally obtains the node embeddings Z of original network G by concatenating Z 0 , Z 1 , ..., Z K .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>An exampling of compressing a network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The framework of HSRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>DeepWalk, HARP(DW), and HSRL(DW). The number of random walks t, the length of each walk l, window size of Skip-Gram model w, representation size d, and learning rate η for DeepWalk, HARP(DW), and HSRL(DW) are set as t = 10, l = 40, w = 5, d = 64, η = 0.025. The largest number of compressing levels for HSRL(DW) K is set as 3.• node2vec, HARP(N2V), and HSRL(N2V). The parameter setting for node2vec, HARP(N2V), and HSRL(N2V) is t = 10, l = 40, w = 5, d = 64, η = 0.025. The largest number of compressing levels for HSRL(N2V) K is set as 3. • LINE, HARP(LINE), and HSRL(LINE). The number of negative sampling λ, learning rate η, and representation size d for LINE, HARP(LINE), and HSRL(LINE) are set as λ = 5, η = 0.025, d = 64. The largest number of compressing levels for HSRL(LINE) K is set as 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>The compressing ratio of nodes/edges of compressed networks to the original network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 Fig. 5 .</head><label>35</label><figDesc>Examples of hierarchical sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 Fig. 6 .Fig. 7 .</head><label>667</label><figDesc>shows the actual running time of all NRL methods on five testing networks. All experiments are conducted on a single machine with 32GB memory, 16 CPU cores at 3.2 GHZ. The results show that the actual running time of HSRL is at most three times higher than others. The running time of HSRL is linear to the corresponding baselines as the Parameter sensitivity on link prediction. Running time input networks growing. Moreover, the running time of HSRL can be reduced by parallelizing the training processes on all compressed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and the brief descriptions of each dataset are presented as below. Movielens is a user-movie interest network which contains three types of nodes: users, movies, and terms. DBLP is a bibliographic network in computer science collected from four research areas: database, data mining, machine learning, and information retrieval. Nodes in the network including authors, papers, venues, and terms. MIT is a Facebook friendship network at one hundred American colleges and universities at a single point in time. It contains a single type of nodes, users. Douban is a user-movie interest network collected from a user review website Douban in China. The network contains four types of nodes including users, movies, actors, and directors. Yelp is a user-business network collected from a website Yelp in America. It contains four types of nodes including users, businesses, locations, and business categories.</figDesc><table /><note>• Movielens:• DBLP:• MIT:• Douban:• Yelp:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF FIVE DATASETS.</figDesc><table><row><cell>Datasets</cell><cell cols="3"># Nodes # Edges Network Types</cell></row><row><cell>Movielens</cell><cell>1332</cell><cell>2592</cell><cell>User-movie</cell></row><row><cell>DBLP</cell><cell>37791</cell><cell>170794</cell><cell>Bibliography</cell></row><row><cell>MIT</cell><cell>6402</cell><cell>251230</cell><cell>Friendship</cell></row><row><cell>Douban</cell><cell>13786</cell><cell>214392</cell><cell>User-movie</cell></row><row><cell>Yelp</cell><cell>28759</cell><cell>247698</cell><cell>User-business</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table .</head><label>.</label><figDesc>II as following:</figDesc><table><row><cell>• HSRL significantly outperforms all baselines on all</cell></row><row><cell>datasets. For the small and sparse network movielens,</cell></row><row><cell>the improvements of HSRL(DW), HSRL(N2V), and</cell></row><row><cell>HSRL(LINE) are 3.6%, 2.5%, and 16.6% respectively.</cell></row><row><cell>For the dense network MIT, HSRL(DW), HSRL(N2V),</cell></row><row><cell>and HSRL(LINE) outperform the baselines by 2.9%,</cell></row><row><cell>8.5%, and 0.5%. For three large networks, DBLP, Yelp,</cell></row><row><cell>and Douban, the improvement of HSRL is striking:</cell></row><row><cell>the improvements of HSRL(DW), HSRL(N2V), and</cell></row><row><cell>HSRL(LINE) are 6.3%, 19.9%, 3.5% for DBLP, 6.5%,</cell></row><row><cell>16.8%, 5.9% for Yelp, and 18.4%, 30.5%, 17.5% for</cell></row><row><cell>Douban.</cell></row></table><note>• The results of HARP on movielens, DBLP, Yelp, and Douban are worse than the original NRL methods. More- over, the performance of HARP(LINE) is drastically worse than LINE. It only works better than DeepWalk, node2vec on MIT which is a small and dense network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II AUC</head><label>II</label><figDesc>OF LINK PREDICTION.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Movielens</cell><cell>DBLP</cell><cell>MIT</cell><cell>Yelp</cell><cell>Douban</cell></row><row><cell>DeepWalk</cell><cell>0.847</cell><cell>0.794</cell><cell>0.899</cell><cell>0.842</cell><cell>0.687</cell></row><row><cell>HARP(DW)</cell><cell>0.817</cell><cell>0.659</cell><cell>0.902</cell><cell>0.743</cell><cell>0.559</cell></row><row><cell>HSRL(DW)</cell><cell>0.879  †</cell><cell>0.847  †</cell><cell>0.926  †</cell><cell>0.901  †</cell><cell>0.842  †</cell></row><row><cell>Gain of HSRL(%)</cell><cell>3.6</cell><cell>6.3</cell><cell>2.9</cell><cell>6.5</cell><cell>18.4</cell></row><row><cell>node2vec</cell><cell>0.843</cell><cell>0.673</cell><cell>0.843</cell><cell>0.742</cell><cell>0.569</cell></row><row><cell>HARP(N2V)</cell><cell>0.828</cell><cell>0.647</cell><cell>0.879</cell><cell>0.708</cell><cell>0.552</cell></row><row><cell>HSRL(N2V)</cell><cell>0.865  †</cell><cell>0.840  †</cell><cell>0.921  †</cell><cell>0.892  †</cell><cell>0.819  †</cell></row><row><cell>Gain of HSRL(%)</cell><cell>2.5</cell><cell>19.9</cell><cell>8.5</cell><cell>16.8</cell><cell>30.5</cell></row><row><cell>LINE</cell><cell>0.613</cell><cell>0.641</cell><cell>0.814</cell><cell>0.752</cell><cell>0.624</cell></row><row><cell>HARP(LINE)</cell><cell>0.220</cell><cell>0.387</cell><cell>0.702</cell><cell>0.306</cell><cell>0.399</cell></row><row><cell>HSRL(LINE)</cell><cell>0.735  †</cell><cell>0.664  †</cell><cell>0.819</cell><cell>0.799  †</cell><cell>0.756  †</cell></row><row><cell>Gain of HSRL(%)</cell><cell>16.6</cell><cell>3.5</cell><cell>0.5</cell><cell>5.9</cell><cell>17.5</cell></row><row><cell cols="6">† denotes the performance of HSRL is significantly better than the</cell></row><row><cell cols="6">other peers according to the Wilcoxons rank sum test at a 0.05</cell></row><row><cell>significance level.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://movielens.org/ 2 https://www.yelp.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathsim: Meta pathbased top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network visualization and analysis of gene expression data using biolayout express 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theocharidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Dongen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature protocols</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1535</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="78" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Like like alike: joint friendship and interest propagation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The benefits of facebook friends: social capital and college students use of online social network sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steinfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Mediated Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1168" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis of weighted networks</title>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56131</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting the hierarchical organization of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sales-Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guimera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A N</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="15" to="224" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding community structure in very large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66111</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Traud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recommendation in heterogeneous information network via dual similarity regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
