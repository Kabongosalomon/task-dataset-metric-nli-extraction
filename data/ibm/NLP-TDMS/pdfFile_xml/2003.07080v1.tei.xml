<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PS-RCNN: DETECTING SECONDARY HUMAN INSTANCES IN A CROWD VIA PRIMARY OBJECT SUPPRESSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
							<email>yoshie@waseda.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PS-RCNN: DETECTING SECONDARY HUMAN INSTANCES IN A CROWD VIA PRIMARY OBJECT SUPPRESSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human Body Detection</term>
					<term>Crowded Scenes</term>
					<term>PS-RCNN</term>
					<term>Human-Shaped Mask</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting human bodies in highly crowded scenes is a challenging problem. Two main reasons result in such a problem: 1). weak visual cues of heavily occluded instances can hardly provide sufficient information for accurate detection; 2). heavily occluded instances are easier to be suppressed by Non-Maximum-Suppression (NMS). To address these two issues, we introduce a variant of two-stage detectors called PS-RCNN. PS-RCNN first detects slightly/none occluded objects by an R-CNN [1] module (referred as P-RCNN), and then suppress the detected instances by human-shaped masks so that the features of heavily occluded instances can stand out. After that, PS-RCNN utilizes another R-CNN module specialized in heavily occluded human detection (referred as S-RCNN) to detect the rest missed objects by P-RCNN. Final results are the ensemble of the outputs from these two R-CNNs. Moreover, we introduce a High Resolution RoI Align (HRRA) module to retain as much of fine-grained features of visible parts of the heavily occluded humans as possible. Our PS-RCNN significantly improves recall and AP by 4.49% and 2.92% respectively on CrowdHuman [2], compared to the baseline. Similar improvements on Widerperson [3] are also achieved by the PS-RCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Human body detection is one of the most important research fields in computer vision. Although rapid developments have been seen in recent years, detecting human in crowded scenarios with various gestures still remains challenging. In the crowded situation, occlusions between human instances are common, making the visual patterns of occluded humans less discriminative and hard to be detected. In this work, we call those slightly/none occluded human instances as "Primary Objects", and those heavily occluded human instances as "Secondary Objects", and abbreviate them to P-Objects and S-objects, respectively.</p><p>There are two reasons leading to the poor detection performance on S-Objects. First, visual cues for S-Objects are weak compared to P-Objects which make them hard to be distinguished from backgrounds or other human bodies. Besides, the remarkable difference of visual features between P-Objects and S-Objects poses great challenge for a single detector in handling both of them. A preliminary experiment verifies such an observation. We evaluate on both P-Objects and S-Objects with a Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> trained on CrowdHuman <ref type="bibr" target="#b1">[2]</ref>. Results show that the missing rate for P-Objects and S-Objects are 4% and 18%, respectively, which reveals the poor detection performance on S-Objects. Second, S-Objects usually share large overlaps with other human instances, and thus they are easier to be treated as duplicate detections and then get suppressed during post-processing (i.e. NMS). To verify this point, we print the statistics of pair-wise IoU in CrowdHuman and WiderPerson <ref type="bibr" target="#b2">[3]</ref> in <ref type="table">Table 1</ref>. As shown in <ref type="table">Table 1</ref>, there are 2.4 pairs of human instances with their overlaps (IoU) over 0.5 in CrowdHuman dataset. It means if we use NMS with the IoU threshold of 0.5 (NMS@0.5) as the default post-processing method, at least 2.4 instances will be inevitably missed per-image. Such missing detections can never be made up by strengthening the detector, while using a high IoU threshold like 0.7 will introduce a great number of false positives in the final results. Similar phenomenon can be observed on the WiderPerson.</p><p>To deal with the first issue, OR-CNN <ref type="bibr" target="#b4">[5]</ref> proposes a new aggregation loss and occlusion-aware RoI Pooling to encourage the model to learn the existence of human bodies based on the occurrence of different human body parts. RepLoss <ref type="bibr" target="#b5">[6]</ref> imposes additional penalty terms on the BBoxes that appear in the middle of two persons to achieve more accurate localization accuracy in crowded scenes. However, these methods do not pay any attention to the post-processing methods, which makes their models suffer from a hidden upper boundary of recall rate.</p><p>To alleviate the second issue, instead of discarding the suppressed BBoxes, Soft-NMS <ref type="bibr" target="#b6">[7]</ref> lowers the classification scores for overlapped BBoxes according to their overlaps with the most confident one. Adaptive NMS <ref type="bibr" target="#b7">[8]</ref> argues that a uniform threshold of NMS is not suitable in crowded scenes. They predict a density map to allow NMS to run with different thresholds at different locations according to the spatial density of ground-truth objects. While these methods can improve the recall of detectors, they also suffer from a high risk of introducing lots of false positives, which makes them become sub-optimal solutions.</p><p>In this paper, we propose PS-RCNN, a variant of twostage detectors to address the above two issues. PS-RCNN consists of two R-CNN modules on top of a shared backbone. The first R-CNN module which is trained in the exact same way with the standard Faster R-CNN is called Primary R-CNN Module (i.e. P-RCNN). It aims to detect the primary human instances. The detected primary instances are then suppressed to facilitate the overlapped secondary human detection. To this end, we introduce a primary instance binary mask which effectively erases the primary instance, such that the weak feature of the occluded secondary instance can stand out. The second R-CNN module (i.e. S-RCNN) which is specialized in occluded human detection, is then introduced to detect the secondary instances, based on the modified features. The whole structure of PS-RCNN can been seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. PS-RCNN can be trained in an end-to-end manner. By avoiding NMS@0.5 being the last post-processing method, our PS-RCNN can yield denser predictions. Moreover, since each R-CNN module is only responsible for detecting one kind of human instances (slightly/none or heavily occluded instances), the individual task of both primary object detection and secondary object detection can be improved.</p><p>Our baseline is the naive Faster R-CNN with FPN <ref type="bibr" target="#b8">[9]</ref>, which augments a standard CNN with a top-down pathway, each level of which can be used for detecting objects at different scales. To further improve the visibility of S-Objects, we pair/image Crowdhuman Widerperson IoU&gt;0. <ref type="bibr" target="#b2">3</ref> 9.02 9.21 IoU&gt;0. <ref type="bibr" target="#b3">4</ref> 4.89 4.78 IoU&gt;0. <ref type="bibr" target="#b4">5</ref> 2.40 2.15 IoU&gt;0. <ref type="bibr" target="#b5">6</ref> 1.01 0.81 <ref type="table">Table 1</ref>. Pair-wise overlaps between human instances in CrowdHuman and WiderPerson datasets.</p><p>introduce High Resolution RoI Align (HRRA) module which only extracts features from the layer with the highest resolution. We also incorporate COCOPerson <ref type="bibr" target="#b9">[10]</ref> into our training pipeline to achieve more accurate instance masks. Combining the used of HRRA and training with COCOPerson leads to an AP improvement of 2.41% on CrowdHuman.</p><p>To summarize, our contributions are as follows: (1) a novel PS-RCNN to fight against poor performance of S-Objects in crowded human detection; (2) a High Resolution RoI Align (HRRA) module to improve the visibility of S-Objects to detectors; (3) state-of-the-art performance of recall and great improvements of AP on both CrowdHuman and WiderPerson datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>General object detection. As one of the predominant detectors, Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> first generates a set of region proposals and then refines them by a post classification and regression network. FPN <ref type="bibr" target="#b10">[11]</ref> extends Faster R-CNN by introducing a top-down pathway of features to tackle large scale variance of target objects. Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> proposes RoI Align and uses another instance segmentation branch to improve the localization ability of detectors. Cascade R-CNN <ref type="bibr" target="#b12">[13]</ref> proposes a multi-stage R-CNN module trained with a set of increasing IoU thresholds, which is able to refine predict bounding boxes stage-wise. Occlusion handling. Crowd occlusion (intra-class occlusion and inter-class occlusion) is the main challenge in pedestrian detection. <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref> propose two crowd human datasets (i.e. CrowdHuman and WiderPerson) to better evaluate detectors in crowded scenarios. <ref type="bibr" target="#b13">[14]</ref> adopts attention mechanism across channels to represent various occlusion patterns. In <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>, part-based model is utilized to handle occluded pedestrians. <ref type="bibr" target="#b17">[17]</ref> starts from the condition with insufficient illumination, then generates pedestrian proposals by a multi-spectral proposal network and proposes a subsequent multi spectral classification network. Occlusion-aware R-CNN <ref type="bibr" target="#b4">[5]</ref> designs an aggregation loss and an Occlusion-aware RoI Pooling to help detectors notice the existence of human instances based on the occurrence of human body parts. RepLoss <ref type="bibr" target="#b5">[6]</ref> proposes additional penalties to the BBoxes which appear in the middle of two persons to force predicted BBoxes locate firmly and compactly to the ground-truth objects. Adaptive NMS <ref type="bibr" target="#b7">[8]</ref> predicts a density map of an image to perform a modified version of NMS with dynamic suppression thresholds. <ref type="bibr" target="#b18">[18]</ref> incorporates mask guided attention to help model focus on visible regions of proposals to obtain better classification prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PS-RCNN</head><p>We propose PS-RCNN, a variant of two-stage detectors to handle the human body detection task in crowded scenarios. We further propose a High Resolution RoI Align (HRRA) module to help Secondary R-CNN Mudole (S-RCNN) read S-Objects at a layer with higher resolution. Finally, we introduce COCOPerson into our training pipeline to improve the quality of human-shaped masks, which leads to better performance of S-RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Structure of PS-RCNN</head><p>The structure of PS-RCNN can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>, PS-RCNN contains two parallel R-CNN modules (i.e. P-RCNN and S-RCNN) on a shared backbone. Inspired by the merit of divide-and-conquer, P-RCNN is designed to detect those slightly/none occluded human instances (P-Objects) while S-RCNN needs to pick up those missed instances which are heavily occluded (S-Objects).</p><p>In the naive Faster R-CNN, RPN <ref type="bibr" target="#b3">[4]</ref> needs to generate proposals with respect to all the ground-truth targets. In PS-RCNN, RPN plays the same role with Faster R-CNN, aiming to provide proposals w.r.t. all the human instances. The two R-CNN modules in PS-RCNN are designed to detect two sets of human instances with different occlusions. P-RCNN carries out the first round of detection, aiming to detect non/slightly occluded instances. S-RCNN is then leveraged to detect the heavily occluded ones missed by P-RCNN. Therefore, we train P-RCNN with all the ground-truth instances, while train S-RCNN using only the missed ones by P-RCNN to make it more complementary with P-RCNN. Formally speaking, we define G = {bbox 1 , bbox 2 , ..., bbox n } as all the ground-truth bounding boxes for image I. In the training phase, G is used as the ground-truth objects for P-RCNN. The instances detected by P-RCNN are defined as G d , and the missed instances are defined as G m . After G d is detected, a human-shaped binary mask is covered at the location of ground-truth of each detected BBox in G d on feature maps, which can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. After binary masking, the visual cues for G d should be invisible for S-RCNN. Thus only the missed instances G m are set as ground-truth objects to train S-RCNN. In this way, the two R-CNNs handle two separate sets of human instances, following in a divide-and-conquer manner. What's more, the division of non-occluded and occluded instances are automatically defined by the model itself, instead of using a hand-designed hard occlusion thresholding strategy. The final outputs of PS-RCNN are the union of the results from two R-CNN modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">High resolution RoI Align</head><p>Feature Pyramid Network (FPN <ref type="bibr" target="#b8">[9]</ref>) is an effective way to handle large scale variance of objects in the real world, thus it has become a standard module in many recent state-of-theart detectors. We also incorporate FPN into our PS-RCNN. FPN assigns objects with different scales to different levels of feature maps. Specifically, the larger object's scale is, the lower resolution level it will be assigned to for richer semantic information. Small objects will be assigned to feature maps at layers in higher resolution level because they may become indistinguishable if not so. An RoI Extractor (e.g. RoI Align <ref type="bibr" target="#b11">[12]</ref>) is then used to extract features of these RoIs from different feature levels according to their scales. In our PS-RCNN, the RoI extractor for P-RCNN also follows this standard routine. However, when extracting RoI features for S-RCNN, we only extract features from the feature maps in the highest resolution level. Such an operation is called High Resolution RoI Align (HRRA). The introduction of HRRA module is based on an observation that though the full body of S-Objects are usually large, their informative visible regions could be extreme small. Directly assigning S-Objects to the layers in small resolutions according to the full-body scales would make the informative visible regions become even smaller and hard to identify. Therefore, we propose to perform RoI extraction of S-Objects from feature maps in higher resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Higher quality instance masks from COCOPerson</head><p>The quality of human-shaped binary masks is crucial. An imperfect mask which leaves parts of a primary instance uncovered will lead to plenty of duplicate detection results in the S-RCNN detection stage, bringing about lots of false positives. A hand-crafted human-shaped mask is qualified in most cases. However, it can not handle complicated cases when human body performs some uncommon gestures like dancing or bending their arms on others' shoulders, etc. Incorporating instance segmentation to acquire more accurate instance masks can alleviate this issue to some extent. Thus beyond the handcrafted binary masks, we propose another enhanced version of PS-RCNN, which incorporates an instance segmentation branch after P-RCNN module to get instance masks for P-Objects. The instance segmentation branch is trained on CO-COPerson and can be easily plugged into current PS-RCNN. S-RCNN does not need to be finetuned after the introduction of instance branch according to our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>To validate the effectiveness of our proposed PS-RCNN, we conduct experiments on two crowded human detection datasets: CrowdHuman <ref type="bibr" target="#b1">[2]</ref> and WiderPerson <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation metrics</head><p>The log average missing rate over 9 points ranging from 10 −2 to 10 0 FPPI (i.e. mMR <ref type="bibr" target="#b19">[19]</ref>) is broadly used in pedestrian detection field. The score threshold for mMR is dynamic and usually determined by the number of false positive predictions. mMR is qualified in common pedestrian detection scenarios because it achieves a good balance between the number of true positive and false positive predictions. However, when detecting human body in crowded scenarios with various gestures, detectors may generate more false positives because the heavy overlaps between human instances. Thus mMR usually yields extremely high score thresholds when dealing with crowded scenes. Specifically, in our experiments, we find that the score threshold for mMR can reach 0.95 which means the predicted BBoxes whose class scores are less than 0.95 will not be considered in the stage of evaluation. Such a high score threshold is not able to fairly reflect the capability of human body detectors. Thus in this work, we use COCO <ref type="bibr" target="#b9">[10]</ref> style AP at IoU threshold of 0.5 and recall as our evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on CrowdHuman</head><p>Dataset. As a benchmark for detecting human body in the crowded situation, CrowdHuman provides three kinds of annotations: full body, visible body and head to help researchers fully explore the potential of human body detection. It contains 15, 000, 4, 370, and 5, 000 images for training, validation and testing, respectively. On average, there are around 23 persons per-image, making CrowdHuman a challenging benchmark. We conduct all of our experiments on full body, because to acquire the BBox for full body, the detector not only needs to localize where the visible regions are, but also needs to estimate the boundaries of occluded areas, which makes it more challenging. Implementation details. We adopt FPN with ResNet-50 <ref type="bibr" target="#b20">[20]</ref> as our baseline. RoI-Align is also used for better RoI feature alignment. For all of our experiments, we train detectors on 8 GPUs (2 images per-GPU) with an initial learning rate of 0.02, and decrease it by 0.1 at the 8th and 11th epoch. The training process finishes at the end of the 12th epoch. Standard SGD with momentum 0.9 is used as the optimizer. We use the same set of anchor ratios with <ref type="bibr" target="#b1">[2]</ref> which are 1.0,1.5,2.0,2.5,3.0. The input image is re-scaled such that its shortest edge is 800 pixels, and the longest side is not beyond 1400 pixels, considering the large scale variance of images in CrowdHuman. For the enhanced version of PS-RCNN, we train the additional instance segmentation branch with extra 9 epochs on COCOPerson, during which only the parameters of instance prediction branch are updated. Since the R-CNN module we use in our model only contains a few fully-connected layers, so our PS-RCNN only brings limited extra computation cost. Main results. Results on CrowdHuman are presented in <ref type="table" target="#tab_0">Table 2</ref>. Our re-implemented baseline is slightly better than <ref type="bibr" target="#b1">[2]</ref>, in which we replace RoI Pooling with RoI Align. "IM" is the abbreviation of "instance masks". "Soft-NMS" is implemented by simply replacing NMS in "Baseline" with "Soft-NMS". As can be seen in <ref type="table" target="#tab_0">Table 2</ref>, without HRRA for S-RCNN, the improvement on recall is only 2.05% because the visual cues of S-Objects are very weak. After adopting HRRA, our PS-RCNN with HRRA can bring 1.03% and 3.15% improvements on AP and recall, respectively. Moreover, after replacing the hand-crafted human-shaped masks with predicted instance masks, total improvements on AP and recall reach 2.92% and 4.49%. It is worth mentioning that if we feed ground-truth annotations of full body into NMS@0.5, the recall of the final outputs is 90.9% 1 which means using NMS@0.5 as the final post-processing method can hardly surpass the recall of 90.9%. Soft-NMS and Adaptive NMS break this hidden upper limit of recall to some extent. Our method further pushes the state-of-the-art of recall to 95.11%. Discussion. To validate the effectiveness of our PS-RCNN on detecting S-Objects, we count the number of detected P-Objects and S-Objects before and after adopting PS-RCNN. Because there is no clear definition on P-Objects and S-Objects, here we define a term V to represent the degree of visibility, where V = S v /S f . S v and S f stand for the area of visible body region and full body region, respectively. As can be seen in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, PS-RCNN improves the recall of instances whose V &lt;= 0.5 by 10% (0.82 v.s. 0.92) compared to its baseline, while the recall for instances whose V &gt; 0.5 also increases a little. Moreover, to prove that our PS-RCNN does not work like Soft-NMS which keeps plenty of detected BBoxes with extremely low confidence scores, we plot the distribution of confidence scores from both P-RCNN and S-RCNN in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. From <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, we can draw a conclusion that S-RCNN actually yields considerable amount of high score predictions which can be beneficial to real applications. Ablation study. The effectiveness of high quality instance masks and HRRA has been verified in <ref type="table" target="#tab_0">Table 2</ref>. Here, we explore other two factors that might influence the performance of PS-RCNN. The first is where should instance masks be applied -original image or feature maps? <ref type="table">Table 3</ref> shows that  <ref type="table" target="#tab_1">Table 4</ref> show that IoU threshold of 0.5 yields better recall while 0.6 achieves the best AP. AP is usually more important than Recall, so we recommend 0.6 as IoU threshold of positive proposals when using PS-RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on WiderPerson</head><p>WiderPerson <ref type="bibr" target="#b2">[3]</ref> is another dense human detection dataset which is collected from various kinds of scenarios. It contains five types of annotations -pedestrians, riders, partiallyvisible persons, crowd and ignored regions. In our experiments, we merge the former four types into one category in both training and testing phase. WiderPerson contains 8,000, 1,000 and 4,382 images for training, validation and testing sets. We use the exactly same division as the original widerperson dataset. We train our PS-RCNN on the training set and validate on the validation set. The other settings follow our experiments on CrowdHuman. Results are presented in <ref type="table" target="#tab_2">Table 5</ref>. As seen in <ref type="table" target="#tab_2">Table 5</ref>, our PS-RCNN with HRRA and IM improves AP and recall by 1.63% and 2.01%, respectively, verifying the effectiveness of our proposed PS-RCNN across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we propose a new two-stage human body detector called PS-RCNN, which utilizes two parallel R-CNN modules to detect slightly/none occluded and heavily occluded human instances. We also introduce high quality instance masks into our model via few extra epochs of training on COCOPerson and a High Resolution RoI Align (HRRA) module to improve the capability of Secondary R-CNN module. Experiments on CrowdHuman show that our PS-RCNN can totally improve the AP and recall by 2.92% and 4.49%, respectively. Great improvements can also be observed on WiderPerson dataset, showing the effectiveness of PS-RCNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a). The BBox in red will be suppressed by the BBox in blue if NMS@0.5 is applied. (b). Illustration of detected instances from P-RCNN. (c). Illustration of detected instances from S-RCNN. (d). Final outputs of PS-RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of PS-RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a). Recall of P-Objects and S-Objects for Faster R-CNN and PS-RCNN. (b). Distribution of predicted scores from P-RCNN and S-RCNN. Only predicted BBoxes whose score&gt;0.5 are counted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on CrowdHuman val set. '*' denotes our re-implemented results. Two region feature extraction methods in the rows of PS-RCNN stand for the RoI feature extraction method for P-RCNN and S-RCNN, respectively.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Region Feature Extraction</cell><cell>AP</cell><cell cols="2">Recall ∆ AP ∆ Recall</cell></row><row><cell cols="2">Baseline Shao et al. [2]</cell><cell>RoI Pooling [4]</cell><cell cols="3">84.95 90.24 -0.07</cell><cell>-0.38</cell></row><row><cell>Baseline*</cell><cell></cell><cell>RoI Align</cell><cell cols="2">85.02 90.62</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Adaptive NMS [8]</cell><cell>-</cell><cell cols="3">84.71 91.27 -0.31</cell><cell>1.03</cell></row><row><cell cols="2">Repulsion Loss* [6]</cell><cell>RoI Align</cell><cell cols="2">85.71 90.74</cell><cell>0.69</cell><cell>0.12</cell></row><row><cell>Soft-NMS* [7]</cell><cell></cell><cell>RoI Align</cell><cell cols="2">85.66 92.88</cell><cell>0.64</cell><cell>2.26</cell></row><row><cell>PS-RCNN</cell><cell></cell><cell>RoI Align; RoI Align</cell><cell cols="2">85.53 92.67</cell><cell>0.51</cell><cell>2.05</cell></row><row><cell>PS-RCNN</cell><cell></cell><cell>RoI Align; HRRA</cell><cell cols="2">86.05 93.77</cell><cell>1.03</cell><cell>3.15</cell></row><row><cell cols="2">PS-RCNN w/ IM</cell><cell>RoI Align; HRRA</cell><cell cols="2">87.94 95.11</cell><cell>2.92</cell><cell>4.49</cell></row><row><cell cols="3">Table 3. Ablation study about where to apply human-shaped</cell><cell></cell><cell></cell></row><row><cell>masks on.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>humanoid masks</cell><cell>AP</cell><cell>Recall</cell><cell></cell><cell></cell></row><row><cell>on input image</cell><cell cols="2">87.86 95.40</cell><cell></cell><cell></cell></row><row><cell cols="3">on feature maps 87.94 95.11</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Varying the IoU threshold for positive proposals</figDesc><table><row><cell>when training S-RCNN.</cell><cell></cell><cell></cell></row><row><cell>threshold</cell><cell>AP</cell><cell>Recall</cell></row><row><cell>0.5</cell><cell cols="2">87.53 95.46</cell></row><row><cell>0.6</cell><cell cols="2">87.94 95.11</cell></row><row><cell>0.7</cell><cell cols="2">86.91 93.50</cell></row><row><cell cols="3">applying masks on original image or feature maps does not</cell></row><row><cell cols="3">make much difference. But considering time efficiency, di-</cell></row><row><cell cols="3">rectly applying masks on feature maps is preferred in real ap-</cell></row><row><cell cols="3">plications. The second is which IoU threshold should we use</cell></row><row><cell cols="3">to define positive proposals for S-RCNN? S-Objects are usu-</cell></row><row><cell cols="3">ally heavily occluded. Estimating the BBoxes for them are</cell></row><row><cell cols="3">tougher than P-Objects, thus better proposals may help. We</cell></row><row><cell cols="3">try 0.5, 0.6 and 0.7 as the IoU thresholds of positive propos-</cell></row><row><cell>als for S-RCNN. Results in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Experimental results on WiderPerson. Method AP Recall Faster R-CNN (Ours) 88.89 93.60 PS-RCNN 89.34 94.36 PS-RCNN w/ HRRA 89.96 94.71 PS-RCNN w/ HRRA and IM 90.52 95.61</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Slightly different values of recall are possible because we set scores for all ground-truth BBoxes as 1.0, so that the order of BBoxes is random.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Widerperson: A diverse dataset for dense pedestrian detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3258" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-label learning of part detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3506" to="3515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multispectral pedestrian detection via simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04818</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
