<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
						</author>
						<title level="a" type="main">StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Generative models</term>
					<term>Generative Adversarial Networks (GANs)</term>
					<term>multi-stage GANs</term>
					<term>multi-distribution approximation</term>
					<term>photo-realistic image generation</term>
					<term>text-to-image synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative Adversarial Networks (GANs) were proposed by Goodfellow et al. <ref type="bibr" target="#b10">[12]</ref>. In the original setting, GANs are composed of a generator and a discriminator that are trained with competing goals. The generator is trained to generate samples towards the true data distribution to fool the discriminator, while the discriminator is optimized to distinguish between real samples from the true data distribution and fake samples produced by the generator. Recently, GANs have shown great potential in simulating complex data distributions, such as those of texts <ref type="bibr" target="#b4">[6]</ref>, images <ref type="bibr" target="#b30">[32]</ref> and videos <ref type="bibr" target="#b46">[48]</ref>.</p><p>Despite the success, GANs are known to be difficult to train. The training process is usually unstable and sensitive to the choices of hyper-parameters. Several papers argued that the instability is partially due to the disjoint supports of the data distribution and the implied model distribution <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b0">[2]</ref>.</p><p>• H. <ref type="bibr">Zhang</ref>  This problem is more severe when training GANs to generate high-resolution (e.g., 256×256) images because the chance is very low for the image distribution and the model distribution to share supports in a high-dimensional space. Moreover, a common failure phenomenon for GANs training is mode collapse, where many of the generated samples contain the same color or texture pattern. In order to stabilize the GANs' training process and improve sample diversity, several methods tried to address the challenges by proposing new network architectures <ref type="bibr" target="#b30">[32]</ref>, introducing heuristic tricks <ref type="bibr" target="#b38">[40]</ref> or modifying the learning objectives <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr">[1]</ref>. But most of the previous methods are designed to approximate the image distribution at a single scale. Due to the difficulty in directly approximating the highresolution image data distribution, most previous methods are limited to generating low-resolution images. To circumvent this difficulty, we observe that, real world data, especially natural images, can be modeled at different scales <ref type="bibr" target="#b36">[38]</ref>. One can view multi-resolution digitized images as samples from the same continuous image signal with different sampling rates. Henceforth, the distributions of images at multiple discrete scales are related. Apart from multiple distributions of different scales, images coupled with or without auxiliary conditioning variables (e.g., class labels or text descriptions) can be viewed as conditional distributions or unconditional distributions, which are also related distributions. Motivated by these observations, we argue that GANs can be stably trained to generate high resolution images by breaking the difficult generative task into sub-problems with progressive goals. Thus, we propose Stacked Generative Adversarial Networks (StackGANs) to model a series of low-to-high-dimensional data distributions.</p><p>First, we propose a two-stage generative adversarial network, StackGAN-v1, to generate images from text descriptions through a sketch-refinement process <ref type="bibr" target="#b53">[55]</ref>. Low-resolution images are first generated by our Stage-I GAN. On top of the Stage-I GAN, we stack Stage-II GAN to generate highresolution (e.g., 256×256) images. By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details. Further, we propose a novel Conditioning Augmentation (CA) technique to encourage smoothness in the latent conditioning manifold <ref type="bibr" target="#b53">[55]</ref>. It allows small random perturbations in the conditioning manifold and increases the diversity of synthesized images.</p><p>Second, we propose an advanced multi-stage generative adversarial network architecture, StackGAN-v2, for both conditional and unconditional generative tasks. StackGAN-v2 has multiple generators that share most of their parameters in a tree-like structure. As shown in <ref type="figure">Fig. 2</ref>, the input to the network can be viewed as the root of the tree, and multiscale images are generated from different branches of the tree. The generator at the deepest branch has the final goal of generating photo-realistic high-resolution images. Generators at intermediate branches have progressive goals of generating small to large images to help accomplish the final goal. The whole network is jointly trained to approximate different but highly related image distributions at different branches. The positive feedback from modeling one distribution can improve the learning of others. For conditional image generation tasks, our proposed StackGAN-v2 simultaneously approximates the unconditional image-only distribution and the image distribution conditioned on text descriptions. Those two types of distributions are complementary to each other. Moreover, we propose a color-consistency regularization term to guide our generators to generate more coherent samples across different scales. The regularization provides additional constraints to facilitate multi-distribution approximation, which is especially useful in the unconditional setting where there is no instancewise supervision between the image and the input noise vector.</p><p>In summary, the proposed Stacked Generative Adversarial Networks have three major contributions. (i) Our StackGAN-v1 for the first time generates images of 256×256 resolution with photo-realistic details from text descriptions. (ii) A new Conditioning Augmentation technique is proposed to stabilize the conditional GANs' training and also improve the diversity of the generated samples. (iii) Our StackGAN-v2 further improves the quality of generated images and stabilizes the GANs' training by jointly approximating multiple distributions. In the remainder of this paper, we first discuss related work and preliminaries in section 2 and section 3, respectively. We then introduce our StackGAN-v1 <ref type="bibr" target="#b53">[55]</ref> in section 4 and StackGAN-v2 in section 5. In section 6, extensive experiments are conducted to evaluate the proposed methods. Finally, we make conclusions in section 7. The source code for StackGAN-v1 is available at https://github.com/hanzhanggit/StackGAN, and the source code for StackGAN-v2 is available at https://github.com/hanzhanggit/StackGAN-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Generative image modeling is a fundamental problem in computer vision. There has been remarkable progress in this direction with the emergence of deep learning techniques. Variational Autoencoders (VAEs) <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b35">[37]</ref> formulate the problem with probabilistic graphical models with the goal of maximizing the lower bound of data likelihood. Autoregressive models (e.g., PixelRNN) <ref type="bibr" target="#b42">[44]</ref> that utilize neural networks to model the conditional distribution of the pixel space have also generated appealing synthetic images. Recently, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[12]</ref> have shown promising performance for generating sharper images. But the training instability makes it hard for GANs to generate high-resolution (e.g., 256×256) images. A lot of works have been proposed to stabilize the training and improve the image qualities <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b27">[29]</ref>.</p><p>Built upon these generative models, conditional image generation has also been studied. Most methods utilize simple conditioning variables such as attributes or class labels <ref type="bibr" target="#b50">[52]</ref>, <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b29">[31]</ref>. There are also works conditioned on images to generate images, including photo editing <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b55">[57]</ref>, domain transfer <ref type="bibr" target="#b41">[43]</ref>, <ref type="bibr" target="#b16">[18]</ref> and super-resolution <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b21">[23]</ref>. However, super-resolution methods <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b21">[23]</ref> can only add limited details to low-resolution images and can not correct large defects. In contrast, the latter stages in our proposed StackGANs can not only add details to low-resolution images generated by earlier stages but also correct defects in them. Recently, several methods have been developed to generate images from unstructured text. Mansimov et al. <ref type="bibr" target="#b23">[25]</ref> built an AlignDRAW model by learning to estimate alignment between text and the generating canvas. Reed et al. <ref type="bibr" target="#b34">[36]</ref> used conditional PixelCNN to generate images using text descriptions and object location constraints. Nguyen et al. <ref type="bibr" target="#b27">[29]</ref> used an approximate Langevin sampling approach to generate images conditioned on text. However, their sampling approach requires an inefficient iterative optimization process. With conditional GANs, Reed et al. <ref type="bibr" target="#b33">[35]</ref> successfully generated plausible 64×64 images for birds and flowers based on text descriptions. Their follow-up work <ref type="bibr" target="#b31">[33]</ref> was able to generate 128×128 images by utilizing additional annotations on object part locations.</p><p>Given the difficulties in modeling details of natural images, many works have been proposed to use multiple GANs to improve sample quality. Wang et al. <ref type="bibr" target="#b48">[50]</ref> utilized a structure GAN and a style GAN to synthesize images of indoor scenes. Yang et al. <ref type="bibr" target="#b51">[53]</ref> factorized image generation into foreground and background generation with layered recursive GANs. Huang et al. <ref type="bibr" target="#b14">[16]</ref> added several GANs to reconstruct the multilevel representations of a pre-trained discriminative model. But they were unable to generate high resolution images with photo-realistic details. Durugkar et al. <ref type="bibr" target="#b8">[10]</ref> used multiple discriminators along with one generator to increase the chance of the generator receiving effective feedback. However, all discriminators in their framework are trained to approximate the image distribution at a single scale. Some methods <ref type="bibr" target="#b6">[8]</ref>, <ref type="bibr" target="#b17">[19]</ref> follow the same intuition as our work. We all agree that it is beneficial to break the high-resolution image generation task into several easier subtasks to be accomplished in multiple stages. Denton et al. <ref type="bibr" target="#b6">[8]</ref> built a series of GANs within a Laplacian pyramid framework (LAPGANs). At each level of the pyramid, a residual image conditioned on the image of the previous stage is generated and then added back to the input image to produce the input for the next stage. Instead of producing a residual image, our StackGANs directly generate high resolution images that are conditioned on their low-resolution inputs. Concurrent to our work, Kerras et al. <ref type="bibr" target="#b17">[19]</ref> incrementally add more layers in the generator and discriminator for high resolution image generation. The main difference in terms of experimental setting is that they used a more restrained upsampling rule: starting from 4×4 pixels, their image resolution is increased by a factor of 2 between consecutive image generation stages. Furthermore, although StackGANs, LAPGANs <ref type="bibr" target="#b6">[8]</ref> and Progressive GANs <ref type="bibr" target="#b17">[19]</ref> all put emphasis on adding finer details in higher resolution images, our StackGANs can also correct incoherent artifacts or defects in low resolution results by utilizing an encoderdecoder network before the upsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[12]</ref> are composed of two models that are alternatively trained to compete with each other. The generator G is optimized to reproduce the true data distribution p data by generating images that are difficult for the discriminator D to differentiate from real images. Meanwhile, D is optimized to distinguish real images and synthetic images generated by G. Overall, the training procedure is a minmax two-player game with the following objective function,</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼p data [log D(x)] + E z∼pz [log(1 − D(G(z)))],<label>(1)</label></formula><p>where x is a real image from the true data distribution p data , and z is a noise vector sampled from the prior distribution p z (e.g., uniform or Gaussian distribution). In practice, the generator G is modified to maximize log(D(G(z))) instead of minimizing log(1 − D(G(z))) to mitigate the problem of gradient vanishing <ref type="bibr" target="#b10">[12]</ref>. We use this modified non-saturating objective in all our experiments.</p><p>Conditional GANs <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b26">[28]</ref> are extension of GANs where both the generator and discriminator receive additional conditioning variables c, yielding G(z, c) and D(x, c). This formulation allows G to generate images conditioned on variables c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STACKGAN-V1: TWO-STAGE GENERATIVE ADVERSARIAL NETWORK</head><p>To generate high-resolution images with photo-realistic details, we propose a simple yet effective two-stage generative adversarial network, StackGAN-v1. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, it decomposes the text-to-image generative process into two stages. Stage-I GAN sketches the primitive shape and basic colors of the object conditioned on the given text description, and draws the background layout from a random noise vector, yielding a low-resolution image. Stage-II GAN corrects defects in the low-resolution image from Stage-I and completes details of the object by reading the text description again, producing a high-resolution photo-realistic image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conditioning Augmentation</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the text description t is first encoded by an encoder, yielding a text embedding ϕ t . In previous works <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b31">[33]</ref>, the text embedding is nonlinearly transformed to generate conditioning latent variables as the input of the generator. However, latent space for the text embedding is usually high dimensional (&gt; 100 dimensions). With limited amount of data, it usually causes discontinuity in the latent data manifold, which is not desirable for learning the generator. To mitigate this problem, we introduce a Conditioning Augmentation technique to produce additional conditioning variablesĉ. In contrast to the fixed conditioning text variable c in <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b31">[33]</ref>, we randomly sample the latent variablesĉ from an independent Gaussian distribution N (µ(ϕ t ), Σ(ϕ t )), where the mean µ(ϕ t ) and diagonal covariance matrix Σ(ϕ t ) are functions of the text embedding ϕ t . The proposed Conditioning Augmentation yields more training pairs given a small number of imagetext pairs, and thus encourages robustness to small perturbations along the conditioning manifold. To further enforce the smoothness over the conditioning manifold and avoid overfitting <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b20">[22]</ref>, we add the following regularization term to the objective of the generator during training,</p><formula xml:id="formula_1">D KL (N (µ(ϕ t ), Σ(ϕ t )) || N (0, I)) ,<label>(2)</label></formula><p>which is the Kullback-Leibler divergence (KL divergence) between the standard Gaussian distribution and the conditioning Gaussian distribution. The randomness introduced in the Conditioning Augmentation is beneficial for modeling text to image translation as the same sentence usually corresponds to objects with various poses and appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stage-I GAN</head><p>Instead of directly generating a high-resolution image conditioned on the text description, we simplify the task to first generate a low-resolution image with our Stage-I GAN, which focuses on drawing only rough shape and correct colors for the object. Let ϕ t be the text embedding of the given description. The Gaussian conditioning variablesĉ 0 for text embedding are sampled from N (µ 0 (ϕ t ), Σ 0 (ϕ t )) to capture the meaning of ϕ t with variations. Conditioned onĉ 0 and random variable z, Stage-I GAN trains the discriminator D 0 and the generator G 0 by alternatively maximizing L D0 in Eq. (3) and minimizing L G0 in Eq. (4),</p><formula xml:id="formula_2">L D0 = E (I0,t)∼p data [log D 0 (I 0 , ϕ t )] + E z∼pz,t∼p data [log(1 − D 0 (G 0 (z,ĉ 0 ), ϕ t ))],<label>(3)</label></formula><formula xml:id="formula_3">L G0 = E z∼pz,t∼p data [− log D 0 (G 0 (z,ĉ 0 ), ϕ t )] + λD KL (N (µ 0 (ϕ t ), Σ 0 (ϕ t )) || N (0, I)),<label>(4)</label></formula><p>where the real image I 0 and the text description t are from the true data distribution p data . z is a noise vector randomly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downsampling</head><p>This bird is grey with white on its chest and has a very short beak  sampled from a given distribution p z (Gaussian distribution in this paper). λ is a regularization parameter that balances the two terms in Eq. (4). We set λ = 1 for all our experiments.</p><p>Using the reparameterization trick introduced in <ref type="bibr" target="#b19">[21]</ref>, both µ 0 (ϕ t ) and Σ 0 (ϕ t ) are learned jointly with the rest of the network. To extract a visually-discriminative text embedding of the given description, we follow the approach of Reed et al. <ref type="bibr" target="#b32">[34]</ref> to pre-train a text encoder. It is a character level CNN-RNN model that maps text descriptions to the common feature space of images by learning a correspondence function between texts with images <ref type="bibr" target="#b32">[34]</ref>.</p><p>Model Architecture. For the generator G 0 , to obtain text conditioning variableĉ 0 , the text embedding ϕ t is first fed into a fully connected layer to generate µ 0 and σ 0 (σ 0 are the values in the diagonal of Σ 0 ) for the Gaussian distribution N (µ 0 (ϕ t ), Σ 0 (ϕ t )).ĉ 0 are then sampled from the Gaussian distribution. Our N g dimensional conditioning vectorĉ 0 is computed byĉ 0 = µ 0 + σ 0 (where is the elementwise multiplication, ∼ N (0, I)). Then,ĉ 0 is concatenated with a N z dimensional noise vector to generate a W 0 × H 0 image by a series of up-sampling blocks.</p><p>For the discriminator D 0 , the text embedding ϕ t is first compressed to N d dimensions using a fully-connected layer and then spatially replicated to form a M d × M d × N d tensor. Meanwhile, the image is fed through a series of downsampling blocks until it has M d × M d spatial dimension. Then, the image filter map is concatenated along the channel dimension with the text tensor. The resulting tensor is further fed to a 1×1 convolutional layer to jointly learn features across the image and the text. Finally, a fully-connected layer with one node is used to produce the decision score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stage-II GAN</head><p>Low-resolution images generated by Stage-I GAN usually lack vivid object parts and might contain shape distortions. Some details in the text might also be omitted in the first stage, which is vital for generating photo-realistic images. Our Stage-II GAN is built upon Stage-I GAN results to generate highresolution images. It is conditioned on low-resolution images and also the text embedding again to correct defects in Stage-I results. The Stage-II GAN completes previously ignored text information to generate more photo-realistic details.</p><p>Conditioning on the low-resolution result s 0 = G 0 (z,ĉ 0 ) and Gaussian latent variablesĉ, the discriminator D and generator G in Stage-II GAN are trained by alternatively maximizing L D in Eq. (5) and minimizing L G in Eq. <ref type="formula" target="#formula_5">(6)</ref>,</p><formula xml:id="formula_4">L D = E (I,t)∼p data [log D(I, ϕ t )] + E s0∼p G 0 ,t∼p data [log(1 − D(G(s 0 ,ĉ), ϕ t ))],<label>(5)</label></formula><formula xml:id="formula_5">L G = E s0∼p G 0 ,t∼p data [− log D(G(s 0 ,ĉ), ϕ t )] + λD KL (N (µ(ϕ t ), Σ(ϕ t )) || N (0, I)) ,<label>(6)</label></formula><p>Different from the original formulation of GANs, the random noise z is not used in this stage with the assumption that the randomness has already been preserved by s 0 . Gaussian conditioning variablesĉ used in this stage andĉ 0 used in Stage-I GAN share the same pre-trained text encoder, generating the same text embedding ϕ t . However, Stage-I and Stage-II Conditioning Augmentation have different fully connected layers for generating different means and standard deviations. In this way, Stage-II GAN learns to capture useful information in the text embedding that is omitted by Stage-I GAN.</p><p>Model Architecture. We design Stage-II generator as an encoder-decoder network with residual blocks <ref type="bibr" target="#b12">[14]</ref>. Similar to the previous stage, the text embedding ϕ t is used to generate the N g dimensional text conditioning vectorĉ, which is spatially replicated to form a M g × M g × N g tensor. Meanwhile, the Stage-I result s 0 generated by Stage-I GAN is fed into several down-sampling blocks (i.e., encoder) until it has a spatial size of M g × M g . The image features and the text features are concatenated along the channel dimension. The encoded image features coupled with text features are fed into several residual blocks, which are designed to learn multi-modal representations across image and text features. Finally, a series of up-sampling layers (i.e., decoder) are used to generate a W × H high-resolution image. Such a generator is able to help rectify defects in the input image while add more details to generate the realistic high-resolution image.</p><p>For the discriminator, its structure is similar to that of Stage-I discriminator with only extra down-sampling blocks since the image size is larger in this stage. To explicitly enforce GANs to learn better alignment between the image and the conditioning text, rather than using the vanilla discriminator, we adopt the matching-aware discriminator proposed by Reed et al. <ref type="bibr" target="#b33">[35]</ref> for both stages. During training, the discriminator takes real images and their corresponding text descriptions as positive sample pairs, whereas negative sample pairs consist of two groups. The first is real images with mismatched text embeddings, while the second is synthetic images with their corresponding text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation details</head><p>The up-sampling blocks consist of the nearest-neighbor upsampling followed by a 3×3 stride 1 convolution. Batch normalization <ref type="bibr" target="#b15">[17]</ref> and ReLU activation are applied after every convolution except the last one. The residual blocks consist of 3×3 stride 1 convolutions, Batch normalization and ReLU. Two residual blocks are used in 128×128 StackGAN-v1 models while four are used in 256×256 models. The downsampling blocks consist of 4×4 stride 2 convolutions, Batch normalization and LeakyReLU, except that the first one does not have Batch normalization.</p><p>By default, N g = 128, N z = 100, M g = 16, M d = 4, N d = 128, W 0 = H 0 = 64 and W = H = 256. For training, we first iteratively train D 0 and G 0 of Stage-I GAN for 600 epochs by fixing Stage-II GAN. Then we iteratively train D and G of Stage-II GAN for another 600 epochs by fixing Stage-I GAN. All networks are trained using ADAM <ref type="bibr" target="#b18">[20]</ref> solver with beta1 = 0.5. The batch size is 64, and the learning rate is initialized to be 0.0002 and decayed to 1/2 of its previous value every 100 epochs. The source code for StackGAN-v1 is available at https://github.com/hanzhanggit/StackGAN for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STACKGAN-V2: MULTI-DISTRIBUTION GENERATIVE ADVERSARIAL NETWORK</head><p>As discussed above, our StackGAN-v1 has two separate networks, Stage-I GAN and Stage-II GAN, to model low-tohigh resolution image distributions. To make the framework more general, in this paper, we propose a new end-to-end network, StackGAN-v2, to model a series of multi-scale image distributions. As shown in <ref type="figure">Fig. 2</ref>, StackGAN-v2 consists of multiple generators (Gs) and discriminators (Ds) in a tree-like structure. Images from low-resolution to high-resolution are generated from different branches of the tree. At each branch, the generator captures the image distribution at that scale and the discriminator estimates the probability that a sample came from training images of that scale rather than the generator. The generators are jointly trained to approximate the multiple distributions, and the generators and discriminators are trained in an alternating fashion. In this section, we explore two types of multi-distributions: (1) multi-scale image distributions; and (2) joint conditional and unconditional image distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-scale image distributions approximation</head><p>Our StackGAN-v2 framework has a tree-like structure, which takes a noise vector z ∼ p noise as the input and has multiple generators to produce images of different scales. The p noise is a prior distribution, which is usually chosen as the standard normal distribution. The latent variables z are transformed to hidden features layer by layer. We compute the hidden features h i for each generator G i by a non-linear transformation,</p><formula xml:id="formula_6">h 0 = F 0 (z); h i = F i (h i−1 , z), i = 1, 2, ..., m − 1, (7)</formula><p>where h i represents hidden features for the i th branch, m is the total number of branches, and F i are modeled as neural networks (see <ref type="figure">Fig. 2</ref> for illustration). In order to capture information omitted in preceding branches, the noise vector z is concatenated to the hidden features h i−1 as the inputs of F i for calculating h i . Based on hidden features at different layers (h 0 , h 1 , ..., h m−1 ), generators produce samples of small-tolarge scales (s 0 , s 1 , ..., s m−1 ),</p><formula xml:id="formula_7">s i = G i (h i ), i = 0, 1, ..., m − 1,<label>(8)</label></formula><p>where G i is the generator for the i th branch. Following each generator G i , a discriminator D i , which takes a real image x i or a fake sample s i as input, is trained to classify inputs into two classes (real or fake) by minimizing the following cross-entropy loss, <ref type="bibr" target="#b7">(9)</ref> where x i is from the true image distribution p datai at the i th scale, and s i is from the model distribution p Gi at the same scale. The multiple discriminators are trained in parallel, and each of them focuses on a single image scale.</p><formula xml:id="formula_8">LD i = −Ex i ∼p data i [log Di(xi)] − Es i ∼p G i [log(1 − Di(si)],</formula><p>Guided by the trained discriminators, the generators are optimized to jointly approximate multi-scale image distributions (p data0 , p data1 , ..., p datam−1 ) by minimizing the following loss function,</p><formula xml:id="formula_9">LG = m i=1 LG i , LG i = −Es i ∼p G i [log Di(si)] ,<label>(10)</label></formula><p>where L Gi is the loss function for approximating the image distribution at the i th scale (i.e., p datai ). During the training process, the discriminators D i and the generators G i are alternately optimized till convergence. The motivation of the proposed StackGAN-v2 is that, by modeling data distributions at multiple scales, if any one of those model distributions shares support with the real data distribution at that scale, the overlap could provide good gradient signal to expedite or stabilize training of the whole network at multiple scales. For instance, approximating the low-resolution image distribution at the first branch results in images with basic color and structures. Then the generators at the subsequent branches can focus on completing details for generating higher resolution images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Joint conditional and unconditional distribution approximation</head><p>For unconditional image generation, discriminators in StackGAN-v2 are trained to distinguish real images from fake ones. To handle conditional image generation, conventionally, images and their corresponding conditioning variables are input into the discriminator to determine whether an imagecondition pair matches or not, which guides the generator to approximate the conditional image distribution. We propose conditional StackGAN-v2 that jointly approximates conditional and unconditional image distributions. For the generator of our conditional StackGAN-v2, F 0 and F i are converted to take the conditioning vector c as input, such that h 0 = F 0 (c, z) and h i = F i (h i−1 , c). For F i , the conditioning vector c replaces the noise vector z to encourage the generators to draw images with more details according to the conditioning variables. Consequently, multi-scale samples are now generated by s i = G i (h i ). The objective function of training the discriminator D i for conditional StackGAN-v2 now consists of two terms, the unconditional loss and the conditional loss,</p><formula xml:id="formula_10">L D i = −Ex i ∼p data i [log Di(xi)] − Es i ∼p G i [log(1 − Di(si)] unconditional loss + −Ex i ∼p data i [log Di(xi, c)] − Es i ∼p G i [log(1 − Di(si, c)] conditional loss .<label>(11)</label></formula><p>The unconditional loss determines whether the image is real or fake while the conditional one determines whether the image and the condition match or not. Accordingly, the loss function for each generator G i is converted to</p><formula xml:id="formula_11">L Gi = −E si∼p G i [log D i (s i )] unconditional loss + −E si∼p G i [log D i (s i , c)] conditional loss .<label>(12)</label></formula><p>The generator G i at each scale therefore jointly approximates unconditional and conditional image distributions. The final loss for jointly training generators of conditional StackGAN-v2 is computed by substituting Eq. (12) into Eq. (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Color-consistency regularization</head><p>As we increase the image resolution at different generators, the generated images at different scales should share similar basic structure and colors. A color-consistency regularization term is introduced to keep samples generated from the same input at different generators more consistent in color and thus to improve the quality of the generated images.</p><p>Let x k = (R, G, B) T represent a pixel in a generated image, then the mean and covariance of pixels of the given image can be defined by µ = k x k /N and Σ = k (x k − µ)(x k − µ) T /N , where N is the number of pixels in the image. The color-consistency regularization term aims at minimizing the differences of µ and σ between different scales to encourage the consistency, which is defined as</p><formula xml:id="formula_12">LC i = 1 n n j=1 λ1 µ s j i − µ s j i−1 2 2 + λ2 Σ s j i − Σ s j i−1 2 F ,<label>(13)</label></formula><p>where n is the batch size, µ s j i and Σ s j i are mean and covariance for the j th sample generated by the i th generator. Empirically, we set λ 1 = 1 and λ 2 = 5 by default. For the j th input vector, multi-scale samples s j 1 , s j 2 , ..., s j m are generated from m generators of StackGAN-v2. L Ci can be added to the loss function of the i th generator defined in Eq. (10) or Eq. (12), where i = 2, 3, ..., m. Therefore, the final loss for training the i th generator is defined as L Gi = L Gi + α * L Ci . Experimental results indicate that the color-consistency regularization is very important (e.g., α = 50.0 in this paper) for the unconditional task, while it is not needed (α = 0.0) for the text-to-image synthesis task which has a stronger constraint, i.e., the instance-wise correspondence between images and text descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation details</head><p>As shown in <ref type="figure">Fig. 2</ref>    and c 1 for conditional StackGAN-v2) is first transformed to a 4×4×64N g feature tensor, where N g is the number of channels in the tensor. Then, this 4×4×64N g tensor is gradually transformed to 64×64×4N g , 128×128×2N g , and eventually 256×256×1N g tensors at different layers of the network by six up-sampling blocks. The intermediate 64×64×4N g , 128×128×2N g , and 256×256×1N g features are used to generate images of corresponding scales with 3×3 convolutions. Conditioning variables c or unconditional variables z are also directly fed into intermediate layers of the network to ensure encoded information in c or z is not omitted. All the discriminators D i have down-sampling blocks and 3×3 convolutions to transform the input image to a 4×4×8N d tensor, and eventually the sigmoid function is used for outputting probabilities. For all datasets, we set N g = 32, N d = 64 and use two residual blocks between every two generators. ADAM <ref type="bibr" target="#b18">[20]</ref> solver with beta1 = 0.5 and a learning rate of 0.0002 is used for all models. The source code for StackGAN-v2 is available at https://github.com/hanzhanggit/StackGAN-v2 for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We conduct extensive experiments to evaluate the proposed methods. In section 6.1, several state-of-the-art methods on text-to-image synthesis and on unconditional image synthesis are compared with the proposed methods. We first evaluate the effectiveness of our StackGAN-v1 for text-to-image synthesis by comparing it with GAWWN <ref type="bibr" target="#b31">[33]</ref> and GAN-INT-CLS <ref type="bibr" target="#b33">[35]</ref>. And then, StackGAN-v2 is compared with StackGAN-v1 on different datasets to show its advantages and limitations. Moreover, StackGAN-v2 as a more general framework also works well on unconditional image synthesis tasks, and on such 1. The conditioning variable c for StackGAN-v2 is also generated by Conditioning Augmentation tasks, it is compared with several state-of-the-art methods <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b11">[13]</ref>. In section 6.2, several baseline models are designed to investigate the overall design and important components of our StackGAN-v1. For the first baseline, we directly train Stage-I GAN for generating 64×64 and 256×256 images to investigate whether the proposed two-stage stacked structure and the Conditioning Augmentation are beneficial. Then we modify our StackGAN-v1 to generate 128×128 and 256×256 images to investigate whether larger images by our method can result in higher image quality. We also investigate whether inputting text at both stages of StackGAN-v1 is useful. In section 6.3, experiments are designed to validate important components of our StackGAN-v2, including designs with fewer multi-scale image distributions, the effect of jointly approximating conditional and unconditional distributions, and the effectiveness of the proposed color-consistency regularization.</p><p>Datasets. We evaluate our conditional StackGAN for textto-image synthesis on the CUB <ref type="bibr" target="#b47">[49]</ref>, Oxford-102 <ref type="bibr" target="#b28">[30]</ref> and COCO <ref type="bibr" target="#b22">[24]</ref> datasets. CUB <ref type="bibr" target="#b47">[49]</ref> contains 200 bird species with 11,788 images. Since 80% of birds in this dataset have object-image size ratios of less than 0.5 <ref type="bibr" target="#b47">[49]</ref>, as a preprocessing step, we crop all images to ensure that bounding boxes of birds have greater-than-0.75 object-image size ratios. Oxford-102 <ref type="bibr" target="#b28">[30]</ref> contains 8,189 images of flowers from 102 different categories. To show the generalization capability of our approach, a more challenging dataset, COCO <ref type="bibr" target="#b22">[24]</ref> is also utilized for evaluation. Different from CUB and Oxford-102, the COCO dataset contains images with multiple objects and various backgrounds. Each image in COCO has 5 descriptions, while 10 descriptions are provided by <ref type="bibr" target="#b32">[34]</ref> for every image in CUB and Oxford-102 datasets. Following the experimental setup in <ref type="bibr" target="#b33">[35]</ref>, we directly use the training and validation sets provided by COCO, meanwhile we split CUB and Oxford-102 into class-disjoint training and test sets. Our unconditional StackGAN utilizes bedroom and church subsets of LSUN <ref type="bibr" target="#b52">[54]</ref>, a dog-breed 2 and a cat-breed 3 sub-sets of ImageNet <ref type="bibr" target="#b37">[39]</ref> to synthesize different types of images. The statistics of datasets are presented in TABLE 1.</p><p>Evaluation metrics. It is difficult to evaluate the performance of generative models (e.g., GANs). In this paper, we choose inception score (IS) <ref type="bibr" target="#b38">[40]</ref> and fréchet inception distance (FID) <ref type="bibr" target="#b13">[15]</ref> for quantitative evaluation. Inception score (IS) <ref type="bibr" target="#b38">[40]</ref> is the first well-known metric for evaluating GANs. IS = exp (E x D KL (p (y|x) || p (y))) , where x denotes one generated sample, and y is the label predicted by the inception model <ref type="bibr" target="#b39">[41]</ref>  <ref type="bibr" target="#b2">4</ref> . The intuition behind this metric is that good models should generate diverse but meaningful images. Therefore, the KL divergence between the marginal distribution p(y) and the conditional distribution p(y|x) should be large. As suggested in <ref type="bibr" target="#b38">[40]</ref>, we compute the inception score on a large number of samples (i.e., 30k samples randomly generated for the test set) for each model <ref type="bibr" target="#b3">5</ref> .</p><p>Fréchet inception distance (FID) <ref type="bibr" target="#b13">[15]</ref> was recently proposed 2. Using the wordNet IDs provided by Vinyals et al., <ref type="bibr" target="#b45">[47]</ref> 3. The wordNet IDs for this dataset: 'n02121808', 'n02124075', 'n02123394', 'n02122298', 'n02123159','n02123478', 'n02122725', 'n02123597', 'n02124484', 'n02124157', 'n02122878', 'n02123917', 'n02122510', 'n02124313', 'n02123045', 'n02123242', 'n02122430'. <ref type="bibr" target="#b2">4</ref>. In our experiments, for fine-grained datasets, CUB and Oxford-102, we fine-tune an inception model for each of them. For other datasets, we directly use the pre-trained inception model. <ref type="bibr" target="#b3">5</ref>. The mean and standard derivation inception scores of ten splits are reported.</p><p>as a metric that considers not only the synthetic data distribution but also how it compares to the real data distribution. It directly measures the distance between the synthetic data distribution p(.) and the real data distribution p r (.). In practice, images are encoded with visual features by the inception model. Assuming the feature embeddings follow a multidimensional Gaussian distribution, the synthetic data's Gaussian with mean and covariance (m, C) is obtained from p(.) and the real data's Gaussian with mean and covariance (m r , C r ) is obtained from p r (.). The difference between the synthetic and real Gaussians is measured by the Fréchet distance, i.e., F ID = ||m−m r || 2 2 +T r C + C r − 2(CC r ) 1/2 . Lower FID values mean closer distances between synthetic and real data distributions. To compute the FID score for a unconditional model, 30k samples are randomly generated. To compute the FID score for a text-to-image model, all sentences in the corresponding test set are utilized to generate samples.</p><p>To better evaluate the proposed methods, especially to see whether the generated images are well conditioned on the given text descriptions, we also conduct user studies. We randomly select 50 text descriptions for each class of CUB and Oxford-102 test sets. For COCO dataset, 4k text descriptions are randomly selected from its validation set. For each sentence, 5 images are generated by each model. Given the same text descriptions, 30 users (not including any of the authors) are asked to rank the results by different methods. The average ranks by human users are calculated to evaluate all compared methods.</p><p>In addition, we use t-SNE <ref type="bibr" target="#b44">[46]</ref> embedding method to visualize a large number (e.g., 30k on the CUB test set) of high-dimensional images in a two-dimensional map. We observe that t-SNE is a good tool to examine the distribution of synthesized images and identify collapsed modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with state-of-the-art GAN models</head><p>To demonstrate the effectiveness of the proposed method, we compare it with state-of-the-art GAN models on text-to-image synthesis <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b31">[33]</ref> and unconditional image synthesis <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b11">[13]</ref>.</p><p>Text-to-image synthesis. We compare our StackGAN models with several state-of-the-art text-to-image methods <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b33">[35]</ref> on CUB, Oxford-102 and COCO datasets. The inception scores, fréchet inception distances and average human ranks for the proposed StackGAN models and compared methods are reported in TABLE 2. Representative examples are compared in <ref type="figure" target="#fig_3">Fig. 3, Fig. 4</ref>. For meaningful and fair comparisons with previous methods, the inception scores (IS/IS*) and fréchet inception distances (FID/FID*) are computed in two settings. In the first setting, 256×256 images produced by StackGAN, 128×128 images generated by GAWWN <ref type="bibr" target="#b31">[33]</ref> and 64×64 images yielded by GAN-INT-CLS <ref type="bibr" target="#b33">[35]</ref> are used directly to compute IS and FID. Thus, in this setting, the different models are compared directly using their generated images, which have different resolutions. In the second setting, before computing IS* and FID*, all generated images are re-sized to the same resolution of 64×64 for fair comparison.</p><p>Compared with previous GAN models <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b31">[33]</ref>, on the text-to-image synthesis task, our StackGAN-v1 model achieves the best FID*, IS and average human rank on all three datasets. As shown in  <ref type="bibr">3.20)</ref>. When we compare images of different models at the same resolution of 64×64, our StackGAN-v1 still achieves higher inception scores (IS*) than GAN-INT-CLS, but produces a slightly worse inception score (IS*) than GAAWN <ref type="bibr" target="#b31">[33]</ref> because GAWWN uses additional supervision. Meanwhile, the FID* of StackGAN-v1 is nearly one half of the FID* of GAN-INT-CLS on each dataset. It means that the StackGAN-v1 can better model and estimate the 64×64 image distribution. As comparison, the FID of StackGAN-v1 is higher than that of GAN-INT-CLS <ref type="bibr" target="#b33">[35]</ref> on COCO. The reason is that the FID of GAN-INT-CLS is the distance between two 64×64 image distributions while the FID of StackGAN-v1 is the distance between two 256×256 image distributions. It is clear that estimating the 64×64 image distribution is much easier than estimating the 256×256 image distribution. It is also the reason why the FID is higher than the FID* for the same model. Finally, the better average human rank of our StackGAN-v1 also indicates our proposed method is able to generate more realistic samples conditioned on text descriptions. On the other hand, representative examples are shown in <ref type="figure" target="#fig_3">Fig. 3</ref> and <ref type="figure" target="#fig_4">Fig. 4</ref> for visualization comparison. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the 64×64 samples generated by GAN-INT-CLS <ref type="bibr" target="#b33">[35]</ref> can only reflect the general shape and color of the birds. Their results lack vivid parts (e.g., beak and legs) and convincing details in most cases, which make them neither realistic enough nor have sufficiently high resolution. By using additional conditioning variables on location constraints, GAWWN <ref type="bibr" target="#b31">[33]</ref> obtains a better inception score on CUB dataset, which is still slightly lower than ours. It generates higher resolution images with more details than GAN-INT-CLS, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. However, as mentioned by its authors, GAWWN fails to generate any plausible images when it is only conditioned on text descriptions <ref type="bibr" target="#b31">[33]</ref>. In comparison, our StackGAN-v1 for the first time generates  images of 256×256 resolution with photo-realistic details from only text descriptions.</p><p>Comparison between StackGAN-v1 and StackGAN-v2. The comparison between StackGAN-v1 and StackGAN-v2 by different quantitative metrics as well as human evaluations are reported in TABLE 3. For unconditional generation, the samples generated by StackGAN-v2 are consistently better than those by StackGAN-v1 (last four columns in TABLE 3) from a human perspective. The end-to-end training scheme together with the color-consistency regularization enables StackGAN-v2 to produce more feedback and regularization for each branch so that consistency is better maintained during the multi-step generation process. This is especially useful for unconditional generation as no extra conditions (e.g., text) are applied. On the text-to-image datasets, the scores are mixed for StackGAN-v1 and StackGAN-v2. The reason is partially due to the fact that the text information, which is a strong constraint, is added in all the stages to keep coherence. The comparison results of FIDs are consistent with the comparison results of human ranks on all datasets. On the other hand, the inception score draws different conclusions on LSUNbedroom, LSUN-church, and ImageNet-cat. We think that the reason is because the inception model is pre-trained on ImageNet with 1000 classes, which makes it less suitable for   class-specific datasets. Compared with ImageNet-cat which has 17 classes, the inception score for ImageNet-dog correlates better with human ranks because ImageNet-dog covers more (i.e. 118) classes from ImageNet. Hence we believe that, using class-specific datasets, it is more reasonable to use FID to directly compare feature distances between generated samples with that of the real world samples <ref type="bibr" target="#b13">[15]</ref>.</p><p>For visual comparison of the results by the two models, we utilize the t-SNE <ref type="bibr" target="#b44">[46]</ref> algorithm. For each model, a large number of images are generated and embedded into the 2D plane. We first extract a 2048d CNN feature from each generated image using a pre-trained Inception model. Then, t-SNE algorithm is applied to embed the CNN features into a 2D plane, resulting a location for each image in the 2D plane. Due to page limits, <ref type="figure">Fig. 5</ref> only shows a 50×50 grid with compressed images for each dataset, where each generated image is mapped to its nearest grid location. By visualizing a large number of images, the t-SNE is a good tool to examine the synthesized distribution and evaluate its diversity. We also follow <ref type="bibr" target="#b29">[31]</ref> to use the multiscale structural similarity (MS-SSIM) <ref type="bibr" target="#b49">[51]</ref> as a metric to measure the variability of samples. We observe that the MS-SSIM is useful to find large-scale mode collapses but often fails to detect small-scale mode col-lapses or fails to measure the loss of variation in the generated samples' color or texture. This observation is consistent with the one found in <ref type="bibr" target="#b17">[19]</ref>. For example, in <ref type="figure" target="#fig_1">Fig 5, StackGAN-v1</ref> has two small collapsed modes (nonsensical images) while StackGAN-v2 does not have any collapsed nonsensical mode. However, the MS-SSIM score of StackGAN-v1 (0.0945) is better than that of StackGAN-v2 (0.1311) and even better than that of the real data (0.1007). Thus, we argue that the MS-SSIM is not a good metric to capture small-scale mode collapses. On the contrary, the t-SNE visualization of the generated samples can easily help us identify any collapsed modes in the samples as well as evaluate sample variability in texture, color and viewpoint.</p><p>More visual comparison of StackGAN-v1 and StackGAN-v2 on different datasets can be found in <ref type="figure" target="#fig_3">Fig 3, Fig 4, Fig 6,  Fig 7, Fig 8, and Fig. 9</ref>. Specially, <ref type="figure" target="#fig_9">Fig. 9</ref> illustrates failure cases of StackGAN-v1 and StackGAN-v2. We categorize the failures in these cases into three groups: mild, moderate, and severe. The "mild" group means that the generated images have smooth and coherent appearance but lack vivid objects. The "moderate" group means that the generated images have obvious artifacts, which usually are signs of mode collapse. The "severe" group indicates that the generated images fall   <ref type="table" target="#tab_5">TABLE 3</ref>). However, because of the same reason, compared with StackGAN-v1, it is harder for StackGAN-v2 to converge on more complex datasets, such as COCO. In contrast, StackGAN-v1 optimizes subtasks separately by training stage by stage. It produces slightly more appealing images on COCO than StackGAN-v2 based on human rank results, but also generates more images that are moderate or severe failure cases. Consequently, while StackGAN-v2 is more advanced than StackGAN-v1 in many aspects (such as end-to-end training and more stable training behavior), StackGAN-v1 has the advantage of stage-by-stage training, which converges faster and requires less GPU memory. Unconditional image synthesis. We evaluate the effectiveness of StackGAN-v2 for the unconditional image generation task by comparing it with DCGAN <ref type="bibr" target="#b30">[32]</ref>, WGAN <ref type="bibr" target="#b1">[3]</ref>, EBGAN-PT <ref type="bibr" target="#b54">[56]</ref>, LSGAN <ref type="bibr" target="#b24">[26]</ref>, and WGAN-GP <ref type="bibr" target="#b11">[13]</ref> on the LSUN bedroom dataset. As shown in <ref type="figure">Fig. 6</ref>, our StackGAN-v2 is able to generate 256×256 images with more photo-realistic details. In <ref type="figure" target="#fig_7">Fig. 7</ref>, we also compare the 256×256 samples generated by StackGAN-v2 and EBGAN-PT. As shown in the figure, the samples generated by the two methods have the same resolution, but StackGAN-v2 generates more realistic ones (e.g., more recognizable dog faces with eyes and noses). While on LSUN bedroom dataset, only qualitative results are reported in <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b11">[13]</ref>, a DCGAN model <ref type="bibr" target="#b30">[32]</ref> is trained for quantitative comparison using the public available source code 6 on the ImageNet Dog dataset. The inception score of DCGAN is 8.19 ± 0.11 which is much lower than the inception achieved by our StackGAN-v2 (9.55 ± 0.11). These experiments demonstrate that our StackGAN-v2 outperforms the state-of-the-art methods for unconditional image generation. Example images generated by StackGAN on LSUN church and ImageNet cat datasets are presented in <ref type="figure" target="#fig_8">Fig. 8</ref>. <ref type="bibr" target="#b4">6</ref>. https://github.com/carpedm20/DCGAN-tensorflow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The component analysis of StackGAN-v1</head><p>In this section, we analyze different components of StackGAN-v1 on CUB dataset with baseline models.</p><p>The design of StackGAN-v1. As shown in the first four rows of TABLE 4, if Stage-I GAN is directly used to generate images, the inception scores decrease significantly. Such performance drop can be well illustrated by results in <ref type="figure" target="#fig_1">Fig. 11</ref>. As shown in the first row of <ref type="figure" target="#fig_1">Fig. 11</ref>, Stage-I GAN fails to generate any plausible 256×256 samples without using Conditioning Augmentation (CA). Although Stage-I GAN with CA is able to generate more diverse 256×256 samples, those samples are not as realistic as samples generated by StackGAN-v1. It demonstrates the necessity of the proposed stacked structure. In addition, by decreasing the output resolution from 256×256 to 128×128, the inception score decreases from 3.70 to <ref type="bibr">3.35</ref>. Note that all images are scaled to 299 × 299 before calculating the inception score. Thus, if our StackGAN-v1 just increases the image size without adding more information, the inception score would remain the same for samples of different resolutions. Therefore, the decrease in inception score by 128×128 StackGAN-v1 demonstrates that our 256×256 StackGAN-v1 does add more details into the larger images. For the 256×256 StackGAN-v1, if the text is only input to Stage-I (denoted as "no Text twice"), the inception score decreases from 3.70 to 3.45. It indicates that processing text descriptions again at Stage-II helps refine Stage-I results. The same conclusion can be drawn from the results of 128×128 StackGAN-v1 models. <ref type="figure" target="#fig_1">Fig. 10</ref> illustrates some examples of the Stage-I and Stage-II images generated by our StackGAN-v1. As shown in the first row of <ref type="figure" target="#fig_1">Fig. 10</ref>, in most cases, Stage-I GAN is able to draw rough shapes and colors of objects given text descriptions. However, Stage-I images are usually blurry with various defects and missing details, especially for foreground objects. As shown in the second row, Stage-II GAN generates 4× higher resolution images with more convincing details to better reflect corresponding text descriptions. For cases where Stage-I GAN has generated plausible shapes and colors, Stage-II GAN completes the details. For instance, in the 1st column of <ref type="figure" target="#fig_1">Fig. 10</ref>, with a satisfactory Stage-I result, Stage-II GAN focuses on drawing the short beak and white color described in the text as well as details for the tail and legs. In all other examples, different degrees of details are added to Stage-II images. In many other cases, Stage-II GAN is able to correct the defects of Stage-I results by processing the text description again. For example, while the Stage-I image in the 5th column has a blue crown rather than the reddish brown crown described in the text, the defect is corrected by Stage-II GAN. In some extreme cases (e.g., the 7th column of <ref type="figure" target="#fig_1">Fig. 10</ref>), even when Stage-I GAN fails to draw a plausible shape, Stage-II GAN is able to generate reasonable objects. We also observe that StackGAN-v1 has the ability to transfer background from Stage-I images and fine-tune them to be more realistic with higher resolution at Stage-II.</p><p>Importantly, the StackGAN-v1 does not achieve good results by simply memorizing training samples but by capturing the complex underlying language-image relations. By feeding our  generated images and all training images to the Stage-II discriminator D of our StackGAN-v1, their visual features are extracted from the last Conv. layer of D. And then, we can compute the similarity between two images, based on their visual features. Finally, for each generated image, its nearest neighbors from the training set can be retrieved. By visually inspecting the retrieved images (see <ref type="figure" target="#fig_1">Fig. 12</ref>), we conclude that</p><p>The bird is completely red → The bird is completely yellow This bird is completely red with black wings and pointy beak → this small blue bird has a short pointy beak and brown on its wings <ref type="figure" target="#fig_1">Fig. 13</ref>: (Left to right) Images generated by interpolating two sentence embeddings. Gradual appearance changes from the first sentence's meaning to that of the second sentence can be observed. The noise vector z is fixed to be zeros for each row. the generated images have some similar characteristics with the training samples but are essentially different.</p><p>Conditioning Augmentation. We also investigate the efficacy of the proposed Conditioning Augmentation (CA). By removing it from StackGAN-v1 256×256 (denoted as "no CA" in TABLE 4), the inception score decreases from 3.70 to 3.31. <ref type="figure" target="#fig_1">Fig. 11</ref> also shows that 256×256 Stage-I GAN (and StackGAN-v1) with CA can generate birds with different poses and viewpoints from the same text embedding. In contrast, without using CA, samples generated by 256×256 Stage-I GAN collapse to nonsensical images due to the unstable training dynamics of GANs. Consequently, the proposed Conditioning Augmentation helps stabilize the conditional GAN training and improves the diversity of the generated samples because of its ability to encourage robustness to small perturbations along the latent manifold.</p><p>Sentence embedding interpolation. To further demonstrate that our StackGAN-v1 learns a smooth latent data manifold, we use it to generate images from linearly interpolated sentence embeddings, as shown in <ref type="figure" target="#fig_1">Fig. 13</ref>. We fix the noise vector z, so the generated image is inferred from the given text description only. Images in the first row are generated by simple sentences made up by us. Those sentences contain only simple color descriptions. The results show that the generated images from interpolated embeddings can accurately reflect color changes and generate plausible bird shapes. The second row illustrates samples generated from more complex sentences, which contain more details on bird appearances. The generated images change their primary color from red to blue, and change the wing color from black to brown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The component analysis of StackGAN-v2</head><p>In this section, we analyze important components of the proposed StackGAN-v2. TABLE 5 lists models with different settings and their inception scores on the CUB test set. <ref type="figure" target="#fig_1">Fig. 14</ref> shows example images generated by different baseline models.</p><p>Our baseline models are built by removing or changing a certain component of the StackGAN-v2 model. By approximating the image distribution directly at the 256×256 scale without intermediate branches, the inception scores on CUB dramatically decrease from 4.04 to 3.49 for "StackGAN-v2-G 3 " and to 2.89 for "StackGAN-v2-all256" <ref type="table" target="#tab_11">(See TABLE 5</ref> and <ref type="figure" target="#fig_1">Figures 14 (e-f)</ref>). This demonstrates the importance of the multi-scale, multi-stage architecture in StackGAN-v2. Inspired by <ref type="bibr" target="#b8">[10]</ref>, we also build a baseline model with multiple discriminators at the 256×256 scale, namely "StackGAN-v2-3G 3 ". Those discriminators have the same structure but different initializations. However, the results do not show improvement over "StackGAN-v2-G 3 ". Similar comparisons have also been done for the unconditional task on the LSUN bedroom dataset. As shown in <ref type="figure" target="#fig_1">Figures 14(a-c)</ref>, those baseline models fail to generate realistic images because they suffer from severe mode collapses.</p><p>To further demonstrate the effectiveness of jointly approximating conditional and unconditional distributions, "StackGAN-v2-no-JCU" replaces the jointly conditional and unconditional discriminators with the conventional ones, resulting in much lower inception score than that of "StackGAN-v2". Another baseline model does not use the colorconsistency regularization term. Results on various datasets (see <ref type="figure" target="#fig_1">Fig. 15</ref>) show that the color-consistency regularization has significant positive effects for the unconditional image synthesis task. Quantitatively, removing the color-consistency regularization decreases the inception score from 9.55 ± 0.11 to 9.02 ± 0.14 on the ImageNet dog dataset. It demonstrates that the additional constraint provided by the color-consistency regularization is able to facilitate multi-distribution approximation and help generators at different branches produce more coherent samples. It is worth mentioning that there is no need to utilize the color-consistency regularization for the text-toimage synthesis task because the text conditioning appears to provide sufficient constraints. Experimentally, adding the color-consistency regularization did not improve the inception score on CUB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, Stacked Generative Adversarial Networks, StackGAN-v1 and StackGAN-v2, are proposed to decompose the difficult problem of generating realistic high-resolution images into more manageable sub-problems. The StackGAN-v1 with Conditioning Augmentation is first proposed for textto-image synthesis through a novel sketch-refinement process. It succeeds in generating images of 256×256 resolution with photo-realistic details from text descriptions. To further improve the quality of generated samples and stabilize GANs' training, the StackGAN-v2 jointly approximates multiple related distributions, including (1) multi-scale image distributions and (2) jointly conditional and unconditional image distributions. In addition, a color-consistency regularization is proposed to facilitate multi-distribution approximation. Extensive quantitative and qualitative results demonstrate that our proposed methods significantly improve the state of the art in both conditional and unconditional image generation tasks.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The architecture of the proposed StackGAN-v1. The Stage-I generator draws a low-resolution image by sketching rough shape and basic colors of the object from the given text and painting the background from a random noise vector. Conditioned on Stage-I results, the Stage-II generator corrects defects and adds compelling details into Stage-I results, yielding a more realistic high-resolution image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 2 :</head><label>22</label><figDesc>The overall framework of our proposed StackGAN-v2 for the conditional image synthesis task. c is the vector of conditioning variables which can be computed from the class label, the text description, etc.. Ng and N d are the numbers of channels of a tensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Example results by our StackGANs, GAWWN<ref type="bibr" target="#b31">[33]</ref>, and GAN-INT-CLS<ref type="bibr" target="#b33">[35]</ref> conditioned on text descriptions from CUB test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Example results by our StackGANs and GAN-INT-CLS [35] conditioned on text descriptions from Oxford-102 test set (leftmost four columns) and COCO validation set (rightmost four columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) StackGAN-v1 has two collapsed modes (in red rectangles). (b) StackGAN-v2 contains no collapsed nonsensical mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 : 6 :</head><label>56</label><figDesc>Utilizing t-SNE to embed images generated by our StackGAN-v1 and StackGAN-v2 on the CUB test set. 64×64 samples by DCGAN (Reported in [32]) 64×64 samples by WGAN (Reported in [3]) 64×64 samples by EBGAN-PT (Reported in [56]) 112×112 samples by LSGAN (Reported in [26]) 128×128 samples by WGAN-GP (Reported in [13]) 256×256 samples by our StackGAN-v1 256×256 samples by our StackGAN-v2 Fig. Comparison of samples generated by models trained on LSUN bedroom dataset (Zoom in for better comparison).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7 :</head><label>7</label><figDesc>256×256 samples by EBGAN-PT (Reported in [56]) 256×256 samples by our StackGAN-v1 256×256 samples by our StackGAN-v2 Fig. Comparison of samples generated by models trained on ImageNet dog dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>256×256 samples generated by our StackGAN-v1 (top) and StackGAN-v2 (bottom) on ImageNet cat (left) and LSUN church (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Examples of failure cases of StackGAN-v1 (top) and StackGAN-v2 (bottom) on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>For generated images (column 1), retrieving their nearest training images (columns 2-6) by utilizing Stage-II discriminator of StackGAN-v1 to extract visual features. The L2 distances between features are calculated for nearest-neighbor retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with the Department of Computer Science, Rutgers University, Piscataway, NJ, 08854. E-mail: han.zhang@cs.rutgers.edu • T. Xu is with the Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, 18015. E-mail: tax313@lehigh.edu • H. Li is with the Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E-mail: hsli@ee.cuhk.edu.hk • S. Zhang is with the Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC 28223. E-mail: szhang16@uncc.edu • X. Wang is with the Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E-mail: xg-wang@ee.cuhk.edu.hk • X. Huang is with the Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, 18015. E-mail: xih206@lehigh.edu • D. N. Metaxas is with the Department of Computer Science, Rutgers University, Piscataway, NJ, 08854. E-mail: dnm@cs.rutgers.</figDesc><table /><note>edu The first two authors contributed equally to this work. Asterisk indicates the corresponding author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of datasets. We do not split LSUN or ImageNet because they are utilized for the unconditional tasks.</figDesc><table><row><cell>Metric</cell><cell>GAN-INT-CLS</cell><cell>CUB GAWWN</cell><cell>Our StackGAN-v1</cell><cell>GAN-INT-CLS</cell><cell>Oxford Our StackGAN-v1</cell><cell>GAN-INT-CLS</cell><cell>COCO Our StackGAN-v1</cell></row><row><cell>FID ↓</cell><cell>68.79</cell><cell>67.22</cell><cell>51.89</cell><cell>79.55</cell><cell>55.28</cell><cell>60.62</cell><cell>74.05</cell></row><row><cell>FID* ↓</cell><cell>68.79</cell><cell>53.51</cell><cell>35.11</cell><cell>79.55</cell><cell>43.02</cell><cell>60.62</cell><cell>33.88</cell></row><row><cell>IS ↑</cell><cell>2.88 ± .04</cell><cell>3.62 ± .07</cell><cell>3.70 ± .04</cell><cell>2.66 ± .03</cell><cell>3.20 ± .01</cell><cell>7.88 ± .07</cell><cell>8.45 ± .03</cell></row><row><cell>IS* ↑</cell><cell>2.88 ± .04</cell><cell>3.10 ± .03</cell><cell>3.02 ± .03</cell><cell>2.66 ± .03</cell><cell>2.73 ± .03</cell><cell>7.88 ± .07</cell><cell>8.35 ± .11</cell></row><row><cell>HR ↓</cell><cell>2.76 ± .01</cell><cell>1.95 ± .02</cell><cell>1.29 ± .02</cell><cell>1.84 ± .02</cell><cell>1.16 ± .02</cell><cell>1.82 ± .03</cell><cell>1.18 ± .03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Inception scores (IS), fréchet inception distance (FID) and average human ranks (HR) of GAN-INT-CLS<ref type="bibr" target="#b33">[35]</ref>, GAWWN<ref type="bibr" target="#b31">[33]</ref> and our StackGAN-v1 on CUB, Oxford-102, and COCO. (* means that images are re-sized to 64×64 before computing IS* and FID*)</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>CUB</cell><cell>Oxford-102</cell><cell>COCO</cell><cell>LSUN-bedroom</cell><cell>LSUN-church</cell><cell>ImageNet-dog</cell><cell>ImageNet-cat</cell></row><row><cell>FID ↓</cell><cell>StackGAN-v1 StackGAN-v2</cell><cell>51.89 15.30</cell><cell>55.28 48.68</cell><cell>74.05 81.59</cell><cell>91.94 35.61</cell><cell>57.20 25.36</cell><cell>89.21 44.54</cell><cell>58.73 28.59</cell></row><row><cell>IS ↑</cell><cell>StackGAN-v1 StackGAN-v2</cell><cell>3.70 ± .04 4.04 ± .05</cell><cell>3.20 ± .01 3.26 ± .01</cell><cell>8.45 ± .03 8.30 ± .10</cell><cell>3.59 ± .05 3.02 ± .04</cell><cell>2.87 ± .05 2.38 ± .03</cell><cell>8.84 ± .08 9.55 ± .11</cell><cell>4.77 ± .06 4.23 ± .05</cell></row><row><cell>HR ↓</cell><cell>StackGAN-v1 StackGAN-v2</cell><cell>1.81 ± .02 1.19 ± .02</cell><cell>1.70 ± .03 1.30 ± .03</cell><cell>1.45 ± .04 1.55 ± .05</cell><cell>1.95 ± .01 1.05 ± .01</cell><cell>1.86 ± .02 1.14 ± .02</cell><cell>1.90 ± .01 1.10 ± .01</cell><cell>1.88 ± .02 1.12 ± .02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of StackGAN-v1 and StackGAN-v2 on different datasets by inception scores (IS), fréchet inception distance (FID) and average human ranks (HR).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 ,</head><label>2</label><figDesc>compared with GAN-INT-CLS [35], StackGAN-v1 achieves 28.47% improvement in terms of inception score (IS) on CUB dataset (from 2.88 to 3.70), and 20.30% improvement on Oxford-102 (from 2.66 to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Inception scores calculated with 30,000 samples generated on CUB by different baseline models of our StackGAN-v1.</figDesc><table><row><cell>into collapsed modes. Based on such criterion, on the simple</cell></row><row><cell>dataset, Oxford-102, all failure cases of StackGAN-v1 belong</cell></row><row><cell>to the "mild" group, while on other datasets all three groups</cell></row><row><cell>of failure cases are observed. As comparison, we observe that</cell></row><row><cell>all failure cases of StackGAN-v2 belong to the "mild" group,</cell></row><row><cell>meaning StackGAN-v2-generated images have no collapsed</cell></row><row><cell>nonsensical mode (see Fig. 5). By jointly optimizing multiple</cell></row><row><cell>distributions (objectives), StackGAN-v2 shows more stable</cell></row><row><cell>training behavior and results in better FID and inception</cell></row><row><cell>scores on most datasets (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Samples generated by our StackGAN-v1 from unseen texts in CUB test set. Each column lists the text description, images generated from the text by Stage-I and Stage-II of StackGAN-v1.Fig. 11: Conditioning Augmentation (CA) helps stabilize the training of conditional GAN and improves the diversity of the generated samples. (Row 1) without CA, Stage-I GAN fails to generate plausible 256×256 samples. Although different noise vector z is used for each column, the generated samples collapse to be the same for each input text description. (Row 2-3) with CA but fixing the noise vectors z, methods are still able to generate birds with different poses and viewpoints.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>This bird is</cell><cell>The bird has</cell><cell>This is a small,</cell><cell>This bird is</cell></row><row><cell>Text description</cell><cell>This bird is blue with white and has a very</cell><cell cols="2">This bird has wings that are brown and has</cell><cell>A white bird with a black crown and</cell><cell>white, black, and brown in color, with a</cell><cell>small beak, with reddish brown crown</cell><cell>black bird with a white breast and white on</cell><cell>white black and yellow in color, with a short</cell></row><row><cell></cell><cell>short beak</cell><cell cols="2">a yellow belly</cell><cell>yellow beak</cell><cell>brown beak</cell><cell>and gray belly</cell><cell>the wingbars.</cell><cell>black beak</cell></row><row><cell>Stage-I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage-II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 10: A small bird with a black head and</cell><cell cols="2">This bird is completely red with black</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">wings and features grey wings</cell><cell></cell><cell>wings and pointy beak</cell><cell></cell><cell></cell><cell></cell></row><row><cell>256x256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage-I GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>without CA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256x256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage-I GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with CA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256x256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StackGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with CA,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Text twice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Images generated from</cell><cell cols="4">Five nearest neighbors from training sets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>text in test sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 :</head><label>5</label><figDesc>Inception scores by our StackGAN-v2 and its baseline models on CUB test set. "JCU" means using the proposed discriminator that jointly approximates conditional and unconditional distributions. Example images generated by the StackGAN-v2 and its baseline models on LSUN bedroom (top) and CUB (bottom) datasets. Example images generated without (top) and with (bottom) the proposed color-consistency regularization for our StackGAN-v2 on ImageNet dog, ImageNet cat and LSUN church datasets. (Left to right) 64×64, 128×128, and 256×256 images by G1, G2, G3, respectively. Hongsheng Li received the bachelors degree in automation from the East China University of Science and Technology, and the masters and doctorate degrees in computer science from Lehigh University, Pennsylvania, in 2006, 2010, and 2012, respectively. He is currently with the Department of Electronic Engineering at the Chinese University of Hong Kong. His research interests include computer vision, medical image analysis, and machine learning.</figDesc><table><row><cell>(a) StackGAN-v2-all256</cell><cell>(b) StackGAN-v2-G3</cell><cell>(c) StackGAN-v2-3G3</cell><cell>(d) StackGAN-v2</cell></row><row><cell cols="4">This black and white and grey bird has a black bandit marking around it's eyes</cell></row><row><cell>(e) StackGAN-v2-all256</cell><cell>(f) StackGAN-v2-G3</cell><cell>(g) StackGAN-v2-no-JCU</cell><cell>(h) StackGAN-v2</cell></row><row><cell>Fig. 14: ImageNet dog [39]</cell><cell cols="2">ImageNet cat [39]</cell><cell>LSUN church [54]</cell></row><row><cell>Fig. 15:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural photo editing with introspective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07983</idno>
		<title level="m">Maximum-likelihood augmented discrete generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Generating interpretable images with controllable structure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation In Neural Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Snderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR- 2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-scale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals, Systems, and Computers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LR-GAN: layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. student of Department of Computer Science at Rutgers University, Piscataway, NJ. His current research interests include computer vision</title>
	</analytic>
	<monogr>
		<title level="m">2009 and M.E. in communication and information systems from Beijing University of Posts and Telecommunications</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Han Zhang received his B.S. in information science from China Agricultural University</orgName>
		</respStmt>
	</monogr>
	<note>deep learning and medical image processing</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">She is currently a Ph.D. student of Department of</title>
	</analytic>
	<monogr>
		<title level="m">2010, and M.S. in computer science from the Institute of Computing Technology</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China; Bethlehem, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Tao Xu received her B.E. in agricultural mechanization and automatization from China Agricultural University ; Computer Science and Engineering at Lehigh University</orgName>
		</respStmt>
	</monogr>
	<note>Her current research interests include deep learning, computer vision, and medical image processing</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Xiaogang Wang received the BS degree from the University of Science and Technology of China in 2001, the MS degree from the Chinese University of Hong Kong in 2003, and the PhD degree from the Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology in 2009. He is currently an associate professor in the Department of Electronic Engineering, The Chinese University of Hong Kong. His research interests include computer vision and machine learning</title>
		<imprint/>
	</monogr>
	<note>He is a member of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Her research interests are in the areas of Computer Vision, Biomedical Image Analysis, Computer Graphics, and Machine Learning</title>
	</analytic>
	<monogr>
		<title level="m">these areas she has published articles in journals such as TPAMI, MedIA, TMI, TOG, and Scientific Reports. She also regularly contributes research papers to conferences such as CVPR, MICCAI, and ICCV</title>
		<meeting><address><addrLine>Bethlehem, PA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Xiaolei Huang received her doctorate degree in computer science from Rutgers University-New Brunswick, and her bachelor&apos;s degree in computer science from Tsinghua University (China ; Computer Science and Engineering Department at Lehigh University</orgName>
		</respStmt>
	</monogr>
	<note>She serves as an Associate Editor for the Computer Vision and Image Understanding journal. She is a member of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">He is a professor in the Computer Science Department, Rutgers University. He is directing the Computational Biomedicine Imaging and Modeling Center (CBIM). He has been conducting research toward the development of formal methods upon which computer vision, computer graphics, and medical imaging can advance synergistically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dimitris</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Metaxas received the BE degree from the National Technical University of Athens Greece in 1986, the MS degree from the University of Maryland in 1988, and the PhD degree from the University of Toronto in 1992. He is a fellow of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
