<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cnhaozhu@zju.edu.cnmaoyh16@mails.tsinghua.edu.cnfhaoshu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person pose estimation is fundamental to many computer vision tasks and has made significant progress in recent years. However, few previous methods explored the problem of pose estimation in crowded scenes while it remains challenging and inevitable in many scenarios. Moreover, current benchmarks cannot provide an appropriate evaluation for such cases. In this paper, we propose a novel and efficient method to tackle the problem of pose estimation in the crowd and a new dataset to better evaluate algorithms. Our model consists of two key components: joint-candidate single person pose estimation (SPPE) and global maximum joints association. With multipeak prediction for each joint and global association using graph model, our method is robust to inevitable interference in crowded scenes and very efficient in inference. The proposed method surpasses the state-of-the-art methods on CrowdPose dataset by 5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of our method. Source code and dataset will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating multi-person poses in images plays an important role in the area of computer vision. It has attracted tremendous interest for its wide applications in activity understanding <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref>, human re-identification <ref type="bibr" target="#b26">[27]</ref>, human parsing <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> etc. Currently, most of the methods can be roughly divided into two categories: i) top-down approaches that firstly detect each person and then perform single person pose estimation, or ii) bottom-up approaches which detect each joint and then associate them into a whole person.</p><p>To evaluate the performance of multi-person pose estimation algorithms, several public benchmarks were established, such as MSCOCO <ref type="bibr" target="#b18">[19]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref> and AI Challenger <ref type="bibr" target="#b29">[30]</ref>. In these benchmarks, the images are usually collected from daily life where crowded scenes appear less <ref type="bibr">Figure 1</ref>. Qualitative comparison of Mask R-CNN and our Crowd-Pose method in crowded scenes. Though current methods achieve good performance in public benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, they fail in crowded cases. There are mainly two types of errors: i) assemble wrong joints into a pose; ii) predict redundant poses in crowded scenes.</p><p>frequently. As a result, most of the images in these benchmarks have few mutual occlusion among humans. For example, in MSCOCO dataset (persons subset), 67.01% of the images have no overlapped person. Current methods have obtained encouraging success on these datasets.</p><p>However, despite the good performance that current methods have achieved on previous benchmarks, we observe an obvious degradation of their performance in crowded cases. As shown in <ref type="figure">Figure 1</ref>, for the current stateof-the-art methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref> of both bottom-up and top-down approaches, their performance decreases dramatically as the crowd level increases (as <ref type="figure" target="#fig_1">Figure 3</ref>). Few previous methods aimed to tackle the problem of pose estimation in crowded scene, and no public benchmark has been built for this purpose. Meanwhile, crowded scenes are inevitable  in many scenarios.</p><p>In this paper, we propose a novel method to tackle the problem of pose estimation in a crowd, using a global view to address interference problem. Our method follows the top-down framework, which first detects individual persons and then performs single person pose estimation (SPPE). We propose a joint-candidate SPPE and a global maximum joints association algorithm. Different from previous methods that only predict target joints for input human proposals, our joint-candidate SPPE outputs a list of candidate locations for each joint. The candidate list includes target and interference joints. Then our association algorithm utilizes these candidates to build a person-joint connection graph. At last, we solve the joint association problem in this graph model with a global maximum joints association algorithm. Moreover, the computational complexity of our graph optimization algorithm is the same as the conventional NMS algorithm.</p><p>To better evaluate human pose estimation algorithms in crowded scenes and promote the development in this area, we collect a dataset of crowded human poses. We define a Crowd Index to measure the crowding level of an image. Images in our dataset have a uniform distribution of Crowd Index among [0, 1], which means only an algorithm that performs well on both uncrowded and crowded scenes can achieve a high score in our dataset.</p><p>To sum up, the contributions of this paper are as follows: i) we propose a novel method to tackle the crowded problem of pose estimation; ii) we collect a new dataset of crowded human poses to better evaluate algorithms in crowded scenes. We conduct experiments on our proposed method. When using a same ResNet-101 based network backbone, our method surpasses all the state-of-theart methods by 5.2 mAP on our dataset. Moreover, we replace the SPPE and post-processing steps in the state-ofthe-art method with our module and brings 0.8 mAP improvement on MSCOCO dataset. That is, our method can generally work in non-crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2D Pose Estimation Dataset</head><p>Pioneer works on 2D human pose estimation dataset on RGB images involve LSP <ref type="bibr" target="#b16">[17]</ref>, FashionPose <ref type="bibr" target="#b8">[9]</ref>, PASCAL Person Layout <ref type="bibr" target="#b9">[10]</ref>, J-HMDB <ref type="bibr" target="#b15">[16]</ref>, etc. These datasets have contributed to encouraging progress of human pose estimation. However, they only evaluate for single person pose estimation. With the improvement of algorithms, more researchers focus on multi-person pose estimation problems, and several datasets are established, e.g., MPII <ref type="bibr" target="#b1">[2]</ref>, MSCOCO <ref type="bibr" target="#b18">[19]</ref>, AI Challenger <ref type="bibr" target="#b29">[30]</ref>. In spite of the prevalence of these datasets, they suffer from a low-density problem, which makes the current model overfitted to uncrowded scenes. The performance of the state-of-the-art methods decreases as the number of human increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Person Pose Estimation</head><p>Part-Based Framework Representative works on the part-based framework <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18]</ref> are reviewed. Part-based methods detect joints and associate them into a whole person. The state-of-the-art part-based methods are mainly different on their association methods. Cao et al. <ref type="bibr" target="#b3">[4]</ref> associate joints with a part affinity field and greedy algorithm. Papandreou et al. <ref type="bibr" target="#b20">[21]</ref> detect individual joints and predict relative displacements for association. Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> propose a multi-task model and assign joints to detected persons by a pose residual network. The joint detectors in part-based approached are relatively vulnerable because they only consider small local regions and output smaller response heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-</head><p>Step Framework Our work follows the two-step approach. A two-step approach first detects human proposals <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref> and then performs single person pose estimation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>. The state-of-the-art two-step methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> achieve significantly higher scores than the part-based methods. However, the two-step approaches highly depend on human detection results, and it fails in crowded scenes <ref type="bibr" target="#b25">[26]</ref>. When people stay close to each other in a crowd, it is improbable to crop a bounding box that only contains one person. Some works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref> aim at human tracking in the crowd. As a supplement to them, we propose a novel and efficient method that significantly increases pose estimation performance in crowded scenes, which is robust to human detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>The pipeline of our proposed method is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Human bounding box proposals obtained by human detector are fed into joint-candidate (JC) single person pose estimator (SPPE). JC SPPE locates the joint candidates with different response scores on the heatmap (Sec. 3.1). Then our joint association algorithm takes these results and builds a person-joint connection graph (Sec. 3.2). Finally, we solve the graph matching problem to find the best joint association result with a global maximum joints association algorithm (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint-Candidates SPPE</head><p>Joint-candidate SPPE receives a human proposal image and outputs a group of heatmaps to indicate human joint locations. Though a human proposal should indicates only one human instance, in the crowded scenarios, we inevitably need to handle a large number of joints from other human instances. Previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref> use SPPE to suppress interference joints. However, SPPE fails in crowded scenes because their receptive fields are limited by the input human proposals. To address this problem, we propose joint-candidate SPPE with a novel loss designed in a more global view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Loss Design</head><p>For the i th human proposal, we input its region R i into our SPPE network and get the output heatmap P i . There are two types of joints in R i , that is, the joints belong to the i th person, and the joints belong to other human instances (not the i th person). We name them as target joints and interference joints respectively.</p><p>Our goal is to enhance target joints response and suppress interference joints response. However, we don't suppress them directly since interference joints for the current proposal can be regarded as target joints for other proposals. Thus, we can leverage interference joints to estimate human poses with other human proposals in a global manner. Therefore, to utilize those two kinds of joint candidates, we output them with different intensities.</p><p>Heatmap Loss For the k th joint in the i th person, we denote the target joint heatmap as T k i , consisting of a 2D Gaussian G(p k i |σ), centered at the target joint location p k i , with standard deviation σ.</p><p>For interference joints, we denote them as a set Ω k i . The heatmap of interference joints is denoted as C k i , consisting of a Gaussian mixture distribution p∈Ω k i G(p|σ). Our proposed loss is defined as,</p><formula xml:id="formula_0">Loss i = 1 K K k=1 M SE[P k i , T k i + µC k i ]<label>(1)</label></formula><p>where µ is an attenuation factor ranged in [0,1]. As aforementioned, interference joints will be useful in indicating joints of other human instances. Therefore, we should consider it in a global view by cross-validation. Finally, we have µ = 0.5, which fits our intuition: interference joints should be attenuated but not over-suppressed. The conventional heatmap loss function can be regarded as our special case where µ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Discussion</head><p>A conventional SPPE depends on a high-quality human detection result. Its tasks are locating and identifying target joints according to the given human proposal. If SPPE mistakes interference joints for target joints, it will be an unrecoverable error. Missing joints cannot be restored in the post-processing step like pose-NMS. Our proposed joints candidate loss is aimed to tackle this limitation. This loss function encourages JC SPPE network to predict multi-peak heatmaps and sets all the possible joints as candidates. In crowded scenes, while conventional SPPE is hard to identify target joints, JC SPPE can still predict a list of joint candidates and guarantee high recall. We leave the association problem to the next procedure, where we have more global information from other JC SPPEs (on other human proposals) to solve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person-Joint Graph</head><p>Due to our joint-candidate mechanism and redundant human proposals from human detector, joint candidates are numerically much greater than the actual joint numbers. To reduce redundant joints, we build a person-joint graph and apply a maximum person-joint matching algorithm to construct the final human poses. <ref type="figure">Figure 4</ref>. In crowded scenes, human proposals are highly overlapped. Overlapped human proposals tend to predict same actual joint. In this example, if we directly connect the highest response to build final poses, two human proposals will locate same right knee and right leg. Our proposed association algorithm can solve this problem by globally best matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Joint Node Building</head><p>Since highly overlapped human proposals tend to predict the same actual joint (as <ref type="figure">Figure 4</ref>), we first group these candidates that represent the same actual joint as one joint node.</p><p>Thanks to the high-quality joint prediction, candidate joints that indicate the same joint are always close to each other. Thus, we can group them using the following criterion: given two candidate joints located at p k 1 and p k 2 with control deviation δ k , we label them as the same group, if</p><formula xml:id="formula_1">||p (k) 1 − p (k) 2 || 2 ≤ min{u k 1 , u k 2 }δ (k) ,<label>(2)</label></formula><p>where u k 1 and u k 2 are the Gaussian response size of two joints on heatmaps, determined by the Gaussian response deviation. δ (k) is the parameter for controlling deviation of the k th joint, which we directly adopt from MSCOCO keypoint dataset <ref type="bibr" target="#b18">[19]</ref>. The reason why we use min{u 1 , u 2 } rather than a constant threshold is to guarantee that, only if p 1 and p 2 fall into each others' control domain (radii are u k 1 δ k , u k 2 δ k ) simultaneously, we group them together. One node represents a group of joints that cluster together by the above criterion. Now, by building a joint group as one node, we have joint</p><formula xml:id="formula_2">node set J = {v k j : for k ∈ {1, . . . , K}, j ∈ {1, . . . , N k }},</formula><p>where N k is the number of joint nodes of body part k, v k j is the j th node of body part k. The total number of joint nodes in J is k N k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Person Node Building</head><p>Person nodes represent the human proposals detected by human detector. We denote person node set as H = {h i : ∀i ∈ {1 . . . M }}, where h i is the i th person node, and M is the number of detected human proposals.</p><p>Ideally, a qualified human proposal tightly bounds a human instance. However, in crowded scenes, this condition is not always satisfied. The human detector will produce many redundant proposals, including truncated and incompact bounding boxes. We will eliminate these lowquality person nodes during global person-joint matching in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Person-Joint Edge</head><p>After obtaining the node of both joints and persons, we connect them to construct our person-joint graph. If a joint node v k j contains a candidate joint from person node h i , we build an edge e k i,j between them. The weight of e k i,j is the response score of that candidate joint, which is denoted as w k i,j . In this way, we can construct edge set E = {e k i,j : ∀i, j, k}. The person-joints graph can then be written as:</p><formula xml:id="formula_3">G = ((H, J ), E).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Globally Optimizing Association</head><p>From now on, our goal of estimating human poses in the crowd is transformed into solving the above person-joint graph and maximizing the total edge weights. We have our objective function as:</p><formula xml:id="formula_4">max d G = max d i,j,k w (k) i,j · d (k) i,j<label>(4)</label></formula><formula xml:id="formula_5">s.t. j d (k) i,j ≤ 1, ∀k ∈ {1, . . . , K}, ∀i ∈ {1, . . . , M } (5) i d (k) i,j ≤ 1, ∀k ∈ {1, . . . , K}, ∀j ∈ {1, . . . , N k } (6) d (k) i,j ∈ {0, 1}, ∀i, j, k<label>(7)</label></formula><p>where d (k)</p><p>i,j indicates whether we keep the edge e k i,j in our final graph or not. The constraints of Eq. 5 and 6 enforce that each human proposal can only match at most one k th joint.</p><p>Note that G can be decomposed into K sub-graph</p><formula xml:id="formula_6">G k = ((H, J (k) ), E (k) ), where J k = {v (k) j : ∀j ∈ {1 . . . N k }} and E k = {e (k) i,j : ∀i ∈ {1 . . . M }, j ∈ {1 . . . N k }}.</formula><p>Thus, our objective function can be formulated as</p><formula xml:id="formula_7">max d G = max d i,j,k w (k) i,j · d (k) i,j (8) = K k=1 (max d (k) i,j w (k) i,j · d (k) i,j ) (9) = K k=1 max d (k) G k .<label>(10)</label></formula><p>As shown in Eq. 10, solving the global assignment problem in person-joint graph G is mathematically equivalent to solving its sub-graph G k separately. G k is a bipartite graph that composed of person subset and the k th joint subset. For each sub-graph, the updated Kuhn-Munkres algorithm <ref type="bibr" target="#b5">[6]</ref> is applied to get the optimized result. By addressing each G k respectively, we obtain the final result set R.</p><p>Given the graph matching result, if d (k) i,j = 1 the weighted center of v k j is assigned to the i th human proposal as its k th joint. Here, weighted center means the linear combination of candidate joints coordinate in v k j and the weights are their heatmap response scores. In this way, the pose of each human proposal can be constructed. The person nodes that can not match any joint will be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Complexity</head><p>The inference speed of pose estimation is essential in many applications. We prove that our global association algorithm is as efficient as common greedy NMS algorithms.</p><p>As the hereditary property identified by White and Whiteley <ref type="bibr" target="#b28">[29]</ref>, a graph G is (k, l) − sparse if every nonempty sub-graph X has at most k|X| − l edges, where |X| is the number of vertices in sub-graph X and 0 ≤ l &lt; 2k.</p><p>Consider the sub-graph G (k) = ((H, J (k) ), E (k) ). It represents the connection between human proposals and the k th type of joints. According to our statistics ( <ref type="figure" target="#fig_2">Fig. 5</ref>), every human bounding box covers four persons at most in crowded scenes. Therefore, one person node builds connection edges to 4 joints at most. In other words, our personjoint sub-graph G (k) is (4, 0) − sparse since</p><formula xml:id="formula_8">|E (k) | ≤ 4|G (k) | − 0.<label>(11)</label></formula><p>Due to the sparsity of our person-joint graph, we can solve the association problem efficiently. We transform E (k) into an adjacency matrix M e k (unconnected nodes refer to 0). According to the work of Carpaneto et al. <ref type="bibr" target="#b5">[6]</ref>, this linear assignment problem for the sparse matrix can be solved in O(n 2 ), i.e., O((|H| + |J (k) |) 2 ). Since we have eliminated the redundant joints and there is a one-to-one correspondence between joints and persons, the expectation of |J (k) | is equal to |H|. Thus we have O((|H| + |J (k) |) 2 ) = O(|H| 2 ). Such computation complexity is the same as the complexity of conventional greedy NMS algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Our method adopts the graph-based approach to associate joints with human proposals in a globally optimal manner. Human proposals compete with each other for joint nodes. In this way, unqualified human proposals without dominant human instance would fail to be assigned any joints, since their joint response scores are all relatively low due to missing dominant human instance. Therefore, many redundant and poor human proposals are rejected. In comparison to our approach, conventional NMS is a greedy and instance-based algorithm, which is less effective. Although <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> proposed pose-NMS to utilize pose information, their algorithms are based on instances and cannot tackle the missing joints and wrong assembling problem. Our globally optimizing association method can deal with such situations well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CrowdPose Dataset</head><p>In this section, we introduce another contribution of our paper, namely, CrowdPose dataset, including crowded scenes definition, data collection process, and the dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Crowding Level Definition</head><p>To build a dataset of crowded human pose, we need to define a Crowd Index first, which measure the crowding level in a given image.</p><p>Intuitively, the number of persons in an image seems to be a good measurement. However, the principal obstacle to solving crowded cases is not caused by the number of persons, but rather by occlusion in a crowd. Therefore, we need  <ref type="figure">Figure 6</ref>. CrowdIndex distributions of current popular datasets and our dataset. Three public datasets are dominated by uncrowded images. Meanwhile, our CrowdPose dataset has a near uniform distribution. a new Crowd Index to indicate crowding level. In the bounding box of the i th human instance, we denote the number of joints that belonging to the i th person and other (not i th ) persons as N a i and N b i respectively. N b i /N a i is the crowd ratio of the i th human instance. Our Crowd Index is derived by averaging the crowd ratio of all persons in an image:</p><formula xml:id="formula_9">Crowd Index = 1 n n i=1 N b i N a i ,<label>(12)</label></formula><p>where n indicates the total number of persons in the image. We evaluate the Crowd Index distribution of three public benchmarks: MSCOCO (person subset), MPII and AI Challenger. As shown in <ref type="figure">Figure 6</ref>, uncrowded scenes dominate these benchmarks, which leads the state-of-the-art methods only focus on these simple cases and ignore the crowded ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Collection</head><p>To set up a benchmark that covers various scenes and encourages models to adapt to different kinds of situations, we wish our benchmark covers not only crowded cases but also simple daily life scenes. To achieve that, we first analyze three public benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30]</ref> and divide their images into 20 groups according to Crowd Index, ranging from 0 to 1. The step among different groups is 0.05. Then we sample 30,000 images uniformly from these groups in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Annotation</head><p>Although these images have been annotated, their label formats are not fully aligned. In terms of annotated joints number, MSCOCO has 17 keypoints, while MPII has 16 and AI Challenger annotates 14 keypoints. Meanwhile, human annotators are easier to make mistakes in the crowded cases. Thus, we re-annotate these images by the following steps.</p><p>• We use 14 keypoints definition and annotate keypoints and full-body bounding boxes for persons in 30,000 images.</p><p>• We analyze the Crowd Index for 30,000 images again with new annotations, and select 20,000 high-quality images.</p><p>• We further crop each person in the images, and then annotate the interference keypoints in each bounding box.</p><p>We use cross annotation, which means at least two annotators annotate each image. If these two annotations have a large deviation, we regard them as mistakes and re-annotate this image. Finally, we take the average value of each keypoint location to ensure the annotation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Dataset Statistics</head><p>Dataset Size In total, our dataset consists of 20,000 images, containing about 80,000 persons. The training, validation and testing subset are split in proportional to 5:1:4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crowd Index Distribution</head><p>The Crowd Index distribution of CrowdPose is shown in <ref type="figure">Figure 6 (d)</ref>. Unlike the other datasets, CrowdPose has a uniform distribution of Crowd Index. Note that we do not simply force CrowdPose to reach high Crowd Index. If a model is only trained on crowded scenes, it may degrade its performance on uncrowded cases due to the bias of training set. Uniform distribution can promote a model to adapt to various scenes.</p><p>Average IoU We further calculate the average intersection over union(IoU) of human bounding boxes. It turns out that CrowdPose has an average bounding box IoU of 0.27, while MSCOCO, MPII, and AI Challenger have 0.06, 0.11 and 0.12 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce datasets and settings for evaluation. Then we report our results and comparisons with state-of-the-art methods, and finally conduct ablation studies on components in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>CrowdPose Our proposed CrowdPose dataset contains 20,000 images in total and 80,000 human instances. Its Crowd Index satisfies uniform distribution in [0, 1]. Crowd-Pose dataset aims to promote performance in crowded cases and make models generalize to different scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mAP @0.5:0.95 mAP@0.5 mAP @0.75 mAR @0.5:0.95 mAR @0.5 mAR @0.75 Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> 57. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO Keypoints</head><p>We also evaluate our method on the MSCOCO Keypoints dataset <ref type="bibr" target="#b18">[19]</ref>. It contains over 150,000 instances for training and 80,000 instances for testing. The persons in this dataset overlap less frequently than CrowdPose, and it has a Crowd Index centralized near zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metric</head><p>We follow the evaluation metric of MSCOCO, using average precision (AP) and average recall (AR) to evaluate the result. Object keypoint similarity (OKS) plays the same role as the IoU to adopt AP/AR for keypoints detection. We consider mAP, averaged over multiple OKS values (.50:.05:.95), as our primary metric. Moreover, we divide the CrowdPose dataset into three crowding levels by Crowd Index: easy (0-0.1), medium (0.1-0.8) and hard (0.8-1), to better evaluate our model performance in different crowded scenarios. We use the same keypoint standard deviations as MSCOCO when calculating OKS in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>Our method follows the two-step framework. Since human detector and pose estimation network are not what we focus on, we simply adopt the human detector (YoloV3 <ref type="bibr" target="#b23">[24]</ref>) and pose estimation network provided by Al-phaPose <ref type="bibr" target="#b12">[13]</ref> which is a state-of-the-art two-step method. During the training step, we adopt rotation (±30), scaling (±30%) and flipping data augmentation. The input resolution is 320 × 256 and the output heatmap resolution is 80 × 64. The learning rate is set to 1 × 10 −4 and 1 × 10 −5 after 80 epochs. Mini-batch size is set to 64, and RM-Sprop <ref type="bibr" target="#b27">[28]</ref> optimizer is used. During testing, the detected human bounding boxes are first extended by 30% along both the height and width directions and then forwarded through the Joint-candidate SPPE. The locations of joint candidates are obtained from the averaged output heatmaps of original and flipped input image. We conduct our experiments on two Nvidia 1080Ti GPUs. Our whole framework is implemented in PyTorch. Method mAP @0.5:0.95 mAR @0.5:0.95 Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> 64. For comparison with current state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> on CrowdPose dataset, we retrain them based on the configuration provided by the authors. To be fair, we use ResNet-101 as backbone for all SPPE networks and use the same human detector for <ref type="bibr" target="#b31">[32]</ref>. For Mask R-CNN we also use the FPN-based ResNet-101 backbone. Same training batch size is used for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results</head><p>CrowdPose Quantitative results on CrowdPose test set are given in <ref type="table">Table 1</ref>. Our method achieves 5.2 mAP higher than state-of-the-art methods. It demonstrates the effectiveness of our proposed method to tackle the problem of pose estimation in crowded scenes. To further evaluate our method in crowded scenes, we report the results on three crowding level in <ref type="table" target="#tab_0">Table 2</ref>, i.e., uncrowded, medium crowded and extremely crowded. Notably, our method improves 4.1 mAP in uncrowded scenes, while achieves 4.9 and 6.2 mAP higher in medium crowded and extremely crowded scenes separately. This result demonstrates that our method has superior performance in crowded scenes. We present some qualitative results in <ref type="figure" target="#fig_4">Figure 7</ref>. More results will be given in supplementary file.</p><p>MSCOCO We also evaluate our method on MSCOCO dataset to show the generalization ability of our method and results are given in <ref type="table" target="#tab_1">Table 3</ref>. Without bells and whistles, our method achieves 70.9 mAP on COCO test-dev set. It brings 0.8 mAP improvements over AlphaPose <ref type="bibr" target="#b12">[13]</ref> given the same human detector and SPPE network, which proves that our method can perform general improvement on pose estimation problem. Note that for <ref type="bibr" target="#b31">[32]</ref>, the results reported in their paper use a strong human detector which is not open-sourced. To make a fair comparison, we report the results using YOLOV3 as human detector.</p><p>Inference Speed The runtime speed of fully opensourced method is tested and shown in <ref type="table" target="#tab_0">Table 2</ref>. We obtain the FPS results by averaging the inference time on the test set. To achieve the best performance, we use the most accurate configuration for OpenPose <ref type="bibr" target="#b4">[5]</ref>, which has an input resolution of 1024×736. As shown in the table, our method achieves 10.1 FPS on the test set, which is slightly slower than AlphaPose <ref type="bibr" target="#b12">[13]</ref> but faster than other methods. It proves that our method works the most accurate yet very efficient in crowded cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Studies</head><p>We study different components of our method on Crowd-Pose test set, as reported in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joints Candidate Loss</head><p>We first evaluate the effectiveness of our joint-candidate loss. In this experiment, we replace our joint-candidate loss with mean square loss, which is widely used in state-of-the-art pose estimation methods. The experimental result is shown in <ref type="table">Table 4</ref>(a). The final mAP drops from 66.0% to 61.7%. It proves that our loss function can encourage SPPE to predict more possible joints and resist interference.</p><p>Globally Optimizing Association Next, we compare our association algorithm to several NMS algorithms, including bounding box NMS, poseNMS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and parametric poseNMS <ref type="bibr" target="#b12">[13]</ref>. The experimental results are shown in Table 4(b). We can see that our association algorithm greatly outperforms previous methods. We note that these NMS algorithms are all instance-based. They eliminate redundancy on instance level. Instance-based elimination is not the best solution for pose estimation problem. Because of the complexity of human pose, we need to reduce redundancy on joints level. Meanwhile, all the NMS algorithms are greedy algorithms essentially. Therefore, their results may be not the global optimum. Our association algorithm can give the global optimal result while running as efficient as previous NMS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel method to tackle the occlusion problem of pose estimation. By building a person-joint graph based on the outputs of our joint candidates SPPE, we transform the pose estimation problem into a graph matching problem and optimize the results in a global manner. To better evaluate the model performance in crowded scenes, we set up a CrowdPose dataset with a normal distribution of Crowd Index. Our proposed method significantly outperforms the state-of-the-art methods in CrowdPose dataset. Experiments on MSCOCO dataset also demonstrate that our method can generalize to different scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Pipeline of our proposed method. JC SPPE uses joint-candidate loss function during training phase. In inference phase, JC SPPE receives human proposals and generates joint candidates. Then we utilize human proposals and joint candidates to build a person-joint graph. Finally, we associate joints with human proposals by solving the assignment problem in our graph model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>State-of-the-art methods evaluation results on MSCOCO dataset. The x-axis is the Crowd Index which we defined to measure the crowding level of an image. Compared to uncrowded scenes, the accuracy (mAP@0.5:0.95) of state-of-the-art methods are about 20 mAP lower in crowded cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Instance-Joint connection distribution. The x-axis denote the number of human bounding boxes that cover a same joint. This statistical result is based on the ground truth annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of our models predictions is presented. Different person poses are painted in different colors to achieve better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results on CrowdPose test set. Test set is divided into three part and we report the results respectively. FPS column reports the runtime speed on the whole test set.</figDesc><table><row><cell></cell><cell></cell><cell>2</cell><cell>83.5</cell><cell>60.3</cell><cell>65.9</cell><cell>89.5</cell><cell>69.4</cell></row><row><cell>AlphaPose [13]</cell><cell></cell><cell>61.0</cell><cell>81.3</cell><cell>66.0</cell><cell>67.6</cell><cell>86.7</cell><cell>71.8</cell></row><row><cell>Xiao et al. [32]</cell><cell></cell><cell>60.8</cell><cell>81.4</cell><cell>65.7</cell><cell>67.3</cell><cell>86.3</cell><cell>71.8</cell></row><row><cell>Ours</cell><cell></cell><cell>66.0</cell><cell>84.2</cell><cell>71.5</cell><cell>72.7</cell><cell>89.5</cell><cell>77.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Table 1. Results on CrowdPose test set.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">APEasy AP M edium AP Hard FPS</cell><cell></cell><cell></cell></row><row><cell>OpenPose [5]</cell><cell>62.7</cell><cell>48.7</cell><cell>32.3</cell><cell>5.3</cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [14]</cell><cell>69.4</cell><cell>57.9</cell><cell>45.8</cell><cell>2.9</cell><cell></cell><cell></cell></row><row><cell>AlphaPose [13]</cell><cell>71.2</cell><cell>61.4</cell><cell>51.1</cell><cell>10.9</cell><cell></cell><cell></cell></row><row><cell>Xiao et al. [32]</cell><cell>71.4</cell><cell>61.2</cell><cell>51.2</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>75.5</cell><cell>66.3</cell><cell>57.4</cell><cell>10.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Results on MSCOCO test-dev set. We compare the stateof-the-art methods with same detection backbone.</figDesc><table><row><cell></cell><cell></cell><cell>8</cell><cell></cell><cell>71.1</cell></row><row><cell cols="2">AlphaPose [13]</cell><cell>70.1</cell><cell></cell><cell>74.4</cell></row><row><cell cols="2">Xiao et al. [32]</cell><cell>69.8</cell><cell></cell><cell>74.1</cell></row><row><cell></cell><cell>Ours</cell><cell>70.9</cell><cell></cell><cell>76.4</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell cols="2">mAP mAR</cell></row><row><cell></cell><cell cols="3">Ours (SPPE + +Association) 66.0</cell><cell>72.7</cell></row><row><cell>(a)</cell><cell cols="2">w/o joint-candidate Loss</cell><cell>61.7</cell><cell>68.7</cell></row><row><cell>(b)</cell><cell cols="2">Greedy NMS Parametric Pose-NMS [13]</cell><cell>49.1 64.2</cell><cell>63.1 71.1</cell></row><row><cell cols="5">Table 4. Results on CrowdPose test set. mAP and mAR are the</cell></row><row><cell cols="5">average value over multiple OKS values (0.50:0.05:0.95). "w/o X"</cell></row><row><cell cols="5">means without X module in our pipeline. "Greedy NMS" means a</cell></row><row><cell cols="5">conventional NMS method based on proposal scores and IoU.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple human tracking in highdensity crowds. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Dailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Merging pose estimates across space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithm for the solution of the assignment problem for sparse matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carpaneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-pose multi-target tracking for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MultiPoseNet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">PersonLab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08225</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07240</idno>
		<title level="m">MegDet: A large mini-batch object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-grained activity recognition with holistic and pose based features. GCPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">CrowdHuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Some matroids from discrete applied geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Whiteley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<title level="m">A largescale dataset for going deeper in image understanding</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
