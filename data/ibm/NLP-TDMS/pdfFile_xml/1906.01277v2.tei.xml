<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wasserstein Weisfeiler-Lehman Graph Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Togninalli</surname></persName>
							<email>matteo.togninalli@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DEPARTMENT OF BIOSYSTEMS SCIENCE AND ENGINEERING</orgName>
								<orgName type="institution">ETH ZURICH</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB SWISS INSTITUTE OF BIOINFORMATICS</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabetta</forename><surname>Ghisu</surname></persName>
							<email>elisabetta.ghisu@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DEPARTMENT OF BIOSYSTEMS SCIENCE AND ENGINEERING</orgName>
								<orgName type="institution">ETH ZURICH</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB SWISS INSTITUTE OF BIOINFORMATICS</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Llinares-López</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DEPARTMENT OF BIOSYSTEMS SCIENCE AND ENGINEERING</orgName>
								<orgName type="institution">ETH ZURICH</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB SWISS INSTITUTE OF BIOINFORMATICS</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
							<email>bastian.rieck@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DEPARTMENT OF BIOSYSTEMS SCIENCE AND ENGINEERING</orgName>
								<orgName type="institution">ETH ZURICH</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB SWISS INSTITUTE OF BIOINFORMATICS</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
							<email>karsten.borgwardt@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DEPARTMENT OF BIOSYSTEMS SCIENCE AND ENGINEERING</orgName>
								<orgName type="institution">ETH ZURICH</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB SWISS INSTITUTE OF BIOINFORMATICS</orgName>
								<address>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wasserstein Weisfeiler-Lehman Graph Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* These authors contributed equally</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most graph kernels are an instance of the class of R-Convolution kernels, which measure the similarity of objects by comparing their substructures. Despite their empirical success, most graph kernels use a naive aggregation of the final set of substructures, usually a sum or average, thereby potentially discarding valuable information about the distribution of individual components. Furthermore, only a limited instance of these approaches can be extended to continuously attributed graphs. We propose a novel method that relies on the Wasserstein distance between the node feature vector distributions of two graphs, which allows finding subtler differences in data sets by considering graphs as high-dimensional objects rather than simple means. We further propose a Weisfeiler-Lehman-inspired embedding scheme for graphs with continuous node attributes and weighted edges, enhance it with the computed Wasserstein distance, and thereby improve the state-of-the-art prediction performance on several graph classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data have become ubiquitous across domains over the last decades, with examples ranging from social and sensor networks to chemo-and bioinformatics. Graph kernels <ref type="bibr" target="#b44">[45]</ref> have been highly successful in dealing with the complexity of graphs and have shown good predictive performance on a variety of classification problems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. Most graph kernels rely on the R-Convolution framework <ref type="bibr" target="#b17">[18]</ref>, which decomposes structured objects into substructures to compute local similarities that are then aggregated. Although being successful in several applications, R-Convolution kernels on graphs have limitations: (1) the simplicity of the way in which the similarities between substructures are aggregated might limit their ability to capture complex characteristics of the graph; (2) most proposed variants do not generalise to graphs with high-dimensional continuous node attributes, and extensions are far from being straightforward. Various solutions have been proposed to address point <ref type="bibr" target="#b0">(1)</ref>. For example, Fröhlich et al. <ref type="bibr" target="#b14">[15]</ref> introduced kernels based on the optimal assignment of node labels for molecular graphs, although these kernels are not positive definite <ref type="bibr" target="#b42">[43]</ref>. Recently, another approach was proposed by Kriege et al. <ref type="bibr" target="#b24">[25]</ref>, which employs a Weisfeiler-Lehman based colour refinement scheme and uses an optimal assignment of the nodes to compute the kernel. However, this method cannot handle continuous node attributes, leaving point <ref type="bibr" target="#b1">(2)</ref> as an open problem.</p><p>To overcome both limitations, we propose a method that combines the most successful vectorial graph representations derived from the graph kernel literature with ideas from optimal transport theory, which have recently gained considerable attention. In particular, improvements of the computational strategies to efficiently obtain Wasserstein distances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> have led to many applications in machine learning that use it for various purposes, ranging from generative models <ref type="bibr" target="#b1">[2]</ref> to new loss functions <ref type="bibr" target="#b13">[14]</ref>. In graph applications, notions from optimal transport were used to tackle the graph alignment problem <ref type="bibr" target="#b45">[46]</ref>. In this paper, we provide the theoretical foundations of our method, define a new graph kernel formulation, and present successful experimental results. Specifically, our main contributions can be summarised as follows:</p><p>• We present the graph Wasserstein distance, a new distance between graphs based on their node feature representations, and we discuss how kernels can be derived from it. • We introduce a Weisfeiler-Lehman-inspired embedding scheme that works for both categorically labelled and continuously attributed graphs, and we couple it with our graph Wasserstein distance; • We outperform the state of the art for graph kernels on traditional graph classification benchmarks with continuous attributes.</p><p>2 Background: graph kernels and Wasserstein distance</p><p>In this section, we introduce the notation that will be used throughout the manuscript. Moreover, we provide the necessary background on graph kernel methods and the Wasserstein distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph kernels</head><p>Kernels are a class of similarity functions that present attractive properties to be used in learning algorithms <ref type="bibr" target="#b35">[36]</ref>. Let X be a set and k : X × X → R be a function associated with a Hilbert space H, such that there exists a map φ : X → H with k(x, y) = φ(x), φ(y) H . Then, H is a reproducing kernel Hilbert space (RKHS) and k is said to be a positive definite kernel. A positive definite kernel can be interpreted as a dot product in a high-dimensional space, thereby permitting its use in any learning algorithm that relies on dot products, such as support vector machines (SVMs), by virtue of the kernel trick <ref type="bibr" target="#b34">[35]</ref>. Because ensuring positive definiteness is not always feasible, many learning algorithms were recently proposed to extend SVMs to indefinite kernels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>We define a graph as a tuple G = (V, E), where V and E denote the set of nodes and edges, respectively; we further assume that the edges are undirected. Moreover, we denote the cardinality of nodes and edges for G as |V | = n G and |E| = m G . For a node v ∈ V , we write N (v) = {u ∈ V | (v, u) ∈ E} and |N (v)| = deg(v) to denote its first-order neighbourhood. We say that a graph is labelled if its nodes have categorical labels. A label on the nodes is a function l : V → Σ that assigns to each node v in G a value l(v) from a finite label alphabet Σ. Additionally, we say that a graph is attributed if for each node v ∈ V there exists an associated vector a(v) ∈ R m . In this paper, a(v) are the node attributes and l(v) are the categorical node labels of node v. In particular, the node attributes are high-dimensional continuous vectors, whereas the categorical node labels are assumed to be integer numbers (encoding either an ordered discrete value or a category). With the term "node labels", we will implicitly refer to categorical node labels. Finally, a graph can have weighted edges, and the function w : E → R defines the weight w(e) of an edge e := (v, u) ∈ E.</p><p>Kernels on graphs are generally defined using the R-Convolution framework by <ref type="bibr" target="#b17">[18]</ref>. The main idea is to decompose graph G into substructures and to define a kernel value k(G, G ) as a combination of substructure similarities. A pioneer kernel on graphs was presented by <ref type="bibr" target="#b18">[19]</ref>, where node and edge attributes are exploited for label sequence generation using a random walk scheme. Successively, a more efficient approach based on shortest paths <ref type="bibr" target="#b4">[5]</ref> was proposed, which computes each kernel value k(G, G ) as a sum of the similarities between each shortest path in G and each shortest path in G . Despite the practical success of R-Convolution kernels, they often rely on aggregation strategies that ignore valuable information, such as the distribution of the substructures. An example is the Weisfeiler-Lehman (WL) subtree kernel or one of its variants <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, which generates graphlevel features by summing the contribution of the node representations. To avoid these simplifications, we want to use concepts from optimal transport theory, such as the Wasserstein distance, which can help to better capture the similarities between graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wasserstein distance</head><p>The Wasserstein distance is a distance function between probability distributions defined on a given metric space. Let σ and µ be two probability distributions on a metric space M equipped with a ground distance d, such as the Euclidean distance.</p><formula xml:id="formula_0">Definition 1. The L p -Wasserstein distance for p ∈ [1, ∞) is defined as W p (σ, µ) := inf γ∈Γ(σ,µ) M ×M d(x, y) p dγ(x, y) 1 p ,<label>(1)</label></formula><p>where Γ(σ, µ) is the set of all transportation plans γ ∈ Γ(σ, µ) over M × M with marginals σ and µ on the first and second factors, respectively.</p><p>The Wasserstein distance satisfies the axioms of a metric, provided that d is a metric (see the monograph of Villani <ref type="bibr" target="#b43">[44]</ref>, chapter 6, for a proof). Throughout the paper, we will focus on the distance for p = 1 and we will refer to the L 1 -Wasserstein distance when mentioning the Wasserstein distance, unless noted otherwise.</p><p>The Wasserstein distance is linked to the optimal transport problem <ref type="bibr" target="#b43">[44]</ref>, where the aim is to find the most "inexpensive" way, in terms of the ground distance, to transport all the probability mass from distribution σ to match distribution µ. An intuitive illustration can be made for the 1-dimensional case, where the two probability distributions can be imagined as piles of dirt or sand. The Wasserstein distance, sometimes also referred to as the earth mover's distance <ref type="bibr" target="#b33">[34]</ref>, can be interpreted as the minimum effort required to move the content of the first pile to reproduce the second pile.</p><p>In this paper, we deal with finite sets of node embeddings and not with continuous probability distributions. Therefore, we can reformulate the Wasserstein distance as a sum rather than an integral, and use the matrix notation commonly encountered in the optimal transport literature <ref type="bibr" target="#b33">[34]</ref> to represent the transportation plan. Given two sets of vectors X ∈ R n×m and X ∈ R n ×m , we can equivalently define the Wasserstein distance between them as</p><formula xml:id="formula_1">W 1 (X, X ) := min P ∈Γ(X,X ) P, M .<label>(2)</label></formula><p>Here, M is the distance matrix containing the distances d(x, x ) between each element x of X and x of X , P ∈ Γ is a transport matrix (or joint probability), and ·, · is the Frobenius dot product. The transport matrix P contains the fractions that indicate how to transport the values from X to X with the minimal total transport effort. Because we assume that the total mass to be transported equals 1 and is evenly distributed across the elements of X and X , the row and column values of P must sum up to 1 /n and 1 /n , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Wasserstein distance on graphs</head><p>The unsatisfactory nature of the aggregation step of current R-Convolution graph kernels, which may mask important substructure differences by averaging, motivated us to have a finer distance measure between structures and their components. In parallel, recent advances in optimisation solutions for faster computation of the optimal transport problem inspired us to consider this framework for the problem of graph classification. Our method relies on the following steps: (1) transform each graph into a set of node embeddings, (2) measure the Wasserstein distance between each pair of graphs, and (3) compute a similarity matrix to be used in the learning algorithm. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the first two steps, and Algorithm 1 summarises the whole procedure. We start by defining an embedding scheme and illustrate how we integrate embeddings in the Wasserstein distance. Definition 2 (Graph Embedding Scheme). Given a graph G = (V, E), a graph embedding scheme f : G → R |V |×m , f (G) = X G is a function that outputs a fixed-size vectorial representation for each node in the graph. For each v i ∈ V , the i-th row of X G is called the node embedding of v i .</p><p>Note that Definition 2 permits treating node labels, which are categorical attributes, as onedimensional attributes with m = 1. Definition 3 (Graph Wasserstein Distance). Given two graphs G = (V, E) and G = (V , E ), a graph embedding scheme f : G → R |V |×m and a ground distance d : R m × R m → R, we define the Graph Wasserstein Distance (GWD) as <ref type="figure" target="#fig_0">Figure 1</ref>: Visual summary of the graph Wasserstein distance. First, f generates embeddings for two input graphs G and G . Then, the Wasserstein distance between the embedding distributions is computed.</p><formula xml:id="formula_2">D f W (G, G ) := W 1 (f (G), f (G )).<label>(3)</label></formula><p>We will now propose a graph embedding scheme inspired by the WL kernel on categorically labeled graphs, extend it to continuously attributed graphs with weighted edges, and show how to integrate it with the GWD presented in Definition 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating node embeddings</head><p>The Weisfeiler-Lehman scheme. The Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, designed for labelled non-attributed graphs, looks at similarities among subtree patterns, defined by a propagation scheme on the graphs that iteratively compares labels on the nodes and their neighbours. This is achieved by creating a sequence of ordered strings through the aggregation of the labels of a node and its neighbours; those strings are subsequently hashed to create updated compressed node labels. With increasing iterations of the algorithm, these labels represent increasingly larger neighbourhoods of each node, allowing to compare more extended substructures.</p><p>Specifically, consider a graph G = (V, E), let 0 (v) = (v) be the initial node label of v for each v ∈ V , and let H be the number of WL iterations. Then, we can define a recursive scheme to compute h (v) for h = 1, . . . , H by looking at the ordered set of neighbours labels</p><formula xml:id="formula_3">N h (v) = { h (u 0 ), . . . , h (u deg(v)−1 )} as h+1 (v) = hash( h (v), N h (v)).<label>(4)</label></formula><p>We call this procedure the WL labelling scheme. As in the original publication <ref type="bibr" target="#b36">[37]</ref>, we use perfect hashing for the hash function, so nodes at iteration h + 1 will have the same label if and only if their label and those of their neighbours are identical at iteration h.</p><p>Extension to continuous attributes. For graphs with continuous attributes a(v) ∈ R m , we need to improve the WL refinement step, whose original definition prohibits handling the continuous case.</p><p>The key idea is to create an explicit propagation scheme that leverages and updates the current node features by averaging over the neighbourhoods. Although similar approaches have been implicitly investigated for computing node-level kernel similarities <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, they rely on additional hashing steps for the continuous features. Moreover, we can easily account for edge weights by considering them in the average calculation of each neighbourhood. Suppose we have a continuous attribute a 0 (v) = a(v) for each node v ∈ G. Then, we recursively define</p><formula xml:id="formula_4">a h+1 (v) = 1 2   a h (v) + 1 deg(v) u∈N (v) w ((v, u)) · a h (u)   .<label>(5)</label></formula><p>When edge weights are not available, we set w(u, v) = 1. We consider the weighted average of the neighbourhood attribute values instead of a sum and add the 1 /2 factor because we want to ensure a similar scale of the features across iterations; in fact, we concatenate such features for building our proposed kernel (see Definition 4 for more details) and observe better empirical results with similarly scaled features. Although this is not a test of graph isomorphism, this refinement step can be seen as an intuitive extension for continuous attributes of the one used by the WL subtree kernel on categorical node labels, a widely successful baseline. Moreover, it resembles the propagation scheme used in many graph neural networks, which have proven to be successful for node classification on large data sets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Finally, its ability to account for edge weights makes it applicable to all types of graphs without having to perform a hashing step <ref type="bibr" target="#b26">[27]</ref>. Further extensions of the refinement step to account for high-dimensional edge attributes are left for future work. A straightforward example would be to also apply the scheme on the dual graph (where each edge is represented as a node, and connectivity is established if two edges in the primal graph share the same node) to then combine the obtained kernel with the kernel obtained on primal graphs via appropriate weighting.</p><p>Graph embedding scheme. Using the recursive procedure described above, we propose a WL-based graph embedding scheme that generates node embeddings from the node labels or attributes of the graphs. In the following, we use m to denote the dimensionality of the node attributes (m = 1 for the categorical labels). Definition 4 (WL features). Let G = (V, E) and let H be the number of WL iterations. Then, for every h ∈ {0, . . . , H}, we define the WL features as</p><formula xml:id="formula_5">X h G = [x h (v 1 ), . . . , x h (v n G )] T ,<label>(6)</label></formula><p>where x h (·) = h (·) for categorically labelled graphs and x h (·) = a h (·) for continuously attributed graphs. We refer to X h G ∈ R n G ×m as the node features of graph G at iteration h. Then, the node embeddings of graph G at iteration H are defined as</p><formula xml:id="formula_6">f H : G → R n G ×(m(H+1)) G → concatenate(X 0 G , . . . , X H G ).<label>(7)</label></formula><p>We observe that a graph can be both categorically labelled and continuously attributed, and one could extend the above scheme by jointly considering this information (for instance, by concatenating the node features). However, we will leave this scenario as an extension for future work; thereby, we avoid having to define an appropriate distance measure between categorical and continuous data, as this is a long-standing issue <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computing the Wasserstein distance</head><p>Once the node embeddings are generated by the graph embedding scheme, we evaluate the pairwise Wasserstein distance between graphs. We start by computing the ground distances between each pair of nodes. For categorical node features, we use the normalised Hamming distance:</p><formula xml:id="formula_7">d Ham (v, v ) = 1 H + 1 H+1 i=1 ρ(v i , v i ), ρ(x, y) = 1, x = y 0, x = y<label>(8)</label></formula><p>The Hamming distance can be pictured as the normalised sum of discrete metric ρ on each of the features. The Hamming distance equals 1 when two vectors have no features in common and 0 when the vectors are identical. We use the Hamming distance as, in this case, the Weisfeiler-Lehman features are indeed categorical, and values carry no meaning. For continuous node features, on the other hand, we employ the Euclidean distance:</p><formula xml:id="formula_8">d E (v, v ) = ||v − v || 2 .<label>(9)</label></formula><p>Next, we substitute the ground distance into the equation of Definition 1 and compute the Wasserstein distance using a network simplex method <ref type="bibr" target="#b30">[31]</ref>.</p><p>Computational complexity. Naively, the computation of the Wasserstein Distance has a complexity of O(n 3 log(n)), with n being the cardinality of the indexed set of node embeddings, i.e., the number of nodes in the two graphs. Nevertheless, efficient speedup tricks can be employed. In particular, approximations relying on Sinkhorn regularisation have been proposed <ref type="bibr" target="#b7">[8]</ref>, some of which reduce the computational burden to near-linear time while preserving accuracy <ref type="bibr" target="#b0">[1]</ref>. Such speedup strategies become incredibly useful for larger data sets, i.e., graphs with thousands of nodes, and can be easily integrated into our method. See Appendix A.7 for a practical discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">From Wasserstein distance to kernels</head><p>From the graph Wasserstein distance, one can construct a similarity measure to be used in a learning algorithm. In this section, we propose a new graph kernel, state some claims about its (in)definiteness, and elaborate on how to use it for classifying graphs with continuous and categorical node labels. </p><formula xml:id="formula_9">Algorithm 1 Compute Wasserstein graph kernel Input: Two graphs G 1 , G 2 ; graph embedding scheme f H ; ground distance d; λ. Output: kernel value k W W L (G 1 , G 2 ). X G1 ← f H (G 1 ); X G2 ← f H (G 2 ) // Generate node embeddings D ← pairwise_dist(X G1 , X G2 , d) // Compute the ground distance between each pair of nodes D W (G 1 , G 2 ) = min P ∈Γ P, D // Compute the Wasserstein distance k W (G 1 , G 2 ) ← e −λD W (G1,G2)</formula><formula xml:id="formula_10">K WWL = e −λD f WL W . (10)</formula><p>This is an instance of a Laplacian kernel, which was shown to offer favourable conditions for positive definiteness in the case of non-Euclidean distances <ref type="bibr" target="#b10">[11]</ref>. Obtaining the WWL kernel concludes the procedure described in Algorithm 1. In the remainder of this section, we distinguish between the categorical WWL kernel, obtained on graphs with categorical labels, and the continuous WWL kernel, obtained on continuously attributed graphs via the graph embedding schemes described in Section 3.1.</p><p>For Euclidean spaces, obtaining positive definite kernels from distance functions is a well-studied topic <ref type="bibr" target="#b16">[17]</ref>. However, the Wasserstein distance in its general form is not isometric, i.e., there is no metric-preserving mapping to an L 2 -norm, as the metric space it induces strongly depends on the chosen ground distance <ref type="bibr" target="#b11">[12]</ref>. Therefore, despite being a metric, it is not necessarily possible to derive a positive definite kernel from the Wasserstein distance in its general formulation, because the classical approaches <ref type="bibr" target="#b16">[17]</ref> cannot be applied here. Nevertheless, as a consequence of using the Laplacian kernel <ref type="bibr" target="#b10">[11]</ref>, we can show that, in the setting of categorical node labels, the obtained kernel is positive definite. Therefore, to ensure the theoretical and practical correctness of our results in the continuous case, we employ recently developed methods for learning with indefinite kernels. Specifically, we use learning methods for Kreȋn spaces, which have been specifically designed to work with indefinite kernels <ref type="bibr" target="#b29">[30]</ref>; in general, kernels that are not positive definite induce reproducing kernel Kreȋn spaces (RKKS). These spaces can be seen as a generalisation of reproducing kernel Hilbert spaces, with which they share similar mathematical properties, making them amenable to machine learning techniques. Recent algorithms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> are capable of solving learning problems in RKKS; their results indicate that there are clear benefits (in terms of classification performance, for example) of learning in such spaces. Therefore, when evaluating WWL, we will use a Kreȋn SVM (KSVM, <ref type="bibr" target="#b25">[26]</ref>) as a classifier for the case of continuous attributes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental evaluation</head><p>In this section, we analyse how the performance of WWL compares with state-of-the-art graph kernels. In particular, we empirically observe that WWL (1) is competitive with the best graph kernel for categorically labelled data, and (2) outperforms all the state-of-the-art graph kernels for attributed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data sets</head><p>We report results on real-world data sets from multiple sources <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref> and use either their continuous attributes or categorical labels for evaluation. In particular, MUTAG, PTC-MR, NCI1, and D&amp;D are equipped with categorical node labels only; ENZYMES and PROTEINS have both categorical labels and continuous attributes; IMDB-B, BZR, and COX2 only contain continuous attributes; finally, BZR-MD and COX2-MD have both continuous node attributes and edge weights. Further information on the data sets is available in <ref type="table" target="#tab_2">Supplementary Table A.</ref>1. Additionally, we report results on synthetic data (SYNTHIE and SYNTHETIC-NEW) in Appendix A.5. All the data sets have been downloaded from Kersting et al. <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setup</head><p>We compare WWL with state-of-the-art graph kernel methods from the literature and relevant baselines, which we trained ourselves on the same splits (see below). In particular, for the categorical case, we compare with WL <ref type="bibr" target="#b36">[37]</ref> and WL-OA <ref type="bibr" target="#b24">[25]</ref> as well as with the vertex (V) and edge (E) histograms. Because <ref type="bibr" target="#b24">[25]</ref> already showed that the WL-OA is superior to previous approaches, we do not include the whole set of kernels in our comparison. For the continuously attributed data sets, we compare with two instances of the hash graph kernel (HGK-SP; HGK-WL) <ref type="bibr" target="#b26">[27]</ref> and with the GraphHopper (GH) <ref type="bibr" target="#b9">[10]</ref>. For comparison, we additionally use a continuous vertex histogram (VH-C), which is defined as a radial basis function (RBF) kernel between the sum of the graph node embeddings. Furthermore, to highlight the benefits of using the Wasserstein distance in our method, we replace it with an RBF kernel. Specifically, given two graphs G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ), with |V 1 | = n 1 and |V 2 | = n 2 , we first compute the Gaussian kernel between each pair of the node embeddings obtained in the same fashion as for WWL; therefore, we obtain a kernel matrix between node embeddings K ∈ n 1 × n 2 . Next, we sum up the values K s = n1 i=1 n2 j=1 K i,j and set K(G 1 , G 2 ) = K s . This procedure is repeated for each pair of graphs to obtain the final graph kernel matrix. We refer to this baseline as RBF-WL.</p><p>As a classifier, we use an SVM (or a KSVM in the case of WWL) and 10-fold cross-validation, selecting the parameters on the training set only. We repeat each cross-validation split 10 times and report the average accuracy. We employ the same split for each evaluated method, thereby guaranteeing a fully comparable setup among all evaluated methods. Please refer to Appendix A.6 for details on the hyperparameter selection.</p><p>Implementation and computing infrastructure Available Python implementations can be used to compute the WL kernel <ref type="bibr" target="#b40">[41]</ref> and the Wasserstein distance <ref type="bibr" target="#b12">[13]</ref>. We leverage these resources and make our code publicly available 1 . We use the original implementations provided by the respective authors to compute the WL-OA, HGK, and GH methods. All our analyses were performed on a shared server running Ubuntu 14.04.5 LTS, with 4 CPUs (Intel Xeon E7-4860 v2 @ 2.60GHz) each with 12 cores and 24 threads, and 512 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and discussion</head><p>The results are evaluated by classification accuracy and summarised in <ref type="table" target="#tab_0">Table 1 and Table 2</ref> for the categorical labels and continuous attributes, respectively 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Categorical labels</head><p>On the categorical data sets, WWL is comparable to the WL-OA kernel; however, it improves over the classical WL. In particular, WWL largely improves over WL-OA in PTC-MR and is slightly better on D&amp;D, whereas WL-OA is better on NCI1 and PROTEINS.</p><p>Unsurprisingly, our approach is comparable to the WL-OA, whose main idea is to solve the optimal assignment problem by defining Dirac kernels on histograms of node labels, using multiple iterations of WL. This formulation is similar to the one we provide for categorical data, but it relies on the optimal assignment rather than the optimal transport; therefore, it requires one-to-one mappings instead of continuous transport maps. Besides, we solve the optimal transport problem on the concatenated embeddings, hereby jointly exploiting representations at multiple WL iterations. Contrarily, the WL-OA performs an optimal assignment at each iteration of WL and only combines them in the second stage. However, the key advantage of WWL over WL-OA is its capacity to account for continuous attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Continuous attributes</head><p>In this setting, WWL significantly outperforms the other methods on 4 out of 7 data sets, is better on another one, and is on a par on the remaining 2. We further compute the average rank of each method in the continuous setting, with WWL scoring as first. The ranks calculated from <ref type="table" target="#tab_1">Table 2</ref> are WWL = 1, HGK-WL = 2.86, RBF-WL = 3.29, HGK-SP = 4.14, and VH-C = 5.86. This is a remarkable improvement over the current state of the art, and it indeed establishes a new one. When looking at the average rank of the method, WWL always scores first. Therefore, we raise the bar in kernel graph classification for attributed graphs. As mentioned in Section 4, the kernel obtained from continuous attributes is not necessarily positive definite. However, we empirically observe the kernel matrices to be positive definite (up to a numerical error), further supporting our theoretical considerations (see Appendix A.1). In practice, the difference between the results obtained from classical SVMs in RKHS and the results obtained with the KSVM approach is negligible.</p><p>Comparison with hash graph kernels The hash graph kernel (HGK) approach is somewhat related to our propagation scheme. By using multiple hashing functions, the HGK method is capable of extending certain existing graph kernels to the continuous setting. This helps to avoid the limitations of perfect hashing, which cannot express small differences in continuous attributes. A drawback of the random hashing performed by HGK is that it requires additional parameters and introduces a stochastic element to the kernel matrix computation. By contrast, our propagation scheme is fully continuous and uses the Wasserstein distance to capture small differences in distributions of continuous node attributes. Moreover, the observed performance gap suggests that an entirely continuous representation of the graphs provides clear benefits over the hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a new family of graph kernels, the Wasserstein Weisfeiler-Lehman (WWL) graph kernels. Our experiments show that WWL graph kernels outperform the state of the art for 1 https://github.com/BorgwardtLab/WWL <ref type="bibr" target="#b1">2</ref> The best performing methods up to the resolution implied by the standard deviation across repetitions are highlighted in boldface. Additionally, to evaluate significance we perform 2-sample t-tests with a significance threshold of 0.05 and Bonferroni correction for multiple hypothesis testing within each data set, significantly outperforming methods are denoted by an asterisk. graph classification in the scenario of continuous node attributes, while matching the state of the art in the categorical setting. As a line of research for future work, we see great potential in the runtime improvement, thus, allowing applications of our method on regimes and data sets with larger graphs. In fact, preliminary experiments (see Section A.7 as well as <ref type="figure">Figure A</ref>.1 in the Appendix) already confirm the benefit of Sinkhorn regularisation when the average number of nodes in the graph increases. In parallel, it would be beneficial to derive approximations of the explicit feature representations in the RKKS, as this would also provide a consistent speedup. We further envision that major theoretical contributions could be made by defining theoretical bounds to ensure the positive definiteness of the WWL kernel in the case of continuous node attributes. Finally, optimisation objectives based on optimal transport could be employed to develop new algorithms based on graph neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. On a more general level, our proposed method provides a solid foundation of the use of optimal transport theory for kernel methods and highlights the large potential of optimal transport for machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Extended considerations on WWL definiteness</head><p>We will now discuss the positive definite nature of our WWL kernel.</p><p>In general, whether distances obtained from optimal transport problems can be used to create positive definite kernels remains an open research question. Several attempts to draw general conclusions on the definiteness of the Wasserstein distance were unsuccessful, but insightful results on particular cases were obtained along the way. First, we collect some of these contributions and use them to prove that our WWL kernel for categorical embeddings is positive definite. Next, we elaborate further on the continuous embeddings case, for which we provide conjectures on practical conditions to obtain a positive definite kernel.</p><p>Before proceeding, let us reiterate some useful notions. Definition 6. </p><formula xml:id="formula_11">c i c j K ij ≥ 0, with K ij = k(x i , x j ),<label>(11)</label></formula><p>for every c i ∈ R, n ∈ N and x i ∈ X .</p><p>The matrix of kernel values K with entries K ij is called the Gram matrix of k with respect to x 1 , . . . , x n . A conditional positive definite (cpd) kernel is a function that satisfies Equation 11 for all c i ∈ R with n i=1 c i = 0. By analogy, a conditional negative definite (cnd) kernel is a function that satisfies</p><formula xml:id="formula_12">n i,j=1 c i c j K ij ≤ 0 for all c i ∈ R with n i=1 c i = 0.</formula><p>For Euclidean spaces, obtaining kernels from distance functions is a well-studied topic. Proposition 1. <ref type="bibr" target="#b16">[17]</ref> Let d(x, x ) be a symmetric, non-negative distance function with d(x, x) = 0. If d is isometric to an L 2 -norm, then</p><formula xml:id="formula_13">k nd d (x, x ) = −d(x, x ) β , β ∈ [0, 2] (12) is a valid cpd kernel.</formula><p>However, the Wasserstein distance in its general form is not isometric to an L 2 -norm, as the metric space it induces strongly depends on the chosen ground distance <ref type="bibr" target="#b11">[12]</ref>. Recently, Feragen et al. <ref type="bibr" target="#b10">[11]</ref> argued that many types of data, including probability distributions, do not always reside in Euclidean spaces. Therefore, they define the family of exponential kernels relying on a non-Euclidean distance d as k(x, x ) = e −λd(x,x ) q for λ, q &gt; 0, Once again, considerations on the negative definiteness of Wasserstein distance functions cannot be made on the general level. Certain ground distances, however, guarantee the negative definiteness of the resulting Wasserstein distance. In particular, the Wasserstein distance with the discrete metric (i.e., ρ in Equation 8) as the ground distance was proved to be conditional negative definite <ref type="bibr" target="#b15">[16]</ref>.</p><p>We will now leverage these results to prove that the Wasserstein distance equipped with the Hamming ground distance is conditional negative definite; therefore, it yields positive definite kernels for the categorical WL embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 The case of categorical embeddings</head><p>When generating node embeddings using the Weisfeiler-Lehman labelling scheme with a shared dictionary across all the graphs, the solutions to the optimal transport problem are also shared across iterations. We denote the Weisfeiler-Lehman embedding scheme as defined in Definition 4 as f H WL , and let D fWL W be the corresponding GWD on a set of graphs G with categorical labels. Let d Ham (v, v ) of Equation 8 be the ground distance of D fWL W . Then, the following useful results hold.</p><p>Lemma 1. If a transportation plan γ with transport matrix P is optimal in the sense of Definition 1 for distances d Ham between embeddings obtained with f H W L , then it is also optimal for the discrete distances d disc between the H-th iteration values obtained with the Weisfeiler-Lehman procedure.</p><p>Proof. See Appendix A.2.</p><p>Lemma 2. If a transportation plan γ with transport matrix P is optimal in the sense of Definition 1 for distances d Ham between embeddings obtained with f H WL , then it is also optimal for distances d Ham between embeddings obtained with f H−1</p><formula xml:id="formula_15">WL . Proof. See Appendix A.3.</formula><p>Therefore, we postulate that the Wasserstein distance between categorical WL node embeddings is a conditional negative definite function. <ref type="figure">·)</ref> is a conditional negative definite function.</p><formula xml:id="formula_16">Theorem 2. D fWL W (·,</formula><p>Proof. See Appendix A.4.</p><p>Proof of Theorem 1. Theorem 2 in light of Proposition 2 implies that the WWL kernel of Definition 5 is positive definite for all λ &gt; 0.</p><p>We will now consider the case of the definiteness of kernels in the continuous setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 The case of continuous embeddings</head><p>On one hand, in the categorical case, we proved the positive definiteness of our kernel. On the other hand, the continuous case is considerably harder to tackle. We conjecture that, under certain conditions, the same might hold for continuous features. Although we do not have a formal proof yet, in what follows, we discuss arguments to support this conjecture, which seems to agree with our empirical findings. <ref type="bibr" target="#b2">3</ref> The curvature of the metric space induced by the Wasserstein metric for a given ground distance plays an important role. First, we need to define Alexandrov spaces. Definition 7 (Alexandrov space). Given a metric space and a real number k, the space is called an Alexandrov space if its sectional curvature is ≥ k.</p><p>Roughly speaking, the curvature indicates to what extent a geodesic triangle will be deformed in the space. The case of k = 0 is special as no distortion is happening here-hence, spaces that satisfy this property are called flat. The concept of Alexandrov spaces is required in the following proposition, taken from a theorem by Feragen et al. <ref type="bibr" target="#b10">[11]</ref>, which shows the relationship between a kernel and its underlying metric space. Proposition 3. The geodesic Gaussian kernel (i.e., q = 2 in Equation 13) is positive definite for all λ &gt; 0 if and only if the underlying metric space (X, d) is flat in the sense of Alexandrov, i.e., if any geodesic triangle in X can be isometrically embedded in a Euclidean space.</p><p>However, it is unlikely that the space induced by the Wasserstein distance is locally flat, as not even the geodesics (i.e., a generalisation of the shortest path to arbitrary metric spaces) between graph embeddings are necessarily unique, as we subsequently show. Hence, we use the geodesic Laplacian kernel instead of the Gaussian one because it poses less strict requirements on the induced space, as stated in Proposition 2. Specifically, the metric used in the kernel function needs to be cnd. We cannot directly prove this yet, but we can prove that the converse is not true. To this end, we first notice that the metric space induced by the GWD, which we refer to as X, does not have a curvature that is bounded from above. Definition 8. A metric space (X, d) is said to be CAT(k) if its curvature is bounded by some real number k &gt; 0 from above. This can also be seen as a "relaxed" definition, or generalisation, of a Riemannian manifold.</p><p>Theorem 3. X is not in CAT(k) for any k &gt; 0, meaning that its curvature is not bounded by any k &gt; 0 from above.</p><p>Proof. This follows from a similar argument presented by Turner et al. <ref type="bibr" target="#b41">[42]</ref>. We briefly sketch the argument. Let G and G be two graphs. Assume that X is a CAT(k) space for some k &gt; 0. Then, it follows <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">Proposition 2.11,</ref><ref type="bibr">p. 23</ref>] that if D fWL W (G, G ) &lt; π 2 /k, there is a unique geodesic between them. However, we can construct a family of graph embeddings for which this is not the case. To this end, let &gt; 0 and f WL (G) and f WL (G ) be two graph embeddings with node embeddings a 1 = (0, 0), a 2 = ( , ) as well as b 1 = (0, ) and b 2 = ( , 0), respectively. Because we use the Euclidean distance as a ground distance, there will be two optimal transport plans: the first maps a 1 to b 1 and a 2 to b 2 , whereas the second maps a 1 to b 2 and a 2 to b 1 . Hence, we have found two geodesics that connect G and G . Because we may choose to be arbitrarily small, the space cannot be CAT(k) for k &gt; 0.</p><p>Although this does not provide an upper bound on the curvature, we have the following conjecture. Conjecture 1. X is an Alexandrov space with curvature bounded from below by zero.</p><p>For a proof idea, we refer to Turner et al. <ref type="bibr" target="#b41">[42]</ref>; the main argument involves characterizing the distance between triples of graph embeddings. This conjecture is helpful insofar as being a nonnegatively curved Alexandrov space is a necessary prerequisite for X to be a Hilbert space <ref type="bibr" target="#b38">[39]</ref>. In turn, Feragen et al. <ref type="bibr" target="#b10">[11]</ref> shows that cnd metrics and Hilbert spaces are intricately linked. Thus, we have some hope in obtaining a cnd metric, even though we currently lack a proof. Our empirical results, however, indicate that it is possible to turn the GWD into a cnd metric with proper normalisation. Intuitively, for high-dimensional input spaces, standardisation of input features changes the curvature of the induced space by making it locally (almost) flat.</p><p>To support this argumentation, we refer to an existing way to ensure positive definiteness. One can use an alternative to the classical Wasserstein distance denoted as the sliced Wasserstein <ref type="bibr" target="#b31">[32]</ref>. The idea is to project high-dimensional distributions into one-dimensional spaces, hereby calculating the Wasserstein distance as a combination of one-dimensional representations. Kolouri et al. <ref type="bibr" target="#b22">[23]</ref> showed that each of the one-dimensional Wasserstein distances is conditional negative definite. The kernel on high-dimensional representations is then defined as a combination of the one-dimensional positive definite counterparts.</p><p>Let P * be an optimal solution for iteration H. Then, from Lemmas 1 and 2, it is also an optimal solution for D H disc and for all h = 0, . . . , H − 1. We can rewrite the equation as a sum of optimal transport problems:</p><formula xml:id="formula_17">D fWL W (G, G ) = 1 H H h=0 min P * ∈Γ P * , D h disc .<label>(14)</label></formula><p>This corresponds to a sum of 1-dimensional optimal transport problems relying on the discrete metric, which were shown to be conditional negative functions <ref type="bibr" target="#b15">[16]</ref>. Therefore, the final sum is also conditional negative definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Data sets and additional results</head><p>We report additional information on the data sets used in our experimental comparison in Supplementary <ref type="table" target="#tab_2">Table A</ref>.1. Our data sets belong to multiple chemoinformatics domains, including small molecules (MUTAG, PTC-MR, NCI1), macromolecules (ENZYMES, PROTEINS, D&amp;D) and chemical compounds (BZR, COX2). We further consider a movie collaboration data set (IMDB, see <ref type="bibr" target="#b46">[47]</ref> for a description) and two synthetic data sets SYNTHIE and SYNTHETIC-NEW, created by Morris et al. <ref type="bibr" target="#b26">[27]</ref> and Feragen et al. <ref type="bibr" target="#b9">[10]</ref>, respectively. The BZR-MD and COX2-MD data sets do not have node attributes but contain the atomic distance between each connected atom as an edge weight. We do not consider distances between non-connected nodes <ref type="bibr" target="#b23">[24]</ref> and we equip the node with one-hot-encoding categorical attributes representing the atom type, i.e., what is originally intended as a categorical node label. On IMDB-B, IMDB-BINARY was used with the node degree as a (semi-)continuous feature for each node <ref type="bibr" target="#b46">[47]</ref>. For all the other data sets, we use the off-the-shelf version provided by Kersting et al. <ref type="bibr" target="#b19">[20]</ref>.</p><p>Results on synthetic data sets are provided in <ref type="table" target="#tab_2">Table A.</ref>2. We decided not to include those in the main manuscript because of the severely unstable and unreliable results we obtained. In particular, for both data sets, there is a high variation among the different methods. Furthermore, we experimentally observed that even a slight modification of the node features (e.g., normalisation or scaling of the embedding scheme) resulted in a large change of performances (up to 15%). Additionally, it has been previously reported <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref> that on SYNTHETIC-NEW, a WL with degree treated as categorical node label outperforms the competitors, suggesting that the continuous attributes are indeed not informative. Therefore, we excluded these data sets from the main manuscript, as we concluded that they could not fairly assess the quality of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Details on hyperparameter selection</head><p>The following ranges are used for the hyperparameter selection: the parameter of the SVM C = {10 −3 , . . . , 10 3 } (for continuous attributes) and C = {10 −4 , . . . , 10 5 } (for categorical attributes); the WL number of iterations h = {0, . . . , 7}; the λ parameter of the WWL λ = {10 −4 , . . . , 10 1 }. For RBF-WL and VH-C, we use default γ parameter for the Gaussian kernel, i.e., γ = 1/m, where m is the size of node attributes. For the GH kernel, we also fix the γ parameter to 1/m. For HGK, we fix the number of iterations to 20 for each data set, except for SYNTHETICnew where we use  100 (these setups were suggested by the respective authors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>. Furthermore, because HGK is a randomised method, we compute each kernel matrix 10 times and average the results. When the dimensionality of the continuous attributes m &gt; 1, these are normalised to ensure comparability among the different feature scales, in each data set except for BZR and COX2, due to the meaning of the node attributes being location coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Runtime comparison</head><p>Overall, we note that WL and WL-OA scale linearly with the number of nodes; therefore, these methods are faster than our approach. Because of the differences in programming language implementations of the different methods, it is hard to provide an accurate runtime comparison. However, we empirically observe that the Wasserstein graph kernels are still competitive, and a kernel matrix can be computed in a median time of 40 s, depending on the size and number of graphs (see <ref type="figure" target="#fig_0">Figure A.1)</ref>. For the continuous attributes, our approach has a runtime comparable to GH. However, although our approach can benefit from a significant speedup (see discussion below and Section 5.2), GH was shown to empirically scale quadratically with the number of graph nodes <ref type="bibr" target="#b9">[10]</ref>. The HGK, on the other hand, is considerably slower, given the number of iterations and multiple repetitions while taking into account the randomisation.</p><p>To evaluate our approach with respect to the size of the graphs and recalling that computing the Wasserstein distance has complexity O(n 3 log(n)), we simulated a fixed number of graphs with a varying average number of nodes per graph. We generated random node embeddings for 100 graphs, where the number of nodes is taken from a normal distribution centered around the average number of nodes. We then computed the kernel matrix on each set of graphs to compare the runtime of regular Wasserstein with the Sinkhorn regularised optimisation. As shown in <ref type="figure">Supplementary Figure A.</ref>1, the speedup starts to become beneficial at approximately 100 nodes per graph on average, which is larger than the average number of nodes in the benchmark data sets we used.</p><p>To ensure good performance when using the Sinkhorn approximation, we evaluate the obtained accuracy of the model. Recalling that the Sinkhorn method solves the following entropic regularisation problem, P γ = arg min P ∈Γ(X,X ) P, M − γh(P ),</p><p>we further need to select γ. Therefore, on top of the cross-validation scheme described above, we further cross-validate over the regularisation parameter values of γ ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 1, 10} for the ENZYMES data set and obtain an accuracy of 72.08 ± 0.93, which remains above the current state of the art. Values of γ selected most of the time are 0.3, 0.5, and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Performance on isomorphic synthetic graphs</head><p>We performed an additional experiment to evaluate the difference between WL and WWL for noisy Erdős-Rényi graphs (n = 30, p = 0.2). We report the relative distance between G and its permuted and perturbed variant G , w.r.t. a third independent graph G for an increasing noise level (i.e., edge removal) in <ref type="figure">Figure A.</ref>2. We see that WWL is more robust against noise. We also report the time taken to compute the ground distance matrix as distance_time.</p><p>Here, total_time is the sum of the time to compute the ground distance and the time taken to solve the optimal transport (ot) problem for the regular solver or the Sinkhorn-regularised one. The logarithmic scale on the right-side figure shows how, for a small average number of nodes, the overhead to run Sinkhorn is higher than the benefits. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 .</head><label>1</label><figDesc>The categorical WWL kernel is positive definite for all λ &gt; 0.For a proof, see Sections A.1 and A.1.1 in the Appendix. By contrast, for the continuous case, establishing the definiteness of the obtained kernel remains an open problem. We refer the reader to Section A.1.2 in the supplementary materials for further discussions and conjectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b35">[36]</ref> A symmetric function k : X × X → R is called a positive definite (pd) kernel if it satisfies the condition n i,j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and, based on earlier considerations from Berg et al. [4], show that, under certain conditions, the Laplacian kernel (q = 1 in Equation 13) is positive definite. Proposition 2. [11] The geodesic Laplacian kernel is positive definite for all λ &gt; 0 if and only if the geodesic distance d is conditional negative definite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A. 1 :</head><label>1</label><figDesc>Runtime performance of the WWL Kernel computation step with a fixed number of graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 2 :</head><label>2</label><figDesc>Relative distance between (Erdős-Rényi) graph G and its permuted and perturbed variant G w.r.t. a third independent graph G for an increasing noise level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies on graphs with categorical node labels. Comparison of Weisfeiler-Lehman kernel (WL), optimal assignment kernel (WL-OA), and our method (WWL). OA 87.15±1.82 60.58±1.35 86.08±0.27 76.37±0.30 * 79.15±0.33 58.97±0.82 WWL 87.27±1.50 66.31±1.21 * 85.75±0.25 74.28±0.56 79.69±0.50 59.13±0.80</figDesc><table><row><cell>METHOD</cell><cell>MUTAG</cell><cell>PTC-MR</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>D&amp;D</cell><cell>ENZYMES</cell></row><row><cell>V</cell><cell cols="2">85.39±0.73 58.35±0.20</cell><cell cols="2">64.22±0.11 72.12±0.19</cell><cell cols="2">78.24±0.28 22.72±0.56</cell></row><row><cell>E</cell><cell cols="2">84.17±1.44 55.82±0.00</cell><cell cols="2">63.57±0.12 72.18±0.42</cell><cell cols="2">75.49±0.21 21.87±0.64</cell></row><row><cell>WL</cell><cell cols="2">85.78±0.83 61.21±2.28</cell><cell cols="2">85.83±0.09 74.99±0.28</cell><cell cols="2">78.29±0.30 53.33±0.93</cell></row><row><cell>WL-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Definition 5 (Wasserstein Weisfeiler-Lehman). Given a set of graphs G = {G 1 , . . . , G N } and the GWD defined for each pair of graph on their WL embeddings, we define the Wasserstein Weisfeiler- Lehman (WWL) kernel as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies on graphs with continuous node and/or edge attributes. Comparison of hash graph kernel (HGK-WL, HGK-SP), GraphHopper kernel (GH), and our method (WWL). 15±0.79 60.79±0.12 71.64±0.49 74.82±2.13 48.51±0.63 66.58±0.97 64.89±1.06 RBF-WL 68.43±1.47 75.43±0.28 72.06±0.34 80.96±1.67 75.45±1.53 69.13±1.27 71.83±1.61 HGK-WL 63.04±0.65 75.93±0.17 73.12±0.40 78.59±0.63 78.13±0.45 68.94±0.65 74.61±1.74 HGK-SP 66.36±0.37 75.78±0.17 73.06±0.27 76.42±0.72 72.57±1.18 66.17±1.05 68.52±1.00 GH 65.65±0.80 74.78±0.29 72.35±0.55 76.49±0.99 76.41±1.39 69.14±2.08 66.20±1.05 WWL 73.25±0.87 * 77.91±0.80 * 74.37±0.83 * 84.42±2.03 * 78.29±0.47 69.76±0.94 76.33±1.02</figDesc><table><row><cell cols="2">METHOD ENZYMES PROTEINS IMDB-B</cell><cell>BZR</cell><cell>COX2</cell><cell>BZR-MD COX2-MD</cell></row><row><cell>VH-C</cell><cell>47.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">.1: Description of the experimental data sets</cell><cell></cell><cell></cell></row><row><cell>DATA SET</cell><cell cols="6">CLASS RATIO NODE LABELS NODE ATTRIBUTES EDGE WEIGHTS # GRAPHS CLASSES</cell></row><row><cell>MUTAG</cell><cell>63/125</cell><cell></cell><cell>-</cell><cell>-</cell><cell>188</cell><cell>2</cell></row><row><cell>NCI1</cell><cell>2053/2057</cell><cell></cell><cell>-</cell><cell>-</cell><cell>4110</cell><cell>2</cell></row><row><cell>PTC-MR</cell><cell>152/192</cell><cell></cell><cell>-</cell><cell>-</cell><cell>344</cell><cell>2</cell></row><row><cell>D&amp;D</cell><cell>487/691</cell><cell></cell><cell>-</cell><cell>-</cell><cell>1178</cell><cell>2</cell></row><row><cell>ENZYMES</cell><cell>100 PER CLASS</cell><cell></cell><cell></cell><cell>-</cell><cell>600</cell><cell>6</cell></row><row><cell>PROTEINS</cell><cell>450/663</cell><cell></cell><cell></cell><cell>-</cell><cell>1113</cell><cell>2</cell></row><row><cell>BZR</cell><cell>86/319</cell><cell></cell><cell></cell><cell>-</cell><cell>405</cell><cell>2</cell></row><row><cell>COX2</cell><cell>102/365</cell><cell></cell><cell></cell><cell>-</cell><cell>467</cell><cell>2</cell></row><row><cell>SYNTHIE</cell><cell>100 PER CLASS</cell><cell>-</cell><cell></cell><cell>-</cell><cell>400</cell><cell>4</cell></row><row><cell>IMDB-BINARY</cell><cell>500/500</cell><cell>-</cell><cell>( )</cell><cell>-</cell><cell>1000</cell><cell>2</cell></row><row><cell>SYNTHETIC-NEW</cell><cell>150/150</cell><cell>-</cell><cell></cell><cell>-</cell><cell>300</cell><cell>2</cell></row><row><cell>BZR-MD</cell><cell>149/157</cell><cell></cell><cell>-</cell><cell></cell><cell>306</cell><cell>2</cell></row><row><cell>COX2-MD</cell><cell>148/155</cell><cell></cell><cell>-</cell><cell></cell><cell>303</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table A.2: Classification accuracies on synthetic graphs with continuous node attributes. Comparison of hash graph kernel (HGK-WL, HGK-SP), GraphHopper kernel (GH), and our method (WWL).</figDesc><table><row><cell>METHOD</cell><cell cols="2">SYNTHIE SYNTHETIC-NEW</cell></row><row><cell>VH-C</cell><cell>27.51 ± 0.00</cell><cell>60.60 ± 1.60</cell></row><row><cell cols="2">RBF-WL 94.43 ± 0.55</cell><cell>86.37 ± 1.37</cell></row><row><cell cols="2">HGK-WL 81.94 ± 0.40</cell><cell>95.96 ± 0.25  *</cell></row><row><cell cols="2">HGK-SP 85.82 ± 0.28</cell><cell>80.43 ± 0.71</cell></row><row><cell>GH</cell><cell>83.73 ± 0.81</cell><cell>88.83 ± 1.42</cell></row><row><cell>WWL</cell><cell>96.04 ± 0.48  *</cell><cell>86.77 ± 0.98</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33">33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1906.01277v2 [cs.LG] 30 Oct 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We observe that for all considered data sets, after standardisation of the input features before the embedding scheme, GWD matrices are conditional negative definite.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 1</head><p>Proof. We recall the matrix notation introduced in Equation 2 of the main paper, where M is the cost or distance matrix, P ∈ Γ is a transport matrix (or joint probability), and ·, · is the Frobenius dot product. Because we give equal weight (i.e., equal probability mass) to each of the vectors in each set, Γ contains all nonnegative n × n matrices P with</p><p>For notation simplicity, let us denote by D h Ham the Hamming matrix D Ham (f h WL (G), f h WL (G )), where the ij-th entry is given by the Hamming distance between the embedding of the i-th node of graph G and the embedding of the j-th node of graph G at iteration h. Similarly, we define D h disc to be the discrete metric distance matrix, where the ij-th entry is given by the discrete distance between feature h of node embedding i of graph G and feature h of node embedding j of graph G . It is easy to see that</p><p>and, by definition,</p><p>Moreover, because of the WL procedure, two labels that are different at iteration h will also be different at iteration h + 1. Hence, the following identity holds:</p><p>Assuming that P h is not optimal for D h d , we can define P * such that P * , D h disc &lt; P h , D h disc . Because the entries of D h disc are either 0 or 1, we can define the set of indices tuples H = (i, j) | [D h disc ] ij = 1 and rewrite the inequality as</p><p>Considering the constraints on the entries of P * and P h , namely i,j p * ij = i,j p h ij = 1, this implies that, by rearranging the transport map, there is more mass that could be transported at 0 cost. In our formalism,</p><p>However, as stated before, entries of D h d that are 0 are also 0 in D h Ham . Therefore, a better transport plan P * would also be optimal for D h Ham :</p><p>Ham , which contradicts the optimality assumption above. Hence, P h is also optimal for D H disc .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma 2</head><p>Proof. Intuitively, the transportation plan at iteration h is a "refinement" of the transportation plan at iteration h − 1, where only a subset of the optimal transportation plans remains optimal for the new cost matrix D h H . Using the same notation as for the Proof in Appendix A.2, and considering the WL procedure, two labels that are different at iteration h will also be different at iteration h + 1. Hence, the following identities hold:</p><p>Ham ij</p><p>An optimal transportation plan P h for f h W L (G) embeddings satisfies P h , D h Ham ≤ P, D h Ham ∀P ∈ Γ, which can also be written as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The values of D h</head><p>Ham increase in a step-wise fashion for increasing h, and their ordering remains constant, except for entries that were 0 at iteration h − 1 and became 1 h at iteration h. Hence, because our metric distance matrices satisfy monotonicity conditions and because P h is optimal for D h disc according to Lemma 1, it follows that</p><p>Ham ∀P ∈ Γ. Therefore, P h is also optimal for f h−1 WL (G) embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 2</head><p>Proof. Using the same notation as for the Proof in Appendix A. <ref type="bibr" target="#b1">2</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Near-linear time approximation algorithms for optimal transport via sinkhorn iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Altschuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1964" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning with similarity functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="89" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Harmonic analysis on semigroups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P R</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ressel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth IEEE International Conference on Data Mining</title>
		<meeting>the Fifth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Metric spaces of non-positive curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Bridson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Häfliger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic exponential kernels: When curvature and linearity conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3032" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal transport and curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Figalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear PDE&apos;s and Applications</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">POT: Python Optimal Transport library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<ptr target="https://github.com/rflamary/POT" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning with a Wasserstein loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2053" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal assignment kernels for attributed molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sieker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the definiteness of Earth Mover&apos;s Distance and its relation to set intersection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selmic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with distance substitution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haasdonk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM-Symposium</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sliced Wasserstein kernels for probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5258" to="5267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning SVM in Kreȋn spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loosli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1204" to="1216" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster kernels for graphs with continuous attributes via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE International Conference on Data Mining</title>
		<meeting>the 16th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning in reproducing kernel kreın spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oglic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3859" to="3867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning with non-positive kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Computational optimal transport. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wasserstein barycenter and its application to texture mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A persistent Weisfeiler-Lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The kernel trick for distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="301" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast subtree kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1660" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Barycenters in Alexandrov spaces of curvature bounded below</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shin-Ichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Geometry</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="587" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the theory of scales of measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="677" to="680" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">graphkernels: R and python packages for graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="532" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fréchet means for distributions of persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="44" to="70" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0801.4061</idno>
		<title level="m">The optimal assignment kernel is not positive definite</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">338</biblScope>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein learning for graph matching and node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Duke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6932" to="6941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
