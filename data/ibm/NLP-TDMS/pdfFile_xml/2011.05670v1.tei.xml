<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Patch-free global learning</term>
					<term>fully convolutional network</term>
					<term>feature fusion</term>
					<term>hyperspectral image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches. As such, these methods are local learning methods, which have a high computational cost. In this paper, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. The proposed framework consists of three main parts: 1) a designed sampling strategy; 2) an encoder-decoder based fully convolutional network (FCN); and 3) lateral connections between the encoder and decoder. In FPGA, an encoder-decoder based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the FCN's abilities of fast inference and global spatial information mining, a global stochastic stratified (GS 2 ) sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark datasets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification. Code has been made available at: https://github.com/Z-Zheng/FreeNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches. As such, these methods are local learning methods, which have a high computational cost. In this paper, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. The proposed framework consists of three main parts: 1) a designed sampling strategy; 2) an encoder-decoder based fully convolutional network (FCN); and 3) lateral connections between the encoder and decoder. In FPGA, an encoder-decoder based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the FCN's abilities of fast inference and global spatial information mining, a global stochastic stratified (GS 2 ) sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark datasets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification. Code has been made available at: https://github.com/Z-Zheng/FreeNet. H YPERSPECTRAL imaging, as a particularly important technique, is able to obtain abundant spectral information about the ground surface <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. As a result, it is widely applied in the fields of geology, agriculture, forestry, and environmental monitoring <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Hyperspectral image classification with the goal to assign a unique semantic label to each pixel in a hyperspectral image (HSI) <ref type="bibr" target="#b6">[7]</ref>, is a fundamental but challenging part of hyperspectral remote sensing (HRS).</p><p>For HSI classification, the spectral feature-based methods, such as support vector machine (SVM) <ref type="bibr" target="#b7">[8]</ref>, random forest (RF) <ref type="bibr" target="#b8">[9]</ref>, rotation forest (RoF) <ref type="bibr" target="#b9">[10]</ref>, canonical correlation forest (CCF) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and multinomial logistic regression (MLR) <ref type="bibr" target="#b12">[13]</ref>, are the traditional classifiers for HSIs. To further improve the accuracy of HSI classification, spatial information has been integrated into the existing pipelines <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Thereby, spectral-spatial feature based methods, such as the gray level cooccurrence matrix <ref type="bibr" target="#b16">[17]</ref>, wavelet transform <ref type="bibr" target="#b17">[18]</ref>, the Gabor filter <ref type="bibr" target="#b18">[19]</ref>, etc., have been proposed to improve the discrimination of the features. Extended morphological profiles (EMPs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> has also been proposed to leverage the spatial context with multiple morphological operations for HSI classification. However, these spectral-spatial features are handcrafted, and they are strongly reliant on the prior information and empirical hyperparameters <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>To automatically obtain more general spectral-spatial features, deep learning technology <ref type="bibr" target="#b23">[24]</ref>, as a data-driven automatic feature learning framework, has now been introduced into HSI classification. Among the deep learning based methods, convolutional neural networks (CNNs), as hierarchical spectral-spatial feature representation learning frameworks, have been widely used in HSI classification <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>, significantly boosting the accuracy when compared with the traditional methods. More importantly, CNN-based methods can act as an end-to-end training feature extractor and classifier for global optimization to obtain a better accuracy. These CNNbased methods follow a patch-based local learning framework <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, where patch generation is first performed to obtain a dense set of patches with a fixed size S × S and then patchwise classification is applied to each patch.</p><p>However, these methods usually have a high computational complexity under the patch-based local learning framework. This is because these methods first generate overlapping image patches and then assign semantic labels obtained by the CNN to the corresponding central pixels to obtain complete classification map. However this results in redundant arXiv:2011.05670v1 [cs.CV] 11 Nov 2020 computation since the image patches generated by adjacent pixels overlap with each other. This seriously constrains the speed of the methods under the patch-based local learning framework. Meanwhile, the limited patch size constrains the spatial context, making it difficult for the CNN to model longrange dependency.</p><p>In this work, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. The FPGA framework includes a sampling strategy, an encoder-decoder based FCN, and lateral connections between the encoder and decoder. To share the computation in the spatial dimension and leverage the global spatial information, an encoder-decoder based fully convolutional network (FCN) is introduced to endto-end HSI classification. However, training an FCN for HSI classification is difficult since it always fails to converge. The main reason for this is the insufficiently diverse gradients caused by the limited training samples during the backward computation. To guarantee the convergence of the training of the FCN, a global stochastic stratified (GS 2 ) sampling strategy is proposed to obtain diverse gradients during backpropagation. Furthermore, FreeNet, which is a novel network architecture for HSI classification, is proposed to maximize the exploitation of the global spatial information and further boost the performance. FreeNet consists of a spectral attention based encoder and a lightweight decoder. In addition, a lateral connection is applied to fuse the spatial details in the encoder and the semantic features in the decoder, for better exploitation of the encoder-decoder structural characteristics, which can help to recover more clear edges of objects in the classification map.</p><p>The main contributions of our study are summarized as follows:</p><p>1) A fast patch-free global learning (FPGA) framework is proposed for HSI classification. The FPGA framework includes a sampling strategy, an encoder-decoder based FCN, and lateral connections between the encoder and decoder, which can achieve faster patch-free inference and learn from the global spatial information, for a better accuracy. 2) To guarantee the convergence of the training of the FCN, the GS 2 sampling strategy is designed to assist with the training of the FCN. GS 2 strategy transforms all the training samples into a stochastic sequence of stratified samples, to obtain diverse gradients during backpropagation, for more effective parameter updating. 3) To further boost the performance, a novel network architecture, FreeNet, is proposed for HSI classification through exploiting the global spatial information. FreeNet consists of a spectral attention based encoder and a lightweight decoder. Spectral attention involves modeling the interdependencies of the feature maps, using the global spatial context to guide the importance of the feature maps. This ensures sufficient exploitation of the redundant spectral information and the global spatial information. A lightweight decoder is responsible for the progressive recovery of the classification map, with less burden on optimization. 4) To make full use of the encoder-decoder structural characteristics, a lateral connection between the encoder and decoder is designed for the fusion of the spatial details in the encoder with the semantic information in the decoder. This refines the semantic features with the spatial detail features to obtain a clearer classification map. The rest of this paper is organized as follows. Section II briefly introduces the handcrafted feature based and CNN based HSI classifiers via the patch-based local learning framework. Section III then describes the details of the proposed unified patch-free learning framework. Section IV describes the comparative results obtained on three HSI classification benchmark datasets, and further analyzes the proposed modules and the introduced hyperparameters. Finally, Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Deep Learning Based HSI Classifiers Under a Patch-Based Local Learning Framework</head><p>The dominant methods in modern HSI classification are based on deep networks and follow a patch-based local learning framework. The deep networks include stacked autoencoders (SAEs), deep belief networks (DBNs), CNNs, recurrent neural networks (RNNs) and generative adversarial networks (GANs), which have all been explored in HSI classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. Among these methods, CNN-based classifiers, which are regarded as the natural spectral-spatial classification methods, have obvious advantages in accuracy. To conveniently extract features and learn a classifier using CNNs, HSI patches are first generated from the original image by a window with a fixed size S ×S (e.g. 7×7 or 28×28). HSI classification always involves modeling a patch classification task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, which involves learning a mapping f : R S×S → R, as shown in <ref type="figure">Fig. 1</ref> Under the patch-based local learning framework, the main difference between the CNN-based classifiers is in the different deep CNN designs. A simple deep CNN was employed for HSI classification in <ref type="bibr" target="#b25">[26]</ref>. To utilize the spatial context information, a contextual CNN was proposed in <ref type="bibr" target="#b35">[36]</ref>, further improving the classification accuracy. A CNN with pixel-pair features (CNN-PPF) was proposed in <ref type="bibr" target="#b37">[37]</ref> to enhance the original CNN by using deep pixel-pair features. A spectral-spatial feature based classification framework was proposed in <ref type="bibr" target="#b33">[34]</ref>, combining a balanced local discriminant embedding algorithm used as the spectral feature extractor with the CNN used as a spectral-spatial feature extractor for HSI classification. To obtain more discriminative features, the Siamese convolutional neural network (S-CNN) was proposed in <ref type="bibr" target="#b38">[38]</ref> to learn low intraclass and high interclass features via a twobranch network, supervised by a margin ranking loss. The deformable HSI classification networks (DHCNet) method <ref type="bibr" target="#b29">[30]</ref> uses an adaptive spatial context modeling method to capture the complex spatial context in the HSIs and boost the performance. However, the overfitting issue gradually emerges as the model complexity increases. To alleviate this issue, Gabor-CNN <ref type="bibr" target="#b39">[39]</ref> combines Gabor filters with convolutional filters to reduce the feature extraction burden of the CNN. The deep feature fusion network (DFFN) was proposed in <ref type="bibr" target="#b40">[40]</ref> as a multi-layer feature fusion method that adopts residual learning to mitigate the overfitting brought by the introduction of more convolutional layers, significantly improving the classification accuracy for HSIs. Although these patch-based methods have achieved remarkable HSI classification accuracies, obtaining a fast inference speed remains a challenge, which limits the further application of HSI classifiers. The main reason for this is the redundant computation in the overlapping areas between patches, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The pixels in the gray area will take part in multiple computations since these pixels are the neighbors of multiple central pixels. Although the DMS 3 FE-classifier <ref type="bibr" target="#b41">[41]</ref> utilizes a pretrained FCN <ref type="bibr" target="#b43">[42]</ref> to extract features, its own FCN is not trained. A convolution-deconvolution (conv-deconv) network with an optimized extreme learning machine (ELM) method was proposed in <ref type="bibr" target="#b44">[43]</ref> for HSI classification with an FCN, but the method is not an end-to-end classifier.</p><p>Neither of these methods are end-to-end trainable FCNs since they only utilize the FCN to extract features, and apply separate classifiers to label the pixels, which means that the whole pipeline cannot be globally optimized and the global spatial information cannot be exploited sufficiently. To overcome the aforementioned issues, we propose the FPGA framework for HSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FPGA: FAST PATCH-FREE GLOBAL LEARNING</head><p>FRAMEWORK FOR HSI CLASSIFICATION We investigated the main speed bottleneck of the patchbased methods and concluded that the redundant computation on the highly overlapping areas between patches is the central cause. To address this issue, we propose a fast patchfree global learning (FPGA) framework and a variant of an FCN (FreeNet) as a fundamental classification model in the FPGA framework through sharing computation in the spatial dimension.</p><formula xml:id="formula_0">R R  H W  Decoder Encoder ij P full image segmentation * : H W H W f R R    image area receptive field area i j S S  R R  H W  Encoder ij P patch classification : S S f R R  </formula><p>patch area image area receptive field area i j <ref type="figure">Fig. 3</ref>. The patch-free global learning framework for HSI classification. For simplification, the mapping only considers the spatial dimension.</p><p>The FPGA framework aims to learn a mapping f * : R H×W → R H×W for full image segmentation, as shown in <ref type="figure">Fig. 3</ref>. In the FPGA framework, there are three core components: 1) the GS 2 sampler; 2) the encoder-decoder based FCN; and 3) the lateral connections. The GS 2 sampler ensures the convergence of the end-to-end trained FCN based model, and the encoder-decoder based FCN is responsible for oneshot forward computation by sharing the computation in the spatial dimension. The lateral connections are designed to effectively fuse the spatial detail features in the encoder and the semantic features in the decoder. The model architecture follows the classical encoder-decoder framework <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b45">[44]</ref><ref type="bibr" target="#b46">[45]</ref><ref type="bibr" target="#b47">[46]</ref> with semantic-spatial fusion (SSF). The encoder is used to transform spatially finer features to semantically stronger features by progressively learning higher-dimensional feature embedding. The decoder is used to recover the spatial information of the semantic features with the high-dimension feature embedding learned by the encoder for the full classification map. The lateral connection based SSF forwards spatially finer features from the encoder to the decoder, which is beneficial for the recovery of the spatial details of the semantic feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Patch-Free Global Learning</head><p>The core idea of patch-free global learning is to replace the explicit patching with the implicit receptive field of the model, to avoid redundant computation on the overlapping areas and obtain a wider latent spatial context.</p><p>Given an HSI X ∈ R C×H×W , the predicted probability cubeŶ i ∈ R #class×H×W is formulated as:</p><formula xml:id="formula_1">Y i = f * (X)<label>(1)</label></formula><p>full hyperspectral image training samples where the mapping f * : R C×H×W → R #class×H×W is modeled as the classifier without patching, and C is the number of bands of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global stochastic stratified sampler</head><formula xml:id="formula_2">Conv3×3-GN-ReLU Conv3×3, stride 2 Spectral Attention Conv3×3-GN-ReLU Conv3×3, stride 2 Spectral Attention Conv3×3-GN-ReLU Conv3×3, stride 2 Spectral Attention Conv3×3-GN-ReLU Spectral Attention Conv3×3-GN-ReLU Conv3×3-Up2× Semantic-Spatial Fusion Conv3×3-Up2× Semantic-Spatial Fusion Conv3×3-Up2× Semantic-Spatial Fusion Conv3×3 Conv1×1 Encoder Decoder Lateral connectionsˆi Y Y X ( ) ( , ) i i k i Loss Y Y W  ( , ) i i Loss Y Y FreeNet training backward</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPGA： Fast Patch-free Global Learning Framework Input</head><p>Stochastic gradient descent (SGD) <ref type="bibr" target="#b48">[47]</ref> is used to minimize the classification loss l (e.g., cross-entropy loss) over the sampled positions R. The sampling algorithm is described in Section. III-B.</p><p>For the i-th iteration, the k-th weight of the model can be updated as follows:</p><formula xml:id="formula_3">W (k) i+1 = W (k) i − η 1 n p∈Ri ∂l( Y i (p),Ŷ i (p)) ∂W (k) i<label>(2)</label></formula><p>where p is the 2-D spatial position in R i , n = |R i |, η is the learning rate and Y i is the sampled ground truth map. The main difference with patch-based local learning is that all the pixels take part in the forward computation during the training, but only the sampled position can obtain supervised signals for every iteration. In this way, the model inference is consistent during the training and testing, which are both oneshot forward computations. The one-shot forward computation significantly boosts the speed of the model inference for HSI classification. Meanwhile, the patch-free global learning allows the model to leverage the spatial context as much as possible. It thus provides more potential to boost the accuracy of the model by following the patch-free global learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Stochastic Stratified Sampling Strategy</head><p>The global stochastic stratified (GS 2 ) sampling strategy is proposed to ensure the convergence of the end-to-end trained FCN based model. The GS 2 sampling strategy is formally described in Algorithm 1. The key idea of the GS 2 sampler is to transform all the training samples into a stochastic sequence of stratified samples. In this way, the GS 2 sampler ensures the class-balanced distribution of the training samples and simulates the behavior of the mini-batch sampler to obtain</p><formula xml:id="formula_4">Algorithm 1: Global Stochastic Stratified Sampling Input: G = {g i } M i=1</formula><p>: a set of labels for training N : the number of classes α: mini-batch per class Output: T : a list of sets of stratified labels</p><formula xml:id="formula_5">R ← [] // an empty list for k = 0 to N do I k ← {j|g j = k, g j ∈ G} I k ← shuffle(I k ) R[k] ← [] while |I k | &gt; α do fetch α samples from I k , t ← I k .pop(α) R[k].push(t) end R[k].push(I k ) end T ← [] c ← 0 while any(R[i] &gt; 0, i = 1, 2, ..., N − 1) do T [c] ← ∅ for k = 0 to N do if |R[k]| &gt; 0 then fetch 1 element from R[k], t k = R[k].pop(1) T [c] ← T [c] ∩ t k end end T.push(T [c]) c ← c + 1 end</formula><p>stable yet diverse gradients. During the training, this sequence is randomly shuffled to ensure stochasticity of the gradients, to prevent overfitting.</p><p>Firstly, in more detail, we split all the training samples to obtain a list R, where the index of R is the class label, and the element is the training samples of each class. During the splitting, the order of the training samples for each class needs to be shuffled to keep the stochasticity of the combination. Stratification is then performed on the training samples of each class to obtain T , which is a list of the sets of stratified labels. In this sampling strategy, the hyperparameter α (the minibatch per class) is introduced, which is of great significance for the training stability. The smaller the value of α, the greater the number of gradient orientations that can be obtained when optimizing the network, which makes it possible to train an FCN using limited training samples for HSI classification. A more detailed analysis of parameter α is provided in Section V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FreeNet in FPGA</head><p>FreeNet is a simple, unified network made up of an encoder network and a decoder network. The encoder is responsible for computing the hierarchical convolutional feature maps over an entire input HSI. The decoder recovers the spatial dimension of the coarsest convolutional feature map progressively via lateral connection based SSF, outputting a classification probability map of the same spatial size as the input image. To improve the FreeNet compactness, we introduce a compression factor (β) to control the number of feature maps in the whole network, achieving a trade-off between speed and accuracy. FreeNet is a lightweight FCN designed for faster and more accurate HSI classification, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Each component of FreeNet is described in the following.</p><p>1) Encoder Network Architecture: The encoder network follows a modular design, made up of a stem block and four hybrid blocks, all of which contain the basic module. The basic module of the encoder network is a 3×3 convolutional layer followed by group normalization <ref type="bibr" target="#b49">[48]</ref> and rectified linear unit (ReLU) activation. Under the FPGA framework, the batch size is always equal to 1, and the iterative inputs are the same image, because of the entire HSI being used as input. In this case, the error of the batch normalization (BN) increases rapidly due to the inaccurate batch statistics estimation. Therefore, we adopt group normalization (GN) as an alternative to BN, which is independent of batch size and can obtain a comparable performance to BN.</p><p>Due to the different numbers of bands for HSIs, we first introduce a stem block to transform the variable channels of the input to a fixed 64 channels. The stem block is simply implemented by a basic module. The four hybrid blocks share the same network topology, unless otherwise specified. The hybrid block is made up of a spectral attention module, as described in Section. III-C2, a basic module and an optional downsampling module. For the downsampling module, we use a 3×3 convolutional layer with a stride of 2 followed by ReLU activation to replace the commonly used 3×3 maxpool with a stride of 2 to align the projected spatial location with its receptive field center for more robust HSI classification. Block#1 ∼ #3 are the hybrid blocks with downsampling modules and block#4 is the block without a downsampling module. 2) Spectral Attention: Spectral attention models the interdependencies of the feature maps with the global spatial context. The encode function is simply implemented by 3 × 3 convolution. This module reweights the feature maps via global context guiding and highlights the more important feature maps to improve the accuracy of the model. The spectral attention is an in-place module where there is no dimension change between the input and output, which is a similar implementation to the SE-Block <ref type="bibr" target="#b50">[49]</ref>. Given the input tensor X ∈ R C×H×W , we first compute the global context</p><formula xml:id="formula_6">Spectral Attention Conv3×3-GN-ReLU Conv3×3, stride 2 Conv3×3-GN-ReLU Spectral Attention Conv3×3-GN-ReLU Conv3×3, stride 2 Spectral Attention Conv3×3-GN-ReLU Conv3×3, stride 2 Spectral Attention Conv3×3-GN-ReLU Conv 1×1 Conv 1×1 Conv 1×1 Conv 1×1 Conv 3×3 Conv 3×3 Up2x Conv 3×3 Conv 3×3 Up2x Up2x Conv 1×1 C H W X R    #class H W Y R   </formula><formula xml:id="formula_7">C H W C H W X Y 1 1 C   1 1 C   S T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral Attention</head><formula xml:id="formula_8">embedding vector S ∈ R C×1×1 , where S(k, :, :) = 1 H · W H i=1 W j=1 X(k, i, j)<label>(3)</label></formula><p>where k is the index of the channel dimension. In order to model the interdependencies between feature maps, we use two thin fully connected layers (nonlinear transformation), followed by a sigmoid gating function to compute the channel scaling coefficient vector</p><formula xml:id="formula_9">T ∈ R C×1×1 , where T = sigmoid(W 2 δ(W 1 S))<label>(4)</label></formula><p>where δ denotes the ReLU function, W 1 ∈ R C r ×C , and W 2 ∈ R C× C r . The reduction ratio r is introduced to balance the capacity of the model and the computational cost. We find r = 16 works well in practice, so this setting was used in most of the experiments. The output Y ∈ R C×H×W of the spectral attention module is simply computed by:</p><formula xml:id="formula_10">Y (k, i, j) = T (k, :, :) · X(k, i, j)<label>(5)</label></formula><p>3) Decoder Network Architecture: The decoder network also follows a modular design, for simplicity, which is made up of a refinement module for progressive spatial feature refinement and a head subnetwork for pixel classification. The refinement module contains multiple refinement stages, which are simply implemented by stacking the upsampling modules and inserting SSF after each upsampling module. The progressive refinement first upsamples the input feature maps with stronger semantic information and then aggregates the feature map with finer spatial information from the encoder to recover the spatial details of the input. The upsampling module is a 3×3 convolutional layer followed by nearest neighbor upsampling with a factor of 2. The SSF receives two features from the blocks in the encoder and decoder, respectively, and aggregates the features into a new enhanced feature that is forwarded to the next upsampling module. The head subnetwork is used to perform pixel classification with the feature from the top layer of the decoder, and is made up of a 3×3 convolutional layer followed by a 1×1 convolutional layer with N filters. N is the number of categories. 4) Lateral Connection Based SSF: Lateral connection based SSF leverages the spatial detail features of the shallow convolutional layers to enhance the deep semantic features of the convolutional layers and boost the performance. The lateral connection is implemented by a 1×1 convolutional layer, which passes more precise locations of features from the encoder to the decoder. The aggregation function is pointwise addition. Lateral connection based SSF can be formulated as follows:</p><formula xml:id="formula_11">q i+1 = q i + conv(p 4−i ), i = 1, 2, 3<label>(6)</label></formula><p>where q i is the feature map of refinement stage#i in the decoder, and p 4−i is the feature map of block#4 − i in the encoder. q i+1 is the output of SSF, which is forwarded to the next block in the decoder. The design of the lateral connection based SSF follows that of residual learning <ref type="bibr" target="#b51">[50]</ref>. The first item q i is a baseline item obtained by nearest neighbor interpolation and conv(p 4−i ) is the residual item that needs to be learned.</p><p>In this case, the gradients are lossless to flow into the shallow layers, making the optimization easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fully End-to-End HSI Classification Using FreeNet in FPGA</head><p>After convergence of the trained FreeNet, HSI classification can be implemented by one-shot forward computation using FreeNet in FPGA. Compared with patchwise classification for HSIs, FreeNet can perform faster patch-free inference over the whole HSI through sharing the computation in the spatial dimension. Due to the introduction of three 2× upsampling blocks in FreeNet, the input size in the spatial dimension of the HSI should be multiples of 2 3 = 8. To ensure that the raw HSI is unchanged, a "padding-crop" trick is used for the inference, which pads the original input with zeros into a new size with multiples of 8 before the inference. After the inference, the final classification map is obtained by cropping the output using a box with the original input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>Extensive experiments were conducted to validate the effectiveness of the proposed method on three benchmark datasets: the ROSIS-03 Pavia University dataset, the Salinas dataset and the Compact Airborne Spectrographic Imager (CASI) University of Houston dataset. Four patch-based HSI classifiers were compared with the proposed FreeNet and its variants. The patch-based classifiers used in the comparison were classical SVM <ref type="bibr" target="#b7">[8]</ref> and four state-of-the-art deep learning based methods (S-CNN <ref type="bibr" target="#b38">[38]</ref>, Gabor-CNN <ref type="bibr" target="#b39">[39]</ref>, DFFN <ref type="bibr" target="#b40">[40]</ref>, 3D-GAN [51]), following <ref type="bibr" target="#b2">[3]</ref>. All the experiments were performed with an NVIDIA Tesla P100 GPU accelerator (with 16GB GPU memory).</p><p>A. Experimental Settings 1) Network Architecture: The hyperparameter settings of the standard FreeNet, namely FreeNet (β = 1.0), such as the output channels of the layers and the reduction ratio r in the spectral attention module, are listed in <ref type="table">Table.</ref> I. For a fair comparison, this architecture setting was used for all three benchmark datasets, and there was no specific tuning for the dataset.</p><p>2) Optimization: For all the experiments, the patch-free methods were trained for 1k iterations using SGD with a "poly" learning rate policy, where the initial learning rate was set to 0.0001 and multiplied by (1 − iter max iter ) power with power = 0.9. The momentum was set to 0.9 and the weight decay was set to 0.0001. We did not use any data augmentation strategy. Unless otherwise specified, α was set to 20 for the GS 2 sampler.</p><p>3) Metrics: To evaluate the performance of the proposed methods, four common metrics are adopted, which are the accuracy of each class, the overall accuracy (OA), the average accuracy (AA), and the Kappa coefficient (Kappa).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1: ROSIS-03 Pavia University Dataset</head><p>The HSI of this dataset contains 610 × 340 pixels and 103 spectral bands with a spatial resolution of 1.3 m per pixel. This dataset contains nine urban land-cover types. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the false-color composite of the image and the corresponding ground truth.   each class. The training samples were randomly chosen from the ground truth with a fixed random seed and the remaining samples were used to evaluate the accuracy. For the HSI classification task, the visual performance is of importance for the classifier. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the classification maps of the compared methods. As can be seen in <ref type="figure" target="#fig_5">Fig. 7 (b</ref>)-(f), the CNN based classifiers have a better visual performance than the classical SVM classifier due to the strongly discriminative deep features. It is clear that the patch-free method is superior to the patch-based methods in spatial detail when comparing <ref type="figure" target="#fig_5">Fig. 7</ref> (f) and <ref type="figure" target="#fig_5">Fig. 7</ref> (b)-(e). This can be attributed to the introduction of more global spatial context. Among the different methods, the edges of the classification map of FreeNet are smoother than those of the other methods. This indicates that FreeNet can capture finer spatial detail by the lateral connection based SSF and the better design of decoder structure. <ref type="table">Table.</ref> III lists the results of the state-of-the-art patchbased methods and the proposed patch-free methods. When constraining the number of training samples (200 samples for each class), DFFN achieves the best accuracy (OA of 98.57%, AA of 99.16%, and Kappa of 0.9808) among the patch-based methods. However, the patch-free method obtains more accurate results than DFFN. FreeNet obtains a higher OA of 99.81%, exceeding DFFN by ∼1%. Meanwhile, we can observe that the accuracies for each class with the patchfree method are higher than those of the patch-based methods. In fact, the accuracy of the patch-free method has reached saturation. This suggests that the patch-free global learning framework is superior to the patch-based local learning framework with this benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 2: Salinas Dataset</head><p>To further evaluate the effectiveness of the FPGA framework, we also performed experiments on the Salinas dataset. The HSI of Salinas dataset has 512 × 217 pixels and 204 spectral bands with a spatial resolution of 3.7 m. The groundtruth map covers 16 classes of interest. <ref type="figure" target="#fig_7">Fig. 9</ref> shows the three-band false-color composite image and the corresponding ground-truth map. The numbers of training and test samples are listed in <ref type="table">Table.</ref> IV. The selection of samples was random selection with the same random seed as used in Experiment 1. <ref type="figure" target="#fig_8">Fig. 10</ref> shows the visual performance of the different methods. We can observe that the classification maps in <ref type="figure" target="#fig_8">Fig. 10(d)</ref>-  (f) (DFFN, 3D-GAN and FreeNet) are better than those in <ref type="figure" target="#fig_8">Fig. 10 (a)</ref>-(c). Although DFFN and FreeNet obtain a similar accuracy, as shown in <ref type="table">Table.</ref> V, the result of FreeNet contains less noise in the classification map, achieving a better visual performance. This suggests that the global spatial information is important for HSI classification. The results of the CNNbased methods show the over-smoothing problem, whereas, surprisingly, the SVM method can obtain sharper edges. We speculate that this is because these methods model the spatial context information. This causes the label of a pixel to be not only dependent on the center pixel, but also the neighboring pixels. Thus, the pixels near the edges of objects usually have different labels but a highly similar spatial context, which   <ref type="table">Table.</ref> V lists the accuracies of the patch-based methods and the patch-free method, where it can be seen that FreeNet performs much better than most of the patch-based methods in terms off OA, AA, and Kappa. Compared to the stateof-the-art patch-based method of DFFN, FreeNet obtains a slightly higher OA of 99.92%, exceeding DFFN by 0.2%. The proposed FreeNet achieves the highest accuracy among all the methods, slightly surpassing DFFN in all of the metrics. FreeNet benefits from the proposed semantic-spatial feature fusion module (lateral based SSF), which is based on residual learning. Furthermore, FreeNet fuses the features from the shallow layer in the encoder and the deep layer in the decoder to achieve the fusion of semantic information and spatial details. It is interesting that the lateral based SSF in FreeNet and the multi-layer fusion in DFFN both belong to cross-layer feature fusion. This indicates that cross-layer feature fusion is an important component of HSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3: CASI University of Houston Dataset</head><p>The proposed patch-free method achieved saturation on the ROSIS-03 Pavia University dataset and the Salinas dataset. Although the proposed FreeNet surpasses the state-of-theart methods on these two simple benchmark datasets, the performance difference is fairly small due to the saturation of the accuracy. Therefore, we chose the CASI University of Houston dataset for Experiment 3, which is a relatively difficult benchmark dataset, to clearly present the performance difference.</p><p>The CASI University of Houston dataset was published as part of the 2013 IEEE Geoscience and Remote Sensing Society (GRSS) data fusion contest. The HSI has 349 × 1905 pixels with 144 spectral bands and a spatial resolution of 2.5 m, the wavelength of which ranges from 0.38 to 1.05 µm. <ref type="figure" target="#fig_9">Fig. 11 (a)</ref> shows the false-color composite of the hyperspectral image. The dataset provides a ground-truth map of 15 classes, as shown in <ref type="figure" target="#fig_9">Fig. 11 (b)</ref> and (c). The corresponding legend is shown in <ref type="figure" target="#fig_9">Fig. 11 (d)</ref>. <ref type="table">Table.</ref> VI lists the number of training and test samples per class. Differing from the first two datasets, the CASI University of Houston dataset provides officially predefined training and test samples. Thus, the results obtained with this benchmark dataset can be considered as more reliable and stable. <ref type="figure" target="#fig_1">Fig. 12</ref> shows the classification map of the compared methods for the visual performance estimation. It can be clearly observed that the maps in <ref type="figure" target="#fig_1">Fig. 12</ref> (c)-(f) are clearer and contain less noise than those in <ref type="figure" target="#fig_1">Fig. 12 (a)-(b)</ref>. For the Road, Highway, and Railway classes, these three classes in the classification map of FreeNet show better connectivity. Meanwhile, the accuracy for these three classes is higher than  for the other methods as shown in <ref type="table">Table.</ref> VII C9-C11. We speculate that the increased spatial context of these classes enhances the discriminative ability for each class. <ref type="table">Table.</ref> VII lists the classification results obtained on this dataset. FreeNet achieves a state-of-the-art result and achieves a ∼2% improvement over the best result (DFFN) of the patch-based methods. This suggests that the patch-free global learning framework performs even better than the patch-based local learning framework. FreeNet benefits from the increased global spatial information, including the global spatial context and spatial details. By replacing the patch with a larger receptive field, the model under the patch-free global learning framework is able to obtain more global spatial context, which boosts the classification performance. FreeNet makes full use of the global spatial context via the spectral attention module and the lateral connection based SSF. The spectral attention module utilizes the global spatial context embedding vector to re-weight the feature maps for modeling the interdependencies between feature maps, which aims to estimate the importance of the different feature maps by the global spatial context. This benefits the classification of HSIs with redundant spectral information. Furthermore, the lateral connection based SSF leverages the finer spatial detail features to refine semantically stronger but spatially coarser deep features, which are aggregated to enhanced features with finer spatial details and stronger semantic information.</p><p>The accuracy obtained on this dataset is clearly lower than the accuracies obtained on the other two datasets using the same method. To explore the reason for this, we drew the confusion matrix for FreeNet on this dataset for a quantitative analysis, as shown in <ref type="figure">Fig. 13</ref>. We can see that the other categories tend to be wrongly classified as Water. For example, a large amount of GrassHealthy, Commercial, and Railway pixels are wrongly classified as Water. This is because these three categories are distributed in the shadow area of the HSI, as shown in <ref type="figure" target="#fig_9">Fig. 11 (a)</ref>. The shadow significantly influences the spectral information, which misleads the classification. Qualitatively, from the visualization result ( <ref type="figure" target="#fig_1">Fig. 12 (e)</ref>), the shadow area is occupied by water. This indicates that the spectral information is sensitive to the observation conditions, which haves a great impact on the qualitative and quantitative performance of HSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FPGA SENSITIVITY ANALYSIS</head><p>To clearly understand the effectiveness of each component in the proposed FreeNet under the patch-free global learning framework, we conducted extensive module analysis experiments. All the component analysis experiments were performed on the CASI University of Houston dataset as this dataset has official training and test samples. We chose the CASI University of Houston dataset for more stable, reliable and reproducible analysis results. The baseline methods shown in <ref type="table">Table.</ref>VIII (a) are encoder-decoder FCNs with β = 0.75 and 1.0, respectively, which were trained by directly using all the training samples, without any sampling strategy.</p><p>A. The GS 2 Sampling Strategy <ref type="table">Table.</ref> VIII (b) presents the results of the baseline methods with the GS 2 sampling strategy. The results indicate that the GS 2 sampling strategy improves the OA for both β = 0.75 (from 15.23% to 65.12%) and β = 1.0 (from 12.49% to 65.16%). A model with an OA of ∼10% on this dataset means that this model fails to converge. The GS 2 sampling strategy effectively addresses this issue for the baseline encoderdecoder FCN. The GS 2 sampling strategy splits the original training samples into many mini-batch training samples with the same spatial size to obtain diverse gradients and partially supervised signals. These diverse gradients and partially supervised signals make it easier to skip local minimum points. This suggests that it is very important for the end-to-end trainable FCNs used in HSI classification to obtain more diverse gradients.</p><p>The hyperparameter α is introduced in the GS 2 sampling strategy to control the number of samples in the mini-batch per class. The classification results of FreeNet (β = 1.0) with different value of α from 10 to 200 with an interval of 10 are shown in <ref type="figure" target="#fig_2">Fig. 14.</ref> We can observe that the proposed FreeNet shows a stable performance when α is set to a value within 30% of the number of total training samples. This indicates the robustness of the GS 2 sampling strategy with respect to α when the stochasticity is sufficient. When α is set to a value larger than 55% of the number of total training samples, the classification accuracy of FreeNet drops rapidly, to even lower than the SVM baseline. The reason for this is that α indirectly controls the stochasticity of the sampling. A smaller α means greater stochasticity of the sampling. Meanwhile, the stochasticity of the sampling directly influences the diversity of the gradients. Therefore, FreeNet with a smaller α always obtains a higher accuracy, while a larger α brings a worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lateral Connection Based SSF</head><p>Lateral connection based SSF is described in Section III-C4. <ref type="table">Table.</ref> VIII (c) presents the effectiveness of the lateral connection based SSF. When applying FreeNet without the lateral connection (LC) and spectral attention (SA) modules ( <ref type="table">Table.</ref>  <ref type="figure">VIII (b)</ref>), the classification performance shows a significant reduction. The addition of the lateral connection and  spectral attention modules to FreeNet (β = 0.75) results in an OA improvement from 65.12% to 84.23%, and The addition of the lateral connection and spectral attention modules to FreeNet (β = 1.0) results in an OA improvement from 65.16% to 84.91%, achieving a similar performance to the state-of-the-art patch-based HSI classifiers. With the help of the lateral connection based SSF, the spatial detail features of the shallow convolutional layers can be passed on to the decoder to progressively refine the spatial detail of the deep semantic features, thus obtaining spatially finer and semantically stronger fused features. Meanwhile, the aggregation function of pointwise addition can alleviate the gradient vanishing problem, making the optimization easier. This suggests that lateral connection based SSF is important for HSI classification when using an encoder-decoder architecture under the patch-free global learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spectral Attention</head><p>Table. VIII (d) presents the effectiveness of the spectral attention module. Based on FreeNet without the spectral attention module <ref type="table">(Table.</ref> VIII (c)), the spectral attention module brings an additional improvement to FreeNet (β = 0.75) (84.23% to 85.49) and FreeNet (β = 1.0) (84.91% to 86.61%). The spectral attention module models the interdependencies of the feature maps in the encoder of FreeNet via the global spatial context, boosting the classification performance. This indicates that it is valuable to further exploit the raw redundant spectral features guided by spatial context for HSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Complexity and Inference Speed Analysis</head><p>For a fair comparison of the model complexity and inference speed between the patch-based and patch-free methods, we directly used the encoder of FreeNet followed by a 1 × 1 convolutional layer as the classifier, to represent the patchbased method. Three variants (β = 0.5, 0.75 and 1.0) of FreeNet and the corresponding encoders were used as a comparison. During the inference, the batch size of the patchbased methods was set to 1024 for parallel computation. The image used for the benchmarking was from the CASI University of Houston dataset, which was converted to a 3-D float32 tensor of shape <ref type="bibr">[349,</ref><ref type="bibr">1905,</ref><ref type="bibr">144]</ref>.</p><p>We adopted the number of parameters (# params) to measure the model complexity and the giga floating-point operations per second (GFLOPs) to measure the theoretical computational overhead. Meanwhile, the GPU time cost was used to measure the actual efficiency of the models. <ref type="table">Table.</ref> IX lists the measured results. The results suggest that the patch-free methods are much faster than the patch-based methods, in both theory and practice, by avoiding redundant computation on the overlapping area. Note that the practical speedup ratio (∼560) is always larger than the theoretical speedup ratio (∼480) because we ignore the influence of the parallel computation. Although the patch-based methods  have fewer parameters than the patch-free methods, the actual computation of the patch-based methods is slower than that of the patch-free methods. Therefore, for real-time applications, such as HSI classification on unmanned aerial vehicle (UAV) or satellite imagery, the patch-free method is a better proposal than the patch-based method. We also list the detailed running time costs of FreeNet for each dataset in <ref type="table">Table.</ref> X. It can be seen that the inference time cost is much smaller than the training time cost for each HSI. This suggests that FreeNet is suitable for the application scenarios which allow offline learning and online inference, such as training FreeNet on ground station data and performing inference on UAV or satellite data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Impact of the Number of Training Samples</head><p>To study the impact of the number of training samples on FreeNet, we conducted extensive experiments on the Pavia University, Salinas, and CASI University of Houston benchmark datasets. The results are plotted in <ref type="figure" target="#fig_3">Fig. 15</ref>. Overall, reducing the training samples results in a drop in the performance of FreeNet. For the Pavia University dataset and Salinas dataset, the decreases in OA, AA, and Kappa are relatively small when using 1% ∼ 3% of the labeled samples. It suggests that FreeNet is also robust, only using limited training sample number. In order to further explore the effectiveness of FreeNet, we also trained FreeNet using only 10 samples per class. Compared with the Salinas dataset, the decrease in performance on the Pavia dataset is much more significant, at 14.82% of OA, 8.99% of AA, and 0.19 of Kappa. Meanwhile the decrease in accuracy for the Salinas dataset is 4.3% of OA, 1.74% of AA, and 0.048 of Kappa. This indicates that classification on the Salinas dataset with FreeNet is easier than for the Pavia University dataset We speculate that FreeNet benefits from more spectral information via the spectral attention module. This is because the spatial resolution of these two datasets is high, so that the contribution of the spatial information to the performance has achieved saturation. However the Salinas dataset has more bands, which may be the core reason for the easier classification. For the CASI University of Houston dataset, the performance is steady until the training samples are reduced to 5% of the labeled samples. This dataset has fewer labeled samples than the other two datasets. Thus, the entries with a high percentage still have minimal training samples, which causes the performance to drop rapidly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed a fast patch-free global learning (FPGA) framework, pushing HSI classification further on both speed and accuracy. In the FPGA framework, the GS 2 sampling strategy is proposed to ensure encoderdecoder based FCN training convergence by transforming the entire training samples into a stochastic sequence of classstratified samples, to obtain stable and diverse gradients. After ensuring the convergence of the training, FreeNet is proposed, which is a simple and unified encoder-decoder based FCN. FreeNet directly inputs the entire HSI without requiring any dimension reduction and outputs the classification map. Therefore, FreeNet is a fully end-to-end trainable HSI classifier that does not require dimension reduction or any post-processing technology. FreeNet avoids the redundant computation on the overlapping areas between patches, which significantly boosts its inference speed. To maximize the exploitation of the global spatial context and details, a spectral attention module and lateral connection based SSF are proposed. The spectral attention module models the interdependencies of the feature maps guided by the global spatial context to effectively boost the performance of FreeNet. The lateral connection based SSF progressively refines the semantic features with the global spatial detail of the features from the shallow layers. Meanwhile, lateral connection based SSF follows the residual learning approach to fuse the features by pointwise addition, which can alleviate the gradient vanishing problem, thereby, significantly improving the performance of FreeNet.</p><p>In the future, we will further explore memory-efficient HSI classifiers, which will be important for the real-time classification of HSIs from satellite and airborne platforms. We hope that the proposed method will serve as a strong baseline and aid future research in HSI classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Index Terms-Patch-free global learning, fully convolutional network, feature fusion, hyperspectral image classification I. INTRODUCTION This work was supported by National Key Research and Development Program of China under Grant No. 2017YFB0504202, National Natural Science Foundation of China under Grant Nos. 41771385; and the National Natural Science Foundation of China under Grant NO. 41801267, in part by the China Postdoctoral Science Foundation under Grant 2017M622522. (Corresponding authors: Yanfei Zhong, Ailong Ma) The authors are with the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China, with the Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan 430079, China. (e-mail: zhengzhuo@whu.edu.cn; zhongyanfei@whu.edu.cn; maailong007@whu.edu.cn; zlp62@whu.edu.cn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overlap of 5×5 patches under the patch-based local learning framework. The gray cell presents the overlap area. P i,j and P i+1,j+1 are the central pixels of the two patches with red and yellow borders, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The overview of the fast patch-free global learning framework. The FPGA framework includes three core components: the GS 2 sampler, the encoderdecoder based FCN and lateral connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The FreeNet network architecture designed for HSI classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The network architecture of the spectral attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of the classification maps for the ROSIS-03 Pavia University dataset. (a) SVM. (b) S-CNN. (c) Gabor-CNN. (d) DFFN. (e) 3D-GAN. (f) FreeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The ROSIS-03 Pavia University dataset. (a) Three-band false color composite. (b) Ground-truth map (c) Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>The Salinas dataset. (a) Three-band false color composite. (b) Ground truth map (c) Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of the classification maps for the Salinas dataset. (a) SVM. (b) S-CNN. (c) Gabor-CNN. (d) DFFN. (e) 3D-GAN. (f) FreeNet. results in the over-smoothing problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>The CASI University of Houston data set. (a) Color composite representation of the hyperspectral data using bands of 70, 50, and 20, as red, green, and blue, respectively; (b) Training samples; (c) Test samples; (d) Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of the classification maps for the CASI University of Houston dataset. (a) SVM. (b) S-CNN. (c) Gabor-CNN. (d) DFFN. (e) 3D-GAN. (f) FreeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Sensitivity of the mini-batch size per class (α) in the GS 2 sampling strategy on the CASI University of Houston dataset. (a) The impact on the OA of different α settings. (b) the impact on the AA of different α settings. (c) the impact on the Kappa of different α settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell>H W </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R R </cell><cell></cell><cell cols="2">full image segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>j</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>i</cell><cell>ij P</cell><cell>Encoder</cell><cell>Decoder</cell><cell>: H W f R </cell><cell></cell><cell>R</cell><cell>H W </cell></row><row><cell cols="2">image area</cell><cell cols="2">receptive field area</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H W </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R R </cell><cell></cell><cell cols="2">patch classification</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S S </cell><cell>j</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Encoder receptive field area Fig. 1. The patch-based local learning framework for HSI classification. For ij P f R R   patch area image area i : S S simplification, the mapping considers only the spatial dimension.</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>CONFIGURATION DETAILS OF THE STANDARD FREENET (FREENET</figDesc><table><row><cell></cell><cell cols="2">FreeNet (β = 1.0)</cell></row><row><cell></cell><cell>stem</cell><cell>3×3 conv, 64</cell></row><row><cell></cell><cell></cell><cell>spectral attention, r = 16</cell></row><row><cell></cell><cell>block#1</cell><cell>3×3 conv, 64</cell></row><row><cell></cell><cell></cell><cell>3×3 conv, 128, stride 2</cell></row><row><cell></cell><cell></cell><cell>spectral attention, r = 16</cell></row><row><cell>Encoder</cell><cell>block#2</cell><cell>3×3 conv, 128</cell></row><row><cell></cell><cell></cell><cell>3×3 conv, 192, stride 2</cell></row><row><cell></cell><cell></cell><cell>spectral attention, r = 16</cell></row><row><cell></cell><cell>block#3</cell><cell>3×3 conv, 192</cell></row><row><cell></cell><cell></cell><cell>3×3 conv, 256, stride 2</cell></row><row><cell></cell><cell>block#4</cell><cell>spectral attention, r = 16</cell></row><row><cell></cell><cell></cell><cell>3×3 conv, 256</cell></row><row><cell></cell><cell>lateral 4-1</cell><cell>1×1 conv, 128</cell></row><row><cell>Lateral</cell><cell>lateral 3-1</cell><cell>1×1 conv, 128</cell></row><row><cell></cell><cell>lateral 2-2</cell><cell>1×1 conv, 128</cell></row><row><cell></cell><cell>lateral 1-3</cell><cell>1×1 conv, 128</cell></row><row><cell></cell><cell>block#1</cell><cell>3×3 conv, 128</cell></row><row><cell></cell><cell></cell><cell>upsample, 2</cell></row><row><cell></cell><cell>block#2</cell><cell>3×3 conv, 128</cell></row><row><cell>Decoder</cell><cell></cell><cell>upsample, 2</cell></row><row><cell></cell><cell>block#3</cell><cell>3×3 conv, 128</cell></row><row><cell></cell><cell></cell><cell>upsample, 2</cell></row><row><cell></cell><cell>head</cell><cell>3×3 conv, 128</cell></row><row><cell></cell><cell></cell><cell>1×1 conv, N</cell></row></table><note>WITH β = 1.0).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THE</head><label>II</label><figDesc></figDesc><table><row><cell cols="5">NUMBER OF TRAINING SAMPLES AND TEST SAMPLES FOR THE</cell></row><row><cell></cell><cell cols="3">ROSIS-03 PAVIA UNIVERSITY DATASET</cell><cell></cell></row><row><cell>Class</cell><cell>Class name</cell><cell>#Training</cell><cell>#Test</cell><cell>#Total</cell></row><row><cell>C1</cell><cell>Asphalt</cell><cell>200</cell><cell>6431</cell><cell>6631</cell></row><row><cell>C2</cell><cell>Meadows</cell><cell>200</cell><cell>18449</cell><cell>18649</cell></row><row><cell>C3</cell><cell>Gravel</cell><cell>200</cell><cell>1899</cell><cell>2099</cell></row><row><cell>C4</cell><cell>Trees</cell><cell>200</cell><cell>2864</cell><cell>3064</cell></row><row><cell>C5</cell><cell>Metal Sheets</cell><cell>200</cell><cell>1145</cell><cell>1345</cell></row><row><cell>C6</cell><cell>Bare Soil</cell><cell>200</cell><cell>4829</cell><cell>5029</cell></row><row><cell>C7</cell><cell>Bitumem</cell><cell>200</cell><cell>1130</cell><cell>1330</cell></row><row><cell>C8</cell><cell>Bricks</cell><cell>200</cell><cell>3482</cell><cell>3682</cell></row><row><cell>C9</cell><cell>Shadow</cell><cell>200</cell><cell>747</cell><cell>947</cell></row><row><cell>Total</cell><cell>-</cell><cell>1800</cell><cell>40976</cell><cell>42776</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III THE</head><label>III</label><figDesc>CLASSIFICATION RESULTS OF SVM<ref type="bibr" target="#b7">[8]</ref>, S-CNN<ref type="bibr" target="#b38">[38]</ref>, GABOR-CNN<ref type="bibr" target="#b39">[39]</ref>, DFFN<ref type="bibr" target="#b40">[40]</ref>, 3D-GAN<ref type="bibr" target="#b52">[51]</ref> AND FREENET ON THE ROSIS-03 PAVIA UNIVERSITY DATASET.</figDesc><table><row><cell cols="2">Class</cell><cell></cell><cell></cell><cell>Patch-based</cell><cell></cell><cell></cell><cell>Patch-free</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell cols="2">S-CNN Gabor-CNN</cell><cell>DFFN</cell><cell>3D-GAN</cell><cell>FreeNet</cell></row><row><cell></cell><cell>C1</cell><cell>85.49</cell><cell>95.47</cell><cell>99.53</cell><cell>99.53</cell><cell>99.18</cell><cell>99.58</cell></row><row><cell></cell><cell>C2</cell><cell>92.12</cell><cell>98.71</cell><cell>98.21</cell><cell>97.71</cell><cell>98.86</cell><cell>99.88</cell></row><row><cell>Accuracy(%)</cell><cell>C3 C4 C5 C6</cell><cell>85.77 96.41 98.60 92.52</cell><cell>97.32 97.72 100 97.67</cell><cell>89.74 93.02 99.42 98.77</cell><cell>99.89 97.88 99.48 99.69</cell><cell>94.94 90.15 99.49 98.56</cell><cell>99.95 99.27 100 100</cell></row><row><cell></cell><cell>C7</cell><cell>93.79</cell><cell>98.36</cell><cell>98.82</cell><cell>100</cell><cell>92.74</cell><cell>100</cell></row><row><cell></cell><cell>C8</cell><cell>86.56</cell><cell>95.56</cell><cell>94.12</cell><cell>98.59</cell><cell>97.18</cell><cell>99.83</cell></row><row><cell></cell><cell>C9</cell><cell>97.97</cell><cell>100</cell><cell>97.91</cell><cell>99.61</cell><cell>98.51</cell><cell>100</cell></row><row><cell cols="2">OA(%)</cell><cell>90.78</cell><cell>97.93</cell><cell>97.33</cell><cell>98.57</cell><cell>97.81</cell><cell>99.81</cell></row><row><cell cols="2">AA(%)</cell><cell>92.14</cell><cell>97.88</cell><cell>96.62</cell><cell>99.16</cell><cell>96.65</cell><cell>99.83</cell></row><row><cell cols="2">Kappa</cell><cell>0.8813</cell><cell>0.9743</cell><cell>0.9662</cell><cell>0.9808</cell><cell>0.9697</cell><cell>0.9974</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV THE</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">NUMBER OF TRAINING SAMPLES AND TEST SAMPLES FOR THE</cell></row><row><cell></cell><cell cols="2">SALINAS DATASET</cell><cell></cell><cell></cell></row><row><cell>Class</cell><cell>Class name</cell><cell>#Training</cell><cell>#Test</cell><cell>#Total</cell></row><row><cell>C1</cell><cell>Brocoli green weeds 1</cell><cell>200</cell><cell>1809</cell><cell>2009</cell></row><row><cell>C2</cell><cell>Brocoli green weeds 2</cell><cell>200</cell><cell>3526</cell><cell>3726</cell></row><row><cell>C3</cell><cell>Fallow</cell><cell>200</cell><cell>1776</cell><cell>1976</cell></row><row><cell>C4</cell><cell>Fallow rough plow</cell><cell>200</cell><cell>1194</cell><cell>1394</cell></row><row><cell>C5</cell><cell>Fallow smooth</cell><cell>200</cell><cell>2478</cell><cell>2678</cell></row><row><cell>C6</cell><cell>Stubble</cell><cell>200</cell><cell>3759</cell><cell>3959</cell></row><row><cell>C7</cell><cell>Celery</cell><cell>200</cell><cell>3379</cell><cell>3579</cell></row><row><cell>C8</cell><cell>Grapes untrained</cell><cell>200</cell><cell>11071</cell><cell>11271</cell></row><row><cell>C9</cell><cell>Soil vinyard develop</cell><cell>200</cell><cell>6003</cell><cell>6203</cell></row><row><cell>C10</cell><cell>Corn senesced green weeds</cell><cell>200</cell><cell>3078</cell><cell>3278</cell></row><row><cell>C11</cell><cell>Lettuce romaine 4wk</cell><cell>200</cell><cell>868</cell><cell>1068</cell></row><row><cell>C12</cell><cell>Lettuce romaine 5wk</cell><cell>200</cell><cell>1727</cell><cell>1927</cell></row><row><cell>C13</cell><cell>Lettuce romaine 6wk</cell><cell>200</cell><cell>716</cell><cell>916</cell></row><row><cell>C14</cell><cell>Lettuce romaine 7wk</cell><cell>200</cell><cell>870</cell><cell>1070</cell></row><row><cell>C15</cell><cell>Vinyard untrained</cell><cell>200</cell><cell>7068</cell><cell>7268</cell></row><row><cell>C16</cell><cell>Vinyard vertical trellis</cell><cell>200</cell><cell>1607</cell><cell>1807</cell></row><row><cell>Total</cell><cell>-</cell><cell>3200</cell><cell>50929</cell><cell>54129</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">CLASSIFICATION RESULTS OF SVM [8], S-CNN[38], GABOR-CNN</cell></row><row><cell></cell><cell cols="7">[39], DFFN [40], 3D-GAN [51] AND FREENET ON THE SALINAS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DATASET</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Class</cell><cell></cell><cell></cell><cell>Patch-based</cell><cell></cell><cell></cell><cell>Patch-free</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell cols="2">S-CNN Gabor-CNN</cell><cell>DFFN</cell><cell>3D-GAN</cell><cell>FreeNet</cell></row><row><cell></cell><cell>C1</cell><cell>99.61</cell><cell>100</cell><cell>100</cell><cell>99.99</cell><cell>75.35</cell><cell>100</cell></row><row><cell></cell><cell>C2</cell><cell>99.69</cell><cell>97.75</cell><cell>88.06</cell><cell>99.94</cell><cell>98.49</cell><cell>100</cell></row><row><cell></cell><cell>C3</cell><cell>99.56</cell><cell>98.88</cell><cell>99.25</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>C4</cell><cell>99.41</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>99.76</cell><cell>99.92</cell></row><row><cell></cell><cell>C5</cell><cell>98.72</cell><cell>99.93</cell><cell>98.88</cell><cell>99.11</cell><cell>100</cell><cell>99.11</cell></row><row><cell></cell><cell>C6</cell><cell>99.77</cell><cell>89.48</cell><cell>100</cell><cell>99.95</cell><cell>99.97</cell><cell>100</cell></row><row><cell>Accuracy(%)</cell><cell>C7 C8 C9 C10</cell><cell>99.52 76.74 99.37 95.13</cell><cell>99.30 98.69 99.34 100</cell><cell>98.94 99.48 97.27 99.21</cell><cell>99.43 99.56 100 99.79</cell><cell>99.45 98.30 99.95 99.79</cell><cell>100 99.94 100 99.77</cell></row><row><cell></cell><cell>C11</cell><cell>99.32</cell><cell>99.93</cell><cell>100</cell><cell>99.48</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>C12</cell><cell>99.70</cell><cell>99.89</cell><cell>100</cell><cell>99.84</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>C13</cell><cell>99.15</cell><cell>88.62</cell><cell>100</cell><cell>99.96</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>C14</cell><cell>98.38</cell><cell>88.72</cell><cell>99.79</cell><cell>99.95</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>C15</cell><cell>75.56</cell><cell>90.62</cell><cell>94.31</cell><cell>99.45</cell><cell>99.52</cell><cell>99.99</cell></row><row><cell></cell><cell>C16</cell><cell>99.23</cell><cell>99.96</cell><cell>93.38</cell><cell>99.96</cell><cell>90.25</cell><cell>99.88</cell></row><row><cell cols="2">OA(%)</cell><cell>90.92</cell><cell>97.62</cell><cell>97.63</cell><cell>99.71</cell><cell>98.22</cell><cell>99.92</cell></row><row><cell cols="2">AA(%)</cell><cell>96.18</cell><cell>96.94</cell><cell>98.04</cell><cell>99.78</cell><cell>97.75</cell><cell>99.91</cell></row><row><cell cols="2">Kappa</cell><cell>0.8985</cell><cell>0.9510</cell><cell>0.9734</cell><cell>0.9967</cell><cell>0.9793</cell><cell>0.9991</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI THE</head><label>VI</label><figDesc>NUMBERS OF TRAINING SAMPLES AND TEST SAMPLES FOR THE CASI UNIVERSITY OF HOUSTON DATASET</figDesc><table><row><cell>Class</cell><cell>Class name</cell><cell>#Training</cell><cell>#Test</cell><cell>#Total</cell></row><row><cell>C1</cell><cell>Grass-Healthy</cell><cell>198</cell><cell>1053</cell><cell>1251</cell></row><row><cell>C2</cell><cell>Grass-Stressed</cell><cell>190</cell><cell>1064</cell><cell>1254</cell></row><row><cell>C3</cell><cell>Grass-Synthetic</cell><cell>192</cell><cell>505</cell><cell>697</cell></row><row><cell>C4</cell><cell>Tree</cell><cell>188</cell><cell>1056</cell><cell>1244</cell></row><row><cell>C5</cell><cell>Soil</cell><cell>186</cell><cell>1056</cell><cell>1242</cell></row><row><cell>C6</cell><cell>Water</cell><cell>182</cell><cell>143</cell><cell>325</cell></row><row><cell>C7</cell><cell>Residential</cell><cell>196</cell><cell>1072</cell><cell>1268</cell></row><row><cell>C8</cell><cell>Commercial</cell><cell>191</cell><cell>1053</cell><cell>1244</cell></row><row><cell>C9</cell><cell>Road</cell><cell>193</cell><cell>1059</cell><cell>1252</cell></row><row><cell>C10</cell><cell>Highway</cell><cell>191</cell><cell>1036</cell><cell>1227</cell></row><row><cell>C11</cell><cell>Railway</cell><cell>181</cell><cell>1054</cell><cell>1235</cell></row><row><cell>C12</cell><cell>Parking Lot 1</cell><cell>192</cell><cell>1041</cell><cell>1234</cell></row><row><cell>C13</cell><cell>Parking Lot 2</cell><cell>184</cell><cell>285</cell><cell>469</cell></row><row><cell>C14</cell><cell>Tennis Court</cell><cell>181</cell><cell>247</cell><cell>428</cell></row><row><cell>C15</cell><cell>Running Track</cell><cell>187</cell><cell>473</cell><cell>660</cell></row><row><cell>Total</cell><cell>-</cell><cell>2832</cell><cell>12179</cell><cell>15011</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII THE</head><label>VII</label><figDesc>CLASSIFICATION RESULTS OF SVM<ref type="bibr" target="#b7">[8]</ref>, S-CNN<ref type="bibr" target="#b38">[38]</ref>, GABOR-CNN<ref type="bibr" target="#b39">[39]</ref>, DFFN<ref type="bibr" target="#b40">[40]</ref>, 3D-GAN<ref type="bibr" target="#b52">[51]</ref> AND FREENET ON THE CASI UNIVERSITY OF HOUSTON DATASET.</figDesc><table><row><cell cols="2">Class</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Patch-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Patch-free</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SVM</cell><cell></cell><cell cols="6">S-CNN Gabor-CNN</cell><cell cols="2">DFFN</cell><cell cols="2">3D-GAN</cell><cell>FreeNet</cell></row><row><cell></cell><cell cols="2">C1</cell><cell cols="2">82.05</cell><cell></cell><cell cols="2">83.00</cell><cell></cell><cell cols="2">82.30</cell><cell></cell><cell cols="2">77.41</cell><cell></cell><cell>81.58</cell><cell>80.91</cell></row><row><cell></cell><cell cols="2">C2</cell><cell cols="2">80.55</cell><cell></cell><cell cols="2">83.27</cell><cell></cell><cell cols="2">84.24</cell><cell></cell><cell cols="2">81.39</cell><cell></cell><cell>79.74</cell><cell>84.21</cell></row><row><cell></cell><cell cols="2">C3</cell><cell cols="2">100</cell><cell></cell><cell cols="2">98.66</cell><cell></cell><cell cols="2">95.61</cell><cell></cell><cell cols="2">94.59</cell><cell></cell><cell>97.42</cell><cell>98.02</cell></row><row><cell></cell><cell cols="2">C4</cell><cell cols="2">92.52</cell><cell></cell><cell cols="2">93.50</cell><cell></cell><cell cols="2">92.39</cell><cell></cell><cell cols="2">87.82</cell><cell></cell><cell>93.36</cell><cell>91.95</cell></row><row><cell></cell><cell cols="2">C5</cell><cell cols="2">98.11</cell><cell></cell><cell cols="2">96.91</cell><cell></cell><cell cols="2">99.80</cell><cell></cell><cell cols="2">96.05</cell><cell></cell><cell>99.71</cell><cell>100</cell></row><row><cell>Accuracy(%)</cell><cell cols="2">C6 C7 C8 C9</cell><cell cols="2">95.10 75.00 40.17 74.88</cell><cell></cell><cell cols="2">94.12 79.95 68.19 76.39</cell><cell></cell><cell cols="2">96.47 84.58 73.09 78.14</cell><cell></cell><cell cols="2">96.15 80.25 77.78 84.66</cell><cell></cell><cell>95.08 89.90 70.52 54.89</cell><cell>96.50 88.53 74.83 87.72</cell></row><row><cell></cell><cell cols="2">C10</cell><cell cols="2">51.64</cell><cell></cell><cell cols="2">48.10</cell><cell></cell><cell cols="2">58.01</cell><cell></cell><cell cols="2">64.63</cell><cell></cell><cell>49.89</cell><cell>62.25</cell></row><row><cell></cell><cell cols="2">C11</cell><cell cols="2">78.37</cell><cell></cell><cell cols="2">74.64</cell><cell></cell><cell cols="2">73.35</cell><cell></cell><cell cols="2">88.61</cell><cell></cell><cell>77.36</cell><cell>83.40</cell></row><row><cell></cell><cell cols="2">C12</cell><cell cols="2">68.40</cell><cell></cell><cell cols="2">85.49</cell><cell></cell><cell cols="2">86.74</cell><cell></cell><cell cols="2">98.57</cell><cell></cell><cell>60.46</cell><cell>98.84</cell></row><row><cell></cell><cell cols="2">C13</cell><cell cols="2">69.47</cell><cell></cell><cell cols="2">88.58</cell><cell></cell><cell cols="2">91.16</cell><cell></cell><cell cols="2">83.09</cell><cell></cell><cell>81.71</cell><cell>88.42</cell></row><row><cell></cell><cell cols="2">C14</cell><cell cols="2">100</cell><cell></cell><cell cols="2">99.19</cell><cell></cell><cell cols="2">100</cell><cell></cell><cell cols="2">99.72</cell><cell></cell><cell>95.14</cell><cell>96.76</cell></row><row><cell></cell><cell cols="2">C15</cell><cell cols="2">98.10</cell><cell></cell><cell cols="2">96.40</cell><cell></cell><cell cols="2">77.73</cell><cell></cell><cell cols="2">81.90</cell><cell></cell><cell>65.35</cell><cell>94.29</cell></row><row><cell cols="3">OA(%)</cell><cell cols="2">76.88</cell><cell></cell><cell cols="2">82.34</cell><cell></cell><cell cols="2">84.32</cell><cell></cell><cell cols="2">84.56</cell><cell></cell><cell>78.16</cell><cell>86.61</cell></row><row><cell cols="3">AA(%)</cell><cell cols="2">80.29</cell><cell></cell><cell cols="2">84.43</cell><cell></cell><cell cols="2">84.17</cell><cell></cell><cell cols="2">86.18</cell><cell></cell><cell>79.98</cell><cell>88.44</cell></row><row><cell cols="2">Kappa</cell><cell></cell><cell cols="2">0.7513</cell><cell></cell><cell cols="2">0.8052</cell><cell></cell><cell cols="2">0.8114</cell><cell></cell><cell cols="2">0.8328</cell><cell></cell><cell>0.7616</cell><cell>0.8555</cell></row><row><cell cols="2">GrassHealthy</cell><cell>0.809</cell><cell></cell><cell></cell><cell>0.022</cell><cell></cell><cell>0.169</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell cols="2">GrassStressed</cell><cell></cell><cell>0.842</cell><cell></cell><cell>0.009</cell><cell></cell><cell>0.148</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GrassSynthetic Tree</cell><cell></cell><cell>0.004</cell><cell>0.98</cell><cell>0.92</cell><cell></cell><cell cols="2">0.066 0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.002 0.018</cell><cell>0.8</cell></row><row><cell cols="2">Soil</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Water Residential</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">0.965 0.029 0.015 0.885 0.024 0.026</cell><cell></cell><cell></cell><cell></cell><cell>0.035</cell><cell>0.005 0.016</cell><cell>0.6</cell></row><row><cell cols="2">Commercial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">0.012 0.167 0.024 0.748 0.018</cell><cell></cell><cell cols="3">0.007 0.012 0.005</cell><cell>0.007</cell></row><row><cell cols="2">Road Highway</cell><cell></cell><cell>0.008</cell><cell>0.003</cell><cell></cell><cell></cell><cell cols="4">0.002 0.042 0.004 0.877 0.165 0.15</cell><cell cols="2">0.623 0.06</cell><cell>0.066</cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell cols="2">Railway</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.118 0.026</cell><cell></cell><cell cols="4">0.013 0.002 0.834 0.008</cell><cell></cell><cell></cell></row><row><cell cols="2">Parking Lot 1 Parking Lot 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.012 0.018 0.07 0.021 0.004</cell><cell></cell><cell></cell><cell cols="2">0.988 0.004 0.884</cell><cell></cell><cell>0.2</cell></row><row><cell cols="2">Tennis Court</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.032</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.968</cell></row><row><cell cols="2">Running Track</cell><cell>GrassHealthy</cell><cell>GrassStressed 0.023</cell><cell>GrassSynthetic</cell><cell>Tree</cell><cell>Soil</cell><cell>Water</cell><cell>Residential</cell><cell>Commercial</cell><cell>Road 0.013</cell><cell>Highway</cell><cell>Railway</cell><cell>Parking Lot 1</cell><cell>Parking Lot 2</cell><cell>Tennis Court 0.021 0.943 Running Track</cell><cell>0.0</cell></row><row><cell cols="17">Fig. 13. Confusion matrix for FreeNet on the CASI University of Houston</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII HSI</head><label>VIII</label><figDesc>CLASSIFICATION RESULTS EVALUATED ON THE CASI UNIVERSITY OF HOUSTON DATASET. STARTING FROM OUR ENCODER-DECODER BASELINE, THE GS 2 SAMPLING STRATEGY, LATERAL CONNECTION AND SPECTRAL ATTENTION ARE GRADUALLY ADDED IN FREENET FOR THE MODULE ANALYSIS. SA DENOTES SPECTRAL ATTENTION AND LC DENOTES LATERAL CONNECTION BASED SSF.</figDesc><table><row><cell>Compression factor</cell><cell>Method</cell><cell cols="3">GS 2 sampling strategy Lateral connection Spectral attention</cell><cell>OA</cell><cell>AA</cell><cell>Kappa</cell></row><row><cell></cell><cell>(a) Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">15.23 19.54 0.0972</cell></row><row><cell>β = 0.75</cell><cell>(b) FreeNet w/o LC and SA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">65.12 67.45 0.6225</cell></row><row><cell></cell><cell>(c) FreeNet w/o SA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">84.23 85.61 0.8293</cell></row><row><cell></cell><cell>(d) FreeNet</cell><cell></cell><cell></cell><cell></cell><cell cols="2">85.49 86.64 0.8423</cell></row><row><cell></cell><cell>(a) Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">12.49 13.89 0.0657</cell></row><row><cell>β = 1.0</cell><cell>(b) FreeNet w/o LC and SA</cell><cell></cell><cell></cell><cell></cell><cell>65.16</cell><cell>65.2</cell><cell>0.6212</cell></row><row><cell></cell><cell>(c) FreeNet w/o SA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">84.91 86.25 0.8363</cell></row><row><cell></cell><cell>(d) FreeNet</cell><cell></cell><cell></cell><cell></cell><cell cols="2">86.61 88.44 0.8555</cell></row><row><cell>30%</cell><cell>55%</cell><cell>30%</cell><cell>55%</cell><cell>30%</cell><cell>55%</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Fig. 15. Performance versus the number of training samples per class. These three rows represent the performance on Pavia University, Salinas and CASI University of Houston dataset, respectively. The percentage near the dashed line is the percentage of training samples.</figDesc><table><row><cell>Pavia University</cell><cell>1.1%</cell><cell></cell><cell>2.1%</cell><cell>3.2%</cell><cell>1.1%</cell><cell>2.1%</cell><cell>3.2%</cell><cell>1.1%</cell><cell>2.1%</cell><cell>3.2%</cell></row><row><cell></cell><cell>0.74%</cell><cell>2.2%</cell><cell>3.0%</cell><cell>0.74%</cell><cell>2.2%</cell><cell>3.0%</cell><cell>0.74%</cell><cell>2.2%</cell><cell>3.0%</cell></row><row><cell>Salinas</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CASI University of Houston</cell><cell>2 5% 5 0%</cell><cell></cell><cell></cell><cell cols="2">2 5% 5 0%</cell><cell></cell><cell cols="2">2 5% 5 0%</cell><cell></cell></row><row><cell>1.0%</cell><cell></cell><cell></cell><cell></cell><cell>1.0%</cell><cell></cell><cell></cell><cell>1.0%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>180+</cell><cell></cell><cell></cell><cell>180+</cell><cell></cell><cell></cell><cell>180+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX THE</head><label>IX</label><figDesc>MODEL COMPLEXITY AND INFERENCE SPEED OF PATCH-BASED AND PATCH-FREE METHODS. ENCODER (β) IS SIMPLY IMPLEMENTED BY ENCODER IN CORRESPONDING FREENET WITH SAME β AND A 1 × 1</figDesc><table><row><cell cols="3">CONVOLUTIONAL LAYER.</cell><cell></cell></row><row><cell>Methods</cell><cell>Model complexity</cell><cell cols="2">Inference speed</cell></row><row><cell></cell><cell>#Params (M)</cell><cell>GFLOPs</cell><cell>GPU (s)</cell></row><row><cell>Patch-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder (β = 0.5)</cell><cell>0.554</cell><cell>53644.8</cell><cell>55.221</cell></row><row><cell>Encoder (β = 0.75)</cell><cell>1.191</cell><cell>107289.6</cell><cell>69.319</cell></row><row><cell>Encoder (β = 1.0)</cell><cell>2.070</cell><cell>167640.0</cell><cell>84.106</cell></row><row><cell>Patch-free</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FreeNet (β = 0.5)</cell><cell>0.724</cell><cell>112.37</cell><cell>0.094</cell></row><row><cell>FreeNet (β = 0.75)</cell><cell>1.575</cell><cell>220.82</cell><cell>0.122</cell></row><row><cell>FreeNet (β = 1.0)</cell><cell>2.749</cell><cell>364.11</cell><cell>0.146</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X DETAILED</head><label>X</label><figDesc></figDesc><table><row><cell cols="3">RUNNING TIME COSTS OF FREENET</cell></row><row><cell>Dataset</cell><cell>Training time (s)</cell><cell>Test time (s)</cell></row><row><cell>ROSIS-03 Pavia University</cell><cell>202</cell><cell>0.039</cell></row><row><cell>Salinas</cell><cell>158</cell><cell>0.030</cell></row><row><cell>CASI University of Houston</cell><cell>523</cell><cell>0.146</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the Editor, Associate Editor, and anonymous reviewers for their helpful comments and suggestions that improved this article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">New frontiers in spectral-spatial hyperspectral image classification: the latest advances based on mathematical morphology, markov random fields, segmentation, sparse representation, and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarablaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mini-uav-borne hyperspectral remote sensing: From observation and processing to applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="46" to="62" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for hyperspectral image classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2907932</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of hyperspectral remote sensing and its application in vegetation and water resource studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Govender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bulcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Sa</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multispectral and hyperspectral remote sensing for identification and mapping of wetland vegetation: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mutanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rugege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wetlands Ecology and Management</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="296" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Status and future of laser scanning, synthetic aperture radar and hyperspectral remote sensing data for forest biomass assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="581" to="590" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advances in hyperspectral image classification: Earth monitoring with statistical learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random forests for land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Gislason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="300" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperspectral remote sensing image classification based on rotation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="243" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Canonical correlation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05444</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with canonical correlation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iwasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="421" to="431" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse multinomial logistic regression: Fast algorithms and generalization bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="957" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral and spatial classification of hyperspectral data using svms and morphological profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4834" to="4837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral imagery based on partitional clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2973" to="2987" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data using loopy belief propagation and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="844" to="856" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A robust built-up area presence index by anisotropic rotationinvariant textural measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pesaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gerhardinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kayitakire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of selected topics in applied earth observations and remote sensing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Study of remote sensing image texture analysis and classification using wavelet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3197" to="3203" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gabor-filtering-based nearest regularized subspace for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1012" to="1022" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral data from urban areas based on extended morphological profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Palmason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized composite kernel framework for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4816" to="4829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected topics in applied earth observations and remote sensing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spectral-spatial unified networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="468" to="477" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sensors</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new deep convolutional neural network for fast hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="120" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1254" to="1258" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A cnn with multiscale convolution and diversified metric for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2886022</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3599" to="3618" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cascaded recurrent neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2899129</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning compact and discriminative stacked autoencoder for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2893180</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contextual deep cnn based hyperspectral classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3322" to="3325" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using deep pixel-pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="844" to="853" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised deep feature extraction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1909" to="1921" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyperspectral images classification with gabor filtering and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2355" to="2359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with deep feature fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3173" to="3184" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep fully convolutional network-based spatial distribution prediction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5585" to="5599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral imagery using a new fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="296" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
	<note>A stochastic approximation method</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5046" to="5063" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
