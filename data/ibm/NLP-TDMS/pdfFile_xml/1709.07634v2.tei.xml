<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lanzhou University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For most state-of-the-art architectures, Rectified Linear Unit (ReLU) becomes a standard component accompanied with each layer. Although ReLU can ease the network training to an extent, the character of blocking negative values may suppress the propagation of useful information and leads to the difficulty of optimizing very deep Convolutional Neural Networks (CNNs). Moreover, stacking layers with nonlinear activations is hard to approximate the intrinsic linear transformations between feature representations.</p><p>In this paper, we investigate the effect of erasing ReLUs of certain layers and apply it to various representative architectures following deterministic rules. It can ease the optimization and improve the generalization performance for very deep CNN models. We find two key factors being essential to the performance improvement: 1) the location where ReLU should be erased inside the basic module; 2) the proportion of basic modules to erase ReLU; We show that erasing the last ReLU layer of all basic modules in a network usually yields improved performance. In experiments, our approach successfully improves the performance of various representative architectures, and we report the improved results on SVHN, CIFAR-10/100, and ImageNet. Moreover, we achieve competitive single-model performance on CIFAR-100 with 16.53% error rate compared to state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the success of AlexNet <ref type="bibr" target="#b14">[15]</ref> in the ILSVRC-2012 competition <ref type="bibr" target="#b18">[19]</ref>, more and more researchers move their focus on deep CNNs. Features learned from the neural networks significantly improve the performance of the largescale vision recognition task, and can successfully be transferred to a large variety of computer vision tasks, such as object detection <ref type="bibr" target="#b4">[5]</ref>, pose estimation <ref type="bibr" target="#b24">[25]</ref> and human-object interactions <ref type="bibr" target="#b5">[6]</ref>. Due to the powerful transferability of CNN models, "network engineering" has attracted much research interest. This leads researchers to explore more effective and efficient network architectures. The state-of-the-art CNN architectures become increasingly deeper and more complex. The VGGNet <ref type="bibr" target="#b19">[20]</ref> extends the depth of AlexNet from eight to nineteen layers. The GoogleNet <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> designs the Inception module explicitly by incorporating the multi-scale property into the architecture. ResNet <ref type="bibr" target="#b8">[9]</ref> proposes a residual learning framework to ease the training of networks, and can successfully train networks with more than 1000 layers. DenseNet <ref type="bibr" target="#b10">[11]</ref> connects each layer to every other layer, which encourages feature reuse and substantially reduces the number of parameters. These different architectures share a key characteristic: they incorporate many nonlinear units, ReLU, in the networks.</p><p>ReLU layers are widely used in all CNN architectures. It has also been demonstrated to be more powerful compared to other nonlinear layers in most situations, such as sigmoid and tanh <ref type="bibr" target="#b6">[7]</ref>. ReLU can not only increase the nonlinearity but also ameliorate gradient vanish/explosion phenomenon in CNNs. Therefore, a convolution layer or a fully-connected layer is usually accompanied with a ReLU layer by default in the state-of-the-art CNN architectures. Is this design principle necessary and helpful for the classification or some other vision tasks?</p><p>We empirically find that reducing the nonlinearity in very deep CNNs eases the difficulty of neural network train-ing. As networks go deeper, the benefits from depth and complexity become less <ref type="bibr" target="#b28">[29]</ref>. For example, there is about 1.3% accuracy improvement from ResNet-110 to ResNet-164 on CIFAR-10 <ref type="bibr" target="#b13">[14]</ref>. However, six times deeper network, ResNet-1001, even decreases the accuracy about 1.7%. In this way, the trained network is far from the capacity it should achieve. When we simply erase the last ReLU layer in residual blocks, the accuracy of a network with more than 1000 layers can still increase, whereas the very deep ResNet tends to decrease the accuracy.</p><p>In this work, we propose a simple but effective method to improve the performance of deep CNN architectures by erasing the ReLU layers. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our approach erases the last ReLU layer of the basic module in the deep neural network. We thus call our approach "EraseReLU". Our intuition is that some nonlinear layers may suppress information through the forward pass. For example, in some cases <ref type="bibr" target="#b2">[3]</ref>, the feature tensors after ReLU can be dominated by zero values, which harms the performance and cannot recover because the gradient of zero value is also zero. Therefore, as our approach significantly reduces the nonlinearity in CNN models, it can help the information propagation in very deep networks. EraseReLU also benefits the network optimization, which usually leads the model to converge faster in the early training epochs. By investigating various factors of applying EraseReLU, we find that two key factors to improve the performance. 1) the location where ReLU should be erased; 2) the proportion of modules to erase ReLU. Moreover, we empirically demonstrate that erasing the last ReLU layer in all basic modules usually tends to result in performance improvement for various state-of-the-art CNN architectures, ResNet, Preact ResNet <ref type="bibr" target="#b9">[10]</ref>, Wide ResNet <ref type="bibr" target="#b27">[28]</ref>, Inception-V2 <ref type="bibr" target="#b12">[13]</ref> and ResNeXt <ref type="bibr" target="#b25">[26]</ref>. Besides, we provide the theoretical analysis of EraseReLU which proves ReLU layers suppress the gradient back-prorogation in deep CNNs.</p><p>In summary, this paper makes the following contributions:</p><p>1. We propose a simple but effective approach to improve the classification performance of very deep CNN models by erasing the last ReLU layer of a certain proportion of basic modules in CNNs. Moreover, the proposed approach can ease the difficulty of deep CNN training, which makes the CNN model converge faster.</p><p>2. We provide the theoretical analysis of our EraseReLU. We demonstrate gradients in very deep CNN models are suppressed by ReLU layers.</p><p>3. We empirically show significant improvements on various of state-of-the-art CNN architectures. We use four benchmark datasets, including the large-scale dataset, ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Nonlinearity. The nonlinear unit plays an essential role in strengthening the representation ability of a deep neural network. In early years, sigmoid or tanh are standard recipes for building shallow neural networks. Since the rise of deep learning, ReLU <ref type="bibr" target="#b6">[7]</ref> has been found to be more powerful in easing the training of deep architectures and contributed a lot to the success of many record-holders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>. There exist lots of variants of ReLU nowadays, such as Leaky ReLU <ref type="bibr" target="#b16">[17]</ref>, PReLU <ref type="bibr" target="#b7">[8]</ref>, etc. The common ground shared by these units is that the computation will be linear on a subset of neurons. Models trained with such kinds of nonlinear units can be viewed as a combination of an exponential number of linear models which share parameters <ref type="bibr" target="#b17">[18]</ref>. This inspires us that modeling the local linearity explicitly may be useful. In this paper, we extend this linearity from the subset to the full set of neurons in some layer and empirically found that this extension effectively improves the performance of the model.</p><p>Architecture. Krizhevsky won the ILSVRC-2012 competition and revealed that a large and deep CNN <ref type="bibr" target="#b14">[15]</ref> is capable of achieving benchmark results on a highly challenging dataset. <ref type="bibr" target="#b29">[30]</ref> proposes a novel visualization technique providing insight into the CNN features as well as a new architecture ZFNet. NIN <ref type="bibr" target="#b15">[16]</ref> leverages one-by-one convolutional layers to make the network become deeper and yield better performance. VGGNet <ref type="bibr" target="#b19">[20]</ref> further promotes the depth of CNN to 19 weighted layers resulting in a significant improvement.</p><p>Inception module is first proposed in <ref type="bibr" target="#b22">[23]</ref>, which considers the Hebbian and multi-scale principle in CNN. <ref type="bibr" target="#b12">[13]</ref> proposes the Batch Normalization (BN) to accelerate the network training. They also applied BN to a new variant of the GoogleNet, named as BN-Inception. <ref type="bibr" target="#b23">[24]</ref> proposes several general design principles to improve Inception module which leads a new CNN architecture, Inception-v3. <ref type="bibr" target="#b21">[22]</ref> combines the advantages of Inception architectures with residual connections to speedup the CNN training.</p><p>Highway network <ref type="bibr" target="#b20">[21]</ref> is designed to ease gradientbased training of very deep networks. <ref type="bibr" target="#b8">[9]</ref> proposes the deep residual network that achieves a remarkable breakthrough in ImageNet classification and won the 1st places various of ImageNet and COCO competitions. The proposed residual learning can make the network easier to optimize and gain accuracy from considerably increased depth. Following ResNet, <ref type="bibr" target="#b9">[10]</ref> proposes the Pre-activation ResNet improving the ResNet by using pre-activation block. Wide ResNet <ref type="bibr" target="#b27">[28]</ref> decreases the depth and increases the width of residual networks. It tackles the problem of diminishing feature reuse for training very deep residual networks. ResNeXt <ref type="bibr" target="#b25">[26]</ref> optimizes the convolution layer in ResNet by aggregating a set of transformations with the same topology.</p><p>Some researchers incorporate the stochastic procedure in the CNN models. <ref type="bibr" target="#b11">[12]</ref> proposes stochastic depth, a training procedure enabling the seemingly contradictory setup to train short networks and use deep networks at test time. <ref type="bibr" target="#b3">[4]</ref> proposes to use parallel branches with a stochastic affine combination in ResNet to the avoid overfitting problem. Our approach is a different way to improve CNN models compared with them, and we can complement each other. While a contemporary work <ref type="bibr" target="#b30">[31]</ref> argues that 1:1 convolution and ReLU ratio is not the best choice to design the network architectures, we observe that only tuning the convolution and ReLU ratio may not always lead to improvement for different network structures. Instead, the location where ReLUs should be erased is the key factor, which is the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The mapping function between different representations can be linear or non-linear. If the intrinsic relationship between two different representations is a linear mapping, it is hard to learn to approximate such kind of linear mapping through stacking of non-linear transformations, especially when the architecture is quite deep and the data is scarce. For example, <ref type="bibr" target="#b8">[9]</ref> illustrates that optimization can be difficult in approximating an identity mapping by stacking multiple layers with non-linear activations in very deep neural networks. ReLU layers introduce linearity for the subset of neurons with positive responses. However, we find that for a subset of layers, it can be helpful to keep the linearity for the neurons with negative responses, i.e., explicitly forcing the linear mapping for a subset of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">EraseReLU</head><p>In most architectures, the network consists of multiple stacked core modules. Therefore, a network can be formulated 1 as :</p><formula xml:id="formula_0">F (x) = fn • fn−1 • ... • fi • ... • f2 • f1 • x,<label>(1)</label></formula><p>where f i indicates the i-th basic unit in the network and <ref type="figure" target="#fig_1">Figure 2</ref> illustrates five different kinds of f in different typical CNN architectures. As we stack more such modules in CNN, the network tends to overfit the training set and the optimization becomes more difficult. Residual connection <ref type="bibr" target="#b8">[9]</ref> can alleviate this phenomenon.. However, these two problems are still unsolved <ref type="bibr" target="#b28">[29]</ref> We empirically find that reducing nonlinearities of f can be helpful for ameliorating the overfitting and optimization problems in very deep neural networks.</p><formula xml:id="formula_1">f i •x equals f i (x).</formula><p>What is the most efficient way to reduce the nonlinearities of f and maintain the model capacity at the same time? <ref type="bibr" target="#b0">1</ref> For simplification, we ignore the fully-connected and pooling layers. There are usually three operations in CNN models: convolution, batch normalization, and ReLU. Convolution operation is a linear unit and also essential for the model capacity, we thus do not change the convolution operation. BN is not a linear unit strictly but can be approximately regarded as a linear unit. It can avoid the gradient explosion by stabilizing the distribution and reducing the internal covariate, we thus should also retain this operation. Therefore, there left two directions to reduce the nonlinearities, modifying ReLU or optimizing the module structure. The module structure has thousands of combinations of different operations, which is beyond the scope of our paper. Hereto, we eliminate all choices except modifying ReLU.</p><p>We can observe that modules in <ref type="figure" target="#fig_1">Figure 2</ref> share a common character, i.e., the last layer is a ReLU layer:</p><formula xml:id="formula_2">F (x) = M odule(x) = ReLU (M odule (x)),<label>(2)</label></formula><p>where most architectures also have this character. If we erase the last ReLU layer, we can preserve the overall structure. On the contrary, if we erase the middle ReLU layer of these modules, it will destroy the module structure and decrease the module capacity, thus be harmful to the performance (See discussion in Sec.3.4). We empirically observe erasing the last ReLU layer in each module is capable of easing the training difficulty and considered as a regularization to improve the final performance. Modules in <ref type="figure" target="#fig_1">Figure 2</ref> can be categorized as the afteractivation structure <ref type="bibr" target="#b9">[10]</ref>, where activation operations (BN  &amp; ReLU) are after the convolutional layer. Pre-activation structure <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref> is another kind of module. It moves BN+ReLU to the head of the convolution layer, and ReLU thus is not the tail of module. To apply EraseReLU to these architectures, we first transfer them into the after-activation structure and then apply EraseReLU, because the middle ReLU layer is essential for performance as we discuss before.</p><p>Hereto, the locations where ReLU layers should be erased have been discussed. It is still not clear that EraseReLU should be applied to which module. If we arbitrarily choose the module to apply EraseReLU, there exist thousands of choice combinations and some of them are even equivalent. Therefore, we use the proportion of modules which should be applied with EraseReLU for efficiency. Given a specific proportion, we uniformly sample the locations where the module to be applied with EraseReLU. For example, if we apply EraseReLU on ResNet with the proportion of 50%, then we will erase the last ReLU of the 1-th, 3-th, 5-th and etc. modules in ResNet. The location where ReLU should be erased and the proportion of modules to apply EraseReLU are two key factors for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of the EreaseReLU's Effect</head><p>In this section, we analyze the effect of different factors when applying EraseReLU to improve CNN architectures. We perform two models stacked by thirty similar modules on CIFAR datasets. One model uses the VGGstyle module, and the other uses the residual-style module as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Both of them use a 3-by-3 convolutional layer with 16 filters at the first layer. There are three stages following the first convolutional layer, where each stage has ten modules with the different number of output channels, i.e., 16, 32 and 64, respectively. Therefore, each of these two models has thirty-one weighted layers. Moreover, we try a multiple layer perception (MLP) network with 12 fully-connected layers. Each fully-connected layer in this MLP has one thousand output neurons followed by BN+ReLU+Dropout, except for the last one which maps the one thousand input neurons to ten neurons for MNIST classification. (More details can be found in supplementary materials) <ref type="figure" target="#fig_2">Figure 3</ref>(a) shows that reducing ReLU layers in a deep VGG-style network leads to the better performance. It achieves the highest accuracy when the proportion for EraseReLU is 20%. <ref type="figure" target="#fig_2">Figure 3(b)</ref> performs the similar experiments as in <ref type="figure" target="#fig_2">Figure 3</ref>(a), while we replace the VGGstyle network as the residual-style network, which dramatically eases the training procedure. However, the original model is still inferior to the model with EraseReLU. For this residual-style network, one with even a very small proportion of modules to erase ReLU (5%) can outperform the original model. <ref type="figure" target="#fig_2">Figure 3</ref>(c) demonstrates the same phenomenon in MLP, reducing the number of ReLU layers yields better performance. <ref type="figure" target="#fig_2">Figure 3(d)</ref> illustrates that the network trained with EraseReLU will gradually become better than the model without EraseReLU when the network goes deeper. The module in the networks used in <ref type="figure" target="#fig_2">Figure 3</ref> only has one ReLU layer. If we use the proportion of 100% for EraseReLU, there will be no nonlinearity in these models, which significantly reduce the performance, and we thus do not list the proportion of 100% for EraseReLU in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>In practice, we usually use the proportion of 100% for EraseReLU, which means that we apply EraseReLU on all modules. There will usually have more than one ReLU layers in the module of state-of-the-art architectures. Therefore, even the proportion of 100% can maintain enough nonlinearity of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical Analysis</head><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we design a neural network f w : R → R taking scalars to scalars. It contains of N modules, where the first layer and the last layer are fully-connected layers. The middle module is similar to the simple residual module in <ref type="figure" target="#fig_1">Figure 2</ref>, whereas we replace the convolutional and BN layer with a fully-connected layer and a LayerNorm layer <ref type="bibr" target="#b0">[1]</ref>. Each hidden layer contains two hundred rectifier neurons. At initialization, the function f w takes the input x ∈ [−2, 2] in a 1-dim grid of 1000 data points. We initialize the weight and bias by <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the gradient of the network for x ∈ [−2, 2] and its covariance matrix in the first two subfigures of each triplet. As the depth increases, the model using EraseReLU does not hurt the correlations between gradients. On the contrary, the original model (denoted as "With ReLU") shows a decreased tendency of correlations between gradients. The third subfigure in each triplet of <ref type="figure" target="#fig_3">Figure 4</ref> shows the activation distribution of the last hidden layer. If the neuron is greater than 0, it is activated; otherwise, it is deactivated. For the model with ReLU, the distribution becomes increasingly bimodal with depth. It decreases efficiency with which rectifier nonlinearities are utilized <ref type="bibr" target="#b1">[2]</ref>. But the model with EraseReLU can maintain neurons to be utilized more efficiently even the depth becomes 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Connection with other non-linear activations</head><p>There exist many variants of ReLU, e.g., Leaky ReLU and PReLU. They aim to alleviate the gradient vanishing problem. While these variants may benefit the training of deep neural networks compared to ReLU, the performance improvement is negligible <ref type="bibr" target="#b26">[27]</ref>. EraseReLU can be considered as a variant of ReLU. It extends identity mapping from positive domain to the whole domain, but leads to a significant improvement compared to other ReLU variants. Table 1 compares our EraseReLU with other variants of ReLU and stochastic depth ResNet <ref type="bibr" target="#b11">[12]</ref> (denoted as SDR). We can find that PReLU achieves a much lower error on the training data, yet obtains a higher testing error compared with EraseReLU. Given enough training data, PReLU can theoretically learn an identity mapping if our EraseReLU is an optimal solution. But in practical, EraseReLU always yield a much better performance than PReLU. ER* in <ref type="table">Table 1</ref> indicates erasing the middle ReLU layer in residual blocks. The middle ReLU layer is essential for as well as the final accuracy. ER* destroys the overall struc-ture of the basic module, and it thus significantly reduces the performance compared to the original model. Instead, ER, which erases the last ReLU layer, yields the consistent performance improvement. SDR is a technique to boost the CNN training when the network is deep, which can complement our EraseReLU. By combining the mutual benefits from SDR and EraseReLU, we achieve 22.89 error rate on ResNet-110, which improves the SDR by about relative 5% and ResNet by about 19%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN. The Street View House Numbers (SVHN) dataset is obtained from house numbers in Google Street</head><p>View images. This dataset contains ten classes, from digit 0 to digit 9. There are 73257 color digit images in the training set, as well as the 26032 images in the testing set. It also has 531131 additional, somewhat less difficult samples, to use as extra training data. Following the common experiment setting on SVHN, such as <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11]</ref>, we only use the official training and testing data. When training models, we only divide the pixel values by 255 to range into [0,1] without any data augmentation.</p><p>CIFAR. The CIFAR-10 dataset consists of 60000 images categorized into ten classes. There are 50000 training images with 5000 images per class, and 10000 testing images with 1000 images per class. The CIFAR-100 dataset is similar as CIFAR-10 but contains 100 classes. We use the official training and testing sets of these two datasets. Following the common practice <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref>, we normalize the data using the means and standard deviations of RGB channels. We also adopt the random horizontal flip and random crop 2 data argumentations.</p><p>ImageNet. The ILSVRC classification dataset contains 1000 classes. There are about 1.2 million images for training, and 50000 for validation. We use the same data argumentation as in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>   <ref type="table">Table 1</ref>. Classification error (%) on CIFAR-100 with different variant of residual networks. ResNet represents the original residual network <ref type="bibr" target="#b8">[9]</ref>. PReLU all indicates that we replace all ReLU layers by PReLU in ResNet. PReLU sum indicates that we only replace ReLU layers, which are right after the shortcut addition. ER indicates that we apply EraseReLU on the last ReLU layer in the residual block. ER* indicates the location where ReLU is erased is the first ReLU layer in the residual block. SDR represents the stochastic depth ResNet <ref type="bibr" target="#b11">[12]</ref>.</p><p>As SDR train the model with 500 epoch, ER † is ER with the same training settings as SDR. SDR can also be complementary to our approach, and we show the results of the ER trained by SDR as ER †+SDR. We run each model five times and show "mean ± std". only use the single-crop with input image size of 224 2 . We also generate three subsets of ImageNet for empirical studies. These three subsets randomly sample 10%, 20% and 30% images from the ImageNet dataset. Therefore, all of them have 1000 classes for training and testing images, but the number of images is different. We refer these three datasets as ImageNet-10%, ImageNet-20% and ImageNet-30%. To be noticed, ImageNet-10% is a subset of ImageNet-20%, and ImageNet-20% is a subset of ImageNet-30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on SVHN and CIFAR</head><p>In this section, we apply our approach to improve five different architectures, ResNet, Pre-act ResNet, Wide ResNet, ResNeXt and Inception-V2. We demonstrate the comparison on SVHN, CIFAR-10, and CIFAR-100.</p><p>Experiment settings. The proportion for EraseReLU is a hyper-parameter, which is selected from {25%, 50%, 75%, 100%} according to validation sets. As the Inception-V2 was designed for the ImageNet with the input image size of 224 2 , we design a special Inception-V2 model for CIFAR datasets. It is shown as modules in <ref type="figure" target="#fig_1">Figure 2</ref>, where the 5×5 convolutional layer is replaced by two 3 × 3 convolutional layers. The proportions of output channels from the 1 × 1, 3×, 5 × 5 convolution branches and the pooling branch are 1 : 8 : 2 : 1 in one Inception module. Our Inception-V2 on CIFAR has a convolutional layer with 64 output channels at first, followed by three Inception stages. Each stage contains ten Inception modules, where the base output channels are 16, 32 and 64, respectively.</p><p>According to the validation, we use the proportion of 100% for EraseReLU on ResNet, Pre-act ResNet, Wide ResNet and ResNeXt. For Inception-V2, we use the proportion of 50% for EraseReLU. We the Inception-V2 model following the same training and testing strategies as in <ref type="bibr" target="#b27">[28]</ref>. For all other networks, we follow the same training and testing strategies as in their official papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26]</ref>. We run each model five times and report the mean error.</p><p>Experimental Analysis. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the accu-   <ref type="table">Table 2</ref>. Classification error (%) on CIFAR and SVHN datasets. The "original" means the architecture described in the original papers, and the EraseReLU means the architecture by applying our EraseReLU method on the original one. † indicates results run by ourselves. The setting formats follow the original papers, except that the setting of Inception-V2 indicates there are 30 Inception modules in the network. By applying EraseReLU, most of the architectures achieve lower error rates while using the same parameters and computation cost. We run each model five times and report "mean (± std)". '-' indicates they do not report results.</p><p>0.01, but the original model raises more than 0.1. Therefore, the model using EraseReLU results in a much smaller loss increase than the original model. Moreover, with the network going deeper, we can obtain more benefits from EraseReLU. For example, the improvement of EraseReLU on ResNet-164 is much more than ResNet-110.</p><p>We demonstrate comparisons on SVHN and CIFAR in <ref type="table">Table 2</ref>. For ResNet, EraseReLU improves the accuracy compared to the original architectures. The ResNet with 1202 layers achieves a worse performance than the network with 110 layers, but ours can still maintain the accuracy improvement even when the depth becomes very deep. For Pre-act ResNet, we use both after-activation and EraseReLU. For Pre-act ResNet with 1001 layers on CIFAR-100, the error drops from 22.71% to 20.07% by our EraseReLU. We can observe more than 2% accuracy gain on Pre-act ResNet with 1001 layers. As shown in <ref type="bibr" target="#b9">[10]</ref>, the after-activation style does not improve the performance. Thus the main contribution of the accuracy improvement is from EraseReLU. For ResNeXt, the last ReLU of all residual blocks are removed, and we obtain about 0.8% improvement for ResNeXt-29-16x64 on CIFAR-100. The previous state-of-the-art result on CIFAR-100 is DenseNet-BC-190-40, 17.18% error rate. We achieve a better result by applying EraseReLU on a model, which performs worse than DenseNet-BC-190-40. On SVHN, EraseReLU improves the Wide ResNet by about 10% relative accuracy. On CIFAR-10, we can observe general improvements on various kinds of networks. The Wide ResNet-52-10 with EraseReLU does not improve the performance, because our model may be close to the lower bound for the CIFAR-10 dataset. Compared to <ref type="bibr" target="#b30">[31]</ref>, we achieve more superior performance on CIFAR, and they lack the exploration on other different kinds of CNN architectures rather than ResNet. Moreover, we will show the results on the largescale dataset in the following section, whereas they only experiment on small-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on ImageNet</head><p>In this section, we first experiment on the ImageNet subsets to analyze the effect of model complexity and data scale for our EraseReLU. We then evaluate our approach on the full ImageNet set to demonstrate the effectiveness. For all experiments in this section, we use the official training code provided by PyTorch 3 and train each network with 120 epochs with the learning rate initialized by 0.1. The learning rate is divided by 10 every 30 epochs.   <ref type="table" target="#tab_4">0  5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100</ref> Top-  <ref type="figure" target="#fig_5">Figure 6</ref>. Comparison of models with and without EraseReLU on the ImageNet-10% in terms of the top-1 and top-5 accuracy (%). The first subfigure shows the results of ResNet-101, and the second one shows ResNeXt-152 (32x4d). The blue and red lines indicate the original network and the network using EraseReLU, respectively. The solid and dashed lines show the top-1 and top-5 accuracy, respectively. <ref type="table">Table 3</ref> shows the comparison of the original ResNet-50 and the model using EraseReLU with the proportion of 100%. EraseReLU improves the performance of ResNet-50 on ImageNet-10% about 1.4% absolute top-1 accuracy. When the numbers of training and validation images increase to ImageNet-20%, the performance improvement goes down to 0.24%. On ImageNet-30%, we also achieve a comparable results by using EraseReLU with the proportion of 25%. This indicates that the proportion is a key factor on large-scale dataset, which is not much sensitive on smallscale datasets.</p><p>On the ImageNet full set, we apply EraseReLU to two models, ResNet-152 and ResNet-200. To ensure a fair comparison, we adopt the public Torch implementation for ResNet <ref type="bibr" target="#b3">4</ref> and only replace the model definition by the model applying our EraseReLU. Therefore, all other factors are eliminated, such as data pre-process and data argumentations. We keep all the experiment settings the same as those used for ResNet. For both ResNet-152 and ResNet-200, we use the EraseReLU with the proportion of 50%. EraseReLU improves the ResNet-152 on ImageNet dataset by 0.6% top-1 accuracy. For ResNet-200, we achieve 21.4% top-1 accuracy outperforming the original one about 0.4%. The ResNet-200 with EraseReLU also outperforms than the Preact ResNet-200, which is an improved version of ResNet.</p><p>We demonstrate EraseReLU can improve deep models in the large-scale dataset. We do not apply EraseReLU to more sophisticated models on ImageNet because it needs much more hardware resources, which is unaffordable for us. All the compared models in experiments are the stateof-the-art CNN architectures, which have been optimized by many researchers and validated by many systems/papers. It is not easy to remove their components. If one removes a layer or a component arbitrarily, the performance will drop significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) shows a part of VGG-like networks. (b) shows a block of ResNet. "original" and "proposed" mean the original architecture and the improved architecture by EraseReLU. Our proposed model can achieve a higher performance compared to the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>In most CNN architectures, the feature extraction part is stacked by many similar modules with different configurations. (a) and (d) are the modules for VGG style network. (b) and (c) show the residual style modules. (e) shows an example of the Inception module. Our EraseReLU erases the ReLU with dashed boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of classification accuracy with and without EraseReLU. (a) shows the relatively accuracy improvement of a VGG-style network with 31 weighted layers regarding the different proportion of modules to apply our EraseReLU. (b) is similar as (a), whereas we replace the VGG-style network by a residualstyle network. In these two figures, the green and blue lines represent the results of CIFAR-100 and CIFAR-10, respectively. (c) illustrates the absolute accuracy comparison of a multiple layer perception network with 12 weighted layers on MNIST, regarding the different proportion for EraseReLU. (d) shows the accuracy on CIFAR-100 regarding different number of weighted layers, where the architecture is the same as in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between networks with and without EarseReLU regarding different depths. In each triplet, the first figure show the gradients for input ∈ [−2, 2]; the second figure show the visualization of gradients' covariance; the third figure shows the distribution of average activation levels for each neuron in the last layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) Accuracy (L) and Loss (R) of ResNet-164 on CIFAR-10 (b) Accuracy (L) and Loss (R) of ResNet-164 on CIFAR-100 Comparison between EraseReLU and the original model. In each subfigure, the x-axis indicates the train epoch and the yaxis means the accuracy or loss. The red and blue lines are the model with EraseReLU and without EraseReLU. The solid and dashed lines show the performance of the training and testing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>a) compares ResNet-50 and ResNet-50 using EraseReLU with the proportion of 100% on ImageNet-10%. Our EraseReLU outperforms the original model by about 3% top-1 accuracy. Figure 6(b) illustrates the results of ResNeXt-152 (32x4) using EraseReLU with the proportion of 50%. EraseReLU obtains a comparable result compared to the original model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>±0.32 11.18±0.17 12.16±0.18 21.34±0.34 12.47±0.21 5.38±0.20 22.51±0.30 24.09±0.09 Train 56 2.38 ± 0.22 1.70 ±0.24 0.89 ±0.13 13.28±0.79 0.86 ±0.08 0.09±0.02 4.91 ±0.12 4.95±0.03 -ing 110 0.34 ± 0.05 0.21 ±0.04 0.33 ±0.01 9.76 ±0.04 0.16 ±0.10 0.29±0.01 0.56 ±0.02 0.50±0.04 20 32.85 ±0.48 33.40 ±0.59 32.95±0.30 35.15±0.36 32.29±0.33 31.82±0.30 32.87±0.15 33.24±0.36 Test 56 30.86 ±0.81 30.60 ±0.84 29.94±0.73 33.64±0.33 28.56±0.17 27.23±0.01 25.60±0.36 25.01±0.22 -ing 110 28.21 ±0.46 28.20 ±0.98 27.45±0.46 33.55±0.44 26.05±0.44 25.01±0.09 24.01±0.14 22.89±0.17</figDesc><table><row><cell>Depth</cell><cell>ResNet</cell><cell>PReLU all PReLU sum</cell><cell>ER*</cell><cell>ER</cell><cell>ER †</cell><cell>SDR</cell><cell>ER †+SDR</cell></row><row><cell cols="2">20 12.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">for training. For evaluation, we</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">2 Pad 4 pixels on each border and randomly crop a 32x32 region</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>https://github.com/facebook/fb.resnet.torch 47.60 36.76 31.11 top-5 23.68 15.65 11.65 ResNet-50 with ER top-1 46.18 36.42 31.10 top-5 23.20 14.99 11.64 Table 3. Top-1 and top-5 error rate on ImageNet validation subsets using different number of training images. 10%, 20% and 30% indicates ImageNet-10%, -20% and -30%, respectively. For EraseReLU on ImageNet-10% and -20%, we use the proportion of 100%. For ImageNet-30%, we use the proportion of 25%. Top-1 and top-5 error rate on ImageNet full validation set. We compare various residual models with and without EraseReLU. We use the proportion of 100% for EraseReLU on ResNet-152 and Pre-act ResNet-200, referred as "ER". They are evaluated on the ImageNet validation set. We only use single-crop for testing with 224 × 224 image size. † indicates results run by ourselves.</figDesc><table><row><cell>Sample Ratio</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell></row><row><cell>ResNet-50 top-1 P P P P P P Error Model P P ResNet-152</cell><cell cols="3">top-1(%) top-5(%) 22.2 6.2</cell></row><row><cell>ResNet-152 with ER</cell><cell>21.6</cell><cell>5.8</cell><cell></cell></row><row><cell>ResNet-200  †</cell><cell>21.8</cell><cell>6.1</cell><cell></cell></row><row><cell>Pre-act ResNet-200</cell><cell>21.7</cell><cell>5.8</cell><cell></cell></row><row><cell>ResNet-200 with ER</cell><cell>21.4</cell><cell>5.8</cell><cell></cell></row></table><note>4</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/examples/blob/ master/imagenet/main.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we investigate the effect of erasing Re-LUs in deep CNN architectures following deterministic rules. We find two key factors to performance improvement: 1) the location where the ReLU should be erased inside the basic module; 2) the proportion of modules to erase ReLU. By leveraging these two factors, we propose a simple but effective approach to improve CNN models, named "EraseReLU". It can lead to a non-negligible improvement of classification performance because its effectiveness in easing the optimization and regularizing the training of deep neural networks. Our approach improves the classification performance beyond various CNN architectures, such as ResNet, Pre-act ResNet, Wide ResNet and ResNeXt. Most of them achieve a much higher performance while having the same computation cost compared to the original models. We obtain more than 2% absolute accuracy improvement on CIFAR-100 compared to the Pre-act ResNet-1001. Our approach also leads 0.6% accuracy improvement on the large-scale dataset ImageNet compared to ResNet-152.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Detecting and recognizing human-object intaractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07333</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00388</idno>
		<title level="m">Diracnets: Training very deep neural networks without skip-connections</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06247</idno>
		<title level="m">Training better cnns requires to rethink relu</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
