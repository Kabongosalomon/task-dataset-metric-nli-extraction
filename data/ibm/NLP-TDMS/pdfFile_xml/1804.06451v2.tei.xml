<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Reward Reinforced Summarization with Saliency and Entailment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Reward Reinforced Summarization with Saliency and Entailment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and nonredundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these rewards are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results (including human evaluation) on the CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstractive summarization, the task of generating a natural short summary of a long document, is more challenging than the extractive paradigm, which only involves selection of important sentences or grammatical sub-sentences <ref type="bibr" target="#b19">(Jing, 2000;</ref><ref type="bibr" target="#b21">Knight and Marcu, 2002;</ref><ref type="bibr" target="#b5">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b10">Filippova et al., 2015)</ref>. Advent of sequence-to-sequence deep neural networks and large human summarization datasets <ref type="bibr" target="#b16">(Hermann et al., 2015;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016)</ref> made the abstractive summarization task more feasible and accurate, with recent ideas ranging from copypointer mechanism and redundancy coverage, to metric reward based reinforcement learning <ref type="bibr" target="#b33">(Rush et al., 2015;</ref><ref type="bibr" target="#b4">Chopra et al., 2016;</ref><ref type="bibr" target="#b31">Ranzato et al., 2015;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016;</ref><ref type="bibr" target="#b34">See et al., 2017)</ref>.</p><p>A good abstractive summary requires several important properties, e.g., it should choose the most salient information from the input document, be logically entailed by it, and avoid redundancy. Coverage-based models address the latter redundancy issue <ref type="bibr" target="#b36">(Suzuki and Nagata, 2016;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016;</ref><ref type="bibr" target="#b34">See et al., 2017)</ref>, but there is still a lot of scope to teach current state-of-the-art models about saliency and logical entailment. Towards this goal, we improve the task of abstractive summarization via a reinforcement learning approach with the introduction of two novel rewards: 'ROUGESal' and 'Entail', and also demonstrate that these saliency and entailment skills allow for better generalizability and transfer.</p><p>Our ROUGESal reward gives higher weight to the important, salient words in the summary, in contrast to the traditional ROUGE metric which gives equal weight to all tokens. These weights are obtained from a novel saliency scorer, which is trained on a reading comprehension dataset's answer spans to give a saliency-based probability score to every token in the sentence. Our Entail reward gives higher weight to summaries whose sentences logically follow from the ground-truth summary. Further, we also add a length normalization constraint to our Entail reward, to importantly avoid misleadingly high entailment scores to very short sentences.</p><p>Empirically, we show that our new rewards with policy gradient approaches perform significantly better than a cross-entropy based state-of-the-art pointer-coverage baseline. We show further performance improvements by combining these rewards via our novel multi-reward optimization approach, where we optimize multiple rewards simultaneously in alternate mini-batches (hence avoiding complex scaling and weighting issues in reward combination), inspired from how humans take multiple concurrent types of rewards (feedback) to learn a task. Overall, our methods achieve the new state-of-the-art (including human evaluation) on the CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002. Lastly, we present several analyses of our model's saliency, entailment, and abstractiveness skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Earlier summarization work was based on extraction and compression-based approaches <ref type="bibr" target="#b19">(Jing, 2000;</ref><ref type="bibr" target="#b21">Knight and Marcu, 2002;</ref><ref type="bibr" target="#b5">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b10">Filippova et al., 2015)</ref>, with more focus on graph-based <ref type="bibr" target="#b13">(Giannakopoulos, 2009;</ref><ref type="bibr" target="#b11">Ganesan et al., 2010)</ref> and discourse tree-based <ref type="bibr" target="#b12">(Gerani et al., 2014)</ref> models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees <ref type="bibr" target="#b3">(Cheung and Penn, 2014;</ref><ref type="bibr" target="#b37">Wang et al., 2016)</ref>, Abstract Meaning Representations <ref type="bibr" target="#b24">(Liu et al., 2015;</ref><ref type="bibr" target="#b8">Dohare and Karnick, 2017)</ref>, and neural network models with pointercopy mechanism and coverage <ref type="bibr" target="#b33">(Rush et al., 2015;</ref><ref type="bibr" target="#b4">Chopra et al., 2016;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016;</ref><ref type="bibr" target="#b34">See et al., 2017)</ref>, as well as reinforcebased metric rewards <ref type="bibr" target="#b31">(Ranzato et al., 2015;</ref><ref type="bibr" target="#b29">Paulus et al., 2017)</ref>. We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods.</p><p>Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&amp;A and IE tasks <ref type="bibr" target="#b15">(Harabagiu and Hickl, 2006;</ref><ref type="bibr" target="#b6">Dagan et al., 2006;</ref><ref type="bibr" target="#b22">Lai and Hockenmaier, 2014;</ref><ref type="bibr" target="#b18">Jimenez et al., 2014)</ref>. Recent neural network models and large datasets <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b38">Williams et al., 2017)</ref> enabled stronger accuracies. Some previous work <ref type="bibr" target="#b25">(Mehdad et al., 2013;</ref><ref type="bibr" target="#b14">Gupta et al., 2014)</ref> has explored the use of RTE by modeling graphbased relationships between sentences to select the most non-redundant sentences for summarization. Recently, <ref type="bibr" target="#b28">Pasunuru and Bansal (2017)</ref> improved video captioning with entailment-corrected rewards. We instead directly use multi-sentence entailment knowledge (with additional length constraints) as a separate RL reward to improve abstractive summarization, while avoiding their penalty hyperparameter tuning.</p><p>For our saliency prediction model, we make use of the SQuAD reading comprehension dataset <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref>, where the answer spans annotated by humans for important questions, serve as an interesting and effective proxy for keyphrase-style salient information in summarization. Some related previous work has incorporated document topic/subject classification <ref type="bibr" target="#b17">(Isonuma et al., 2017)</ref> and webpage keyphrase extraction <ref type="bibr" target="#b42">(Zhang et al., 2004)</ref> to improve saliency in summarization. Some recent work <ref type="bibr" target="#b35">Subramanian et al. (2017)</ref> has also used answer probabilities in a document to improve question generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Sequence-to-Sequence Model</head><p>Our abstractive text summarization model is a simple sequence-to-sequence single-layer bidirectional encoder and unidirectional decoder LSTM-RNN, with attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, pointer-copy, and coverage mechanism -please refer to <ref type="bibr" target="#b34">See et al. (2017)</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Policy Gradient Reinforce</head><p>Traditional cross-entropy loss optimization for sequence generation has an exposure bias issue and the model is not optimized for the evaluated metrics <ref type="bibr" target="#b31">(Ranzato et al., 2015)</ref>. Reinforce-based policy gradient approach addresses both of these issues by using its own distribution during training and by directly optimizing the non-differentiable evaluation metrics as rewards. We use the RE-INFORCE algorithm <ref type="bibr" target="#b39">(Williams, 1992;</ref><ref type="bibr" target="#b41">Zaremba and Sutskever, 2015)</ref> to learn a policy p θ defined by the model parameters θ to predict the next action (word) and update its internal (LSTM) states. We minimize the loss function L RL = −E w s ∼p θ [r(w s )], where w s is the sequence of sampled words with w s t sampled at time step t of the decoder. The derivative of this loss function with approximation using a single sample along with variance reduction with a bias estimator is:</p><formula xml:id="formula_0">∇ θ L RL = −(r(w s ) − b e )∇ θ log p θ (w s ) (1)</formula><p>There are several ways to calculate the baseline estimator; we employ the effective SCST approach <ref type="bibr" target="#b32">(Rennie et al., 2016)</ref>, as depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, where b e = r(w a ), is based on the reward obtained by the current model using the test time inference algorithm, i.e., choosing the arg-max word w a t of the final vocabulary distribution at each time step t of the decoder. We use the joint cross-entropy and reinforce loss so as to optimize the non-differentiable evaluation metric as reward while also maintaining the readability of the generated sentence <ref type="bibr" target="#b40">(Wu et al., 2016;</ref><ref type="bibr" target="#b29">Paulus et al., 2017;</ref><ref type="bibr" target="#b28">Pasunuru and Bansal, 2017)</ref>, which is de-</p><formula xml:id="formula_1">fined as L Mixed = γL RL + (1 − γ)L XE , where γ is a tunable hyperparameter.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Reward Optimization</head><p>Optimizing multiple rewards at the same time is important and desired for many language generation tasks. One approach would be to use a weighted combination of these rewards, but this has the issue of finding the complex scaling and weight balance among these reward combinations.</p><p>To address this issue, we instead introduce a simple multi-reward optimization approach inspired from multi-task learning, where we have different tasks, and all of them share all the model parameters while having their own optimization function (different reward functions in this case). If r 1 and r 2 are two reward functions that we want to optimize simultaneously, then we train the two loss functions of Eqn. 2 in alternate mini-batches.</p><formula xml:id="formula_2">L RL 1 = −(r 1 (w s ) − r 1 (w a ))∇ θ log p θ (w s ) L RL 2 = −(r 2 (w s ) − r 2 (w a ))∇ θ log p θ (w s )<label>(2)</label></formula><p>4 Rewards ROUGE Reward The first basic reward is based on the primary summarization metric of ROUGE package <ref type="bibr" target="#b23">(Lin, 2004)</ref>. Similar to <ref type="bibr" target="#b29">Paulus et al. (2017)</ref>, we found that ROUGE-L metric as a reward works better compared to ROUGE-1 and ROUGE-2 in terms of improving all the metric scores. 1 Since these metrics are based on simple phrase matching/n-gram overlap, they do not focus on important summarization factors such as salient phrase inclusion and directed logical entailment. Addressing these issues, we next introduce two new reward functions.   <ref type="figure" target="#fig_1">Fig. 2</ref>, given a sentence as input, the predictor assigns a saliency probability to every token, using a simple bidirectional encoder with a softmax layer at every time step of the encoder hidden states to classify the token as salient or not. Finally, we use the probabilities given by this saliency prediction model as weights in the ROUGE matching formulation to achieve the final ROUGESal score (see appendix for details about our ROUGESal weighted precision, recall, and F-1 formulations).</p><p>Entailment Reward A good summary should also be logically entailed by the given source document, i.e., contain no contradictory or unrelated information. <ref type="bibr" target="#b28">Pasunuru and Bansal (2017)</ref> used entailment-corrected phrase-matching metrics (CIDEnt) to improve the task of video captioning; we instead directly use the entailment knowledge from an entailment scorer and its multisentence, length-normalized extension as our 'Entail' reward, to improve the task of abstractive text summarization. We train the entailment classifier (Parikh et al., 2016) on the SNLI <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> and Multi-NLI <ref type="bibr" target="#b38">(Williams et al., 2017)</ref> datasets and calculate the entailment probability score between the ground-truth (GT) summary (as premise) and each sentence of the generated summary (as hypothesis), and use avg. score as our  <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref>. All dataset splits and other training details (dimension sizes, learning rates, etc.) for reproducibility are in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We use the standard ROUGE package <ref type="bibr" target="#b23">(Lin, 2004)</ref> and Meteor package <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref> for reporting the results on all of our summarization models. Following previous work <ref type="bibr" target="#b4">(Chopra et al., 2016;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016;</ref><ref type="bibr" target="#b34">See et al., 2017)</ref>, we use the ROUGE full-length F1 variant. Human Evaluation Criteria: We also performed human evaluation of summary relevance and readability, via Amazon Mechanical Turk (AMT). We selected human annotators that were located in the US, had an approval rate greater than 98%, and had at least 10, 000 approved HITs. For the pairwise model comparisons discussed in Sec. 6, we 2 Since the GT summary is correctly entailed by the source document, we directly (by transitivity) use this GT as premise for easier (shorter) encoding. We also tried using the full input document as premise but this didn't perform as well (most likely because the entailment classifiers are not trained on such long premises; and the problem with the sentence-tosentence avg. scoring approach is discussed below). We also tried summary-to-summary entailment scoring (similar to ROUGE-L) as well as pairwise sentence-to-sentence avg. scoring, but we found that avg. scoring of groundtruth summary (as premise) w.r.t. each generated summary's sentence (as hypothesis) works better (intuitive because each sentence in generated summary might be a compression of multiple sentences of GT summary or source document).  represents previous work on anonymous version. 'XE': cross-entropy loss, 'RL': reinforce mixed loss (XE+RL). Columns 'R': ROUGE, 'M': METEOR.</p><p>showed the annotators the input article, the ground truth summary, and the two model summaries (randomly shuffled to anonymize model identities) -we then asked them to choose the better among the two model summaries or choose 'Not-Distinguishable' if both summaries are equally good/bad. Instructions for relevance were based on the summary containing salient/important information from the given article, being correct (i.e., avoiding contradictory/unrelated information), and avoiding redundancy. Instructions for readability were based on the summary's fluency, grammaticality, and coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Baseline Cross-Entropy Model Results Our abstractive summarization model has attention, pointer-copy, and coverage mechanism. First, we apply cross-entropy optimization and achieve comparable results on CNN/Daily Mail w.r.t. previous work <ref type="bibr" target="#b34">(See et al., 2017)</ref>. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE Reward Results</head><p>First, using ROUGE-L as RL reward (shown as ROUGE in <ref type="table">Table 1</ref>) improves the performance on CNN/Daily Mail in all metrics with stat. significant scores (p &lt; 0.001) as compared to the cross-entropy baseline (and also stat. signif. w.r.t. <ref type="bibr" target="#b34">See et al. (2017)</ref>). Similar to <ref type="bibr" target="#b29">Paulus et al. (2017)</ref>, we use mixed loss function (XE+RL) for all our reinforcement experiments, to ensure good readability of generated summaries.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Output Analysis</head><p>Saliency Analysis We analyzed the output summaries generated by <ref type="bibr" target="#b34">See et al. (2017)</ref>, and our baseline, ROUGE-reward and ROUGESal-reward models, using our saliency prediction model (Sec. 4) as the keyword detection classifier. We annotated the ground-truth and model summaries with this keyword classifier and computed the % match, i.e., how many salient words from the ground-truth summary were also generated in the model summary 6 , and the scores are 27.95%, 28.00%, 28.80%, and 30.86%. We also used the original CNN/Daily Mail Cloze Q&amp;A setup <ref type="bibr" target="#b16">(Hermann et al., 2015)</ref> with the fill-in-the-blank answers treated as salient information, and the results are 60.66%, 59.36%, 60.67%, and 64.66% for the four models. Further, we also calculated the ROUGESal scores (based on our reward formulation in Sec. 4), and the results are 42.04%, 42.14%, 43.05%, and 46.56% for the four models. All three of these saliency analysis experiments illustrate that our ROUGESal reward model is stat. signif. better in saliency than the <ref type="bibr" target="#b34">See et al. (2017)</ref>, our baseline, and ROUGE-reward models (p &lt; 0.001 for all three experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entailment Analysis</head><p>We also analyzed the entailment scores of the generated summaries from <ref type="bibr" target="#b34">See et al. (2017)</ref>, and our baseline, ROUGEreward, and Entail-reward models, and the results are 27. <ref type="bibr">33%, 27.21%, 28.23%, and 28.98%. 7</ref> We observe that our Entail-reward model achieves stat. significant entailment scores (p &lt; 0.001) w.r.t. all the other three models.</p><p>Models 2-gram 3-gram 4-gram <ref type="bibr" target="#b34">See et al. (2017)</ref> 2.24 6.03 9.72 Baseline <ref type="bibr">(XE)</ref> 2.23 5.58 8.81 ROUGE <ref type="bibr">(RL)</ref> 2.69 6.57 10.23 ROUGESal <ref type="bibr">(RL)</ref> 2.37 6.00 9.50 Entail <ref type="bibr">(RL)</ref> 2.63 6.56 10.26 <ref type="table">Table 4</ref>: Abstractiveness: novel n-gram percentage.</p><p>Abstractiveness Analysis In order to measure the abstractiveness of our models, we followed the 'novel n-gram counts' approach suggested in <ref type="bibr" target="#b34">See et al. (2017)</ref>. First, we found that all our rewardbased RL models have significantly (p &lt; 0.01) more novel n-grams than our cross-entropy baseline (see <ref type="table">Table 4</ref>). Next, the Entail-reward model 'maintains' stat. equal abstractiveness as the ROUGE-reward model, likely because it encourages rewriting to create logical subsets of information, while the ROUGESal-reward model does a bit worse, probably because it focuses on copying more salient information (e.g., names). Compared to previous work <ref type="bibr" target="#b34">(See et al., 2017)</ref>, our Entailreward and ROUGE-reward models achieve statistically significant improvement (p &lt; 0.01) while ROUGESal is comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a summarization model trained with novel RL reward functions to improve the saliency and directed logical entailment aspects of a good summary. Further, we introduced the novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate minibatches. We achieve the new state-of-the-art on CNN/Daily Mail and also strong test-only improvements on a DUC-2002 transfer setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Saliency Rewards</head><p>Here, we describe the ROUGE-L formulation at summary-level and later describe how we incorporate saliency information into it. Given a reference summary of u sentences containing a total of m tokens ({w r,k } m k=1 ) and a generated summary of v sentences with a total of n tokens ({w c,k } n k=1 ), let r i be the reference summary sentence and c j be the generated summary sentence. Then, the precision (P lcs ), recall (R lcs ), and F-score (F lcs ) for ROUGE-L are defined as follows:</p><formula xml:id="formula_3">P lcs = u i=1 LCS ∪ (r i , C) n (4) R lcs = u i=1 LCS ∪ (r i , C) m (5) F lcs = (1 + β 2 )R lcs P lcs R lcs + β 2 P lcs<label>(6)</label></formula><p>where LCS ∪ takes the union Longest Common Subsequence (LCS) between a reference summary sentence r i and every generated summary sentence c j (c j ∈ C), and β is defined in <ref type="bibr" target="#b23">Lin (2004)</ref>.</p><p>In the above ROUGE-L scores, we assume that every token has equal weight, i.e, 1. However, every summary has salient tokens which should be rewarded with more weight. Hence, we use the weights obtained from our novel saliency predictor to modify the ROUGE-L scores with salient information as follows: </p><formula xml:id="formula_4">P s lcs = u i=1 LCS * ∪ (r i , C) n k=1 η(w c,k )<label>(7)</label></formula><formula xml:id="formula_5">R s lcs = u i=1 LCS * ∪ (r i , C) m k=1 η(w r,k )<label>(8)</label></formula><p>where η(w) is the weight assigned by the saliency predictor for token w, and β is defined in <ref type="bibr" target="#b23">Lin (2004)</ref>. 8 Let {w k } p k=1 be the union LCS set, then LCS * ∪ (r i , C) is defined as follows:</p><formula xml:id="formula_7">LCS * ∪ (r i , C) = p k=1 η(w k )<label>(10)</label></formula><p>8 If a token is repeated at multiple times in the input sentence, we average the probabilities of those instances.  <ref type="bibr" target="#b16">(Hermann et al., 2015;</ref><ref type="bibr" target="#b26">Nallapati et al., 2016)</ref> is a collection of online articles and their summaries. The summaries are based on the human written highlights of these articles. The dataset has 287, 226 training pairs, 13, 368 validation pairs, and 11, 490 test pairs. We use the non-anonymous version of the dataset as described in <ref type="bibr" target="#b34">See et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC Test Corpus</head><p>We use the DUC-2002 single document summarization dataset 9 as a test-only setup where we directly take the pretrained models trained on CNN/Daily Mail dataset and test them on DUC-2002, in order to check for our model's domain transfer capabilities. This corpus consists of 567 documents with one or two human annotated reference summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI and MultiNLI corpus</head><p>We use the full Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> and the recent Multi-NLI corpus <ref type="bibr" target="#b38">(Williams et al., 2017)</ref> data for building our entailment classifier. We use the standard splits following previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD Dataset</head><p>We use Stanford Question Answering Dataset (SQuAD) for our saliency prediction model. We process the SQuAD dataset to collect the sentence and their corresponding salient phrases pairs. Here again, we use the standard split following previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Training Details</head><p>During training, all our LSTM-RNNs are set with hidden state size of 256. We use a vocabulary size of 50k, where word embeddings are represented in 128 dimension, and both the encoder and decoder share the same embedding for each word. We encode the source document using a 400 timestep unrolled LSTM-RNN and 100 time-step unrolled LSTM-RNN for decoder. We clip the gradients to a maximum gradient norm value of 2.0 and use Adam optimizer <ref type="bibr" target="#b20">(Kingma and Ba, 2015)</ref> with a learning rate of 1 × 10 −3 for pointer baseline and 1 × 10 −4 while training along with coverage loss, and 1×10 −6 for reinforcement learning. Following <ref type="bibr" target="#b34">See et al. (2017)</ref>, we add coverage mechanism to a converged pointer model. For mixed-9 http://www-nlpir.nist.gov/projects/duc/ guidelines/2002.html Models Accuracy Entailment Classifier 74.50% Saliency Predictor 16.87% <ref type="table">Table 5</ref>: Performance of our entailment classifier and saliency predictor. loss (XE+RL) optimization, we use the following γ values for various rewards: 0.9985 for ROUGE, 0.9999 for Entail and ROUGE+Entail, and 0.9995 for ROUGESal and ROUGESal+Entail. For reinforcement learning, we only use 5000 training samples (&lt; 2% of the actual data) to speed up convergence, but we found it to work well in practice.</p><p>During inference time, we use a beam search of size 4. <ref type="table">Table 5</ref> presents the performance of our saliency predictor (on the SQuAD-based dev set for answer span classification accuracy) and entailment classifier (on the Multi-NLI dev set accuracy). Our entailment classifier is comparable to the state-ofthe-art models. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Saliency and Entailment Scorer</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our sequence generator with RL training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our saliency predictor model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label></label><figDesc>http://www-nlpir.nist.gov/projects/duc/ guidelines/2002.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Entail reward. 2 Finally, we add a length normalization constraint to avoid very short sentences achieving misleadingly high entailment scores:</figDesc><table><row><cell>#tokens in generated summary #tokens in reference summary</cell><cell>(3)</cell></row><row><cell>5 Experimental Setup</cell><cell></cell></row><row><cell>5.1 Datasets and Training Details</cell><cell></cell></row><row><cell cols="2">CNN/Daily Mail dataset (Hermann et al., 2015;</cell></row><row><cell cols="2">Nallapati et al., 2016) is a collection of online</cell></row><row><cell cols="2">news articles and their summaries. We use the</cell></row><row><cell cols="2">non-anonymous version of the dataset as described</cell></row><row><cell cols="2">in See et al. (2017). For test-only generaliza-</cell></row><row><cell cols="2">tion experiments, we use the DUC-2002 single</cell></row><row><cell cols="2">document summarization dataset 3 . For entailment</cell></row><row><cell cols="2">reward classifier, we use a combination of the</cell></row></table><note>Entail = Entail ×full Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and the recent Multi-NLI corpus (Williams et al., 2017) training datasets. For our saliency prediction model, we use the Stanford Question Answering (SQuAD) dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>14.57 32.19 14.36 ROUGE (RL) 35.97 15.45 32.72 14.50 ROUGESal+Ent (RL) 38.95 17.05 35.52 16.47</figDesc><table><row><cell>Models</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>M</cell></row><row><cell>Baseline (XE)</cell><cell>35.50</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>ROUGE F1 full length scores of our models on test-only DUC-2002 generalizability setup.</figDesc><table><row><cell>Models</cell><cell cols="3">Relevance Readability Total</cell></row><row><cell>ROUGESal+Ent</cell><cell>55</cell><cell>54</cell><cell>109</cell></row><row><cell>See et al. (2017)</cell><cell>34</cell><cell>33</cell><cell>67</cell></row><row><cell>Non-distinguish.</cell><cell>11</cell><cell>13</cell><cell>24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Human Evaluation: pairwise comparison of relevance and readability between our ROUGE-Sal+Entail multi-reward model and See et al. (2017).</figDesc><table><row><cell>ity/transfer skills, where we take the models</cell></row><row><cell>trained on CNN/Daily Mail and directly test them</cell></row><row><cell>on DUC-2002 in a test-only setup. As shown in</cell></row><row><cell>Table 2, our final ROUGESal+Entail multi-reward</cell></row><row><cell>RL model is statistically significantly better than</cell></row><row><cell>both the cross-entropy (pointer-generator + cov-</cell></row><row><cell>erage) baseline as well as ROUGE reward RL</cell></row><row><cell>model, in terms of all 4 metrics with a large mar-</cell></row><row><cell>gin (with p &lt; 0.001). This demonstrates that our</cell></row><row><cell>ROUGESal+Entail model learned better transfer-</cell></row><row><cell>able and generalizable skills of saliency and logi-</cell></row><row><cell>cal entailment.</cell></row><row><cell>ROUGESal and Entail Reward Results With</cell></row><row><cell>our novel ROUGESal reward, we achieve stat.</cell></row><row><cell>signif. improvements in all metrics w.r.t. the</cell></row><row><cell>baseline as well as w.r.t. ROUGE-reward results</cell></row><row><cell>(p &lt; 0.001), showing that saliency knowledge</cell></row><row><cell>is strongly improving the summarization model.</cell></row><row><cell>For our Entail reward, we achieve stat. signif.</cell></row><row><cell>improvements in ROUGE-L (p &lt; 0.001) w.r.t.</cell></row><row><cell>baseline and achieve the best METEOR score by</cell></row><row><cell>a large margin. See Sec. 7 for analysis of the</cell></row><row><cell>saliency/entailment skills learned by our models.</cell></row><row><cell>Multi-Reward Results Similar to ROUGESal,</cell></row><row><cell>Entail is a better reward when combined with</cell></row><row><cell>the complementary phrase-matching metric in-</cell></row><row><cell>formation in ROUGE; Table 1 shows that the</cell></row><row><cell>ROUGE+Entail multi-reward combination per-</cell></row><row><cell>forms stat. signif. better than ROUGE-reward</cell></row><row><cell>in ROUGE-1, ROUGE-L, and METEOR (p &lt;</cell></row><row><cell>0.001), and better than Entail-reward in all</cell></row><row><cell>ROUGE metrics. Finally, we combined our two</cell></row><row><cell>rewards ROUGESal+Entail to incorporate both</cell></row><row><cell>saliency and entailment knowledge, and it gives</cell></row><row><cell>the best results overall (p &lt; 0.001 in all metrics</cell></row><row><cell>w.r.t. both baseline and ROUGE-reward models),</cell></row><row><cell>setting the new state-of-the-art. 5</cell></row><row><cell>Human Evaluation Table. 3 shows the MTurk</cell></row><row><cell>anonymous human evaluation study (based on 100</cell></row><row><cell>samples), where we do pairwise comparison be-</cell></row><row><cell>tween our ROUGESal+Entail multi-reward's out-</cell></row><row><cell>put summaries w.r.t. See et al. (2017) summaries</cell></row><row><cell>on CNN/Daily Mail (see setup details in Sec. 5.2).</cell></row><row><cell>As shown, our multi-reward model is better on</cell></row><row><cell>both relevance and readability.</cell></row><row><cell>Test-Only Transfer (DUC-2002) Results Fi-</cell></row><row><cell>nally, we also tested our model's generalizabil-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the rest of the paper, we mean ROUGE-L whenever we mention ROUGE-reward models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our baseline is statistically equal to the paper-reported scores of<ref type="bibr" target="#b34">See et al. (2017)</ref> (seeTable 1) on ROUGE-1, ROUGE-2, based on the bootstrap test<ref type="bibr" target="#b9">(Efron and Tibshirani, 1994)</ref>. Our baseline is stat. significantly better (p &lt; 0.001) in all ROUGE metrics w.r.t. the github scores(R-1: 38.82,  R-2: 16.81, R-3: 35.71, M: 18.14)  of<ref type="bibr" target="#b34">See et al. (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our last three rows inTable 1are all stat. signif. better in all metrics with p &lt; 0.001 compared to<ref type="bibr" target="#b34">See et al. (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In order to select the keywords for this analysis, we used a 0.2 probability threshold on the saliency classifier (based on the scale of the classifier's distribution).7  Based on our ground-truth summary to output summary sentences' average entailment score (see Sec. 4); similar trends hold for document-to-summary entailment scores.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">RepEval leaderboard: https://repeval2017. github.io/shared/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. This work was supported by DARPA (YFA17-D17AP00022), Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised sentence enhancement for automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="775" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Text summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibhansh</forename><surname>Dohare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01678</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on computational linguistics. ACL</title>
		<meeting>the 23rd international conference on computational linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic summarization from multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Text summarization through entailment-based minimum vertex cover. Lexical and Computational Semantics (* SEM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manpreet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Mirkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Methods for using textual entailment in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extractive summarization using multi-task learning with document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="732" to="742" />
		</imprint>
	</monogr>
	<note>Av Juan Dios Bátiz, and Av Mendizábal</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence reduction for automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Illinois-LH: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. SemEval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL: HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization with entailment and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th European Workshop on Natural Language Generation</title>
		<meeting>of the 14th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforced video captioning with entailment rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural models for key phrase detection and question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rnn-based encoder-decoder approach with word frequency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A sentence compression based framework to query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07548</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521362</idno>
		<title level="m">Reinforcement learning neural turing machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">World wide web site summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nur</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Intelligence and Agent Systems: An International Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="53" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
