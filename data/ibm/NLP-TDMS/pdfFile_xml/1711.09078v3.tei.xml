<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Enhancement with Task-Oriented Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Baian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Donglai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
						</author>
						<title level="a" type="main">Video Enhancement with Task-Oriented Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a selfsupervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and video super-resolution. Most motion-based video processing algorithms use a two-step approach <ref type="bibr" target="#b24">(Liu and Sun 2011;</ref><ref type="bibr" target="#b3">Baker et al 2011;</ref><ref type="bibr" target="#b23">Liu and Freeman 2010)</ref>: they first estimate motion between input frames for frame registration, and then process the registered frames to generate the final output. Therefore, the accuracy of flow estimation greatly affects the performance of these two-step approaches.</p><p>However, precise flow estimation can be challenging and slow for many video enhancement tasks. The brightness constancy assumption, which many motion estimation algorithms rely on, may fail due to variations in lighting and pose, as well as the presence of motion blur and occlusion. Also, many motion estimation algorithms involve solving a large-scale optimization problem, making it inefficient for real-time applications.</p><p>Moreover, solving for a motion field that matches objects in motion may be sub-optimal for video processing. <ref type="figure">Figure 1</ref> shows an example in frame interpolation. EpicFlow <ref type="bibr" target="#b40">(Revaud et al 2015)</ref>, one of the state-of-the-art motion estimation algorithms, calculates a precise motion field (I-b) whose boundary is well-aligned with the fingers in the image (I-c); however, the interpolated frame (I-c) based on it still contains obvious artifacts due to occlusion. This is because EpicFlow only matches the visible parts between the two frames; however, for interpolation we also need to inpaint the occluded regions, where EpicFlow cannot help. In contrast, task-oriented flow, which we will soon introduce, learns to handle occlusions well (I-e), though its estimated motion field (I-d) differs from the ground truth optical flow. Similarly, in video denoising, EpicFlow can only estimate the movement of the girl's hair (II-b), but our task-oriented flow (II-d) can remove the noise in the input. Therefore, the frame denoised by ours is much cleaner than that by EpicFlow (II-e). For specific video processing tasks, there exist motion representations that do not match the actual object movement, but lead to better results.  <ref type="figure">Fig. 1</ref>: Many video processing tasks, e.g., temporal frame-interpolation (top) and video denoising (bottom), rely on flow estimation. In many cases, however, precise optical flow estimation is intractable and could be suboptimal for a specific task. For example, although EpicFlow <ref type="bibr" target="#b40">(Revaud et al 2015)</ref> predicts precise movement of objects (I-b, the flow field aligns well with object boundaries), small errors in estimated flow fields result in obvious artifacts in interpolated frames, like the obscure fingers in (I-c). With the task-oriented flow proposed in this work <ref type="bibr">(I-d)</ref>, those interpolation artifacts disappear as in <ref type="bibr">(I-e)</ref>. Similarly, in video denoising, our task-oriented flow (II-d) deviates from EpicFlow (II-b), but leads to a cleaner output frame (II-e). Flow visualization is based on the color wheel shown on the corner of (I-b).</p><p>In this paper, we propose to learn this task-oriented flow (TOFlow) representation with an end-to-end trainable convolutional network that performs motion analysis and video processing simultaneously. Our network consists of three modules: the first estimates the motion fields between input frames; the second registers all input frames based on estimated motion fields; and the third generates target output from registered frames. These three modules are jointly trained to minimize the loss between output frames and ground truth. Unlike other flow estimation networks <ref type="bibr" target="#b39">(Ranjan and Black 2017;</ref><ref type="bibr" target="#b9">Fischer et al 2015)</ref>, the flow estimation module in our framework predicts a motion field tailored to a specific task, e.g., frame interpolation or video denoising, as it is jointly trained with the corresponding video processing module.</p><p>Several papers have incorporated a learned motion estimation network in burst processing <ref type="bibr" target="#b42">(Tao et al 2017;</ref><ref type="bibr" target="#b26">Liu et al 2017)</ref>. In this paper, we move beyond to demonstrate not only how joint learning helps, but also why it helps. We show that a jointly trained network learns task-specific features for better video processing. For example, in video denoising, our TOFlow learns to reduce the noise in the input, while traditional optical flow keeps the noisy pixels in the registered frame. TOFlow also reduces artifacts near occlusion boundaries. Our goal in this paper is to build a standard framework for better understanding when and how task-oriented flow works.</p><p>To evaluate the proposed TOFlow, we have also built a large-scale, high-quality video dataset for video processing. Most existing large video datasets, such as Youtube-8M (Abu-El-Haija et al 2016), are designed for high-level vision tasks like event classification. The videos are often of low resolutions with significant motion blurs, making them less useful for video processing. We introduce a new dataset, Vimeo-90K, for a systematic evaluation of video processing algorithms. Vimeo-90K consists of 89,800 high-quality video clips (i.e. 720p or higher) downloaded from Vimeo. We build three benchmarks from these videos for interpolation, denoising or deblocking, and super-resolution, respectively. We hope these benchmarks will also help improve learning-based video processing techniques with their highquality videos and diverse examples. This paper makes three contributions. First, we propose TOFlow, a flow representation tailored to specific video processing tasks, significantly outperforming standard optical flow. Second, we propose an end-to-end trainable video processing framework that handles frame interpolation, video denoising, and video super-resolution. The flow network in our framework is fine-tuned by minimizing a task-specific, self-supervised loss. Third, we build a large-scale, highquality video processing dataset, Vimeo-90K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Optical flow estimation. Dated back to <ref type="bibr" target="#b13">Horn and Schunck (1981)</ref>, most optical flow algorithms have sought to minimize hand-crafted energy terms for image alignment and flow smoothness <ref type="bibr" target="#b32">(Mémin and Pérez 1998;</ref><ref type="bibr" target="#b4">Brox et al 2004</ref><ref type="bibr" target="#b5">Brox et al , 2009</ref><ref type="bibr" target="#b45">Wedel et al 2009)</ref>. Current state-of-the-art methods like EpicFlow <ref type="bibr" target="#b40">(Revaud et al 2015)</ref> or DC Flow <ref type="bibr" target="#b48">(Xu et al 2017)</ref> further exploit image boundary and segment cues to improve the flow interpolation among sparse matches. Recently, end-to-end deep learning methods were proposed for faster inference <ref type="bibr" target="#b9">(Fischer et al 2015;</ref><ref type="bibr" target="#b39">Ranjan and Black 2017;</ref><ref type="bibr" target="#b51">Yu et al 2016)</ref>. We use the same network structure for motion estimation as SpyNet <ref type="bibr" target="#b39">(Ranjan and Black 2017)</ref>. But instead of training it to minimize the flow estimation error, as SpyNet does, we train it jointly with a video processing network to learn a flow representation that is the best for a specific task.</p><p>Low-level video processing. We focus on three video processing tasks: frame interpolation, video denoising, and video super-resolution. Most existing algorithms in these areas explicitly estimate the dense correspondence among input frames, and then reconstruct the reference frame according to image formation models for frame interpolation <ref type="bibr" target="#b3">(Baker et al 2011;</ref><ref type="bibr" target="#b47">Werlberger et al 2011;</ref><ref type="bibr" target="#b52">Yu et al 2013;</ref><ref type="bibr" target="#b17">Jiang et al 2018;</ref><ref type="bibr" target="#b41">Sajjadi et al 2018)</ref>, video super-resolution <ref type="bibr" target="#b25">(Liu and Sun 2014;</ref><ref type="bibr" target="#b22">Liao et al 2015)</ref>, and denoising <ref type="bibr" target="#b23">(Liu and Freeman 2010;</ref><ref type="bibr" target="#b43">Varghese and Wang 2010;</ref><ref type="bibr" target="#b29">Maggioni et al 2012;</ref><ref type="bibr" target="#b33">Mildenhall et al 2018;</ref><ref type="bibr" target="#b12">Godard et al 2017)</ref>. We refer readers to survey articles <ref type="bibr" target="#b34">(Nasrollahi and Moeslund 2014;</ref><ref type="bibr" target="#b11">Ghoniem et al 2010)</ref> for comprehensive literature reviews on these flourishing research topics.</p><p>Deep learning for video enhancement. Inspired by the success of deep learning, researchers have directly modeled enhancement tasks as regression problems without representing motions, and have designed deep networks for frame interpolation <ref type="bibr" target="#b31">(Mathieu et al 2016;</ref><ref type="bibr" target="#b36">Niklaus et al 2017a;</ref><ref type="bibr" target="#b16">Jiang et al 2017;</ref><ref type="bibr" target="#b35">Niklaus and Liu 2018)</ref>, super-resolution <ref type="bibr" target="#b14">(Huang et al 2015;</ref><ref type="bibr" target="#b19">Kappeler et al 2016;</ref><ref type="bibr" target="#b42">Tao et al 2017;</ref><ref type="bibr" target="#b6">Bulat et al 2018;</ref><ref type="bibr" target="#b1">Ahn et al 2018;</ref><ref type="bibr" target="#b18">Jo et al 2018)</ref>, <ref type="bibr">denoising Mildenhall et al (2018)</ref>, deblurring <ref type="bibr" target="#b50">Yang et al (2018)</ref>; <ref type="bibr" target="#b2">Aittala and Durand (2018)</ref>, rain drops removal , and video compression artifacts removal <ref type="bibr" target="#b27">(Lu et al 2018)</ref>.</p><p>Recently, with differentiable image sampling layers in deep learning <ref type="bibr" target="#b15">(Jaderberg et al 2015)</ref>, motion information can be incorporated into networks and trained jointly. Such approaches have been applied to video interpolation <ref type="bibr" target="#b26">(Liu et al 2017)</ref>, light-field interpolation , novel view synthesis <ref type="bibr" target="#b53">(Zhou et al 2016)</ref>, eye gaze manipulation <ref type="bibr" target="#b10">(Ganin et al 2016)</ref>, object detection , denoising <ref type="bibr" target="#b46">(Wen et al 2017)</ref>, and super-resolution <ref type="bibr" target="#b8">(Caballero et al 2017;</ref><ref type="bibr" target="#b42">Tao et al 2017;</ref><ref type="bibr" target="#b30">Makansi et al 2017)</ref>. Although many of these algorithms also jointly train the flow estimation with the rest parts of network, there is no systematical study on the advantage of joint training. In this paper, we illustrate the advantage of the trained task-oriented flow through toy examples, and also demonstrate its superiority over general flow algorithm on various real-world tasks. We also present a general framework that can easily adapt to different video processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks</head><p>In the paper, we explore three video enhancement tasks: frame interpolation, video denoising/deblocking, and video super-resolution.</p><p>Temporal frame interpolation. Given a low frame rate video, a temporal frame interpolation algorithm generates a high frame rate video by synthesizing additional frames between two temporally neighboring frames. Specifically, let I 1 and I 3 be two consecutive frames in an input video, the task is to estimate the missing middle frame I 2 . Temporal frame interpolation doubles the video frame rate, and can be recursively applied to generate even higher frame rates.</p><p>Video denoising/deblocking. Given a degraded video with artifacts from either the sensor or compression, video denoising/deblocking aims to remove the noise or compression artifacts to recover the original video. This is typically done by aggregating information from neighboring frames. Specifically, Let {I 1 , I 2 , . . . , I N } be N consecutive, degraded frames in an input video, the task of video denoising is to estimate the middle frame I * ref .</p><p>For the ease of description, in the rest of paper, we simply call both tasks as video denoising.</p><p>Video super-resolution. Similar to video denoising, given N consecutive low-resolution frames as input, the task of video super-resolution is to recover the high-resolution middle frame. In this work, we first upsample all the input frames to the same resolution as the output using bicubic interpolation, and our algorithm only needs to recover the high-frequency component in the output image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Task-Oriented Flow for Video Processing</head><p>Most motion-based video processing algorithms has two steps: motion estimation and image processing. For example, in temporal frame interpolation, most algorithms first estimate how pixels move between input frames (frame 1 and 3), and then move pixels to the estimated location in the output frame (frame 2) <ref type="bibr" target="#b3">(Baker et al 2011)</ref>. Similarly, in video denoising, algorithms first register different frames based on estimated motion fields between them, and then remove noises by aggregating information from registered frames.</p><p>In this paper, we propose to use task-oriented flow (TOFlow) to integrate the two steps, which greatly improves the performance. To learn task-oriented flow, we design an end-to-end trainable network with three parts ( <ref type="figure">Figure 2</ref>): a flow estimation module that estimates the movement of pixels between input frames; an image transformation module that warps all the frames to a reference frame; and a task-specific image processing module that performs video interpolation, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow Estimation Transformation Image Processing</head><p>Not used in interp. <ref type="figure">Fig. 2</ref>: Left: our model using task-oriented flow for video processing. Given an input video, we first calculate the motion between frames through a task-oriented flow estimation network. We then warp input frames to the reference using spatial transformer networks, and aggregate the warped frames to generate a high-quality output image. Right: the detailed structure of flow estimation network (the orange network on the left).</p><p>denoising, or super-resolution on registered frames. Because the flow estimation module is jointly trained with the rest of the network, it learns to predict a flow field that fits to a particular task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy Example</head><p>Before discussing the details of network structure, we first start with two synthetic sequences to demonstrate why our TOFlow can outperform traditional optical flows. The left of <ref type="figure" target="#fig_1">Figure 3</ref> shows an example of frame interpolation, where a green triangle is moving to the bottom in front of a black background. If we warp both the first and the third frames to the second, even using the ground truth flow (Case I, left column), there is an obvious doubling artifact in the warped frames due to occlusion (Case I, middle column, top two rows), which is a well-known problem in the optical flow literature <ref type="bibr" target="#b3">(Baker et al 2011)</ref>. The final interpolation result based on these two warp frames still contains the doubling artifact (Case I, right column, top row). In contrast, TOFlow does not stick to object motion: the background should be static, but it has non-zero motion (Case II, left column). With TOFlow, however, there is barely any artifact in the warped frames (Case II, middle column) and the interpolated frame looks clean (Case II, right column). This is because TOFlow not only synthesize the movement of visible object, but also guide how to inpaint occluded background region by copy-ing pixels from its neighborhood. Also, if the ground truth occlusion mask is available, the interpolation result using ground truth flow will also contain little doubling artifacts (Case I, bottom rows). However, calculating the ground occlusion mask is even harder task than estimate flow, as it also requires inferring the correct depth ordering. On the other side, TOFlow can handle occlusion and synthesize frames better than the ground truth flow without using ground truth occlusion masks and depth ordering information.</p><p>Similarly, on the right of <ref type="figure" target="#fig_1">Figure 3</ref>, we show an example of video denoising. The random small boxes in the input frames are synthetic noises. If we warp the first and the third frames to the second using the ground truth flow, the noisy patterns (random squares) remain, and the denoised frame still contains some noise (Case I, right column. There are some shadows of boxes on the bottom). But if we warp these two frames using TOFlow (Case II, left column), those noisy patterns are also reduced or eliminated (Case II, middle column), and the final denoised frame base on them contains almost no noise, even better than the result by denoising results with ground truth flow and occlusion mask (Case I, bottom rows). This also shows that TOFlow learns to reduce the noise in input frames by inpainting them with neighboring pixels, which traditional flow cannot do. Now we discuss the details of each module as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case I: With Ground Truth Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case II: With Task-Oriented Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Flow Estimation Module</head><p>The flow estimation module calculates the motion fields between input frames. For a sequence with N frames (N = 3 for interpolation and N = 7 for denoising and super-resolution), we select the middle frame as the reference. The flow estimation module consists of N − 1 flow networks, all of which have the same structure and share the same set of parameters. Each flow network (the orange network in <ref type="figure">Figure 2</ref>) takes one frame from the sequence and the reference frame as input, and predicts the motion between them.</p><p>We use the multi-scale motion estimation framework proposed by <ref type="bibr" target="#b39">Ranjan and Black (2017)</ref> to handle the large displacement between frames. The network structure is shown in the right of <ref type="figure">Figure 2</ref>. The input to the network are Gaussian pyramids of both the reference frame and another frame rather than the reference. At each scale, a sub-network takes both frames at that scale and upsampled motion fields from previous prediction as input, and calculates a more accurate motion fields. We uses 4 sub-networks in a flow network, three of which are shown <ref type="figure">Figure 2</ref> (the yellow networks).</p><p>There is a small modification for frame interpolation, where the reference frame (frame 2) is not an input to the network, but what it should synthesize. To deal with that, the motion estimation module for interpolation consists of two flow networks, both taking both the first and third frames as input, and predict the motion fields from the second frame to the first and the third respectively. With these motion fields, the later modules of the network can transform the first and the third frames to the second frame for synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Transformation Module</head><p>Using the predicted motion fields in the previous step, the image transformation module registers all the input frames to the reference frame. We use the spatial transformer networks (Jaderberg et al 2015) (STN) for registration, which is a differentiable bilinear interpolation layer that synthesizes the new frame after transformation. Each STN transforms one input frame to the reference viewpoint, and all N − 1 STNs forms the image transformation module. One important property of this module is that it can back-propagate the gradients from the image processing module to the flow estimation module, so we can learn a flow representation that adapts to different video processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image Processing Module</head><p>We use another convolutional network as the image processing module to generate the final output. For each task, we use a slightly different architecture. Please refer to appendices for details.</p><p>Occluded regions in warped frames. As mentioned Section 4.1, occlusion often results in doubling artifacts in the warped frames. A common way to solve this problem is to mask out occluded pixels in interpolation, for example, <ref type="bibr" target="#b26">Liu et al (2017)</ref> proposed to use an additional network that estimates the occlusion mask and only uses pixels are not occluded.</p><p>Similar to <ref type="bibr" target="#b26">Liu et al (2017)</ref>, we also tried the mask prediction network. It takes the two estimated motion fields as input, one from frame 2 to frame 1, and the other from frame 2 to frame 3 (v 21 and v 23 in <ref type="figure">Figure 4</ref>). It predicts two occlusion masks: m 21 is the mask of the warped frame 2 from frame 1 (I 21 ), and m 23 is the mask of the warped frame 2 from frame 3 (I 23 ). The invalid regions in the warped frames (I 21 and I 23 ) are masked out by multiplying them with their corresponding masks. The middle frame is then calculated through another convolutional neural network with both the warped frames (I 21 and I 23 ) and the masked warped frames (I 21 and I 23 ) as input. Please refer to appendices for details.</p><p>An interesting observation is that, even without the mask prediction network, our flow estimation is mostly robust to occlusion. As shown in the third column of <ref type="figure" target="#fig_2">Figure 5</ref>, the warped frames using TOFlow has little doubling artifacts. Therefore, just from two warped frames without the learned masks, the network synthesizes a decent middle frame (the  top image of the right most column). The mask network is optional, as it only removes some tiny artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>To accelerate the training procedure, we first pre-train some modules of the network and then fine-tune all of them together. Details are described below.</p><p>Pre-training the flow estimation network. Pre-training the flow network consists of two steps. First, for all tasks, we pre-train the motion estimation network on the Sintel dataset <ref type="bibr" target="#b7">(Butler et al 2012)</ref>, a realistically rendered video dataset with ground truth optical flow.</p><p>In the second step, for video denoising and super-resolution, we fine-tune it with noisy or blurry input frames to improve its robustness to these input. For video interpolation, we fine-tune it with frames I 1 and I 3 from video triplets as input, minimizing the l 1 difference between the estimated optical flow and the ground truth flow v 23 (or v 21 ). This enables the flow network to calculate the motion from the unknown frame I 2 to frame I 3 given only frames I 1 and I 3 as input.</p><p>Empirically we find that this two-step pre-training can improve the convergence speed. Also, because the main purpose of pre-training is to accelerate the convergence, we simply use the l 1 difference between estimated optical flow and the ground truth as the loss function, instead of endpoint error in flow literature <ref type="bibr" target="#b5">(Brox et al 2009;</ref><ref type="bibr" target="#b7">Butler et al 2012)</ref>. The choice loss function in the pre-training stage has a minor impact on the final result.</p><p>Pre-training the mask network. We also pre-train our occlusion mask estimation network for video interpolation as an optional component of video processing network before joint training. Two occlusion masks (m 21 and m 23 ) are estimated together with the same network and only optical flow v 21 , v 23 as input. The network is trained by minimizing the l 1 loss between the output masks and pre-computed occlusion masks.</p><p>Joint training. After pre-training, we train all the modules jointly by minimizing the l 1 loss between recovered frame and the ground truth, without any supervision on estimated flow fields. For optimization, we use ADAM (Kingma and Ba 2015) with a weight decay of 10 −4 . We run 15 epochs with batch size 1 for all tasks. The learning rate for denoising/deblocking and super-resolution is 10 −4 , and the learning rate for interpolation is 3 × 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Vimeo-90K Dataset</head><p>To acquire high quality videos for video processing, previous methods <ref type="bibr" target="#b25">(Liu and Sun 2014;</ref><ref type="bibr" target="#b22">Liao et al 2015)</ref> took videos by themselves, resulting in video datasets that are small in size and limited in terms of content. Alternatively, we resort to Vimeo where many videos are taken with professional cameras on diverse topics. In addition, we only search for videos without inter-frame compression (e.g., H.264), so that each frame is compressed independently, avoiding artificial signals introduced by video codecs. As many videos are composed of multiple shots, we use a simple thresholdbased shot detection algorithm to break each video into consistent shots and further use GIST feature <ref type="bibr" target="#b38">(Oliva and Torralba 2001)</ref> to remove shots with similar scene background.</p><p>As a result, we collect a new video dataset from Vimeo, consisting of 4,278 videos with 89,800 independent shots that are different from each other in content. To standardize the input, we resize all frames to the fixed resolution 448×256. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, frames sampled from the dataset contain diverse content for both indoor and outdoor scenes. We keep consecutive frames when the average motion magnitude is between 1-8 pixels. The right column of <ref type="figure" target="#fig_3">Figure 6</ref> shows the histogram of flow magnitude over the whole dataset, where the flow fields are calculated using SpyNet <ref type="bibr" target="#b39">(Ranjan and Black 2017)</ref>.</p><p>We further generate three benchmarks from the dataset for the three video enhancement tasks studied in this paper.</p><p>Vimeo interpolation benchmark. We select 73,171 frame triplets from 14,777 video clips with the following three cri-  teria for the interpolation task. First, more than 5% pixels should have motion larger than 3 pixels between neighboring frames. This criterion removes static videos. Second, l 1 difference between the reference and the warped frame using optical flow (calculated using SpyNet) should be at most 15 intensity levels (the maximum intensity level of an image is 255). This removes frames with large intensity change, which are too hard for frame interpolation. Third, the average difference between motion fields of neighboring frames (v 21 and v 23 ) should be less than 1 pixel. This removes nonlinear motion, as most interpolation algorithms, including ours, are based on linear motion assumption.</p><p>Vimeo denoising/deblocking benchmark. We select 91,701 frame septuplets from 38,990 video clips for the denoising task, using the first two criteria introduced for the interpolation benchmark. For video denoising, we consider two types of noises: a Gaussian noise with a standard deviation of 0.1, and mixed noises including a 10% salt-andpepper noise in addition to the Gaussian noise. For video deblocking, we compress the original sequences using FFmpeg with codec JPEG2000, format J2k, and quantization factor q = {20, 40, 60}.</p><p>Vimeo super-resolution benchmark. We also use the same set of septuplets for denoising to build the Vimeo superresolution benchmark with down-sampling factor of 4: the resolution of input and output images are 112 × 64 and 448×256 respectively. To generate the low-resolution videos from high-resolution input, we use the MATLAB imresize function, which first blurs the input frames using cubic filters and then downsamples videos using bicubic interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate two variations of the proposed network. The first one is to train each module separately: we first pre-train motion estimation, and then train video processing while fixing the flow module. This is similar to the two-step video processing algorithms, and we refer to it as Fixed Flow. The other one is to jointly train all modules as described in Section 4.5, and we refer to it as TOFlow. Both networks are trained on Vimeo benchmarks we collected. We evaluate these two variations on three different tasks and also compare with other state-of-the-art image processing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Frame Interpolation</head><p>Datasets. We evaluate on three datasets: Vimeo interpolation benchmark, the dataset used by Liu et al <ref type="formula">(2017)</ref>  Metrics. We use two quantitative measure to evaluate the performance of interpolation algorithms: peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index.</p><p>Baselines. We first compare our framework with two-step interpolation algorithms. For the motion estimation, we use EpicFlow <ref type="bibr" target="#b40">(Revaud et al 2015)</ref> and SpyNet <ref type="bibr" target="#b39">(Ranjan and Black 2017)</ref>. To handle occluded regions as mentioned in Section 4.4, we calculate the occlusion mask for each frame using the algorithm proposed by <ref type="bibr" target="#b55">Zitnick et al (2004)</ref> and only use non-occluded regions to interpolate the middle frame. Further, we compare with state-of-the-art end-to-end models, Deep Voxel Flow (DVF) <ref type="bibr">(</ref> also compare with Fixed Flow, which is another baseline two-step interpolation algorithm 1 .</p><p>Results. <ref type="table">Table 1</ref> shows our quantitative results 2 . On Vimeo interpolation benchmark, TOFlow in general outperforms the others interpolation algorithms, both the traditional twostep interpolation algorithms (EpicFlow and SpyNet) and recent deep-learning based algorithms (DVF, AdaConv, and SepConv), with a significant margin. Though our model is trained on our Vimeo-90K dataset, it also outperforms DVF on DVF dataset in both PSNR and SSIM. There is also a significant boost over Fixed Flow, showing that the network does learn a better flow representation for interpolation during joint training. <ref type="figure">Figure 7</ref> also shows qualitative results. All the two-step algorithms (EpicFlow and Fixed Flow) generate a doubling artifacts, like the hand in the first row or the head in the second row. AdaConv on the other sides does not have the doubling artifacts, but it tends to generate blurry output, by 1 Note that Fixed Flow or TOFlow only uses 4-level structure of SpyNet for memory efficiency, while the original SpyNet network has 5 levels. <ref type="bibr">2</ref> We did not evaluate AdaConv on DVF dataset, as neither the implementation of AdaConv nor the DVF dataset is publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>PMMST DeepFlow SepConv TOFlow TOFlow Mask   <ref type="bibr" target="#b26">(Liu et al 2017)</ref>, and our TOFlow (with and without mask). Follow the convention of Middlebury flow dataset, we reported the square root error (SSD) between ground truth image and interpolated image in 1) entire images, 2) regions of motion discontinuities, and 3) regions without texture.</p><p>directly synthesizing interpolated frames without a motion module. SepConv increases the sharpness of output frame compared with AdaConv, but there are still artifacts (see the hat on the bottom row). Compared with these methods, TOFlow correctly recovers sharper boundaries and fine details even in presence of large motion. <ref type="table" target="#tab_5">Table 2</ref> shows a qualitative comparison of the proposed algorithms with the best four alternatives on Middlebury <ref type="bibr" target="#b3">(Baker et al 2011)</ref>. We use the sum of square difference (SSD) reported on the official website as the evaluation metric. TOFlow performs better than other interpolation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Video Denoising/Deblocking</head><p>Setup. We first train and evaluate our framework on Vimeo denoising benchmark, with three types of noises: Gaussian noise with standard deviation of 15 intensity levels (Vimeo-Gauss15), Gaussian noise with standard deviation of 25 (Vimeo-Gauss25), and mixture of Gaussian noise and 10% salt-and-pepper noise (Vimeo-Mixed). To compare our network with V-BM4D <ref type="bibr" target="#b29">(Maggioni et al 2012)</ref>, which is a monocular video denoising algorithm, we also transfer all videos in Vimeo Denoising Benchmark to grayscale to create Vimeo-    BW (Gaussian noise only), and retrain our network on it. We also evaluate our framework on the a mono video dataset in V-BM4D.</p><p>Baselines. We compare our framework with the V-BM4D, with the standard deviation of Gaussian noise as its additional input on two grayscale datasets (Vimeo-BW and V-BM4D). As before, we also compare with the Fixed Flow variant of our framework on three RGB datasets (Vimeo-Gauss15, Vimeo-Gauss25, and Vimeo-Mixed).</p><p>Results. We first evaluate TOFlow on the Vimeo dataset with three different noise levels <ref type="table" target="#tab_7">(Table 3)</ref>. TOFlow outper-forms Fixed Flow by a significant margin, demonstrating the effectiveness of joint training. Also, when the noise level increases to 25 or when additional salt-and-pepper noise is added, the PSNR of TOFlow is still round 34dB, showing its robustness to different noise levels. This is qualitatively demonstrated in the right half of <ref type="figure" target="#fig_5">Figure 8</ref>. On two grayscale datasets, Vimeo-BW and V-BM4D, TOFlow outperforms V-BM4D in SSIM. Here we do not fine-tune it on V-BM4D. Though TOFlow only achieves a comparable performance with V-BM4D in PSNR, the output of TOFlow is much sharper than V-BM4D. As shown in <ref type="figure" target="#fig_5">Figure 8</ref>, the details of the beard and collar are kept in the denoised frame by TOFlow (the mid left of <ref type="figure" target="#fig_5">Figure 8)</ref>, and leaves on the tree are also clearer (the bottom left of <ref type="figure" target="#fig_5">Figure 8)</ref>. Therefore, TOFlow beats V-BM4D in SSIM, which better reflects human's perception than PSNR.</p><p>For video deblocking, <ref type="table" target="#tab_8">Table 4</ref> shows that TOFlow outperforms V-BM4D. <ref type="figure">Figure 9</ref> also shows the qualitative comparison between TOFlow, Fixed Flow, and V-BM4D. Note that the compression artifacts around the girl's hair <ref type="formula">(</ref> due to a blocky compression is also removed by our algorithm. To demonstrate the robustness of our algorithms on video deblocking with different quantization levels, we also evaluate the three algorithms on input videos generated under three different quantization levels, and TOFlow consistently outperforms other two baselines. <ref type="figure">Figure 10</ref> also shows that when the quantization level increases, the deblocking output remains mostly the same, suggesting the robustness of TOFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Video Super-Resolution</head><p>Datasets. We evaluate our algorithm on two dataset: Vimeo super-resolution benchmark and the dataset provided by <ref type="bibr">Liu and Sun (2011) (BayesSR)</ref>. The later one consists of four sequences, each having 30 to 50 frames. Vimeo super-resolution benchmark only contains 7 frames, so there is no full-clip evaluation for it.</p><p>Baselines. We compare our framework with bicubic upsampling, three video SR algorithms: BayesSR (we use the version provided by <ref type="bibr" target="#b28">Ma et al (2015)</ref>), DeepSR , and SPMC <ref type="formula">(</ref>  DeepSR can take various number of frames as input. Therefore, on BayesSR dataset, we report two numbers: one on the whole sequence, the other on the seven frames in the middle, as SPMC, TOFlow, and Fixed Flow only take 7 frames as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth DeepSR</head><p>Fixed Flow TOFlow SPMC   Results. <ref type="table" target="#tab_10">Table 5</ref> shows our quantitative results. Our algorithm performs better than baseline algorithms when using 7 frames as input, and it also achieves comparable performance to BayesSR when BayesSR uses all 30-50 frames as input while our framework only uses 7 frames. We show qualitative results in <ref type="figure" target="#fig_7">Figure 11</ref>. Compared with either DeepSR or Fixed Flow, the jointly trained TOFlow generates sharper output. Notice the text on the cloth (top) and the tip of the knife (bottom) are clearer in the high-resolution frame synthesized by TOFlow. This shows the effectiveness of joint training.</p><p>To better understand how many input frames are sufficient for super-resolution, we also train our TOFlow with different number of input frames, as shown in <ref type="table" target="#tab_11">Table 6</ref>. There is a big improvement when switching from 3-frame to 5frame, and the improvement becomes minor when further switching to 7-frame. Therefore, 5 or 7 frames should be enough for super-resolution.</p><p>Besides, the down-sampling kernels (a.k.a. the pointspread function) used to create the low-resolution images may also affect the performance super-resolution . To evaluate how down-sampling kernels affect the performance of our algorithm, we evaluate on three differ- ent kernels: cubic kernels, box down-sampling kernels, and Gaussian kernels with a variance of 2 pixels), and <ref type="table" target="#tab_12">Table 7</ref> shows the result. There is a 1 dB drop in PSNR when switching to box kernels, and another 1 dB drop when switching to Gaussian kernels. This is because that down-sampling kernels remove high-frequency aliasing in low-resolution input images, making super-resolution harder. In most of experiments here, we follow the convention in previous multiframe super-solution papers <ref type="bibr" target="#b24">(Liu and Sun 2011;</ref><ref type="bibr" target="#b42">Tao et al 2017)</ref>, which creates low-resolution images through bicubic interpolation with no blur kernels. However, the results with blur kernels are also interesting, as it is closer the actual formation of low-resolution images captured by cameras.</p><p>In all the experiments, we train and evaluate our network on an NVIDIA Titan X GPU. For an input clip with resolution 256 × 448, our network takes about 200ms for interpolation and 400ms for denoising or super-resolution (the resolution of the input to the super-resolution network is 64 × 112), where the flow module takes 18 ms for each estimated motion field.  <ref type="table">Table 8</ref>: PSNR of TOFlow on tasks including but not limited to the one it was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Flows Learned from Different Tasks</head><p>We now compare and contrast the flow learned from different tasks to understand if learning flow in such a taskoriented fashion is necessary.</p><p>We conduct an ablation study by replacing the flow estimation network in our model by a flow network trained on a different task ( <ref type="figure" target="#fig_8">Figure 12</ref> and <ref type="table">Table 8</ref>). There is a significant performance drop when we use a flow network that is not trained on that task. For example, with the flow network trained on deblocking or super-resolution, the performance of the denoising algorithm drops by 5dB, and there are noticeable noises in the images (the first column of <ref type="figure" target="#fig_8">Figure 12</ref>). There are also ringing artifacts when we apply the flow network trained on super-resolution for deblocking ( <ref type="figure" target="#fig_8">Figure 12</ref> row 2, col 2). Therefore, our task-oriented flow network is indeed tailored to a specific task. Besides, in all these three tasks, Fixed Flow performs better than TOFlow if trained and tested on different tasks, but worse than TOFlow if trained and tested on the same task. This suggests that joint training improves the performance of a flow network on one task, but decreases its performance on the others. <ref type="figure" target="#fig_1">Figure 13</ref> contrasts the motion fields learned from different tasks: the flow field for interpolation is very smooth, even on the occlusion boundaries, while the flow field for super-resolution has artificial movements along the texture edges. This indicates that the network may learn to encode different information that is useful for different tasks in the learned motion fields.   <ref type="bibr" target="#b40">(Revaud et al 2015)</ref>. We report errors over full images, matched regions, and unmatched regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Accuracy of Retrained Flow</head><p>As shown in the <ref type="figure">Figure 1</ref>, tailoring a motion estimation network to a specific task will reduce the accuracy of the estimated flow. To verify that, we evaluate the flow estimation accuracy on the Sintel Flow Dataset by <ref type="bibr" target="#b7">Butler et al (2012)</ref>. Three variants of flow estimation networks are tested: first, a network pre-trained on the Flying Chair dataset, second, the network after fine-tuning on denoising, and third, the network after fine-tuning on super-resolution. All fine-tuning is on the Vimeo-90K dataset. As shown in <ref type="table" target="#tab_15">Table 9</ref>, the accuracy of TOFlow is much worse than either EpicFlow (Revaud et al 2015) or Fixed Flow 3 , but as shown in <ref type="table" target="#tab_7">Table 3</ref>, 4, and 5, TOFlow outperforms Fixed Flow on specific tasks. This is consistent with the intuition that TOFlow is a motion representation that does not match the actual object movement, but leads to better video processing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Different Flow Estimation Network Structure</head><p>Our task-oriented video processing pipeline is not limited to one flow estimation network structure, although in all previous experiments, we use SpyNet by <ref type="bibr" target="#b39">Ranjan and Black (2017)</ref>    <ref type="bibr" target="#b9">(Fischer et al 2015)</ref> as the motion estimation module.</p><p>256×192 and upsample it to the target resolution. Its performance is therefore worse than the model using SpyNet, as shown in <ref type="table" target="#tab_17">Table 10</ref>. Still, in all these three tasks, TOFlow outperforms than Fixed Flow. This demonstrates the generalization ability of the TOFlow framework to other flow estimation modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we have proposed a novel video processing model that exploits task-oriented motion cues. Traditional video processing algorithms normally consist of two steps: motion estimation and video processing based on estimated motion fields. However, a genetic motion for all tasks might be sub-optimal and the accurate motion estimation would be neither necessary nor sufficient for these tasks. Our selfsupervised, task-oriented flow (TOFlow) bypasses this difficulty by modeling motion signals in the loop. To evaluate our algorithm, we have also created a new dataset, Vimeo-90K, for video processing. Extensive experiments on temporal frame interpolation, video denoising/deblocking, and video super-resolution demonstrate the effectiveness of TOFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>Additional qualitative results. We show additional results on the following benchmarks: Vimeo interpolation benchmark <ref type="figure">(Figure 14)</ref>, Vimeo denoising benchmark <ref type="figure" target="#fig_2">(Figure 15</ref> for RGB videos, and <ref type="figure" target="#fig_3">Figure 16</ref> for grayscale videos), Vimeo deblocking benchmark <ref type="figure">(Figure 17)</ref>, and Vimeo super-resolution benchmark <ref type="figure" target="#fig_5">(Figure 18</ref>). We randomly select testing images from test datasets. Differences between different algorithms are more clearer when zoomed in.</p><p>Flow estimation module. We use SpyNet (Ranjan and Black 2017) as our flow estimation module. It consists of four sub-networks with the same network structure, but each sub-network has an independent set of parameters. Each sub-network consists of five sets of 7×7 convolutional (with zero padding), batch normalization and ReLU layers. The number of channels after each convolutional layer is 32, 64, 32, 16, and 2. The input motion to the first network is a zero motion field.</p><p>Image processing module. We use slight different structures in the image processing module for different tasks. For temporal frame interpolation both with and without masks, we build a residual network that consists of an averaging network and a residual network. The averaging network simply averages the two transformed frames (from frame 1 and frame 3). The residual network also takes the two transformed frames as input, but calculates the difference between the actual second frame and the average of two transformed frames through a convolutional network consists of three convolutional layers, each of which is followed by a ReLU layer. The kernel sizes of three layers are 9×9, 1×1, and 1×1 (with zero padding), and the numbers of output channels are 64, 64, and 3. The final output is the summation of the output of the averaging network and the residual network. For video denoising/deblocking, the image processing module uses the same six-layer convolutional structure (three convolutional layers and three ReLU layers) as interpolation, but without the residual structure. We have also tried the residual structure for denoising/deblocking, but there is no significant improvement.</p><p>For video super-resolution, the image processing module consists of four pairs of convolutional layers and ReLU layers. The kernel sizes for these four layers are 9×9, 9×9, 1×1, and 1×1 (with zero padding), and the numbers of output channels are 64, 64, 64, and 3.</p><p>Mask network. Similar to our flow estimation module, our mask estimation network is also a four-level convolutional neural network pyramid as in <ref type="figure">Figure 4</ref>. Each level consists of the same sub-network structure with five sets of 7×7 convolutional (with zero padding), batch normalization and ReLU layers, but an independent set of parameters (output channels are 32, 64, 32, 16, and 2). For the first level, the input to the network is a concatenation of two estimated optical flow fields (four channels after concatenation), and the output is a concatenation of two estimated masks (one channel per mask). From the second level, the input to the network switch to a concatenation of, first, two estimated optical flow fields at that resolution, and second, bilinearupsampled masks from the previous level (the resolution is twice of the previous level). In this way, the first level mask network estimates a rough mask, and the rest refines high frequency details of the mask.</p><p>We use cycle consistencies to obtain the ground truth occlusion mask for pre-training the mask network. For two consecutive frames I 1 and I 2 , we calculate the forward flow v 12 and the backward flow v 21 using the pre-trained flow network. Then, for each pixel p in image I 1 , we first map it to I 2 using v 12 and then map it back to I 1 using v 21 . If it maps to a different point rather to p (up to an error threshold of two pixels), then this point is considered to be occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EpicFlow</head><p>AdaConv SepConv Fixed Flow TOFlow Ground Truth </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>d) Task-oriented Flow (II-c) Denoise by EpicFlow (II-e) Denoise by Task-oriented Flow (II-a) Input Noisy Videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>A toy example that demonstrates the effectiveness of task oriented flow over the traditional optical flow. See Section 4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison between Epicflow (Revaud et al 2015)and TOFlow interpolation (both with and without mask).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>The Vimeo-90K dataset. (a) Sampled frames from the dataset, which show the high quality and wide coverage of our dataset; (b) The histogram of flow magnitude of all pixels in the dataset; (c) The histogram of mean flow magnitude of all images (the flow magnitude of an image is the average flow magnitude of all pixels in that image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(DVF), and Middlebury flow dataset (Baker et al 2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>$"1&amp;%,$%2 !"#$%&amp;"'()*&amp;+,-./) !"#$%&amp;"'()*&amp;+,-.Qualitative results on video denoising. The differences are clearer when zoomed-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>top) and the man's nose (bottom) are completely removed by TOFlow. The vertical line around the man's eye (bottom) Qualitative results on video deblocking. The differences are clearer when zoomed-in. Results on frames with different encoding qualities. The differences are clearer when zoomed-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Qualitative results on super-resolution. Close-up views are shown on the top left of each result. The differences are clearer when zoomed-in. 0.9375 33.04 0.9415 33.08 0.9417</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Qualitative results of TOFlow on tasks including but not limited to the one it was trained on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 :Fig. 15 :Fig. 16 :Fig. 17 :Fig. 18 :</head><label>1415161718</label><figDesc>Qualitative results on video interpolation. Samples are randomly selected from the Vimeo interpolation benchmark. The differences between different algorithms are clear only when zoomed in. Qualitative results on RGB video denoising. Samples are randomly selected from the Vimeo denoising benchmark. The differences between different algorithms are clear only when zoomed in. Qualitative results on grayscale video denoising. Samples are randomly selected from the Vimeo denoising benchmark. The differences between different algorithms are clear only when zoomed in. Qualitative results on video deblocking. Samples are randomly selected from the Vimeo deblocking benchmark. The differences between different algorithms are clear only when zoomed in. Qualitative results on video super-resolution. Samples are randomly selected from the Vimeo super-resolution benchmark. The differences between different algorithms are clear only when zoomed in. DeepSR was originally trained on 30-50 images, but evaluated on 7 frames in this experiment, so there are some artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Qualitative results on frame interpolation. Zoomed-in views are shown in lower right.</figDesc><table><row><cell>EpicFlow</cell><cell>AdaConv</cell><cell></cell><cell>SepConv</cell><cell>Fixed Flow</cell><cell>TOFlow</cell><cell>Ground truth</cell></row><row><cell>Fig. 7: Methods</cell><cell>Vimeo Interp.</cell><cell cols="2">DVF Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM</cell><cell></cell><cell></cell></row><row><cell>SpyNet</cell><cell cols="3">31.95 0.9601 33.60 0.9633</cell><cell></cell><cell></cell></row><row><cell>EpicFlow</cell><cell cols="3">32.02 0.9622 33.71 0.9635</cell><cell></cell><cell></cell></row><row><cell>DVF</cell><cell cols="3">33.24 0.9627 34.12 0.9631</cell><cell></cell><cell></cell></row><row><cell>AdaConv</cell><cell>32.33 0.9568</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>SepConv</cell><cell cols="3">33.45 0.9674 34.69 0.9656</cell><cell></cell><cell></cell></row><row><cell>Fixed Flow</cell><cell cols="3">29.09 0.9229 31.61 0.9544</cell><cell></cell><cell></cell></row><row><cell cols="4">Fixed Flow + Mask 30.10 0.9322 32.23 0.9575</cell><cell></cell><cell></cell></row><row><cell>TOFlow</cell><cell cols="3">33.53 0.9668 34.54 0.9666</cell><cell></cell><cell></cell></row><row><cell>TOFlow + Mask</cell><cell cols="3">33.73 0.9682 34.58 0.9667</cell><cell></cell><cell></cell></row><row><cell cols="4">Table 1: Quantitative results of different frame interpolation</cell><cell></cell><cell></cell></row><row><cell cols="4">algorithms on the Vimeo interpolation test set and the DVF</cell><cell></cell><cell></cell></row><row><cell>test set (Liu et al 2017).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Liu et al 2017), Adaptive Con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">volution (AdaConv) (Niklaus et al 2017a), and Separable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Convolution (SepConv) (Niklaus et al 2017b). At last, we</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Quantitative results of five frame interpolation al-</cell></row><row><cell>gorithms on Middlebury flow dataset (Baker et al 2011):</cell></row><row><cell>PMMST (Xu et al 2015), SepConv (Niklaus et al 2017b),</cell></row><row><cell>DeepFlow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on video denoising. Left: Vimeo RGB datasets with three different types of noise; Right: two grayscale dataset: Vimeo-BW and V-BM4D.</figDesc><table><row><cell></cell><cell>Vimeo-Blocky</cell><cell>Vimeo-Blocky</cell><cell>Vimeo-Blocky</cell></row><row><cell>Methods</cell><cell>(q=20)</cell><cell>(q=40)</cell><cell>(q=60)</cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>V-BM4D</cell><cell cols="3">35.75 0.9587 33.72 0.9402 32.67 0.9287</cell></row><row><cell cols="4">Fixed flow 36.52 0.9636 34.50 0.9485 33.06 0.9168</cell></row><row><cell>TOFlow</cell><cell cols="3">36.92 0.9663 34.97 0.9527 34.02 0.9447</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results on video deblocking.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Results on video super-resolution. Each clip in</cell></row><row><cell>Vimeo-SR contains 7 frames, and each clip in BayesSR</cell></row><row><cell>contains 30-50 frames.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: Results on video super-resolution with a different</cell></row><row><cell cols="2">number of input frames.</cell><cell></cell></row><row><cell>Cubic Kernel</cell><cell>Box Kernel</cell><cell>Gaussian Kernel</cell></row><row><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell cols="3">33.08 0.9417 32.08 0.9372 31.15 0.9314</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results of TOFlow on video super-resolution when different downsampling kernels are used for building the dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: End-point-error (EPE) of estimated flow fields on</cell></row><row><cell>the Sintel dataset. We evaluate TOFlow (trained on two</cell></row><row><cell>different tasks), Fixed Flow, and EpicFlow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Fixed Flow 24.685 0.8297 36.028 0.9672 31.834 0.9291 TOFlow 24.689 0.8374 36.496 0.9700 33.010 0.9411</figDesc><table><row><cell>Methods</cell><cell>Denoising</cell><cell>Deblocking</cell><cell>Super-resolution</cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>as the flow estimation module for its memory ef-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ficiency. To demonstrate the generalization ability of our</cell></row><row><cell></cell><cell></cell><cell></cell><cell>framework, we also experiment with the FlowNetC (Fis-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cher et al 2015) structure, and evaluate it on video denois-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ing, deblocking, and super-resolution. Because FlowNetC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>has larger memory consumption, we only estimate flow at</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Results of TOFlow on three different tasks, using FlowNetC</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The EPE of Fixed Flow on Sintel dataset is different from EPE of SpyNet (Ranjan and Black 2017) reported on Sintel website, as it is trained differently from SpyNet as we mentioned before.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by NSF RI #1212849, NSF BIGDATA #1447476, Facebook, Shell Research, and Toyota Research Institute. This work was done when Tianfan Xue and Donglai Wei were graduate students at MIT CSAIL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno>arXiv:160908675 2</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, accurate, and, lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Burst image deblurring using permutation invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="731" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">To learn image superresolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepwarp: Photorealistic image resynthesis for gaze manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlocal video denoising, simplification and inpainting using discrete regularization on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghoniem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chahir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2445" to="2455" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Deep burst denoising</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth estimation with occlusion handling from a sparse set of light field views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Pendu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video superresolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video rain streak removal by multiscale convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6644" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A high-quality video denoising algorithm based on reliable motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep kalman filtering network for video compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="568" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end learning of video superresolution with motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense estimation and object-based segmentation of the optical flow with robust techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mémin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="703" to="719" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Super-resolution: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1423" to="1468" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video denoising based on a spatiotemporal gaussian scale mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1032" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Light field video capture using a learning-based hybrid imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno>SIGGRAPH 3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-and motionadaptive regularization for high accuracy optic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint adaptive sparsity and low-rankness on the fly: an online tensor reconstruction scheme for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optical flow guided tv-l1 video interpolation and restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pm-pm: Patchmatch with potts model for object segmentation and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2182" to="2196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-frame quality enhancement for compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6664" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops 3</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-level video frame interpolation: Exploiting the interaction among different levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1235" to="1248" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision 3</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="600" to="608" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
