<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
							<email>linchaobao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
							<email>wubaoyuan1987@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>wliu@ee.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatiotemporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very highorder dependencies. To this end, we propose a novel CNNembedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation refers to a task of extracting pixel-level masks for class-agnostic objects in videos. This task can be further divided into two settings <ref type="bibr" target="#b35">[36]</ref>, namely unsupervised and semi-supervised. While the unsupervised task does not provide any manual annotation, the semisupervised task provides information about objects of interest in the first frame of a video. In this paper, we focus on the latter task, where the initial masks for objects of interest are provided in the first frame. The task is important for many applications such as video editing, video sum-marization, action recognition, etc. Note that the semantic class/type of the objects of interest cannot be assumed known and the task is thus class-agnostic. It is usually treated as a temporal label propagation problem and solved with spatio-temporal graph structures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b3">4]</ref> like a Markov Random Field (MRF) model <ref type="bibr" target="#b45">[46]</ref>. Recent advances on the task show significant improvements over traditional approaches when incorporating deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Despite the remarkable progress achieved with CNNs, video object segmentation is still challenging when applied in real-world environments. One example is that even the top performers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> on the DAVIS 2016 benchmark <ref type="bibr" target="#b35">[36]</ref> show significantly worse performance on the more challenging DAVIS 2017 benchmark <ref type="bibr" target="#b37">[38]</ref>, where interactions between objects, occlusions, motions, object deformation, etc., are more complex and frequent in the videos.</p><p>Reviewing the top performing CNN-based methods and traditional spatio-temporal graph-based methods, there is a clear gap between the two lines. The CNN-based methods usually treat each video frame individually or only use simple heuristics to propagate information along the temporal axis, while the well established graph-based models cannot utilize the powerful representation capabilities of neural networks. In order to fully exploit the appearance/shape information about the given objects, as well as the temporal information flow along the time axis, a better solution should be able to combine the best from both. For example, built on the top-performing CNN-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>, there should be a temporal averaging between the CNN outputs of an individual frame and its neighboring frames, so that the segmentation results are temporally consistent. The temporal averaging, however, is heuristic and likely to degrade the segmentation performance due to outliers. A more principled method should be developed. In this paper, we propose a novel approach along this direction.</p><p>Specifically, we build a spatio-temporal MRF model over a video sequence, where each random variable represents the label of a pixel. While the pairwise temporal dependencies between random variables are established us-ing optical flow between neighboring frames, the spatial dependencies in our model are not modeled as pairwise potentials like conventional MRF models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b49">50]</ref>. The problem of spatial pairwise potential is that it has very restricted expressive power and thus cannot model complicated dependencies among pixels in natural images. Some higher-order potentials are proposed to incorporate learned patterns <ref type="bibr" target="#b39">[40]</ref> or enforce label consistency in pre-segmented regions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>. Yet the expressive power of them is still rather limited.</p><p>In our model, we instead use a CNN to encode even higher-order spatial potentials over pixels. Given a labeled object mask in the first frame, we can train a mask refinement CNN for the object to refine a coarse mask in a future frame. Assuming that the mask refinement CNN is so reliable that it can consistently refine a coarse mask to a better one and keep a good mask unchanged, we can define an objective function based on the CNN to assess a given mask as a whole. Then the spatial potential over the pixels within a frame can be defined using the CNN-based function. In this case, more complicated dependencies among pixels can be represented for the object. As a result, the MRF model will enforce the inference result in each frame to be more like the specific object. Yet, the inference in the resulting MRF model is very difficult due to the CNN-based potential function. In this paper, we overcome the difficulty by proposing a novel approximate inference algorithm for the MRF model. We first decouple the inference problem into two subproblems by introducing an auxiliary variable. Then we show that one subproblem involving the CNNbased potential function can be approximated by a feedforward pass of the mask refinement CNN. Consequently, we do not even need to explicitly compute the CNN-based potential function during the inference. The entire inference algorithm alternates between a temporal fusion step and a feed-forward pass of the CNN. When initialized with a simple one-shot segmentation CNN <ref type="bibr" target="#b12">[13]</ref>, our algorithm shows outstanding performance on challenging benchmarks like the DAVIS 2017 test-dev dataset <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Video Object Segmentation We briefly review recent work focusing on the semi-supervised setting. The task is usually formulated as a temporal label propagation problem. Spatio-temporal graph-based methods tackle the problem by building up graph structures over pixels <ref type="bibr" target="#b44">[45]</ref>, patches <ref type="bibr" target="#b3">[4]</ref>, superpixels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46]</ref>, or even object proposals <ref type="bibr" target="#b36">[37]</ref> to infer the labels for subsequent frames. The temporal connections are established using regular spatio-temporal lattices <ref type="bibr" target="#b33">[34]</ref>, optical flow <ref type="bibr" target="#b17">[18]</ref>, or other similar techniques like nearest neighbor fields <ref type="bibr" target="#b3">[4]</ref>. Some methods even build up long-range connections using appearance-based methods <ref type="bibr" target="#b36">[37]</ref>. Among these methods, some algorithms infer the labels using greedy strategies by only considering two or more neighboring frames one time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>, while other algorithms strive to find globally optimal solutions by considering all the frames together <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Although various nicely designed models and algorithms are proposed to tackle the problem, deep learning shows overwhelming power when introduced to this area. It is shown that a merely appearance-based CNN named OS-VOS <ref type="bibr" target="#b12">[13]</ref>, trained on the first frame of a sequence and tested on each subsequent frame individually, achieves significant improvements over top-performing traditional methods (79.8% vs 68.0% accuracy on the DAVIS 2016 dataset <ref type="bibr" target="#b35">[36]</ref>). A concurrent work named MaskTrack <ref type="bibr" target="#b34">[35]</ref> achieves similar performance by employing a slightly different CNN where the mask of a previous frame is fed to the CNN as an additional channel besides the RGB input image. Some other CNN-based methods also demonstrate pretty nice results <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48]</ref>. Among these methods, an online adaptation version of OSVOS, namely OnAVOS <ref type="bibr" target="#b47">[48]</ref>, achieves the best performance (86.1% accuracy) on the DAVIS 2016 dataset.</p><p>Since the best performance on the DAVIS 2016 dataset tends to be saturated, the authors of the dataset released a larger, more challenging dataset, namely DAVIS 2017 <ref type="bibr" target="#b37">[38]</ref>, to further push the research in video object segmentation for more practical use cases. The new dataset adds more distractors, smaller objects and finer structures, more occlusions and faster motions, etc. Hence the top performer on DAVIS 2016 performs much worse when it comes to the new dataset. For example, the accuracy of OnAVOS <ref type="bibr" target="#b47">[48]</ref> on DAVIS 2016 is 86.1%, while its accuracy drops to 50.1% on the DAVIS 2017 test-dev dataset. Although the best performance (on the test-dev dataset) is further improved to around 66% during the DAVIS 2017 Challenge by Li et al. <ref type="bibr" target="#b30">[31]</ref> and the LucidTracker <ref type="bibr" target="#b25">[26]</ref>, the score is achieved with engineering techniques like model ensembling, multi-scale training/testing, dedicated object detectors, etc. We show in this paper that our proposed approach can achieve better performance without resorting to these techniques. CNN + MRF/CRF The idea of combining the best from both CNN and MRF/CRF is not new. We here briefly review some attempts to combine CNN and MRF/CRF for the segmentation task. For a more thorough review please refer to <ref type="bibr" target="#b2">[3]</ref>. The first idea to take advantage of the representation capability of CNN and the fine-grained probabilistic modeling capability of MRF/CRF is to append an MRF/CRF inference to a CNN as a separate step. For example, the semantic segmentation framework DeepLab <ref type="bibr" target="#b13">[14]</ref> utilizes fully-connected CRFs <ref type="bibr" target="#b28">[29]</ref> as a post-processing step to improve the semantic labelling results produced by a CNN, similar to performing an additional edge-preserving filtering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref> on the segmentation masks. The video object segmentation method by Jang and Kim <ref type="bibr" target="#b22">[23]</ref> performs MRF optimization to fuse the outputs of a triple-branch CNN. However, the loosely-coupled combination cannot fully exploit the strength of MRF/CRF models. Schwing and Urtasun <ref type="bibr" target="#b40">[41]</ref> proposed to jointly train CNN and MRF by back-propagating gradient obtained during the MRF inference to CNN. Unfortunately, the approach does not show clear improvements over the separated training scheme. Arnab et al. <ref type="bibr" target="#b1">[2]</ref> successfully demonstrated performance gains via a joint training of CNN and MRF, even with higher-order potentials modeled by object detection or superpixels. Note that their focus is on the back-propagation of high-order potentials during the joint training, while our work focuses on the higher-order modeling with CNNs. The CRF-RNN work <ref type="bibr" target="#b52">[53]</ref> formulates the mean-field approximate inference for CRFs as a Recurrent Neural Network (RNN) and integrates it with a CNN to obtain an endto-end trainable deep network, which shows an outstanding performance boost in an elegant way. Taking one step further, the Deep Parsing Network (DPN) <ref type="bibr" target="#b31">[32]</ref> is designed to approximate the mean-field inference for MRFs in one pass. The above work demonstrates promising directions of using neural networks to approximate the inference of MRFs, which is different from our work that is trying to model higher-order potentials in MRFs with CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>The main contributions of this paper are as follows:</p><p>• We propose a novel spatio-temporal Markov Random Field (MRF) model for the video object segmentation problem. The novelty of the model is that the spatial potentials are encoded by CNNs trained for objects of interest, so higher-order dependencies among pixels can be modeled to enforce the holistic segmentation of object instances.</p><p>• We propose an effective iterative algorithm for video object segmentation. The algorithm alternates between a temporal fusion operation and a feed-forward CNN to progressively refine the segmentation results. Initialized with an appearance-based one-shot video object segmentation CNN, our algorithm achieves stateof-the-art performance on public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>We start by considering the case of the single object segmentation, where the goal is to label pixels as binary values. Handling multiple objects is described in Sec. 4.1. Note that in the semi-supervised setting, the ground-truth object mask for the first frame of a video is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations &amp; Preliminaries</head><p>We define a discrete random field X over all the pixels V = {1, 2, ..., N } in a video sequence. Each random variable X i ∈ X is associated with a pixel i ∈ V and takes a value x i from the label set L = {0, 1}. We use x to denote a possible assignment of labels (namely a labeling or a configuration) to the random variables in X. The data of video frames is denoted as D. Denoting a clique in the field by c and the set of variables in that clique by x c , the distribution of the random variables in the field can be written as a product of potential functions over the maximal cliques <ref type="bibr" target="#b10">[11]</ref> </p><formula xml:id="formula_0">p(x|D) = 1 Z c ψ c (x c |D) = 1 Z c exp{−E c (x c |D)},<label>(1)</label></formula><p>where Z is the normalization constant and E c (x c |D) is the energy function corresponding to the potential function ψ c (x c |D). Our goal is to infer the maximum a posteriori (MAP) labeling x * of the random field as</p><formula xml:id="formula_1">x * = arg max x log p(x|D) = arg min x c E c (x c |D). (2)</formula><p>By defining the graph structures of the random field and their associated energy functions, the MAP labeling can be obtained via minimizing the total energy in the field. Note that the energy functions defined in our model will be conditioned on the data D. To be concise, we will drop D in the notations hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Structures &amp; Energies</head><p>The total energy in our model is defined as follows</p><formula xml:id="formula_2">E(x) = i∈V E u (x i ) + (i,j)∈N T E t (x i , x j ) + c∈S E s (x c ),<label>(3)</label></formula><p>where E u is the unary energy, and E t and E s are the energies associated with temporal and spatial dependencies, respectively. The notation N T refers to the set of all temporal connections, while S refers to the set of all spatial cliques. The concrete definitions are as follows.</p><p>The unary energy is defined by the negative log likelihood of the labeling for each individual random variable as</p><formula xml:id="formula_3">E u (x i ) = −θ u log p(X i = x i ),<label>(4)</label></formula><p>where θ u is to balance the weight of this term with other energy terms. The set of temporal connections N T is established using semi-dense optical flow, such that each pixel is only connected to pixels in neighboring frames when the motion estimation is reliable enough. We use a forwardbackward consistency check to filter reliable motion vectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>. The yellow dashed lines in <ref type="figure">Fig. 1</ref> show an example of an one-step temporal dependencies for the red pixel. Note that the one-step temporal dependencies can be further extended to k-step temporal dependencies by directly computing optical flow between a frame and the frame that is k frames away. We use k 2 (k = 0) in our model, which means for a certain frame t, all the following links are established:</p><formula xml:id="formula_4">{t ←→ t − 2}, {t ←→ t − 1}, {t ←→ t + 1}, {t ←→ t + 2}.</formula><p>As a result, each pixel is connected to at most 4 temporal neighbors (note that some invalid connections are removed by the forward-backward consistency check). The temporal energy function is defined as</p><formula xml:id="formula_5">E t (x i , x j ) = θ t w ij (x i − x j ) 2 ,<label>(5)</label></formula><p>where θ t is a balancing parameter for this term and w ij is a data-dependent weight to measure the confidence of the temporal connection between variables X i and X j . The energy encourages a temporally consistent labeling when the temporal connection is confident.</p><p>For spatial dependencies, we define all the pixels in a frame as a clique, in which the labeling for each pixel depends on all other pixels in the same frame (shown as the green shaded region in <ref type="figure">Fig. 1</ref>). In order to construct a spatial energy function defined over all the pixels in a frame, we need to design an energy function f (·) that can assess the quality of a given mask x c as a whole. Ideally, it is easy to construct the function f (·) if the ground-truth mask x * c of an input mask x c is given. For example, we can define f (·)</p><formula xml:id="formula_6">f (x c ) = x c − x * c 2 2 ,<label>(6)</label></formula><p>which gives lower energies to masks that are more similar to the ground-truth mask. However, x * c is unknown and indeed what we need to solve for. We here resort to a feed-forward CNN to approximate x * c and define f (·) as follows</p><formula xml:id="formula_7">f (x c ) = x c − g CNN (x c ) 2 2 ,<label>(7)</label></formula><p>where g CNN (·) is a mask refinement CNN that accepts as input a given mask x c and outputs a refined mask. Note that the operator g CNN (·) here is a feed-forward pass of a CNN. Intuitively, the above definition assigns a lower energy to a mask whose mapping through g CNN (·) is more similar to itself. With a well-trained g CNN (·) that can reliably refine a coarse mask to a better one and keep a good mask unchanged, the function f (·) could assign better masks lower energies. Fortunately, it is shown that such a CNN can be trained in a two-stage manner using the first frame of a given video and performs very reliably during the inference for the following frames <ref type="bibr" target="#b34">[35]</ref>. We leave the detailed discussion of g CNN (·) in the next section. We here define the spatial energy in Eq. <ref type="formula" target="#formula_2">(3)</ref> as</p><formula xml:id="formula_8">E s (x c ) = θ s f (x c ),<label>(8)</label></formula><p>where θ s is a balancing parameter for this term. The above spatial energy definition has a much more expressive power than traditional pairwise smoothness energies <ref type="bibr" target="#b42">[43]</ref>, higher-order energies enforcing label consistency … … <ref type="figure">Figure 1</ref>: The spatio-temporal dependencies for a pixel (in red) in our model. The temporal dependencies are established by optical flow (indicated by yellow dashed lines).</p><p>The spatial dependencies are modeled by a CNN, as shown in the center frame where the green shaded region indicates pixels belonging to the same spatial clique as the red pixel (in this case it indicates all the pixels within the same image as the red pixel). Best viewed in color.</p><p>in pre-segmented regions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>, or energies encouraging labels to follow certain learned patterns <ref type="bibr" target="#b39">[40]</ref>. However, the inference in the MRF with the CNN-based energy is very difficult. In the next section, we present an efficient approximate algorithm for the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Inference</head><p>The exact MAP inference in MRF models is NP-hard in general <ref type="bibr" target="#b27">[28]</ref>. The higher-order energy function in our model makes the inference problem intractable. Even with efficient approximate algorithms like belief propagation or mean-field approximation, finding a solution minimizing Eq. (3) is still computationally infeasible, due to the very high-order spatial cliques. Intuitively, each time evaluating the total energy in the MRF, a feed-forward CNN pass for every frame in the video is required. The computational cost quickly becomes unaffordable as the number of energy evaluations grows.</p><p>In order to make the problem tractable, we decouple the temporal energy E t and spatial energy E s by introducing an auxiliary variable y, and minimize the following approximation of Eq. (3) instead:</p><formula xml:id="formula_9">E(x, y) = i∈V E u (x i ) + (i,j)∈N T E t (x i , x j ) + β 2 x − y 2 2 + c∈S E s (y c ),<label>(9)</label></formula><p>where β is a penalty parameter such that y is a close approximation of x. Eq. (9) can be minimized by alternating steps updating either x or y iteratively. Specifically, in the k-th iteration, the two updating steps are:</p><p>1. with y fixed, update x by</p><formula xml:id="formula_10">x (k) ← arg min xÊ (x, y (k−1) ),<label>(10)</label></formula><p>2. with x fixed, update y by</p><formula xml:id="formula_11">y (k) ← arg min yÊ (x (k) , y).<label>(11)</label></formula><p>Note that Eq. (10) in step 1 is essentially the regularized total energy in Eq. (3) ignoring spatial dependencies, while Eq. (11) in step 2 only considers spatial dependencies for each frame c. The two steps are essentially performing temporal fusion and mask refinement, respectively. Solving step 1 requires solving a large quadratic integer programming problem with an N × N Laplacian matrix, where N is the total number of pixels in a video. For efficiency considerations, we here resort to a classical iterative method named Iterated Conditional Modes (ICM) <ref type="bibr" target="#b10">[11]</ref> to find an approximate solution of step 1. Specifically, each time one random variable X i is updated to minimize Eq. (10), fixing the rest of the random variables X. The original ICM algorithm repeats the variable updating until converged. In our algorithm, we only perform the updating for a fixed number L of iterations, as shown in Algorithm 1.</p><p>The updating in step 2 can be performed for each frame c individually, that is</p><formula xml:id="formula_12">y (k) c ← arg min yc β 2 x (k) c − y c 2 2 + E s (y c ) .<label>(12)</label></formula><p>Note that the problem is highly non-convex due to the CNNbased energy function E s . Intuitively, step 2 is to refine a given mask x is better in terms of E s and at the same time not deviates too much from the input. Directly solving the optimization problem in Eq. (12) is difficult, we instead approximate this step by simply using g CNN (·) to update y c :</p><formula xml:id="formula_13">y (k) c ← g CNN (x (k) c ),<label>(13)</label></formula><p>which we find in the experiments can make the objective function in Eq. (12) non-increasing in most cases (99% of more than 3000 frames in the DAVIS 2017 validation set when θ s = β). As a result, the overall algorithm alternating between the above two steps, as described in Algorithm 1, ensures the non-increasing of the total energy in Eq. (9) in each iteration. We show experimentally in Sec. 4 that the algorithm converges after a few iterations. Now we discuss the details of the CNN operator g CNN (·) in our algorithm. Similar to MaskTrack <ref type="bibr" target="#b34">[35]</ref>, our g CNN (·) accepts a 4-channel input (RGB image + coarse mask), and outputs a refined mask. Also, we train g CNN (·) in a twostage manner. In the first stage, an offline model is trained using object segmentation data available. Then in the second stage, the offline model is fine-tuned using the groundtruth mask in the first frame of a given video. During the training, the input mask to the CNN is a contaminated version of the ground-truth mask with data augmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Our Inference Algorithm</head><p>Parameters: number of outer iterations K, number of inner iterations L, number of pixels N , and number of frames C. Initialization: initial labeling x (0) = y (0) . for k from 1 to K do -Temporal Fusion Step (TF) -</p><formula xml:id="formula_14">x (k,0) ← x (k−1) for l from 1 to L do for i from 1 to N do x (k,l) i ← arg min xi β 2 (x i − y (k−1) i ) 2 + E u (x i ) + (i,j)∈N T E t (x i , x (k,l−1) j ) end for end for x (k) ← x (k,L) -Mask Refinement Step (MR) - for c from 1 to C do y (k) c ← g CNN (x (k)</formula><p>c ) end for end for Output: Binarize y (K) as the final segmentation masks.</p><p>techniques like non-rigid deformation. Note that the twostage training is performed before our inference algorithm. During inference, the operator g CNN (·) in Algorithm 1 is only a feed-forward pass of the CNN.</p><p>The CNN trained in this way can partially encode the appearances of an object of interest. It endows our algorithm with the ability to recover missing parts of an object mask when occlusions happen. Even in the case that there are re-appearing objects after completely occluded, our algorithm can recover high-quality object masks given a poor likelihood obtained from an appearance-based method like OSVOS <ref type="bibr" target="#b12">[13]</ref>. In fact, we will show in the next section that our algorithm achieves outstanding performance on challenging datasets where heavy occlusions are common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Initialization &amp; Pixel Likelihood We use OSVOS <ref type="bibr" target="#b12">[13]</ref> to obtain the initial labeling and pixel likelihood for all frames. As OSVOS tends to produce false-positive results, especially when there are multiple similar objects, we weight the response map output from OSVOS with a Gaussian centered at the most likely location of the target object predicted by a simple linear motion model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. The weighted response map in each frame is then combined (using max at each pixel) with the response map warped from the preceding frame. Then the fused response map is binarized as the initial labeling of our algorithm. To obtain the pixel likelihood p(X i = x i ) in Eq. (4), we use the initial foreground region imposed with a dilated uncertain region similar to <ref type="bibr" target="#b47">[48]</ref>, where we assign the likelihood for pixels in the foreground region as p(X i = 1) = 0.99 and the likelihood for pixels in the uncertain region as a Gaussian peaked with probability p(X i = 1) = 0.7. CNN Implementation We use the Caffe-based DeepLab framework <ref type="bibr" target="#b13">[14]</ref> to implement our mask refinement CNN g CNN (·). The backbone net is a VGG-Net <ref type="bibr" target="#b43">[44]</ref> with the input data layer modified to 4-channel (RGB image + 1channel binary mask). We add additional skip connections from intermediate pooling layers to a final output convolutional layer to enable multi-level feature fusion. The input image to our CNN is cropped around the object using the labeling from a previous iteration and then resized to 513 × 513. In our experiments, we train the offline model using the DAVIS 2017 training set <ref type="bibr" target="#b37">[38]</ref> for 50K iterations with a batch size of 10 and a learning rate of 10 −4 (with "poly" policy), where the initial model weights are obtained from DeepLabv2 VGG16 pre-trained on PASCAL VOC. The training data consists of 60 video clips with all frames annotated in pixel-level. We use the optical flow warped mask of a previous frame as contaminated input for each frame during offline training. For a given test video, the offline model is fine-tuned for 2K iterations using the groundtruth mask of the first frame, augmented using a simplified version of Lucid data dreaming <ref type="bibr" target="#b25">[26]</ref>. We provide the model definition file in supplementary material.</p><p>Handling Multiple Objects When there are multiple objects to be segmented in a video, we handle each object individually in each iteration and deal with overlapped regions before starting the next iteration. Overlapped regions are divided into connected pixel blobs and each blob is assigned to a label that minimizes Eq. (10) for the blob.</p><p>Other Settings We use FlowNet2 <ref type="bibr" target="#b19">[20]</ref> to compute optical flow in our implementation. In the case that NaN error happens, we instead use the TV-L1 Split-Bregman optical flow GPU implementation provided by Bao et al. <ref type="bibr" target="#b4">[5]</ref>. The temporal confidence weighting w ij in Eq. (5) is obtained by incorporating a decaying frame confidence as the frame index grows, which is w ij = ξ ci ξ cj , where ξ c = max(0.9 c−1 , 0.3) for frame c ranging from 1 to number of frames in a video. The energy balancing parameters in Eq. (3) are set to θ u = θ t = 1. The decoupling penalty parameter in Eq. (9) is initially set to β = 1.5, multiplied by 1.2 in each iteration. The number of inner iterations (i.e., the ICM iterations in temporal fusion) is set to L = 5. Runtime Analysis The main portion of the runtime is the online Lucid data augmentation and CNN training for a given video. In our implementation, the data augmentation takes about 1 hour to produce 300 training pairs from the first frame, and the online training of g CNN (·) takes about 1 hour for 2K iterations with a batch size of 10 on an NVIDIA Tesla M40 GPU. The online training of OSVOS takes about 20 minutes for 2K iterations but can be performed in parallel to the training of g CNN (·). During inference, the algorithm is actually pretty efficient. Note that the optical flow for establishing temporal dependencies is only computed once, with each frame only taking a fraction of a second on GPU <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref>. The temporal fusion step in our algorithm is performed locally and the runtime is almost ignorable. The mask refinement step is a feed-forward pass of CNN and takes only a fraction of a second on GPU for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform ablation study of our algorithm on the DAVIS 2017 validation set <ref type="bibr" target="#b37">[38]</ref>, which consists of 30 video clips with pixel-level annotations. We use the region similarity in terms of IoU (J ) and contour accuracy (F) to evaluate quality of the results. <ref type="table">Table 1</ref> shows the evaluation results for our algorithm in different settings. As a comparison, the results from OSVOS <ref type="bibr" target="#b12">[13]</ref> (without boundary snapping, multiple objects conflicts are handled by simply taking the object with the maximum CNN response value at each pixel) are also listed in the table. Note that the performance of OSVOS on the DAVIS 2017 dataset is significantly worse than that on the DAVIS 2016 dataset, since the renewed dataset is much more challenging. Our baseline implementation is essentially OSVOS (without boundary snapping) enhanced with a linear motion model, which serves as the initial labeling in our algorithm.</p><p>In our experiments, when only performing temporal fusion step (TF), we set y with the updated x after each iteration in Algorithm 1 to propagate variable estimation between iterations. Similarly, when only performing mask refinement step (MR), x is set with the latest y in each iteration. From <ref type="table">Table 1</ref> we can see that, performing only TF step will degrade the results of the segmentation. This is  <ref type="table">Table 1</ref>: Ablation study on the DAVIS 2017 validation set. Our baseline is OSVOS enhanced with a linear motion model. TF represents the temporal fusion step in our algorithm, while MR represents the mask refinement step. For example, "TF&amp;MR×3" means that the algorithm is performed for 3 iterations with both TF and MR steps enabled. The "Boost" column shows the performance gain of adding each algorithm variant to the baseline. because TF step completely ignores the rich spatial dependencies between variables in a frame, and tends to propagate erroneous labeling among neighboring frames. On the other hand, performing only MR step can largely boost the baseline method but will stuck at a performance gain around 5%. With both TF and MR steps enabled, our algorithm can improve the baseline method by a performance gain up to 11%. The intuition can be illustrated by <ref type="figure" target="#fig_1">Fig. 2</ref>. The TF step can partially recover missing segments by utilizing information from neighboring frames, but the result is coarse and false-positive outliers may be introduced in this step, as shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>. Fortunately, the coarse segmentation can then be refined in the MR step to produce a high-quality result, as shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>. Note that in this example, MR step itself cannot yield satisfactory results without the help of TF step for enlarging the positive labeling set, as shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>. To obtain the results in the rest of our experiments, we set the number of iterations K = 3 for a good balance between performance and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We first report the performance of our algorithm on the challenging DAVIS 2017 test-dev set <ref type="bibr" target="#b37">[38]</ref>. The dataset consists of 30 video clips in various challenging cases including heavy occlusions, large appearance changes, complex shape deformation, diverse object scales, etc. It was used as a warm-up dataset in the DAVIS 2017 Challenge and remains open after the challenge.  <ref type="bibr" target="#b46">[47]</ref> 0.565 0.534 0.578 0.596 0.654 OnAVOS <ref type="bibr" target="#b47">[48]</ref> 0.528 0.501 -0.554 -OSVOS <ref type="bibr" target="#b12">[13]</ref> 0.505 0.472 0.508 0.537 0.578  <ref type="bibr" target="#b47">[48]</ref> 0.857 0.774 -LucidTracker <ref type="bibr" target="#b25">[26]</ref> 0.837 0.762 0.768 MaskRNN <ref type="bibr" target="#b18">[19]</ref> 0.804 -0.721 OSVOS <ref type="bibr" target="#b12">[13]</ref> 0.798 0.783 0.654 MaskTrack <ref type="bibr" target="#b34">[35]</ref> 0.797 0.726 0.703 SegFlow <ref type="bibr" target="#b14">[15]</ref> 0.761 --STV <ref type="bibr" target="#b48">[49]</ref> 0.736 -0.781 CTN <ref type="bibr" target="#b22">[23]</ref> 0.735 --VPN <ref type="bibr" target="#b21">[22]</ref> 0.702 --PLM <ref type="bibr" target="#b41">[42]</ref> 0.700 --ObjFlow <ref type="bibr" target="#b45">[46]</ref> 0.680 0.776 0.741 BVS <ref type="bibr" target="#b33">[34]</ref> 0.600 -0.584  <ref type="figure">Fig. 3</ref> shows several examples of our segmentation results. Note that this dataset is very challenging and methods purely relying on object appearance or methods mainly utilizing temporal information perform very poorly on the dataset. For example, in the "carousel" sequence, appearance-based methods like OSVOS <ref type="bibr" target="#b12">[13]</ref> will mistakenly switch object identities since the appearances of all the carousels are very similar to each other when they are in a same orientation relative to camera. On the other hand, <ref type="figure">Figure 3</ref>: Examples of our results on the DAVIS 2017 test-dev set. The first column shows the ground-truth mask given in the first frame. The other columns are segmentation results for subsequent frames. From top to bottom, the sequences are "carousel", "girl-dog", and "salsa", respectively. Multiple objects are highlighted with different colors. It can be shown from these examples that the dataset is very challenging. Note that our algorithm does not employ specific object detectors (like a person detector used in a re-identification based method <ref type="bibr" target="#b30">[31]</ref>) to achieve the results. methods like Object Flow <ref type="bibr" target="#b45">[46]</ref> cannot handle occlusions due to lack of the ability to recognize re-appearing objects. Our algorithm handles these cases well as shown in the first row of <ref type="figure">Fig. 3</ref>, thanks to the representation power of CNN for encoding object appearances and shapes and the MRF modeling for establishing spatio-temporal connections.</p><p>For completeness, we also report the performance of our algorithm on three legacy datasets, DAVIS 2016 dataset <ref type="bibr" target="#b35">[36]</ref>, Youtube-Objects dataset <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21]</ref> and SegTrack v2 dataset <ref type="bibr" target="#b29">[30]</ref>, as shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_3">Fig. 4</ref>. With the same parameter settings as before, our algorithm achieves state-of-the-art results on all three datasets. Note that these datasets are less challenging comparing to DAVIS 2017 and the scores tend to be saturated. In Youtube-Objects and SegTrack v2, there are very few occlusions and appearance changes among the video sequences and hence temporal propagation methods like Object Flow <ref type="bibr" target="#b45">[46]</ref> can achieve very high performance. In the more challenging DAVIS 2016 dataset, although there are large appearance changes and complex deformations in the sequences, distractors and occlusions are much fewer than those in DAVIS 2017. Most foreground objects can be correctly identified by CNNbased methods like OSVOS <ref type="bibr" target="#b12">[13]</ref> or its online adaptation version <ref type="bibr" target="#b47">[48]</ref>, without any temporal information leveraged. By taking the advantages from both types of methods (temporal propagation and CNN), our algorithm achieves stateof-the-art performance on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel spatio-temporal MRF model for video object segmentation. By performing inference in the MRF model, we developed an algorithm that alternates between a temporal fusion operation and a mask refinement feed-forward CNN, progressively inferring the results of video object segmentation. We demonstrated the effectiveness of the proposed algorithm through extensive experiments on challenging datasets. Different from previous efforts in combining MRFs and CNNs, our method explores the new direction to embed a feed-forward pass of a CNN inside the inference of an MRF model. We hope that this idea could inspire more future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of the ablation study experiments. Performing TF or MR individually can only yield limited improvements over the baseline method, as shown in (b) and (c). With both TF and MR enabled, the quality of the segmentation result gets largely improved, as shown in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>692 0.096 0.652 0.728 0.732 0.822 +TF&amp;MR×2 0.704 0.108 0.668 0.740 0.740 0.824 +TF&amp;MR×3 0.706 0.110 0.671 0.742 0.741 0.816 +TF&amp;MR×4 0.707 0.111 0.672 0.744 0.742 0.820 +TF&amp;MR×5 0.707 0.111 0.672 0.744 0.742 0.820</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples of our results on DAVIS 2016, Youtube-Objects and SegTrack v2 datasets. From top to bottom, "dancetwirl" from DAVIS 2016, "cat-0001" from Youtube-Objects, and "hummingbird" from SegTrack v2, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Global Mean</cell><cell>Region J Mean Recall Mean Recall Contour F</cell></row><row><cell>Ours</cell><cell>0.675</cell><cell>0.645 0.741 0.705 0.794</cell></row><row><cell>apata[26]</cell><cell>0.666</cell><cell>0.634 0.739 0.699 0.801</cell></row><row><cell>lixx[31]</cell><cell>0.661</cell><cell>0.644 0.735 0.678 0.756</cell></row><row><cell>wangzhe</cell><cell>0.577</cell><cell>0.556 0.632 0.598 0.667</cell></row><row><cell>lalalaf-</cell><cell>0.574</cell><cell>0.545 0.613 0.602 0.688</cell></row><row><cell>voigtla-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results on the DAVIS 2017 test-dev set. The names "apata" and "lixx" are the top entries presented in the DAVIS 2017 Challenge.</figDesc><table><row><cell>The performances of OnAVOS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The results on DAVIS 2016, Youtube-Objects and SegTrack v2 datasets. Only recent work in the semisupervised setting (i.e., ground-truth of the first frame is given) is shown. The results of DAVIS 2016 are evaluated on the validation set (scores mostly obtained from the DAVIS 2016 benchmark website<ref type="bibr" target="#b0">[1]</ref>) and only region IoU J score is shown. Our algorithm achieves state-of-the-art performances on all three datasets. the performance of our algorithm comparing to the topperforming entries presented in the DAVIS 2017 Challenge. Our algorithm outperforms the winning entries without resorting to techniques like model ensembling or multi-scale training/testing. It also does not rely on dedicated object detectors, which makes it more general for class-agnostic object segmentation.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Benchmark Video Object Segmentation on DAVIS</title>
		<ptr target="http://davischallenge.org/soa_compare.html.7" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional random fields meet deep neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of tv-l1 optical flow solvers on gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GTC Posters</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An edge-preserving filtering framework for visibility restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tree filtering: Efficient structure-preserving smoothing with a minimum spanning tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="555" to="569" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.7580</idno>
		<title level="m">Robust piecewise-constant smoothing: M-smoother revisited</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4996" to="5006" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust tracking-by-detection using a detector confidence particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jumpcut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="195" to="196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain transform for edgeaware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human action recognition in unconstrained videos by explicit motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3781" to="3795" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Trajectory-based modeling of human actions with motion reference points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking. The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation with reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7618</idno>
		<title level="m">Multiple object tracking: a literature review</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label mrf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A coupled hidden markov random field model for simultaneous face clustering and tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="373" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simultaneous clustering and tracklet linking for multi-face tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Constrained clustering and its application to face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
