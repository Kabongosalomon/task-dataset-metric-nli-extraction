<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Closer Look at Art Mediums: The MAMe Image Classification Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Parés</surname></persName>
							<email>ferran.pares@bsc.es</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Arias-Duart</surname></persName>
							<email>anna.ariasduart@bsc.es</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Garcia-Gasulla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gema</forename><surname>Campo-Francés</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Viladrich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Eduard</forename><surname>Ayguadé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Labarta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Arias-Duart</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Garcia-Gasulla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gema</forename><surname>Campo-Francés</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Viladrich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Ayguadé</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Barcelona Supercomputing Center (BSC)</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Barcelona Supercomputing Center (BSC)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Conservació-Restauració Universitat de Barcelona (UB)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Conservació-Restauració Universitat de Barcelona (UB)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Barcelona Supercomputing Center (BSC) Universitat Politècnica de Catalunya (UPC) Jesús Labarta Barcelona Supercomputing Center (BSC) Universitat Politècnica de Catalunya (UPC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Closer Look at Art Mediums: The MAMe Image Classification Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Art is an expression of human creativity, skill and technology. An exceptionally rich source of visual content. In the context of AI image processing systems, artworks represent one of the most challenging domains conceivable: Properly perceiving art requires attention to detail, a huge generalization capacity, and recognizing both simple and complex visual patterns. To challenge the AI community, this work introduces a novel image classification task focused on museum art mediums, the MAMe dataset. Data is gathered from three different museums, and aggregated by art experts into 29 classes of mediums (i.e., materials and techniques). For each class, MAMe contains a minimum of 850 high-resolution and variable shape images (700 for training, 150 for test). The combination of volume, resolution and shape allows MAMe to fill a void in current image classification challenges, empowering research in aspects so far overseen by the research community. After reviewing the Ferran Parés Barcelona Supercomputing Center (BSC)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Challenging problems is what drives AI research. What pushes the field and its applications forward. A prime example of that is the ImageNet dataset, together with the corresponding ILSVRC challenge <ref type="bibr" target="#b32">[37]</ref>. The popularization of this competition revitalized the Neural Networks field, particularly in the context of image processing. The outstanding performance of deep neural networks models in the demanding ILSVRC challenge caught the attention of AI researchers and practitioners around the world, who quickly acknowledged the potential behind the combination of deep nets and large sets of data. As a result, the popularity of the field exploded.</p><p>The ImageNet dataset provided an appealing challenge to lure AI researchers, who in turn were able to develop and test new ideas on it. Some of these ideas became powerful principles for the current deep learning (DL) field, such as Inception blocks <ref type="bibr" target="#b42">[47]</ref>, residual connections <ref type="bibr" target="#b14">[19]</ref>, dropout regularization <ref type="bibr" target="#b39">[44]</ref>, ReLU activations <ref type="bibr" target="#b27">[32]</ref> and weight initializations <ref type="bibr" target="#b10">[15,</ref><ref type="bibr" target="#b13">18]</ref>, among others. This amounts for a remarkable set of achievements in a very short time span, and speaks of the contribution of ImageNet to the AI field. That being said, the relevance of the ImageNet image classification challenge today has mostly vanished. The last edition of ILSVRC took place in 2017 <ref type="bibr" target="#b0">[1]</ref>, and the AI community considers it a solved problem with little margin for improvement (by 2019, 98.2% top-5 accuracy <ref type="bibr" target="#b48">[53]</ref> was achieved, while human top-5 classification accuracy is thought to be between 88% and 95% <ref type="bibr" target="#b32">[37]</ref>).</p><p>The ImageNet challenge is defined around two main types of instances: Man-made objects, and living things. These classes are characterized by large distinctive features which require little attention to detail for their recognition. State-of-the-art performance can be achieved on this kind of tasks after applying heavy deformation on the image (i.e., uniform reshape) and losing most visual details (e.g., downsampling to 300x300) <ref type="bibr" target="#b48">[53]</ref>. At the same time, samples of the same class have little intra-class variance, while being affected by large contextual changes (background, scale, perspective, illumination, etc.). To contribute in a direction which has not yet been properly addressed by the AI community, in this paper we present a visual challenge which is different in all these aspects. It is based on museum art mediums (MAMe), where attention to detail is essential, where there is huge intra-class variance, and where contextual information is not a factor.</p><p>The properties of ImageNet and ImageNet-like datasets have popularized the practice of interpolating images. This approach allows to reduce the memory requirements of models, avoiding high-resolution (HR) images, and removing the hindrances of variable-shaped (VS) inputs. The first CNN models tackling the ImageNet challenge inter-polated images to a fixed size of 224x224 pixels <ref type="bibr" target="#b36">[41,</ref><ref type="bibr" target="#b42">47]</ref>. More recent solutions increased that size to 229x229 <ref type="bibr" target="#b43">[48]</ref>, 331x331 <ref type="bibr" target="#b51">[56]</ref>, 480x480 <ref type="bibr" target="#b16">[21]</ref> or even 600x600 <ref type="bibr" target="#b12">[17,</ref><ref type="bibr" target="#b21">26]</ref> pixels, as scaling the image resolution is known to result in better performances on some cases <ref type="bibr" target="#b9">[14,</ref><ref type="bibr" target="#b44">49]</ref>. Even so, the nature of the ImageNet-like problems minimized these inconveniences, resulting in competitive performances even when using relatively small input sizes <ref type="bibr" target="#b48">[53]</ref>. Given the prominence of ImageNet, this particularity biased research. Indeed, beyond this ImageNet-like tasks, there are many current and future visual challenges where exploitation of HR and VS properties are likely to be relevant for improving performance.</p><p>Visual challenges in the medical domain are often based on the identification of small-scale visual patterns, requiring both attention to detail and an understanding of large structures. In domains like breast cancer detection, the benefit of exploiting the highest possible image resolution has already been highlighted <ref type="bibr" target="#b8">[13,</ref><ref type="bibr" target="#b24">29]</ref>, motivating the use of HR images. Similarly, image recognition systems used for autonomous driving also benefits from using HR images, as this entails detection at further distances, which have enormous safety implications. Current solutions already use images that are larger than 0.25 MP <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b46">51]</ref>.</p><p>The motivation for research on VS images derives from the increasing popularity of crowd-sourced datasets, such as Open Images <ref type="bibr" target="#b19">[24]</ref>. These datasets combine data produced from multiple sources, which saves time and effort, at the expense of obtaining data in different resolutions and shapes (e.g., landscape or portrait). In this context, standard training procedures using squared images are forced to interpolate them, hence deforming their image patterns. These image deformations introduce noise within data, potentially decreasing performance.</p><p>The main contribution of this paper is the MAMe dataset itself, which is made available for the research community ( §3). Beyond extensive statistics and expert insights, this work also provides several baselines based on popular architectures: VGG <ref type="bibr" target="#b36">[41]</ref>, ResNet <ref type="bibr" target="#b14">[19]</ref>, DenseNet <ref type="bibr" target="#b15">[20]</ref> and EfficientNet <ref type="bibr" target="#b44">[49]</ref> ( §4). Further experiments ( §4.4) are performed to assess the impact on accuracy when using highresolution, variable shape or both properties in conjunction. One final experiment ( §4.4) highlights whether performance gain comes from increasing the amount of image information or from increasing the models internal representation (as consequence of increasing the input size). This last experiment provide really different results when using the MAMe dataset in contrast to ImageNet <ref type="bibr" target="#b33">[38]</ref>, highlighting the particularity of the MAMe dataset. Finally, we provide a qualitative analysis of the MAMe through a set of expert analysis and explainability experiments ( §5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There are many visual challenge datasets in the current literature. There are however, very few containing images larger than 500x500 pixels, and with a significant variance in their aspect ratio. To illustrate that point we analyze a sample of popular datasets which satisfy three conditions we consider essential for attracting and generating high quality research:</p><p>-The dataset is publicly available.</p><p>-The dataset labels are reliable.</p><p>-The dataset has at least 100 instances per class.</p><p>The first requires data to be as public as possible, to reach the largest possible number of researchers. The second one excludes all datasets that contain labels not validated by humans or that have been crowd-labeled, as these may contain a significant amount of noise (and noise reduces the reliability of experimental results). The third enforces a minimum number of instances. We consider this a necessity for thorough research experimentation. We were nonetheless flexible in this regard, as some datasets of those analized contain some classes with less than 100 instances.</p><p>The sample analyzed contains the following 12 datasets: ImageNet 2012 <ref type="bibr" target="#b32">[37]</ref>, Food101 <ref type="bibr" target="#b4">[8]</ref>, IP102 <ref type="bibr" target="#b47">[52]</ref>, Places365 <ref type="bibr" target="#b50">[55]</ref>, Mit67 <ref type="bibr" target="#b30">[35]</ref>, Flower102 <ref type="bibr" target="#b28">[33]</ref>, CatsDogs <ref type="bibr" target="#b29">[34]</ref>, Stanford-Dogs <ref type="bibr" target="#b17">[22]</ref>, Textures <ref type="bibr" target="#b6">[10]</ref>, Caltech256 <ref type="bibr" target="#b11">[16]</ref>, Microsoft COCO <ref type="bibr" target="#b22">[27]</ref> and Pascal VOC 2012 <ref type="bibr" target="#b7">[11]</ref>. For each one we compute the product size (i.e., width multiplied by height) and aspect ratio (i.e., width divided by height) distributions. For the three datasets with more than 100,000 total samples (Im-ageNet 2012, Places365 and Microsoft COCO) we use a random sample of 100,000 images. Distributions for all 12 datasets can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>In terms of number of pixels (left plot), current image classification datasets rarely contain images with more than 1 megapixel (MP). For reference purposes, none of the 12 datasets contain images bigger than 1,000 x 1,000 pixels, assuming unitary aspect ratio. This already indicates a significant bias in current research, and a mismatch with current technology, as popular image taking resolutions are well above that size. Obviously, there are datasets with images larger than 1 MP, but these are typically either private, unreliability labeled <ref type="bibr" target="#b19">[24]</ref>, or have very few instances per class <ref type="bibr">[12]</ref>. In this context, as shown at the bottom of <ref type="figure" target="#fig_0">Figure 2</ref>, the MAMe dataset stands out, containing a large volume of reliable labeled HR images. In fact, all images in the Q1-Q3 interval of the MAMe dataset are bigger than the largest image found on all analyzed datasets. The mean image size for the MAMe dataset is 6.6MP (e.g., 2350x2350 in a squared image), one order of magnitude larger than all images contained in the analyzed datasets.</p><p>Regarding aspect ratio, the right plot of <ref type="figure" target="#fig_0">Figure 2</ref> shows how the majority of images found in current datasets are landscape. All datasets have their median in the landscape side, only half of the datasets contain Q1 within the portrait side, and only 3 contain a significant amount of portrait images (Food101, CatsDogs and Caltech256). However, even these have their aspect ratio distribution clearly skewed towards landscape images (notice that the median is quite close to the third quartile on all three cases). In contrast, the proposed MAMe dataset has a balanced distribution, containing approximately the same number of portrait and landscape images. The aspect ratio distribution is also much wider than the other datasets, showing how the MAMe dataset contains infrequently wide and tall images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The MAMe dataset</head><p>In this work we propose the Museum Artworks Medium dataset, abbreviated as the MAMe dataset. MAMe is an image classification dataset focused on the recognition of mediums in artworks and heritage held by museums (e.g., Oil on canvas, Bronze or Woodcut). Medium is a broad technical term used to describe several aspects of artworks <ref type="bibr" target="#b25">[30]</ref>. On one hand, it can be used to describe the main physical components used for the creation of an artwork, such as Oil on canvas. However, medium can also refer to the technique used to produce the artwork. Engraving, for example, is the printed result of engraving a metal plate. Both of these interpretations of medium are freely used by museums to organize their collections.</p><p>As detailed in §1, the classes considered in the MAMe dataset comprise a wide variety of mediums according to both interpretations of the term. These can range from simple material aspects (e.g., Bronze, Silver or Gold) to complex, high-level techniques (e.g., Faience, Woodblock or Woven fabric). The variety of relevant features in MAMe requires both attention to detail and to the overall image structure. Meanwhile, the essence of art causes widely different artworks to share the same label. The degree of intra-class variance of MAMe is exemplified in <ref type="figure">Figure 3</ref>. <ref type="figure">Fig. 3</ref>: Example of intra-class variance. Images in the same row belong to the same medium class, but share few visual features. The first row belongs to Ceramic, the second row to Bronze and the third row to Faience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data acquisition</head><p>In the past few years, museums around the world have been endorsing the policy of publicly releasing images of their heritage. Some of these museums release HR images under a CC0 license, allowing a free and unrestricted use of the data. We base our work on the data released by three museums. These were chosen because all three endorse the CC0 license, include a large number of images, provide accessible labels for them, and make it feasible to access their data in an automatized manner:</p><p>-The Metropolitan Museum of Art of New York (from now on the Met museum) <ref type="bibr" target="#b1">[4]</ref>. All three museums hold large artistic collections with a general scope, including artworks from all over the world, from very early cultures to recent ones. For accessing the data, the Cleveland museum publishes an API to automatically download images. Lacma and Met on the other hand provide access to their images only through their webpages. This implies an image-by-image download process, for which we built museum-specific crawlers. By these means we downloaded approximately 232,000 images from the Met museum, 26,000 from the Lacma museum and 32,000 from the Cleveland museum. From this data, we define the MAMe dataset, composed by an expertly-curated subset of the data. The final selection includes 37,407 images belonging to 29 classes. The class selection process was made following several technical criteria, including balance between museums (to avoid potential bias), balance and volume of class instances (to facilitate research), and image resolution (to enable HR exploration). Grey scale images were discarded. Significantly, museum images have a natural tendency towards VS (e.g., human sculptures tend to be tall, while paintings tend to be wide). Although we did not encouraged its presence, this natural feature is shown in the dataset statistics (see right plot of <ref type="figure" target="#fig_2">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label mapping</head><p>All three museums (Met, Lacma and Cleveland) reported the medium used to represent their artworks as metadata. Unfortunately, there is not a unique ontology behind, as each museum uses a different level of detail and interpretation of medium. Some mediums are subtypes of another mediums. Some mediums are reported under different names. And some mediums are combinations of other mediums. Experts from the art domain grouped the medium metadata into coherent classes, following their professional understanding of artistic coherency and visual discriminability. Classes which could not be discriminated visually by a human without technical aid (e.g., a microscope) were discarded. The main expert criteria used to determine the classes are the following:</p><p>-Written coherency: Medium categories written in different forms refering to the same term are aggregated (e.g., Bronze and bronze) -Terminology coherency: Medium categories which are considered to be analogous are aggregated (e.g., Ceramic and Pottery).  After enforcing a minimum amount of 850 samples per medium (adding up train, val and test), the MAMe dataset contains 29 different classes. These are shown in the left column of <ref type="table">Table 1</ref>. Notice we made an exception with the Silk and metal thread medium, which only contains 845 samples. A detailed description of the nature of each class is provided in <ref type="table" target="#tab_2">Table 2</ref>. Visual details on how to discriminate some of these classes are discussed in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset details</head><p>The MAMe dataset is publicly available 1 . The site provides access to all the original images, and a CSV file with metadata for each of them. This metadata includes the following information:</p><p>the image filename the medium of the artwork (i.e., the classification label) the museum from where the image was obtained the artwork ID given by the museum <ref type="table">Table 1</ref>: For each medium class within MAMe, distributions of instances among museums. The Met, Lacma and Cleveland museums are labeled as "Met", "Lac" and "Cle" respectively. Museum distributions are divided by data splits, into training, validation and test ("Train", "Val" and "Test" respectively). The last four columns show values aggregated for all data splits ("All"). The "Test" and "All" sections contain a 4th column indicating the total ("Total"). These values are not provided for "Train" and "Val" since these are constant (700 and 50 respectively).  <ref type="table" target="#tab_2">680  0  20  48  0  2  92  1  2  95  820  1  24  845  Silver  452  81 167  32  5  13  450  83 167  700  934 169 347 1450  Steel  628  0  72  44  0  6  118  1  14  133  790  1  92  883  Wood  577  43  80  41  3  6  576  44  80  700  1194  90 166 1450  Wood engraving  410  15 275  29  1  20  211  9 141  361  650  25 436 1111  Woodblock  259 258 183  18  19  13  258 258 184  700  535 535 380 1450  Woodcut  417  51 232  30  3  17  416  52 232  700  863 106 481 1450  Woven fabric  658  3  39  46  0  4  656  5  39  700  1360  8  82 1450</ref> the data split of the instance (i.e., train, validation or test set) the width of the image the height of the image the product size of the image (i.e., width multiplied by height) the aspect ratio of the image (i.e., width divided by height)</p><p>The dataset contains 29 medium classes. Each class is composed by at least 850 images and, at most 1,450. Each class contains 700 images for training, 50 images for validation and a variable amount of images for the test set (i.e., the test set is unbalanced). The minimum amount of instances in the test set is 100 (except for Silk and metal thread with 95) and the maximum is 700. In total, the MAMe dataset is composed by 37,407 HR images. All images in the MAMe dataset have, at least, a resolution of 0.25MP, equivalent to a squared image of 500x500 pixels. The mean resolution is around 10.3MP, corresponding to an image of more than 3,200x3,200 pixels, and the greatest image has more than 370MP corresponding to an image of 32,683x11,412 pixels (check <ref type="figure" target="#fig_0">Figure 2</ref>). The 37,407 images are divided in subsets as follows: 20,300 images for training and 1,450 images for validation and 15,657 for test. Of those, 24,911 images originate from the Met museum, 5,531 images from the Lacma museum and 6,965 images from the Cleveland museum. An effort was made to keep the data coming from the different museums as balanced as possible, to minimize the possibility of potential biases generated by the nature of artworks and the image taking particularities of each museum. The exact distributions of images per museum, class and data split are shown in <ref type="table">Table 1</ref>. To assess the internal balance of MAMe with regards to HR and VS features, <ref type="figure" target="#fig_2">Figure 4</ref> shows the product size and aspect ratio distributions for each medium class. Besides a few classes with particularly narrow or skewed distributions, most of the categories include a wide variety of product sizes and aspect ratios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medium</head><p>Description Albumen photograph Photographic prints on paper support. Paper is coated with egg white and silver nitrate, and exposed to sunlight in contact with a glass negative. Bronze</p><p>Objects mainly made of bronze (cooper and tin alloy). Includes both polished and hammered bronze. Ceramic</p><p>Includes pottery, stoneware, earthware and terracotta. It may include glazed, slip-painted or painted textures. Clay</p><p>Objects made of clay or mud. In most cases the object has not been baked, or it has at very low temperatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Engraving</head><p>Intaglio printmaking process in which lines are cut into a metal plate in order to hold the ink. The plate can be made of copper or zinc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Etching</head><p>Intaglio printmaking process in which lines or areas are incised using acid into a metal plate in order to hold the ink. The plate can be made of iron, copper, or zinc. Faience</p><p>May contain egyptian faience (sintered quartz with a vitreous coating) or tin-glazed pottery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glass</head><p>Objects mainly made of glass (eg blown, or pressed). Stained glass windows are excluded. Gold</p><p>Objects mainly made of gold. Includes polished gold, hammered gold and other surface textures. Graphite</p><p>Drawings or sketches made with graphite lead on paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand-colored engraving</head><p>Engraving prints hand-colored after the printmaking process. Prints are colored using either watercolor or wash techniques.</p><p>Hand-colored etching Etching prints hand-colored after the printmaking process. Prints are colored using either watercolor or wash techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iron</head><p>Objects mainly made of iron. Includes polished iron, hammered iron and other surface textures. Ivory</p><p>Objects made mainly of ivory (elephant or walrus tusks). Includes watercolor on ivory miniature portraits (medallions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limestone</head><p>Objects mainly made of limestone, a sedimentary rock mainly composed by calcium carbonate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lithograph</head><p>Planographic printmaking process in which a design is drawn onto a flat stone (or prepared metal plate, usually zinc or aluminum) and affixed by means of a chemical reaction. May contain lithographic offset prints and hand-colored monochrome lithographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marble</head><p>Objects mainly made of marble, a metamorphic rock composed of calcite or dolomite. Oil on canvas Fabric streched into frame (stretcher bar), with a preparation layer (or ground layer) painted with linseed oil and pigment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pen and brown ink</head><p>Drawings or sketches on paper, mainly made in brown ink (either with a dip pen, a fountain pen or a brush). Can be supplemented by other procedures such as wash (brown or black ink) or dry media. Some artworks may contain aged iron gall ink, or other similar brown inks such as bister or sepia ink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polychromed wood</head><p>Objects made of painted wood. Includes three-dimensional objects and painted surfaces, such as panel painting (oil on wood or tempera on wood). Porcelain A type of ceramic composed by quartz, feldspar and kaoli cooked at high temperatures. May contain soft-past porcelain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Silk and metal thread</head><p>Woven fabric objects made of silk with metallic threads, typically forming an embroidery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Silver</head><p>Objects mainly made of silver. Includes both polished and hammered silver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steel</head><p>Objects mainly made of steel (alloy of iron with carbon). Wood</p><p>Non polychromed wood objects. Inlcudes several wood types such as oak, boxwood or limewood. Wood engraving A type of woodcut printmaking process characteristic for using a block cut along the end-grain.</p><p>Woodblock A type of woodcut printmaking process typically used by oriental cultures. This type of woodcut is carved along the wood grain and uses a different block for each color printed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Woodcut</head><p>The oldest form of printmaking. Relief process in which knives and other tools are used to carve a design into the surface of a wooden block. The raised areas that remain after the block has been cut are inked and printed, while the recessed areas that are cut away do not retain ink, and will remain blank in the final print.</p><p>Woven fabric Fabric objects woven with a loom. Includes linen, cotton, silk and others. Fabrics appear in several forms such as plain fabrics, embroideries or printed fabrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines and Experiments</head><p>This section introduces and evaluates both a set of baseline models and a set of hypothesis. The purpose of baselines is to illustrate how the task proposed is coherently constructed (i.e., solvable) and worth receiving the attention of researchers. To this end, we employ prototypical solutions from the literature that provide good results on other challenges, and report their performance on the MAMe dataset.</p><p>Additionally, to highlight the differences of high-resolution (HR) and variable shape (VS) properties w.r.t. low-resolution (LR) and fixed shape (FS) in the context of MAMe, we per-form a set of experiments. These are designed to evaluate the following hypothesis:  </p><formula xml:id="formula_0">Hypothesis</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAMe data types</head><p>Most of current solutions in the literature use squared images to feed their models, that is images with a fixed shape. Additionally, these squared images are typically of low resolution. Resolutions used are diverse, but the most common is 256x256 pixels, corresponding to a total amount of 65,536 pixels. For referencing purposes, we use this data type as a starting point and we call it R65k-FS. For comparison purposes, we use a second data type using the same resolution (i.e., same amount of total pixels) but keeping the original aspect ratio of the image, that is with the VS property. This second data type is called R65k-VS. We also produce the HR versions of these two data type. These HR versions contain a total of 360,000 pixels. They are the R360k-FS and the R360k-VS version. Notice that R360k-FS corresponds exactly to an squared image of 600x600 pixels, while R360k-VS contains images of variable shape but fixed number of pixels. See <ref type="figure" target="#fig_4">Figure 5</ref> for an illustration of all data types used.</p><p>The final list of data types used in this section is as follows:</p><p>-R65k-FS: images are downsampled to 256 x 256 pixels, corresponding to an image size of 65,536 pixels. -R65k-VS: the original aspect ratio is maintained forcing the total number of pixels to 65,536. -R360k-FS: images are resized to 600 x 600 pixels (360,000 pixels)</p><p>-R360k-VS: images are rescaled to a total number of pixels to 360,000, maintaining the original aspect ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training configurations</head><p>In this work we use very well-known architectures: VGG <ref type="bibr" target="#b36">[41]</ref>, ResNet <ref type="bibr" target="#b14">[19]</ref>, DenseNet <ref type="bibr" target="#b15">[20]</ref> and EfficientNet <ref type="bibr" target="#b44">[49]</ref>. The specific architecture versions that we use are the following:</p><formula xml:id="formula_1">-VGG11 (configuration A) -VGG16 (configuration D) -ResNet18 -ResNet50 -EfficientNet-B0 -EfficientNet-B3 -DenseNet121</formula><p>Our baselines and experiments use several types of input processing. This is divided into two main components: data processing and data augmentation. The first refers to all image transformations required to obtain each data type (according to subsection 4.1), while the second provides regularization during the training process. The data augmentation is independent of the data type, and is defined as follows:</p><p>1. Random rotation of the image from <ref type="bibr">[-30, 30]</ref> degrees. 2. Random crop of (0.875 x width, 0.875 x height) pixels.</p><p>Width and height refer to current dimensions at this point of the processing.</p><p>3. Random horizontal flip with 50% chance.</p><p>As a final step, images are normalized to have values in the range [0, 1] and standardized with µ = 0.5 and σ = 0.5 (same value for all three channels). Notice that during validation, the data augmentation is adapted. In this phase, steps 1 and 3 are avoided and, step 2 does a center crop instead of random one.</p><p>We use the AMSGrad optimizer <ref type="bibr" target="#b31">[36]</ref> for all the baselines and experiments, a variant of the original Adam optimizer <ref type="bibr" target="#b18">[23]</ref>. Batch sizes and learning rates are optimized for each training, considering memory limitations, training speed and learning convergence. Executions are conducted in a single computing node of the CTE-Power9 cluster at the Barcelona Supercomputing Center, with the following characteristics:</p><p>-2 Sockets x IBM Power9 8335-GTH @ 2.4GHz (20 cores and 4 threads/core, total 160 threads). -4 x GPU NVIDIA V100 (Volta) with 16GB HBM2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines performance</head><p>To show the feasibility and evaluate the difficulty of the MAMe task, we introduce a set of baseline models. Their purpose is to reach the best possible performance using current prototypical solutions. To that end, we employ the following CNN architectures: VGG11, VGG16, ResNet18, ResNet50, EfficientNet-B0, EfficientNet-B3 and DenseNet121. We train these using the four MAMe data types explained in subsection 4.2. Due to memory limitations, we only use a subset of architectures on the R360k data type: VGG11, ResNet18, EfficientNet-B0 and EfficientNet-B3. All baselines are trained on top of the corresponding ImageNet pre-trained models 3,4 . Top 10 baseline results are shown in <ref type="table" target="#tab_4">Table 3</ref>. These reported results correspond to the mean per class test accuracy using the models achieving minimum validation loss.</p><p>After training multiple models with combinations of seven architectures and four MAMe data types, finetuning each of the training models to optimize performance and using pre-trained models from ImageNet, most models achieve accuracies above 80%. The maximum 88.95% accuracy is obtained by the EfficientNet-B3 architecture on the R360k-FS data type. These results show that, indeed, the MAMe task is solvable. It also clearly illustrates the benefits of using higher resolutions, as the top 4 models are based on R360k data types. On the other hand, it seems that models are not properly exploiting the VS property, as only VGG11 manages to be in the top 4 with that shape policy. Next, let us assess the relevance of these properties in further detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hypothesis evaluation</head><p>In this section we aim to validate the three hypothesis introduced in section 4. Our first hypothesis is H 1 : MAMe benefits from HR data w.r.t. LR data. Since HR contains extra information that is not present in LR data, this hypothesis aims to measure to which degree is this additional information relevant for improving performance on MAMe.</p><p>To test H 1 we train a set of models using R65k-FS and R360k-FS MAMe data types, where the only difference is the resolution of images. Notice both data types share the same proportional distortion w.r.t. the original shape of images. We do the same experiment using variable shape (i.e., R65k-VS and R360k-VS data types). In this case the only difference is the resolution of images because there is no distortion added to the aspect ratio. The architectures used for validating this hypothesis are VGG11, ResNet18,  EfficientNet-B0 and EfficientNet-B3. We use this subset of shallow architectures due to the high-memory requirements when using R360k data. The models are trained starting from their corresponding ImageNet pre-trained models. Results are shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref>. H 1 is validated for 6 out of 8 comparison pairs. In these, the models trained on HR data (i.e., R360k) achieve a boost in performance around 4%. This happens for all four architectures tested (VGG11, ResNet18 and the EfficientNet variants). Noticeably, the two cases where using HR data does not yield benefits (and instead degrades around 3%) are VS settings using the EfficientNet architectures. This phenomenon will be further studied and discussed next, when we assess the validity of H 2 hypothesis.</p><p>Let us now consider the second hypothesis H 2 : MAMe benefits from VS data w.r.t. FS data. Since the FS property deforms the original image adding some distortion, this hypothesis aims to measure to which degree is this deformation relevant for performance on MAMe. For this purpose we compare models which have the same resolution and only differ in shape; we compare R65k-FS with R65k-VS, and R360k-FS with R360k-VS. The architectures used for the R65k comparison are all architectures listed in section 4.2, but architectures used in the R360k comparison are a subset of them due to high-memory requirements: VGG11, ResNet18, EfficientNet-B0 and EfficientNet-B3. All models are trained starting from ImageNet pre-trained models. Results are shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_7">Table 6</ref>.</p><p>With regards to H 2 , results do not fully validate nor reject it. Prototypical architectures do not adequately take advantage of the VS property, either by obtaining insignificant accuracy variations (VGG11, VGG16, ResNet18-R65k and ResNet50) or even producing a negative effect on performance due to VS (ResNet18-R360k, EfficientNet-B0 and EfficientNet-B3). Only for the case of DenseNet121 on R65k data, performance increases when moving from FS to VS. Overall, results are inconclusive regarding this second hypothesis H 2 .</p><p>The experiments with regards to H 2 are affected by the padding. To process images of VS in a single batch, it is required that they all have exactly the same shape for computational purposes. Doing so without changing the shape implies the addition of padding pixels, that is, non-informative </p><formula xml:id="formula_2">FS to VS Architecture R65k +0.04% VGG11 +0.01% VGG16 -0.67% ResNet18 -0.22% ResNet50 +2.92% DenseNet121 -1.35% EfficientNet-B0 -1.63% EfficientNet-B3 R360k +0.38% VGG11 -3.56% ResNet18 -9.14%</formula><p>EfficientNet-B0 -8.91%</p><p>EfficientNet-B3 values (typically zeros) that are used to uniform batch shape. However, existing architectures do not differentiate padding pixels from image pixels, counting as noise during the training process. Remarkably, this noise increases considerably under a HR setting, where the absolute amount of padding pixels increases. In this regard, some preliminary experiments conducted on a previous version of the MAMe dataset <ref type="bibr" target="#b37">[42]</ref> indicate that reducing padding in a VS setting can yield to accuracy improvements between 3% and 5%. As illustrated in the results of the H 1 , on the MAMe dataset a gain in resolution implies a gain in performance. However, as suggested by Sandler et. al. <ref type="bibr" target="#b33">[38]</ref>, such increase in performance may be due to an increase of the input size or due to an increase of the internal representation of the model. In their publication <ref type="bibr" target="#b33">[38]</ref>, they evaluate the impact of these two factors with an experiment on the ImageNet dataset. This experiment consist on comparing information gain against resolution gain, by assessing pairs of models trained with images of same resolution but different amount of information:</p><p>-Full-information images: Images that are downsampled to a target resolution and contain their corresponding amount of information. -Capped-information images: Images that are, first, downsampled to a 224x224 resolution and, second, upsampled to a larger target resolution (Bilinear interpolation). Notice that such image will be of same resolution than its corresponding full-information image, but it will contain less original information.</p><p>Full-information images increase the model internal representation and the input information, while capped-information images increase the model internal representation as well, but do not increase the input information. By comparing performances using both, we can isolate the impact of the input information when increasing the image resolution.</p><p>To evaluate the impact of image information gain on the MAMe dataset, we formulated our third hypothesis H 3 : MAMe benefits from information gain w.r.t. only resolution gain. While this hypothesis is rejected in ImageNet according to <ref type="bibr" target="#b33">[38]</ref>, next we test if this is also the case for MAMe. To conduct this experiment, we train FS models on VGG11 and ResNet18 architectures using the following target resolutions: 50k, 90k, 160k, 250k and 360k pixels. These correspond to squared image widths of 224, 300, 400, 500 and 600 pixels. For each target resolution, we use fullinformation and capped-information images. Notice both full-information image and capped-information image are equivalent for the image width of 224 pixels. Results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>.</p><p>Results obtained by Sandler et. al. <ref type="bibr" target="#b33">[38]</ref> on ImageNet dataset <ref type="bibr" target="#b32">[37]</ref> indicate that there is no gain in performance due to information gain (see their <ref type="figure" target="#fig_0">Figure 2b</ref>). Indeed, improvement in their experiments is only caused by the use of larger model internal representations. In the MAMe dataset, this very same experiment shows that improvement occurs for both reasons. Increasing the model internal representation (capped-information) entails some consistent improvement in performance. However, performance is further boosted when also increasing the information (full-information). These results suggest the third hypothesis holds for the MAMe dataset, and further highlights the inherent differences between MAMe and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expert and explainability analysis of MAMe</head><p>The domain of artworks and heritage is defined by human technology, skill and creativity. Art experts can identify a set of visual queues useful for the characterization of art, but remains to be seen if AI models learn these same features. To analyze these features in the context of the MAMe dataset, we analyze several medium classes from an expert point of view and perform explainability experiments. We perform explainability on two models trained from scratch with two data types. On one hand, the R65k-FS is used to characterize a low-resolution and fixed shape setting (LR&amp;FS). On the other hand, to highlight a high-resolution and variable shape (HR&amp;VS) we introduce a new data type version: the A500-VS. This new HR&amp;VS version ensures a minimum size of 500 pixels per axis, preserving the original aspect ratio, as illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. The main reason for using A500-VS instead of R360k-VS is to facilitate visualization. In this case we use the architecture most widely used for this kind of experiments, the VGG <ref type="bibr" target="#b36">[41]</ref>. In our case, we use the shallow version VGG11 to handle the high-memory requirements of A500-VS. By understanding the focus of the LR&amp;FS and HR&amp;VS models, we can detect the most relevant class features according to them. These explanations allow experts to assess the consistence of the decisions made, and detect the potential existence of bias. Finally, comparison between LR&amp;FS and HR&amp;VS explanations offers an additional exploration about the impact of HR and VS properties in the MAMe dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Layer-wise relevance propagation</head><p>In our analysis we use post-hoc interpretability <ref type="bibr" target="#b23">[28]</ref>: Methods used to interpret the model predictions once the model has been trained. For image classification, a widely used visual explanation are the saliency methods. These methods use saliency maps to show the features on the image that contribute to a prediction. In other words, which pixels in the input image are important for the classification task. Among this family of methods <ref type="bibr" target="#b34">[39,</ref><ref type="bibr" target="#b35">40,</ref><ref type="bibr" target="#b38">43,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b49">54]</ref>, we use Layer-wise Relevance Propagation (LRP) <ref type="bibr" target="#b20">[25]</ref> which has been used in different fields performing meaningful explanations <ref type="bibr" target="#b2">[6,</ref><ref type="bibr" target="#b3">7,</ref><ref type="bibr" target="#b40">45,</ref><ref type="bibr" target="#b45">50]</ref>. The LRP technique backpropagates the output prediction to the input image, by computing the contribution of each neuron w.r.t. the output prediction. That is, effectively mapping the relevance of an specific class into the pixels of the input image.</p><p>Although different LRP rules have been proposed, we implement the recent Composite LRP <ref type="bibr" target="#b26">[31]</ref>. This technique proposes to combine different propagation rules depending on the depth of the layer. Our Composite LRP makes use of LRP−0 for last layers, LRP− ( = 0.25) and LRP−γ (γ = 0.25) for intermediate layers, and LRP−z B for the first layer of the network, as illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>So, given an image I and a specific class c, the Composite LRP produces an explanation heatmap E I,c . The color convention for this heatmap is as follows: red is used for positive contributions, while blue indicates negative contributions. That means, the red areas are considered descriptive patterns of the given class by the model. Meanwhile, the blue areas are considered typical patterns of other classes.</p><p>We perform two types of LRP analysis, one for the correctly predicted images, and another one for the incorrectly predicted images. In case of correctly predicting the medium m, we produce its corresponding explanation heatmap E I,m . In this case, the red areas of the heatmap correspond to descriptive patterns of the predicted medium m and blue areas to descriptive patterns of the rest of mediums. In the case of incorrectly predicted images, we computed the explanation as the difference between two heatmaps. The one associated to the real medium r, minus the one associated to the predicted medium p: E I,r,p = E I,r − E I,p</p><p>This difference allows to remove the contributions to the predicted class, focusing on the features that contribute to the real class. In this visualization, the red areas will be considered typical patterns of the real class but not of the predicted class, while blue areas will be considered typical patterns of other classes (most of them probably from the predicted class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Best and worst performances</head><p>First, let us focus on the classes that are best and worst recognized by the two models trained on the two versions of the MAMe dataset introduced before: Among the best ones we can count Albumen photograph, Gold and Graphite. In the case of Albumen photograph, we only have one type of photographic technique in the MAMe dataset, making these images easily distinguishable from other cultural assets. The class Gold is a similar case, since the golden color differentiates it from other metals, despite having other objects in the dataset of similar shapes. Lastly, Graphite is a drawing technique that uses similar grey tones with metallic brightness and smooth strokes that usually end at the edge of the paper. These characteristics help avoiding confusions between Graphite and Lithograph, which in some cases may be similar. For these reasons these mediums are easily recognizable, not only for the LR&amp;FS and HR&amp;VS model, but also for human experts.</p><p>On the other side of the spectrum we have the classes that are most poorly recognized by these two models. These are Woven fabric, Polychromed wood, Etching and Silk and metal thread. These classes are hard to predict because they belong to fine-grained groups of classes, with many common features. Following expert guidelines we identify the following fine-grained groups. These are discussed in further detail next.</p><p>-Prints: Etching, Engraving, Wood engraving, Woodcut, Woodblock, Lithograph -Fabrics: Woven fabric, Silk and metal thread -Paintings: Polychromed Wood, Oil on canvas</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Prints group</head><p>From an expert perspective, the most complex fine-grained group is Prints. They are hard to differentiate because they may look very similar, despite having been printed through different procedures. Common clues used by experts for their discrimination include the definition of lines, the appearance of strokes, the homogeneity of shadows or color areas, as well as the intensity of blacks. A common feature used to identify different kinds of prints is the platemark. Platemark is the rectangular ridge created in the paper of a print by the edge of an intaglio plate. These marks can be essential for the discrimination of certain print classes: While both Engraving or Wood engraving have very defined lines and grid patterns, they can be told apart through platemarks since these only appear on the edges of an Engraving. Within the same group Prints, Woodblocks are distinguishable from the rest because of their oriental aesthetics. They are usually colored prints that use one block for each ink. As a result, colors sometimes overlap, and/or leave gaps in the outlines. However, this last characteristic is also found on other colored prints like Lithographs or Woodcuts. One last example to illustrate the complexity within Prints could be Etching and Engraving. These two techniques are very similar, having the same aforementioned platemarks and often the same grid patterns in their printed areas. In this case, experts need to appreciate the contours of the lines for differentiation. They are more vibrant and less defined in Etchings, and they have convex edges for Engravings.</p><p>In sight of the expert knowledge, image resolution seems key to properly detect main discriminating patterns. In some cases, even our HR&amp;VS images seem to fall short in resolution (e.g., grid patterns are lost). As an example, <ref type="figure" target="#fig_8">Figure 8</ref> shows a rectangular region of an Engraving in original resolution (left side) and in HR&amp;VS (right side). Zoomed area shows the central figure of the print, a fisherman. If we focus on the clothes, we can clearly perceive the characteristic grid pattern of an Engraving in the original resolution image, but these are lost on the HR&amp;VS image, where the grid become a gray blur due to the interpolation when resizing the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Fabrics group</head><p>The second group of fine-grained classes is Fabrics. To discriminate these with total confidence it is necessary to identify the fibers using microscopy techniques. This condition motivated the aggregation of several classes within Woven fabric (e.g., linen, cotton, silk and others). Nonetheless, one particular type of woven fabric can be visually recognized without the aid of external machinery. That is Silk and metal thread, which are clearly distinguishable from other textile fibers due to the glitter of metallic threads.</p><p>In <ref type="figure">Figure 9</ref>, we can see the metallic glitter in LR&amp;FS and HR&amp;VS images (more clearly on the latter). However, both models have been unable to properly discriminate these two classes. If the model does not detect this feature, it will learn other patterns for differentiating these two classes, such as ornamental motifs. However, this is not a reliable discrimi- natory feature and, therefore, it could be a source of error. We performed explainability experiments on several images and found cases where the model focuses on the ornamental motifs as shown in <ref type="figure">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Paintings group</head><p>The third group of fine-grained classes is Paintings. This group contains two classes: Polychromed wood and Oil on canvas. The main reason why these classes are hard to differentiate is because Polychromed wood contains the subclass panel paintings (i.e., a painting on a flat panel made of wood), which are similar to Oil on canvas. Both, Polychromed wood panel paintings and Oil on canvas, hide the support behind the paint layer, complicating the identification of the support material (fabric or wood). In this context, experts pay attention to cracks, leaks or textures that may be characteristic of the support below the paint. Nonetheless, these features may not be properly visible in a single LR&amp;FS or HR&amp;VS images.</p><p>There are several Oil on canvas images that are incorrectly predicted as Polychromed wood, both in LR&amp;FS and CLEVELAND <ref type="bibr">MUSEUM: 1943.324</ref> in HR&amp;VS. It makes sense from an expert point of view since, in several HR&amp;VS images, it is impossible to appreciate any detail that may suggest whether the support is wood or fabric, forcing the model to guess the class based on alternative patterns that may be misleading. For example, one of the key properties that identify an Oil on canvas is the canvas weave pattern. Unfortunately, this seems to be visible only on a few HR&amp;VS images. Within this work, art experts reviewed around150 images where the two models failed to discriminate between Oil on canvas and Polychromed wood, and considered that they could only see the canvas weave pattern in approximately 5% of the HR&amp;VS images. In <ref type="figure" target="#fig_10">Figure 11</ref>, we show an example of an Oil on canvas image where it is possible to perceive the canvas weave pattern. Although this pattern is present in the HR&amp;VS image but not in the LR&amp;FS image, both models misclassified this example, indicating that the HR&amp;VS model does not pay attention to this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">LR&amp;FS and HR&amp;VS comparison</head><p>In this section we explore the classes with greatest difference in accuracy between the two models we are exploring: the one trained on LR&amp;FS images and the one trained on HR&amp;VS images. In order, these classes are Lithograph (+16.28% gain by HR&amp;VS), Bronze (+15.71% gain by HR&amp;VS) and Engraving (+14.85% gain by HR&amp;VS). Lithograph and Engraving are within the Prints group which, as reviewed in §5.2.1, can benefit from more detailed inputs for their discrimination. The third, Bronze is a material which can be easily differentiated by a human expert.</p><p>Let us start with the case of Lithograph. <ref type="figure" target="#fig_0">Figure 12</ref> shows a representative example of this class, illustrating both the input and the LRP for the HR&amp;VS and LR&amp;FS models. Both models focus on the overall texture of the image (the LRP relevance is spread throughout the image), but with different impacts on the prediction: it represents negative evidence for LR&amp;FS (which ends up in mispredicting the class Wood engraving) but positive evidence for HR&amp;VS. Experts highlight the relevance of the texture of Lithographs for their discrimination from other similar classes like Woodblock, Hand-colored etching, Wood engraving or Hand-colored engraving. Lithographs contain a granular texture that is not present on the other classes, but this texture is only visible at a certain resolution, as shown in the zoomed tombstone at the bottom of <ref type="figure" target="#fig_0">Figure 12</ref>. This LRP results indicate that the HR&amp;VS model follows a similar strategy to distinguish Lithograph from other classes, successfully recognizing the textures from prints and properly interpreting them for the final prediction. The LR&amp;FS model, unable to recognize the granular texture, fails at finding relevant features towards Lithograph. <ref type="figure" target="#fig_12">Figure 13</ref> shows an example of the Engraving class, which has been correctly predicted by the HR&amp;VS but not by the LR&amp;FS (mispredicted as Wood engraving).  not. According to experts, these figure contours are dark areas that encode essential information for discriminating the mediums within the Prints group. Contours can only be properly inspected at high-resolutions. Some of this information is retained in HR&amp;VS images, as reviewed by experts. Meanwhile LR&amp;FS images lose all relevant details.</p><p>As mentioned in subsection 5.2.1, another property to distinguish printing techniques is the grid pattern. Although in some cases it can only be perceived in the original resolution image, some HR&amp;VS image retain this information. However, this is always lost in the LR&amp;FS images. On top of that, the image distortion produced by the shape variation of LR&amp;FS images forces the grid lines closer in one axis (unpredictably, as it depends on the original image aspect ratio), complicating its identification. As an example of that, <ref type="figure" target="#fig_2">Figure 14</ref> shows an Engraving image in HR&amp;VS and LR&amp;FS format, where the latter shows a great image distortion. It also shows a zoomed area, highlighting the differences in the grid pattern.</p><p>The last case we consider in this section is the third class with the biggest difference in performance. This is the Bronze class, which includes a great variety of objects (e.g., sculptures, ornaments), but specially coins. One of the main reasons why there are so many coins inside the Bronze class is that, historically, Bronze has been a usual alloy used to mint coins. One of the main characteristics of a coin is its circular shape. However, this property is lost when deforming the image due to the uniformization of aspect ratio inherent to LR&amp;FS inputs. The lack of a uniform shape of coins has a negative impact on their recognition, which is not found on the HR&amp;VS model. A clear example of this can be observed in <ref type="figure" target="#fig_4">Figure 15</ref>. The corresponding LRP explanations show, on one side, the positive impact of the rounded coin contour for the HR&amp;VS image and, on the other side, the negative im- CLEVELAND <ref type="bibr">MUSEUM: 1926.248</ref> pact of the deformed coin for the prediction of the LR&amp;FS image. This particular LR&amp;FS example is mispredicted with Steel, which makes sense from an expert point of view because the model must focus on the detection of the material, as it can not rely on the shape of the coin for the prediction. Indeed, classes like Steel and Iron are among the most frequent confusions for Bronze. As as result, Bronze is significantly better predicted by the HR&amp;VS, with a 15.7% increase in accuracy with respect to LR&amp;FS.</p><p>Another example is shown in <ref type="figure" target="#fig_5">Figure 16</ref>, where we can see the characteristic corrosion and patinas of Bronze. This corrosion or green patinas on the surface comes from the oxidation of copper, which is one of the main components of the Bronze alloy. Experts underline that these properties make quite easy to recognize the class. While these are perfectly visible in HR&amp;VS images, they become hard to perceive in the LR&amp;FS images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we introduce the MAMe dataset, a novel challenge for the prediction of artwork mediums based on its visual appearance. The images of the dataset come from three different museums for a total of 37,407 images. Museums do not share a common scheme for labeling mediums, which required intensive work by art experts for its homogenization. For producing the dataset, we leverage technical requirements (sample size, balance, image resolution, etc.) and domain requirements (visual coherency, taxonomical properties, etc.). At the end, the MAMe is composed by 29 classes of mediums, each containing at least 850 images (always 700 for training) of high-resolution (at least 500 pixels in the smaller axis) and variable shape.</p><p>In comparison with commonly available datasets, the MAMe provides a significantly larger distribution of highresolution and variable-shaped images. These properties are of relevance for future applications in domains such as medicine or autonomous driving; domains where attention to detail, understanding the overall structure and avoiding image pattern deformation/loss is crucial. Recognizing a lack of focus on these topics by the AI community, MAMe provides a good testing environment for new research ideas in the field.</p><p>Baselines and hypothesis results provide several conclusions. Regarding baselines, results presented in <ref type="table" target="#tab_4">Table 3</ref> show the capability of such models to solve the task proposed by the MAMe dataset up to certain degree, with a top performance of 88.95% accuracy achieved by EfficientNet-B3 architecture using R360k-FS data format. Regarding hypothesis evaluation, results shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="figure" target="#fig_5">Figure 6</ref> support H 1 and H 3 hypotheses but not H 2 . We conclude from our first hypothesis H 1 that performance on the MAMe task increases when using images of high-resolution over standard low-resolution ones. Furthermore, based on the validated third hypothesis H 3 , we see that this performance gain comes not only from larger image resolution but also from an increase of the image information (unlike ImageNet <ref type="bibr" target="#b32">[37]</ref>). In contrast, results on the H 2 hypothesis do not validate it. We consider prototypical architectures to be specifically designed for FS data, hence not taking proper advantage of the VS property. Moreover, the current way of handling VS introduce padding on batching, increasing the amount of noise during training, specially in high-resolution settings (i.e., R360k in our case). We consider that there is room for improvement in this area based on studies in a previous version of the MAMe dataset, where padding reduction on VS models provides performance improvements of 3% to 5% <ref type="bibr" target="#b37">[42]</ref>.</p><p>Lastly, we perform an explainability and expert analysis to further understand the differences when training models with either, low-resolution and fixed-shape (R65k-FS) im-ages or high-resolution and variable-shape (A500-VS). The results of these analysis allow us to assess how the models we explored fail to discriminate between certain classes due to a lack of resolution. In several cases we found that even the A500-VS resolution is insufficient to perceive the patterns that experts would pay attention to. This forces the models to learn on alternative patterns that may not generalize well.</p><p>Overall, the MAMe dataset constitutes a large scale challenge which benefits from the use of HR. Benefit from using HR images does not only come from bigger internal representation of the models, but also from an increase of the image information. This is particularly characteristic for MAMe, since differs from the prototypical and well-known ImageNet dataset. Further research is needed to efficiently handle VS data, and MAMe dataset serves as a good candidate for such use case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Product size and aspect ratio distribution over several datasets, both on log scale. The dashed horizontal blue line separates a sample of current image classification datasets, and the proposed MAMe dataset. The vertical red line at aspect ratio 1.0 shows the border between portrait (left side) and landscape (right side) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc>The Los Angeles County Museum of Art (from now on the Lacma museum) [3]. -The Cleveland Museum of Art (from now on the Cleveland museum) [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Product size and aspect ratio distribution over all classes of the MAMe dataset. Distributions are represented in boxplots, both of them on log scale. The vertical red line at aspect ratio 1.0 shows the border between portrait (left side) and landscape (right side) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of the different MAMe data types, exemplified for one particular instance. FS stands for fixed shape and VS for variable shape. R stands for resolution and A for axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Results when training VGG11 and ResNet18 architectures using full-information (downsampled from original) and capped-information (upsampled from 224) images. Target resolutions used are the ones corresponding to image widths of 224, 300, 400, 500 and 600 pixels. Both settings improve with size, indicating that both resolution and information gain contribute to better performances in MAMe. Notice the experiment at 224 width is not capped information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>LRP rules applied to each layer of the VGG11 network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>-</head><label></label><figDesc>The model trained from scratch on images of LR&amp;FS (R65k-FS), using the VGG11 architecture.-The model trained from scratch on images of HR&amp;VS (A500-VS), using the VGG11 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Example of an Engraving artwork at its original size (left side) and HR&amp;VS (right side). The second row shows the same zoomed area for both images, where the grid pattern can only be perceived on the original resolution (left). MET MUSEUM: 53.600.1616</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Example of Silk and metal thread in HR&amp;VS (left) and LR&amp;FS (right). The brightness of the metal threads is visible in both cases. MET MUSEUM: 2002.494.278 Example of Silk and metal thread in HR&amp;VS (left) and its LRP explanation (right). The ornamental motifs (red zones) have positively contributed to the Silk and metal thread class classification. MET MUSEUM: 2002.494.366</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Example of Oil on canvas in HR&amp;VS (left side) and LR&amp;FS (right side). The second row shows the zoomed area where it is possible to perceive the canvas wave pattern in the HR&amp;VS but not in the LR&amp;FS image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 :</head><label>12</label><figDesc>Lithograph example in HR&amp;VS and LR&amp;FS. There is a top side and a bottom side divided by an horizontal black line. Top shows the image in HR&amp;VS and its corresponding LRP explanation. Both models focus on the general texture for their predictions, although LR&amp;FS mispredicts Wood engraving. Bottom side shows a zoomed area of the print in HR&amp;VS (left) and LR&amp;FS (right). In here we can see the granular texture of the surface typical of this class in HR&amp;VS, but not in LR&amp;FS.MET MUSEUM:49.21.53    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Engraving example in HR&amp;VS and LR&amp;FS and its corresponding LRP explanations. Check how the contours of the figures positively contribute to the prediction of the class in HR&amp;VS format. LR&amp;FS loses most these details, and mispredicts it as Wood engraving. CLEVELAND MUSEUM: 1958.105</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>The Figure contains the entire image and its corresponding LRP explanation for both, HR&amp;VS and LR&amp;FS, which target really different aspects of the print: While HR&amp;VS focuses on the contours of the print figures, LR&amp;FS does</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 :</head><label>14</label><figDesc>First row shows an Engraving in HR&amp;VS and LR&amp;FS. Notice the deformation of the latter. Second row shows a zoomed area, to illustrate how the grid lines become blurred in the LR&amp;FS version. MET MUSEUM: 17.3.3169</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 :Fig. 16 :</head><label>1516</label><figDesc>Example of coins within the Bronze class in HR&amp;VS (left side) and LR&amp;FS (right side), and its corresponding LRP explanations (bottom). Shape of coins is lost in LR&amp;FS, which affects the prediction. CLEVELAND MUSEUM: 1916.1877 Zoom in of a Bronze artwork in HR&amp;VS (left) and in LR&amp;FS (right) respectively. Notice how the corrosion and patinas are easier to appreciate in HR&amp;VS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Descriptions of the medium classes. Some descriptions are obtained from the museum sources [5].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 (H 1 ): MAMe benefits from HR data w.r.t. LR data. MAMe benefits from VS data w.r.t. FS data. MAMe benefits from information gain w.r.t. only resolution gain. All baseline models, hypothesis experiments and the code needed to replicate results are publicly available 2 .</figDesc><table><row><cell>Hypothesis 2 (H 2 ): Hypothesis 3 (H 3 ):</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top 10 baseline results for the MAMe dataset. Notice the prevalence of high resolution to a great extent.</figDesc><table><row><cell>Architecture</cell><cell cols="3">Resolution Shape Accuracy</cell></row><row><cell cols="2">EfficientNet-B3 R360k</cell><cell>FS</cell><cell>88.95%</cell></row><row><cell cols="2">EfficientNet-B0 R360k</cell><cell>FS</cell><cell>88.25%</cell></row><row><cell>Resnet18</cell><cell>R360k</cell><cell>FS</cell><cell>88.15%</cell></row><row><cell>VGG11</cell><cell>R360k</cell><cell>VS</cell><cell>85.42%</cell></row><row><cell cols="2">EfficientNet-B3 R65k</cell><cell>FS</cell><cell>85.11%</cell></row><row><cell>VGG11</cell><cell>R360k</cell><cell>FS</cell><cell>85.04%</cell></row><row><cell>Resnet18</cell><cell>R360k</cell><cell>VS</cell><cell>84.59%</cell></row><row><cell>Resnet50</cell><cell>R65k</cell><cell>FS</cell><cell>84.29%</cell></row><row><cell>Resnet50</cell><cell>R65k</cell><cell>VS</cell><cell>84.07%</cell></row><row><cell cols="2">EfficientNet-B0 R65k</cell><cell>FS</cell><cell>83.73%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiment results for H 1 (more resolution is better) and H 2 (less deformation is better) hypotheses. H 1 is assessed vertically (same shape policy, variable resolution), while H 2 is assessed horizontally (same resolution, variable shape policy).</figDesc><table><row><cell></cell><cell>FS</cell><cell>VS</cell><cell>Architecture</cell></row><row><cell></cell><cell cols="3">81.35% 81.39% VGG11</cell></row><row><cell></cell><cell cols="3">81.20% 81.21% VGG16</cell></row><row><cell></cell><cell cols="3">83.33% 82.66% ResNet18</cell></row><row><cell>R65k</cell><cell cols="3">84.29% 84.07% ResNet50</cell></row><row><cell></cell><cell cols="3">73.14% 76.06% DenseNet121</cell></row><row><cell></cell><cell cols="3">83.73% 82.38% EfficientNet-B0</cell></row><row><cell></cell><cell cols="3">85.11% 83.48% EfficientNet-B3</cell></row><row><cell></cell><cell cols="3">85.04% 85.42% VGG11</cell></row><row><cell>R360k</cell><cell cols="3">88.15% 84.59% ResNet18 88.25% 79.11% EfficientNet-B0</cell></row><row><cell></cell><cell cols="3">88.95% 80.04% EfficientNet-B3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Difference in performance between models trained using R65k and R360k data. This results are used to validate hypothesis H 1 .</figDesc><table><row><cell></cell><cell>FS</cell><cell>VS</cell><cell>Architecture</cell></row><row><cell>R65k to R360k</cell><cell cols="3">+3.69% +4.03% VGG11 +4.82% +1.93% ResNet18 +4.52% -3.27% EfficientNet-B0 +3.84% -3.44% EfficientNet-B3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Difference in performance between models trained using FS and VS data. This results are used to validate hypothesis H 2 .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://hpai.bsc.es/MAMe-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/HPAI-BSC/MAMe-baselines</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/vision 4 https://github.com/lukemelas/ EfficientNet-PyTorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://image-net.org/challenges/beyond_ilsvrc.Ac-cessed" />
		<title level="m">Beyond imagenet large scale visual recognition challenge</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://www.metmuseum.org/about-the-met/curatorial-departments/drawings-and-prints/materials-and-techniques/printmaking.Accessed" />
		<title level="m">Met museum: Image and data resources</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note type="report_type">Printmaking descriptions</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying individual facial expressions by deconstructing a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<editor>B. Rosenhahn, B. Andres</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bockmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wienert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hellweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stenzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Parlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Budczies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goeppert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kotani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dietel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://github.com/cvdfoundation/google-landmark#release-history" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<editor>Foundation, C.V.D.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Google landmarks v2 dataset</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07047</idno>
		<title level="m">High-resolution breast cancer screening with multi-view deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reshaping inputs for convolutional neural network: Some common and uncommon methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasipuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="79" to="94" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-256 object category dataset</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-scale cnn and curriculum learning strategy for mammogram classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Paper conservation catalog. The American Institute for Conservation of Historic and Artistic Works Book and Paper Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Maynor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reyden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Ninth Edition.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Layer-Wise Relevance Propagation: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-610</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="193" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<title level="m">Nondiscriminative data or weak model? on the relative importance of data and model resolution</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01228-7</idno>
		<idno>10. 1007/s11263-019-01228-7</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-019-01228-7" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Handling variable shaped &amp; high resolution images for multi-class classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Sotiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interpretable deep neural networks for single-trial EEG classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<idno>abs/1604.08201</idno>
		<ptr target="http://arxiv.org/abs/1604.08201" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Axiomatic attribution for deep networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Interpretable lstms for whole-brain neuroimaging analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Heekeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.09945" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLITS, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ip102: A large-scale benchmark dataset for insect pest recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8787" to="8796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Visualizing and understanding convolutional networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
