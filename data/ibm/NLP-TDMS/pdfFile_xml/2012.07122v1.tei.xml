<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>Qi</surname></persName>
						</author>
						<title level="a" type="main">DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly detection</term>
					<term>anomaly segmentation</term>
					<term>re- gional representation</term>
					<term>feature reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic detecting anomalous regions in images of objects or textures without priors of the anomalies is challenging, especially when the anomalies appear in very small areas of the images, making difficult-to-detect visual variations, such as defects on manufacturing products. This paper proposes an effective unsupervised anomaly segmentation approach that can detect and segments out the anomalies in small and confined regions of images. Concretely, we develop a multi-scale regional feature generator which can generate multiple spatial context-aware representations from pre-trained deep convolutional networks for every subregion of an image. The regional representations not only describe the local characteristics of corresponding regions but also encode their multiple spatial context information, making them discriminative and very beneficial for anomaly detection. Leveraging these descriptive regional features, we then design a deep yet efficient convolutional autoencoder and detect anomalous regions within images via fast feature reconstruction. Our method is simple yet effective and efficient. It advances the state-of-the-art performances on several benchmark datasets and shows great potential for real applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U SUPERVISED anomaly segmentation aims at precisely detecting and localizing anomalous regions within images solely via prior knowledge from the anomaly-free images. This task is significant especially in smart manufacturing processes for ensuring qualified products, such as automatically inspecting and screening defective or flawed products. In these inspection scenarios, it is usually preferable to train machine learning models only with normal images of the products alone to detect the anomalies. Since industrial processes are generally optimized to produce least unqualified samples, it might be impossible to collect a sufficient amount or even a few of defective samples. More importantly, because all sorts of anomalies or defects would possibly occur during manufacturing, a detection model solely trained on limited anomaly samples may fail to generalize on those previously unseen ones.</p><p>In recent literature, many effective unsupervised anomaly and novelty detection algorithms for images have been proposed <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, whereas most of these methods aim to imagelevel classification where the anomalous samples often differ significantly from the normal data either in semantic or visual. For instance, if we take images of cats as normal, then all the other images differ visually will be detected as anomalies, such as images of dogs. As recently Bergmann et al. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> suggested, fewer efforts have been done to improve algorithms for detecting anomalies that deviate subtly from the normal  <ref type="figure">Fig. 1</ref>. Qualitative results of our anomaly detection method with increasing feature scales on the MVTec AD dataset. Input: input image. AM {1: }: anomaly map of our approach where representation scales from 1 to are leveraged. Note that in AM red regions correspond to high score for anomaly.</p><p>data. Defects within images, as <ref type="figure">Fig.1</ref> shows, typically belong to this sort of anomaly. They often appear in very small and confined local regions of images and result in subtle visual deviations from the whole. In this scenario, an image-level anomaly detection algorithm may not be competent, especially when expecting to segment out the anomaly.</p><p>One strategy to address the problem above is to exploit convolutional autoencoders <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Generally, it first trains an autoencoder on anomaly-free images and then can perform pixel-precise anomaly detection during inference by comparing the pixel-wise differences between the input and its reconstructed version via some distance metric, e.g. 2distance <ref type="bibr" target="#b10">[11]</ref> or structural similarity metric (SSIM) <ref type="bibr" target="#b11">[12]</ref>. Approaches based on this strategy assume that autoencoders solely trained on normal images are unable to reproduce the anomalous image subregions that deviate far from the normal ones. Thus, anomalies can be indicated by large reconstruction errors. Deep generative models based on variational autoencoder (VAE) <ref type="bibr" target="#b12">[13]</ref> and generative adversarial nets (GAN) <ref type="bibr" target="#b13">[14]</ref> could also used in a similar way <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Differently, generative methods can further leverage the reconstruction probability or likelihood score as an additional anomaly measurement <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. These methods depended on image regeneration usually struggle to reproduce the sharp edges and complex textural structures. As a consequence, they always get high reconstruction errors in those edge or texture areas of images and incur many false anomaly alarms.</p><p>There are also detection approaches that leverage discriminative image features. They model the normality as well as inference the anomaly in the feature space. Typically, this category of method firstly divides an image into many partially overlapping regions or patches and construct corresponding region-level representations with either handcraft features <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> or learned embeddings with convolutional neural networks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Then, with the obtained regional features of the normal images, many machine learning methods can be used to model the distribution of normal feature patterns, such as gaussian mixture models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, sparse coding <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> and kmeans clustering <ref type="bibr" target="#b24">[25]</ref>. During inference, any regional feature pattern that deviates from the modeled distribution will be classified as an anomaly. Note that these feature-based methods accomplish anomaly detection and location simultaneously because the detection is done over every subregion of images. Since having to extract features for every local region of images, feature-based approaches are not very efficient especially when feature embeddings with deep neural networks are required <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Besides, the spatial size of the region used for producing regional features affects the performance of feature-based models to a large extent. If a smaller region size is selected, the extracted regional features may fail to capture large spatial structures within the image and also be sensitive to local changes of the image content, thus tending to either miss or wrongly report anomalies. In turn, if a larger size is used, the features may predominantly describe anomaly-free characteristics and ignore the traits of small anomalous regions at all. A general practice to tackle the problem is by multi-scale modeling <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Multiple scale can be realized either by tweaking the scales of the input image <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref> or controlling the sizes of the receptive field of convolutional neural networks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. At each scale, an anomaly detection model is trained and tested separately. To obtain the final multi-scale detection result, one has to combine all the detection results from different scales together via some rule, such as weighted averaging. Indeed, the multi-scale strategy usually contributes to improved performance. But it also costs much more time at both training and testing stages.</p><p>In this work, we also propose to leverage the idea of multiscale modeling. However, we propose to make full use of the pre-trained deep convolutional neural networks (CNNs). On the one hand, deep CNNs pre-trained on large datasets, such as ImageNet <ref type="bibr" target="#b25">[26]</ref>, can produce very discriminative features that have successfully transferred to many supervised vision tasks, such as edge detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, as well as unsupervised anomaly detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>. On the other hand, with the deep hierarchical convolution architecture, the feature representations learned by different convolutional layers are inherently multi-scale <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Each feature map, i.e. the output of each convolutional layer, is derived from a specific receptive field, and each location on the feature map perceives a corresponding spatial region of the input image. Therefore, each feature map in fact forms a dense regional representation for the whole image <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, where each feature on the map represents a corresponding local region within the image, and the spatial size or scale of this region corresponds to the size of the specific receptive field. Therefore, if, in a way, we fuse these hierarchical CNN feature maps, then a multi-scale dense regional feature representation of an image will be obtained by nature.</p><p>Based on the observations above, we specifically develop a regional feature generator that can align and aggregate the output feature maps of different convolution layers from a pre-trained deep CNN and produce multi-scale discriminative representations for every subregion of the input image via only one forward pass through the deep network. Since these regional features are very descriptive and can be generated efficiently, they are particularly beneficial for the task of unsupervised anomaly detection. Besides, to leverage these dense regional features for effective and fast detection, we specifically design a deep yet efficient convolutional autoencoder (CAE) and detect possible anomalous regions within images through fast compressing and regenerating the dense regional representation. We term our anomaly detection method as Deep Feature Reconstruction (DFR), where we realize unsupervised anomaly detection and localization by reproducing the dense regional features generated from deep pre-trained CNNs with a deep CAE. Extensive experiments have been carried out to demonstrate that our DFR is both effective and computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In the literature, the approaches specifically developed for unsupervised anomaly segmentation can be roughly divided into two categories: reconstruction-based and feature-based methods.</p><p>A typical group of reconstruction-based methods is based on convolutional autoencoders <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These models solely train on normal images and then detect anomalies within an image by computing the pixel-wise distances between the image and its reconstruction such as 2 -distance <ref type="bibr" target="#b10">[11]</ref> and structural similarity metric (SSIM) <ref type="bibr" target="#b11">[12]</ref>. They assume that autoencoders trained on normal data are unable to reproduce the anomalous ones. Deep generative models based on variational autoencoder (VAE) <ref type="bibr" target="#b12">[13]</ref> and generative adversarial nets (GAN) <ref type="bibr" target="#b13">[14]</ref> can also be used in similar ways. Baur et al. <ref type="bibr" target="#b14">[15]</ref> utilize VAEGAN to detect anomalies or lesions in 2D Brain MR Images where the GAN component is used for adversarial training to enhance the reconstruction quality. During testing, they only use per-pixel 1 -distance to scoring the anomaly. Schlegl et al. <ref type="bibr" target="#b15">[16]</ref> implement similar ideas for detecting anomalies in optical coherence tomography images but using a convolutional autoencoder instead. Except the ordinary distance metrics, detection methods based on deep generative models can further leverage the reconstruction probability <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref> and likelihood score <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> as the additional anomaly measurements. Besides, instead of comparing the differences between the input test image with its reconstruction, some methods propose to compute the residuals between the test image with its nearest normal counterpart. Schlegl et al. <ref type="bibr" target="#b32">[33]</ref> propose AnoGAN where they train a GAN only on the normal images, and then detect anomalies by comparing differences between the test image and its nearest normal counterpart generated by the GAN. Specifically, they have to firstly search the nearest latent code of the test image in the GAN's latent space through an optimization process. With the obtained code, only then can they generate the expected nearest normal image for comparison. David et al. <ref type="bibr" target="#b33">[34]</ref> also propose to detect the anomaly by comparing the differences between the test image and its nearest normal version. They train a VAE on the normal data then find the nearest normal counterpart for the test image by iterative updating the input of the VAE via gradient descent of a reconstruction loss defined on the test image and the VAE's output. Both of the two methods need a searching step, thus they are not very efficient in the practice. Since reconstruction-based methods detect anomalies within images in pixel or image space, they are usually required to produce high qualified images for comparison. However, the problem itself, i.e. high qualified image generation, is still challenging.</p><p>Unlike reconstruction-based models that detect anomalies in image space, feature-based methods detect anomalies in feature space. These approaches devote to construct descriptive representations for every local patch or region of the images with either handcraft features <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> or embeddings produced by neural networks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Then relevant machine learning models, such as sparse coding <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, gaussian mixture models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and kmeans clustering <ref type="bibr" target="#b24">[25]</ref>, are used to learn the distribution of the normal regional features. During inference, if a regional feature corresponding to a local region of the test image deviates from the learned distribution, then an anomalous region is detected. To further enhance the detection performance, multi-scale models are usually adopted <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, where they will combine multiple models derived from different image region sizes together.</p><p>Recently, Bergmann et al. <ref type="bibr" target="#b6">[7]</ref> developed a comprehensive benchmark dataset for unsupervised anomaly segmentation, which consists of various texture and object categories with over 70 different types of anomalies. They evaluated many state-of-the-art reconstruction-based and feature-based methods on this dataset, and found that none of these approaches work consistently well and a considerable improvement room exists. More recently, Bergmann et al. <ref type="bibr" target="#b7">[8]</ref> have proposed a novel unsupervised anomaly segmentation approach based on the student-teacher framework and achieved much better results than previous methods on the MVTec AD dataset.</p><p>They leverage the transferred deep CNN features and detected anomalies in images via feature regression. Specifically, they train a knowledgeable teacher network on a large dataset with the guidance of pre-trained deep CNNs (e.g. resnet18 <ref type="bibr" target="#b34">[35]</ref>), and a group of student networks that imitates the teacher's behaviors solely on the anomaly-free data. During testing, the students are utilized to predict the teacher's output, and anomaly scores are computed based on the corresponding predicting errors and uncertainties. The assumption lies that the student only trained to regress the teacher's output on the normal image patches well will probably predict poorly or fail to follow the teacher on the anomalous ones. Besides, the authors also suggest using multi-scale models to enhance the final detection performance, i.e. an ensemble of multiple student-teacher pairs with different image patch sizes or various receptive fields. In our work, we also propose to leverage the transferred deep CNN features and especially the multi-scale modeling. However, we suggest to build a multiscale feature representation instead of the model ensemble and detect anomalies via feature reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The pipeline of our approach for unsupervised anomaly segmentation is outlined in <ref type="figure" target="#fig_1">Fig.2</ref>. It has four stages, i.e. hierarchical image feature extraction, multi-scale regional feature generation, deep feature reconstruction, and scoring and segmentation. Given an input image, firstly, the discriminative hierarchical image features (feature maps) are extracted via a pre-trained deep CNN. Then, a regional feature generator takes the hierarchical feature maps as input and transforms them into a single feature map of a relatively large volume, which in essence establishes a dense multi-scale regional representation for the whole input image (Detailed explanations are in subsection ). Followed, a deep CAE convolves over the multi-scale representation and attempts to reproduce it again. Finally, to detect and segment the anomalous regions within the image, the reconstruction error and anomaly score map are calculated. The anomalies are segmented out if any scores on the anomaly map are larger than an estimated or user-defined threshold. We detail our pipeline in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchical Image Feature Generation</head><p>We use a pre-trained CNN to generate rich hierarchical discriminative features for the input image and then feed them into the multi-scale regional feature generator.</p><p>Suppose there is a convolutional neural network with convolutional layers, typically each of which implements a composition of functions such as Convolution, Batch Normalization (BN) <ref type="bibr" target="#b35">[36]</ref> and Rectified Linear Units (ReLU) <ref type="bibr" target="#b36">[37]</ref>. Let with height ℎ, width and channel be an image. Passing it through the network, we can obtain a set of output feature maps { 1 ( ), 2 ( ), ..., ( )} from the convolutional layers, where the th feature map is of size ℎ × × .</p><p>Since each feature map is derived from a network layer in a specific depth with a specific receptive field (which can perceive a corresponding spatial region of the image), it comprises a certain level of representation or abstraction  for the input image <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>. The shallow convolutional layers with relatively small receptive fields capture the lowlevel characteristics such as the textural structures within the image. With the layers going deeper and their receptive fields becoming larger, the corresponding output feature maps encode more global or higher-level information such as an object or object parts in the input image. Therefore, an ensemble of the feature maps { ( )} =1 naturally forms a rich hierarchical representation of the input image from the local details to the global semantic information. As an example, we detail the numbered convolutional layers and corresponding receptive field (RF) sizes of the VGG19 <ref type="bibr" target="#b38">[39]</ref> in TABLE I. The VGG19 net consists of 16 convolutional layers, and its receptive field size grows gradually from 3 to 252 as the layer becomes deeper. Thus, here, the VGG19 can produce 16 different levels of representations for an input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale Regional Feature Generation</head><p>With the hierarchical CNN feature maps as input, we design a regional feature generator which can generate discriminative multi-scale representations for every subregion of the image. The overall scheme is shown in <ref type="figure">Fig. 3</ref>.</p><p>We firstly align the CNN feature maps { ( )} =1 that derived from different receptive fields by resizing all of them to the spatial size of the input image (ℎ × ) but with channels retrained:ˆ(</p><formula xml:id="formula_0">) = ( ( ))<label>(1)</label></formula><p>where the aligned th feature mapˆ( ) has a size of ℎ× × . Then a convolution operation is followed, where a mean filter is used to spatially convolve over the every aligned feature map with an appropriate stride. This is the aggregation operation:</p><formula xml:id="formula_1">( ) = (ˆ( ))<label>(2)</label></formula><p>where the size of the th aggregated feature map is ℎ × × .</p><p>The aggregation operation has two functions: first, it smooths the feature variations on the feature maps making the generated feature more robust to noisy input; second, it provides a way to control the spatial size of the aggregated feature representation such as by varying the convolution stride.</p><p>Finally, we concatenate all the aggregated feature maps to a single feature map with the size of ℎ × × :</p><formula xml:id="formula_2">{1: } ( ) = (¯1( ),¯2 ( ), ...,¯( ))<label>(3)</label></formula><p>where {1: } ( ) denotes the resulted feature map combined from the 1 to th aggregated feature maps, and its depth or number of channels is such that = =1 . For convenience, in some places, we also take ( ) instead of {1: } ( ) for short in the rest of the paper.</p><p>Obviously, the obtained final feature map is in fact a fusion of a series of transformed hierarchical CNN feature maps. If we take a closer look, the fused feature map, in essence, forms a dense multi-scale regional description for the input image. As <ref type="figure">Fig.3</ref> illustrates, each branch CNN feature map is derived from a convolutional layer with a specific receptive field. Each feature on a certain branch feature map describes an image subregion of a specific spatial size that equals the corresponding receptive field (Note that, in <ref type="figure">Fig.3</ref>, we have only visualized what the feature on the center location can perceive from the image). When transforming all the hierarchical or branch CNN feature maps with operations such as alignment, aggregation, and concatenation, into a single feature map of a large volume, we naturally get a dense multi-scale representation for every local region of the image. It is dense and multi-scale because every multi-scale feature , ( ) with a dimension of on the obtained feature map ( ) comprises a multi-scale description for a corresponding subregion on the image, where ( , ) denotes a spatial location on this feature map. In particular, each feature corresponds a non-overlapping region of the image with a spatial size of ℎ/ℎ × / . For instance, if the image is of size 256 × 256 and the feature map is of spatial size 64 × 64, then a feature on the map represents a 4×4 pixel region of the image. If we make the final feature representation ( ) the same spatial size as the image, i.e. ℎ = ℎ and = , then a pixel-wise representation will be obtained. Multi-scale Regional Representation … A multi-scale regional feature at the center location <ref type="figure">Fig. 3</ref>. An illustration of the proposed multi-scale regional feature generator. <ref type="figure">Figure best</ref> viewed in color.</p><formula xml:id="formula_3">… ℎ × × 1 ℎ × × 2 ℎ × × 1( ) 2( ) ( ) 1( ) 2( ) ( ) 1( ) 2( ) ( ) ( , ) (ℎ × × ) , ( ) ( )</formula><p>Note that though we use a multi-scale regional feature to represent a corresponding local region of the image, we usually derive the feature at each scale from a larger region or receptive filed. Such a multi-scale regional feature not only describes the local characteristics of the subregion itself but also encodes its multiple spatial context information or global characteristics, making it discriminative and very beneficial for anomaly detection.</p><p>In addition, we define that the scales of our regional representation correspond to the hierarchical layers of the CNN, and the spatial size of a specific scale equals the receptive field size of the corresponding convolutional layer. For example, if using all the hierarchical convolutional layers of the VGG19, we will finally get a multi-scale regional representation with 16 different scales. And the size of each scale that corresponds to the specific receptive field of each convolutional layer can be found in TABLE I. Moreover, one can also flexibly select different combinations of feature scales to meet the application requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Feature Reconstruction</head><p>The multi-scale regional features are discriminative. However, the feature dimension, i.e. , is usually very large. To leverage such high-dimension regional representations for effective and fast anomaly detection, we design an efficient convolutional autoencoder which only includes operations of 1 × 1 convolution and ReLU activation. Specifically, we use the CAE to convolve over the dense multi-scale regional representation ( ) and compress it into a low-dimension latent space, then manage to reproduce the representation again. The input representation ( ) and it reconstructionˆ( ) will be use to score and segment the anomaly at the next stage of our pipeline.</p><p>We train the CAE solely on the regional representations of normal images with a reconstruction loss measured by the averaged pair-wise 2 -distance between the reproduced dense regional representationˆ( ) and its ground truth ( ):</p><formula xml:id="formula_4">L = ℎ ∑︁ =1 ∑︁ =1 || , ( ) −ˆ, ( )|| 2<label>(4)</label></formula><p>Note that both ( ) andˆ( ) are in fact feature maps with the same size of ℎ × × , and each regional feature , ( ) with dimension on the regional feature map ( ) corresponds a local region of the input image. The detailed architecture of our CAE is in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Anomaly Scoring and Segmentation</head><p>At the last stage of our pipeline, we detect all the possible anomalous regions within the input image base on the reproduced regional feature mapˆ( ) and its ground truth ( ). We first inference the anomaly score map by comparison of the ground truth representation ( ) and its reconstructionˆ( ) and then binarize the anomaly map with a certain threshold to segment the anomaly.</p><p>We define the anomaly score map or anomaly map as the pair-wise reconstruction error between the input regional feature map ( ) and its reconstructionˆ( ):</p><formula xml:id="formula_5">, ( ) = || , ( ) −ˆ, ( )|| 2<label>(5)</label></formula><p>where , ( ) is the anomaly score of the regional feature , ( ), and ( , ) denotes the spatial location where the regional feature , ( ) lies on the regional feature map ( ) of the input image . Correspondingly, ( ) is the regional anomaly map of the image with the same spatial size as ( ), i.e. ℎ × . To obtain a pixel-wise anomaly mapˆ( ) for the image, we further bilinearly upsample the regional anomaly map to the same spatial size of the image.</p><p>We assume that the CAE solely trained on the regional features of normal images are unable to reproduce the regional features correspond to anomalous image regions. Therefore, anomalous regions coincide with the large reconstruction errors of the corresponding regional features or the high scores on the anomaly map.</p><p>To get the final segmentation result, we binarize the anomaly mapˆ( ) with a threshold . Specifically, we use the acceptable false positive rate (FPR) on the normal data to estimate the segmentation threshold. For instance, if the acceptable FPR is expected to be zero, then it means that the threshold should be such that no pixels in the normal images are wrongly classified as anomalies. If the FPR is 0.005, then the segmentation threshold should meet that just 0.5 percent of pixels in the normal images are incorrectly detected as anomalous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first present the experimental comparisons with state-of-the-art unsupervised anomaly segmentation methods and then conduct a thorough analysis of our approach.</p><p>A. Experimental Setup 1) Datasets: We evaluate the proposed approach on the challenging MVTec Anomaly Detection (MVTec AD) dataset <ref type="bibr" target="#b6">[7]</ref>, which is specifically developed to benchmark unsupervised algorithms for anomaly segmentation. It includes a collection of 15 sub-datasets (10 for objects and 5 for textures) and contains a total of 5354 high-resolution images with over 70 types of anomalies such as scratches, cracks, stains, and various structural damages. All the datasets are divided into training and testing sets, where the training sets are only consist of normal images while the testing sets contains both normal and anomalous samples. Detailed statistics of the MVTec AD dataset is in Appendix .</p><p>2) Baselines: We compare our approach against the following state-of-the-art unsupervised anomaly segmentation methods:</p><p>• AE-2 and AE- <ref type="bibr" target="#b8">[9]</ref>: approaches based on CAEs, which detect anomalies by pixel-wise comparisons between the input image and its reconstruction via 2distance <ref type="bibr" target="#b10">[11]</ref> and SSIM <ref type="bibr" target="#b11">[12]</ref>  3) Architecture Details: We take the VGG19 <ref type="bibr" target="#b38">[39]</ref> pretrained on ImageNet <ref type="bibr" target="#b25">[26]</ref> to produce hierarchical image features. In particular, we strip the last 3 dense layers and only retain the front 16 convolutional layers. We get CNN feature maps from the ReLU outputs of the convolutional layers and number the feature maps with the order of the corresponding layer in the network. The ordered layer and corresponding receptive field are listed in TABLE I. Since we have defined that the scales and scale sizes of our regional features correspond to the hierarchical convolutional layers and their receptive fields respectively, we will get at most 16 different scales of regional features with the trimmed VGG19. For our regional feature generator, we align the hierarchical CNN feature maps by nearest-neighbor interpolation and use a mean filter with spatial size of 4 × 4 and stride of 4 to aggregate the aligned hierarchical CNN feature maps. As a result, we obtain such a regional feature representation where each location on the regional feature map corresponds to a 4 × 4 pixel region of the input image. As for the CAE, we first randomly sample a subset of regional features from the regional feature map, then estimate the latent code dimension with Principal Component Analysis (PCA) such that 90% variance is just explained. The concrete parameters of our CAE depends on the dataset and the number of CNN feature maps that used. The CAE architecture is detailed in Appendix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Training Details:</head><p>For all experiments, the images are resized to the size of 256 × 256 pixels and their channels are triplicated if gray images are encountered. For all the datasets, we train our model solely on the anomaly-free training sets using the Adam optimizer with a learning rate of 1 × 10 −4 and a batch size of 4 for 700 epochs. During training, we freeze the weights of the pretrained VGG19 and the regional feature generator, and only update the weights of the CAE. We implements our method in Pytorch with a NVIDIA GeForce GTX 1080 Ti. The code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Evaluation Metrics:</head><p>We take the area under the receiver operating characteristic curve (ROC-AUC) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref> and the area under the per-region-overlap curve (PRO-AUC) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> as our evaluation metrics. The ROC-AUC assesses the best potential segmentation result in terms of normal and anomalous pixels, i.e. per pixel overlapping performance. The PRO-AUC metric is suggested in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. It attempts to measure the best possible segmentation performance across normal and anomalous regions at the region level, i.e. per region overlapping performance. Specifically, the PRO-AUC weights all the ground-truth anomalous regions equally so that the segmentation performance is measured with no bias to either large or small ground-truth regions. In other words, it measures a model's ability to segment out all the possible anomalous regions equally no matter what the size of a particular abnormal region is. Simple per-pixel segmentation metrics such as PRO-AUC may fail to measure this property since a large enough correctly segmented region can compensate for many wrongly segmented minor ones. As <ref type="bibr" target="#b7">[8]</ref> suggests, we report the normalized PRO-AUC up to an average per-pixel false positive rate (FPR) of 30%, where the average FPR is the percentage of pixels within all testing images that are incorrectly detected as anomalies. We calculate the PRO-AUC metric as in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons Against Baselines</head><p>We evaluate the segmentation performances of our approach and baselines on the testing sets across 7 object and 5 texture data categories. For comparison, the ROC-AUC and PRO-AUC metrics of our method are calculated under two settings: 1) all the 16 scales of the regional representation are used, i.e. {1:16} ; 2) only the front of 12 scales are taken, i.e. {1:12} . Besides, we take the ROC-AUC results of baselines from <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b33">[34]</ref>, and PRO-AUC results from <ref type="bibr" target="#b7">[8]</ref> respectively. TABLE II shows the ROC-AUC results. Our methods outperform the baselines on most of the data categories, except Tile and Transistor. On average, ours improve the ROC-AUC performances of the baseline methods by a very large margin. TABLE III is the PRO-AUC results. Comparing with AE-, AE-2 and CNN-FD, our approaches achieve overwhelming results across all data categories. Besides, our method shows similar or better results on many data categories when compared with ST models, i.e. ST-m, ST-p65. Averaging the metrics over all categories, ours work on par with ST-m.</p><p>However, the ST model needs to train several different networks simultaneously, i.e. a teacher network and an ensemble of student networks. During inference, the test image has to be separately passed through the students and teacher networks to generate corresponding dense embeddings and compute the anomaly scores. To make a multi-scale model, ST has to independently train multiple such pairs of studentteacher networks and then combine the separately detected results together during testing. While our approach is more direct yet effective, where a multi-scale anomaly detection can be realized with only one forward pass through our pipeline network. That is our method in fact is inherently multi-scale. Besides, we can also flexibly combine different scales to meet specific applications. The only part of our pipeline needing to train is the CAE.</p><p>It is also interesting to note that our methods, ST-m and ST-p65 work much better than the reconstruction-based models such as AE-and AnoGAN. This is likely because that both ST models and ours leverage the transferred discriminative CNN features. This also indicates that, for anomaly detection, approaches which leverage transferred discriminative features may show more potential than the methods which only learn representations from scratch such as models based on autoencoders and GANs. Similar findings are presented in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b39">[40]</ref>. Besides, CNN-FD also use the transferred deep features, but it shows inferior performance on most data categories. Since CNN-FD adopts a shallow algorithm, i.e. kmeans clustering, for anomaly detection, the shallow model is not capable to make full use of the rich CNN features due to its limited model capacity.</p><p>In addition, multi-scale modeling contributes to improved performance. As the results in TABLE III show, the multiscale model ST-m works better than ST-p65 on average. And our approach achieves better average performance either on ROC-AUC or PRO-AUC metric when all the 16 scales of the regional features are used. These results indicate that each feature scale may conveys some useful information for anomaly detection. Thus, if there is no prior knowledge of the anomalies, a multi-scale model is usually a wise choice.</p><p>We have also visualized some qualitative segmentation results over all the data categories in <ref type="figure" target="#fig_2">Fig. 4</ref>. Specifically, the figure shows the obtained anomaly maps and segmentation maps of our approach when the front 12 scales of the regional representation, i.e. {1:12} , are used. For visualization, the anomaly maps are respectively normalized to the range [0, 1] and superimposed on the corresponding testing images.</p><p>C. Analysis 1) Effectiveness of the multi-scale representation: In the above experiments, to implement our approach, we respectively use two different multi-scale regional representations, i.e. {1:12} and {1:16} . Both of them contains multiple feature scales, i.e. 12 and 16 respectively. However, is it reasonable to exploit so many scales? To answer this question, , which means that more and larger scales are gradually exploited to form these different multi-scale representations.</p><p>The results for object data categories are shown in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref>. In terms of ROC-AUC and PRO-AUC, all the object categories benefit from multi-scale representations. With more scales held by the multi-scale representation, the performance of our model becomes better. Similar phenomenon can be seen in <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> for most of the texture categories, except Wood. Though the ROC-AUC on Wood tends to increase when more scales are included in its multi-scale representation, the corresponding PRO-AUC decreases. This indicates that our model on Wood prefers to detect larger anomalous regions when more and larger scales are used. In addition, we can also observe that the metrics on textures saturate at {1:8} or {1:12} and even slightly degrade afterwards. This is because that relatively local statistics are usually enough to represent the textural structures.</p><p>Some qualitative results are also presented in <ref type="figure">Fig. 1</ref>. With more and larger scales taken into account, the corresponding anomaly maps tend to approach their ground truth counterparts progressively. And the false anomalous regions are gradually removed, while the truth anomalous regions are gradually refined. This is because, with more scales leveraged, the corresponding multi-scale regional representation will encode more spatial context information for every subregion of an image, thus making the detection more certain or confident.</p><p>Though we have demonstrated that our approach tends to perform better when leveraging more and larger scales, however, are the small scales still helpful? If not, we can drop them to build a more compact model. To identify this, further experimental scenarios are designed. Concretely, we start from a regional representation derived from a large scale, i.e. {12} , then gradually add the regional features from smaller scales to form a series of multi-scale representations, i.e. {9:12} , {5:12} , {1:12} .</p><p>The results are shown in TABLE IV. With more small scales considered, the average performances improve gradually but by a small margin. The results suggest that each scale of the regional representation conveys some different information that can advance the detection performance. In addition, the regional representations derived from relatively large scales may have contained much useful information for anomaly detection. As it can be seen from the TABLE IV, only with the regional representation {12} , our model can also obtain satisfactory results.</p><p>In general, we can draw the following conclusions about our approach: 1) multi-scale modeling, i.e using multi-scale regional representation, is always beneficial for anomaly detection. With more and larger scales leveraged, the detection performance always tends to become better; 2) for texture categories, it is enough to exploit less and smaller feature scales when compared with the object categories; 3) we can drop some smaller scales to establish more compact models if not pursuing high detection metrics.  <ref type="figure">Fig. 5</ref>. The ROC-AUC metrics of our approach on object categories with different multi-scale regional representations.</p><formula xml:id="formula_6">f {1:2} f {1:4} f {1:8} f {1:12} f {1:16}</formula><p>Multi-scale Regional Representation  <ref type="figure">Fig. 6</ref>. The PRO-AUC metrics of our approach on object categories with different multi-scale regional representations.  <ref type="figure">Fig. 7</ref>. The ROC-AUC metrics of our approach on texture categories with different multi-scale regional representations.</p><formula xml:id="formula_7">f {1:2} f {1:4} f {1:8} f {1:12} f {1:16}</formula><p>Multi-scale Regional Representation  <ref type="figure">Fig. 8</ref>. The PRO-AUC metrics of our approach on texture categories with different multi-scale regional representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Boundary Effects:</head><p>We observe that our approach tends to wrongly report anomalies near the image boundary areas when the foreground (the target we are interested in) fills the whole image, such as the texture categories. One cause may be the zero-padding operation used in the pre-trained VGG19. This operation will inject novel information that is out of the image into the corresponding CNN feature maps, especially the boundary regions. Since these features in boundary areas are not statistically significant compared with the features in relatively center areas, our CAE may not model these feature patterns well. One possible solution is to adopt the reflection padding that only uses information from the image itself. With reflection padding instead, we train and evaluate our model on all the data categories. As <ref type="figure">Fig. 9</ref> shows, the boundary effects are relieved with the reflection padding strategy. In addition, as TABLE V presents, the averaged performances on both object and texture categories are improved by about 1 percent when the reflection padding is used.</p><p>3) Inference Speed: We evaluate the inference speed of our method under many different multi-scale settings. Since the model architecture depends on the specific dataset, we average the inference time across all the object and texture categories on corresponding testing sets. TABLE VI shows the statistics of the inference speed when using different multiscale regional representations. We also list the corresponding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero Padding</head><p>Reflection Padding Input GT <ref type="figure">Fig. 9</ref>. Examples of boundary effects. Input: input anomalous image. GT: ground truth anomalous regions (in red). The last two columns are respectively the resulted anomaly maps when using zero padding and refection padding. Note that red regions correspond to high score for anomaly. <ref type="figure">Figure best</ref> viewed in color. average performances where reflection padding strategy is used. In general, our method can reach over about 100 frames per second (fps) even when 12 different feature scales are used, which indicates our approach is applicable in practice. Besides, leveraging fewer feature scales, we can further obtain more compact and efficient models only with small degradations on performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we have primarily presented a general unsupervised approach, i.e. DFR, to detect anomalous regions within images. We propose to make use of the transferred hierarchical CNN features to build dense discriminative multiscale feature representations for every local region of the images via a specially designed regional feature generator. We also propose to detect possible anomalous regions in images through deep feature reconstruction, i.e. reconstructing the multi-scale regional features via a deep yet efficient CAE. Extensive experiments and analysis on various data categories of objects and textures have demonstrated that our method is effective and achieves state-of-the-art results. In future work, we plan to further optimize our approach for more compact and efficient implementations.   <ref type="table" target="#tab_0">Textures   Carpet  280  117  5  97  Grid  264  78  5  170  Leather  245  124  5  99  Tile  230  117  5  86  Wood  247  79  5  168   Objects   Bottle  209  83  3  68  Cable  224  150  8  151  Capsule  219  132  5  114  Hazelnut  391  110  4  136  Meta Nut  220  115  4  132  Pill  267  167  7  245  Screw  320  160  5  135  Toothbrush  60  42  1  66  Transistor  213  100  4  44  Zipper  240  151  7  177  Total  3629  1725  73  1888</ref> APPENDIX A MVTEC AD DATASET The detailed statistics of the MVTec AD dataset is given in VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ARCHITECTURE OF OUR CAE</head><p>The architecture of our deep convolutional autoencoder for compressing and reproducing the multi-scale regional representation {1: } (x) is designed as in TABLE VIII. It consists of 6 convolutional layers and only contains 1 × 1 convolutions and ReLU activations. For the latent feature dimension , we randomly sample a subset of regional features from the regional feature map and estimate the latent dimension with Principal Component Analysis (PCA) such that 90% variance is just retained. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overview of our unsupervised anomaly segmentation pipeline which consists four stages: hierarchical image feature generation, multi-scale regional feature generation, deep feature reconstruction, and anomaly scoring and segmentation.Figure Bestviewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of our unsupervised anomaly segmentation approach. Input: input anomalous image. GT: ground truth anomalous regions (in red). AM: anomaly map (red regions correspond to high score for anomaly). SM: segmentation map. Note that the segmentation maps are visualized when acceptable FPRs of 0 and 0.005 on corresponding training data are given. Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc></figDesc><table><row><cell cols="9">NUMBERED CONVOLUTIONAL LAYER AND CORRESPONDING</cell></row><row><cell></cell><cell></cell><cell cols="5">RECEPTIVE FIELD SIZE OF VGG19</cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>RF size</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>14</cell><cell>24</cell><cell>32</cell><cell>40</cell><cell>48</cell></row><row><cell>Layer</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row><row><cell>RF size</cell><cell cols="3">68 84 100</cell><cell>116</cell><cell>156</cell><cell cols="2">188 220</cell><cell>252</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>VAE solely on normal data, then attempts to find the nearest normal image for the test image by iterative updating the VAE's input via minimizing a reconstruction loss defined on the test image and the VAE's output, finally detects the anomaly by comparing differences between the test image and its nearest normal version, i.e. the VAE's input that obtained at the final iteration.</figDesc><table><row><cell></cell><cell>respectively.</cell></row><row><cell cols="2">• AnoGAN [33]: a GAN based model, which first tries to</cell></row><row><cell cols="2">generates the nearest normal image for the test image with</cell></row><row><cell cols="2">the GAN generator trained only on the normal data and</cell></row><row><cell cols="2">then detects the anomaly by computing per-pixel residuals</cell></row><row><cell cols="2">between the test and its nearest normal counterpart.</cell></row><row><cell>• VAE-</cell><cell>[34]: a recently proposed reconstruction-based</cell></row><row><cell cols="2">method, which first trains a ST [8]: a recently proposed powerful anomaly segmenta-</cell></row><row><cell cols="2">tion approach, which leverages both deep CNN embed-</cell></row><row><cell cols="2">dings and multi-scale modeling, and detects anomalous</cell></row><row><cell cols="2">regions within images using a student-teacher framework.</cell></row><row><cell cols="2">Specifically, we will compare our method with two best</cell></row><row><cell cols="2">performing models in [8], i.e. the ST-m and ST-p65,</cell></row><row><cell cols="2">where ST-m is a multi-scale ST model and ST-p65</cell></row><row><cell cols="2">a single scale model established on image patches of</cell></row><row><cell cols="2">size 65 × 65.</cell></row></table><note>• CNN-FD [25]: a method which exploits deep CNN fea- tures and uses a shallow model, i.e. kmeans clustering, to learn the normality and inference the anomaly.•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISONS WITH BASELINES (ROC-AUC) {1:2} , {1:4} , {1:8} , {1:12} , {1:16} , are used respectively. Note that, from {1:2} to {1:16} , the number of different feature scales held by the corresponding multiscale representation gradually increases from 2 to 16 and the corresponding maximum scale size goes from 5 to 252 (One can refer to TABLE I for these scale sizes)</figDesc><table><row><cell></cell><cell>Category</cell><cell>AE</cell><cell>AE 2</cell><cell>Ano-GAN</cell><cell>VAE</cell><cell>CNN FD</cell><cell>Ours {1:12}</cell><cell>Ours {1:16}</cell></row><row><cell>Textures</cell><cell>Carpet Grid Leather Tile</cell><cell>0.87 0.94 0.78 0.59</cell><cell>0.59 0.90 0.75 0.51</cell><cell>0.54 0.58 0.64 0.50</cell><cell>0.74 0.96 0.93 0.65</cell><cell>0.72 0.59 0.87 0.93</cell><cell>0.96 0.98 0.99 0.86</cell><cell>0.97 0.98 0.98 0.87</cell></row><row><cell></cell><cell>Wood</cell><cell>0.73</cell><cell>0.73</cell><cell>0.62</cell><cell>0.84</cell><cell>0.91</cell><cell>0.94</cell><cell>0.93</cell></row><row><cell></cell><cell>Bottle</cell><cell>0.93</cell><cell>0.86</cell><cell>0.86</cell><cell>0.92</cell><cell>0.78</cell><cell>0.95</cell><cell>0.97</cell></row><row><cell></cell><cell>Cable</cell><cell>0.82</cell><cell>0.86</cell><cell>0.78</cell><cell>0.91</cell><cell>0.79</cell><cell>0.88</cell><cell>0.92</cell></row><row><cell></cell><cell>Capsule</cell><cell>0.94</cell><cell>0.88</cell><cell>0.84</cell><cell>0.92</cell><cell>0.84</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell>Objects</cell><cell>Hazelnut Meta Nut Pill Screw</cell><cell>0.97 0.89 0.91 0.96</cell><cell>0.95 0.86 0.85 0.96</cell><cell>0.87 0.76 0.87 0.80</cell><cell>0.98 0.91 0.93 0.95</cell><cell>0.72 0.82 0.68 0.87</cell><cell>0.98 0.90 0.96 0.99</cell><cell>0.99 0.93 0.97 0.99</cell></row><row><cell></cell><cell>Toothbrush</cell><cell>0.82</cell><cell>0.93</cell><cell>0.90</cell><cell>0.98</cell><cell>0.77</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell></cell><cell>Transistor</cell><cell>0.90</cell><cell>0.86</cell><cell>0.80</cell><cell>0.92</cell><cell>0.66</cell><cell>0.75</cell><cell>0.80</cell></row><row><cell></cell><cell>Zipper</cell><cell>0.88</cell><cell>0.77</cell><cell>0.78</cell><cell>0.87</cell><cell>0.76</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell></cell><cell>Mean</cell><cell>0.86</cell><cell>0.82</cell><cell>0.74</cell><cell>0.89</cell><cell>0.78</cell><cell>0.94</cell><cell>0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">QUANTITATIVE COMPARISONS WITH BASELINES (PRO-AUC).</cell></row><row><cell></cell><cell>Category</cell><cell>AE</cell><cell>Ano-GAN</cell><cell>CNN FD</cell><cell>ST 65</cell><cell>ST-m</cell><cell>Ours {1:12}</cell><cell>Ours {1:16}</cell></row><row><cell>Textures</cell><cell>Carpet Grid Leather Tile</cell><cell>0.65 0.85 0.56 0.18</cell><cell>0.20 0.23 0.38 0.18</cell><cell>0.47 0.18 0.64 0.80</cell><cell cols="2">0.70 0.88 0.82 0.95 0.82 0.95 0.91 0.95</cell><cell>0.93 0.93 0.97 0.79</cell><cell>0.93 0.93 0.97 0.79</cell></row><row><cell></cell><cell>Wood</cell><cell>0.61</cell><cell>0.39</cell><cell>0.62</cell><cell cols="2">0.73 0.91</cell><cell>0.93</cell><cell>0.91</cell></row><row><cell></cell><cell>Bottle</cell><cell>0.83</cell><cell>0.62</cell><cell>0.74</cell><cell>0.92</cell><cell>0.93</cell><cell>0.92</cell><cell>0.93</cell></row><row><cell></cell><cell>Cable</cell><cell>0.48</cell><cell>0.38</cell><cell>0.56</cell><cell>0.87</cell><cell>0.82</cell><cell>0.77</cell><cell>0.81</cell></row><row><cell></cell><cell>Capsule</cell><cell>0.86</cell><cell>0.31</cell><cell>0.31</cell><cell>0.92</cell><cell>0.97</cell><cell>0.96</cell><cell>0.97</cell></row><row><cell>Objects</cell><cell>Hazelnut Meta Nut Pill Screw</cell><cell>0.92 0.60 0.83 0.89</cell><cell>0.70 0.32 0.78 0.47</cell><cell>0.84 0.36 0.46 0.28</cell><cell cols="2">0.94 0.90 0.94 0.93 0.94 0.97 0.94 0.96</cell><cell>0.97 0.87 0.96 0.95</cell><cell>0.97 0.90 0.96 0.96</cell></row><row><cell></cell><cell cols="2">Toothbrush 0.78</cell><cell>0.75</cell><cell>0.15</cell><cell>0.86</cell><cell>0.93</cell><cell>0.93</cell><cell>0.93</cell></row><row><cell></cell><cell>Transistor</cell><cell>0.73</cell><cell>0.55</cell><cell>0.63</cell><cell cols="2">0.70 0.67</cell><cell>0.77</cell><cell>0.79</cell></row><row><cell></cell><cell>Zipper</cell><cell>0.67</cell><cell>0.47</cell><cell>0.70</cell><cell>0.99</cell><cell>0.95</cell><cell>0.89</cell><cell>0.90</cell></row><row><cell></cell><cell>Mean</cell><cell>0.69</cell><cell>0.45</cell><cell>0.52</cell><cell>0.86</cell><cell>0.91</cell><cell>0.90</cell><cell>0.91</cell></row><row><cell cols="9">we conduct thorough experiments on all the data categories</cell></row><row><cell cols="9">of objects and textures with different multi-scale regional</cell></row><row><cell cols="9">representations. Concretely, we evaluate our model in a series</cell></row><row><cell cols="9">of scenarios where different multi-scale regional represen-</cell></row><row><cell cols="2">tations, i.e.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV METRICS</head><label>IV</label><figDesc>WITH DIFFERENT MULTI-SCALE REGIONAL REPRESENTATIONS.</figDesc><table><row><cell></cell><cell></cell><cell>Textures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell>Carpet Grid</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROC-AUC</cell><cell>0.80 0.85 0.90</cell><cell>Leather Tile Wood</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>f {1:2} f {1:4} Multi-scale Regional Representation f {1:8} f {1:12}</cell><cell>f {1:16}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Category</cell><cell>Metric</cell><cell>{12}</cell><cell>{9:12}</cell><cell>{5:12}</cell><cell>{3:12}</cell><cell>{1:12}</cell></row><row><cell></cell><cell></cell><cell>Textures</cell><cell cols="2">ROC-AUC 0.922 PRO-AUC 0.877</cell><cell>0.938 0.897</cell><cell>0.940 0.903</cell><cell>0.941 0.906</cell><cell>0.945 0.909</cell></row><row><cell></cell><cell></cell><cell>Objects</cell><cell cols="2">ROC-AUC 0.916 PRO-AUC 0.868</cell><cell>0.930 0.889</cell><cell>0.929 0.892</cell><cell>0.930 0.892</cell><cell>0.933 0.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V METRICS</head><label>V</label><figDesc>WITH DIFFERENT PADDING MODES.</figDesc><table><row><cell>Category</cell><cell>Meric</cell><cell>Zero</cell><cell>Refection</cell></row><row><cell>Textures</cell><cell cols="2">ROC-AUC 0.945 PRO-AUC 0.909</cell><cell>0.955 0.920</cell></row><row><cell>Objects</cell><cell cols="2">ROC-AUC 0.933 PRO-AUC 0.898</cell><cell>0.941 0.899</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI INFERENCE</head><label>VI</label><figDesc>SPEED AND METRIC UNDER DIFFERENT MULTIPLE SCALE SETTINGS.</figDesc><table><row><cell></cell><cell>{12}</cell><cell>{9:12}</cell><cell>{5:12}</cell><cell>{3:12}</cell><cell>{1:12}</cell></row><row><cell>Speed (fps)</cell><cell>159</cell><cell>148</cell><cell>116</cell><cell>111</cell><cell>100</cell></row><row><cell>ROC-AUC</cell><cell>0.926</cell><cell>0.942</cell><cell>0.941</cell><cell>0.942</cell><cell>0.946</cell></row><row><cell>PRO-AUC</cell><cell>0.875</cell><cell>0.895</cell><cell>0.899</cell><cell>0.900</cell><cell>0.906</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII MVTEC</head><label>VII</label><figDesc>AD DATASET.</figDesc><table><row><cell>Category</cell><cell>Train</cell><cell>Test</cell><cell>Anomaly Types</cell><cell>Anomalous Regions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII THE</head><label>VIII</label><figDesc>ARCHITECTURE OF OUR DEEP CONVOLUTIONAL AUTOENCODER.</figDesc><table><row><cell>Input: {1: } (x) (ℎ ×</cell><cell>× )</cell></row><row><cell cols="2">[layer 1]: Conv. (1, 1, ( + )//2), stride=1; ReLU;</cell></row><row><cell cols="2">[layer 2]: Conv. (1, 1, 2 × ), stride=1; ReLU;</cell></row><row><cell cols="2">[layer 3]: Conv. (1, 1, ), stride=1;</cell></row><row><cell cols="2">[layer 4]: Conv. (1, 1, 2 × ), stride=1; ReLU;</cell></row><row><cell cols="2">[layer 5]: Conv. (1, 1, ( + )//2), stride=1; ReLU;</cell></row><row><cell cols="2">[layer 6]: Conv. (1, 1, ), stride=1;</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/YoungGod/DFR</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by grants from: National Natural Science Foundation of China (No.71932008, 91546201, and 71331005).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep transfer learning for multiple class novelty detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Space Autoregression for Novelty Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An unsupervised-learning-based approach for automated defect inspection on textured surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1266" to="1277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pougetabadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seebock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidterfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Avid: Adversarial visual irregularity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating prediction and reconstruction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texems: Texture exemplars for defect detection on random textured surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xianghua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1454" to="1464" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time texture error detection on textured surfaces with compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bottger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Defect detection in sem images of nanofibrous materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lanzarone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="551" to="561" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scale-invariant anomaly detection with multiscale group-sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3892" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting anomalous structures by convolutional sparse models</title>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="209" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Not all areas are equal: Transfer learning for semantic segmentation via hierarchical region selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4355" to="4364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deepanomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1992" to="2004" />
		</imprint>
	</monogr>
	<note>Computer Vision and Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seebock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidterfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference information processing</title>
		<meeting>the international conference information processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iterative energybased projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR, 07-09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, F. Bach and D. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning</title>
		<editor>Research, G. Gordon, D. Dunson, and M. Dudík</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics, ser. Machine Learning<address><addrLine>Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multiple-attributeperceived network for real-world texture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Where&apos;s wally now? deep generative and discriminative embeddings for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="507" />
		</imprint>
	</monogr>
	<note>PLACE PHOTO HERE Michael Shell Biography text here</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Jane Doe Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
