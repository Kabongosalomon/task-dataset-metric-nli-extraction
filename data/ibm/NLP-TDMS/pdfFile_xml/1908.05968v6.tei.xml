<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">N2D: (Not Too) Deep Clustering via Clustering the Local Manifold of an Autoencoded Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
							<email>ryan.mcconville@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical and Electronic Engineering, and Engineering Maths</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Santos-Rodríguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical and Electronic Engineering, and Engineering Maths</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Piechocki</surname></persName>
							<email>r.j.piechocki@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical and Electronic Engineering, and Engineering Maths</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Craddock</surname></persName>
							<email>ian.craddock@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical and Electronic Engineering, and Engineering Maths</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">N2D: (Not Too) Deep Clustering via Clustering the Local Manifold of an Autoencoded Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep clustering has increasingly been demonstrating superiority over conventional shallow clustering algorithms. Deep clustering algorithms usually combine representation learning with deep neural networks to achieve this performance, typically optimizing a clustering and non-clustering loss. In such cases, an autoencoder is typically connected with a clustering network, and the final clustering is jointly learned by both the autoencoder and clustering network. Instead, we propose to learn an autoencoded embedding and then search this further for the underlying manifold. For simplicity, we then cluster this with a shallow clustering algorithm, rather than a deeper network. We study a number of local and global manifold learning methods on both the raw data and autoencoded embedding, concluding that UMAP in our framework is able to find the best clusterable manifold of the embedding. This suggests that local manifold learning on an autoencoded embedding is effective for discovering higher quality clusters. We quantitatively show across a range of image and time-series datasets that our method has competitive performance against the latest deep clustering algorithms, including outperforming current state-of-the-art on several. We postulate that these results show a promising research direction for deep clustering. The code can be found at https://github.com/rymc/n2d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Deep clustering has increasingly been demonstrating superiority over conventional shallow clustering algorithms. Deep clustering algorithms usually combine representation learning with deep neural networks to achieve this performance, typically optimizing a clustering and non-clustering loss. In such cases, an autoencoder is typically connected with a clustering network, and the final clustering is jointly learned by both the autoencoder and clustering network. Instead, we propose to learn an autoencoded embedding and then search this further for the underlying manifold. For simplicity, we then cluster this with a shallow clustering algorithm, rather than a deeper network. We study a number of local and global manifold learning methods on both the raw data and autoencoded embedding, concluding that UMAP in our framework is able to find the best clusterable manifold of the embedding. This suggests that local manifold learning on an autoencoded embedding is effective for discovering higher quality clusters. We quantitatively show across a range of image and time-series datasets that our method has competitive performance against the latest deep clustering algorithms, including outperforming current state-of-the-art on several. We postulate that these results show a promising research direction for deep clustering. The code can be found at https://github.com/rymc/n2d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Clustering is a fundamental pillar of unsupervised machine learning. It is widely used in a range of tasks across disciplines and well-known algorithms such as k-means have found success in many applications. For example, in science, data exploration and understanding is a fundamental task which clustering facilitates by uncovering the hidden structure of the data. However, k-means <ref type="bibr" target="#b17">[17]</ref>, along with many conventional clustering algorithms such as Gaussian Mixture Models (GMMs) <ref type="bibr" target="#b23">[23]</ref>, DBSCAN <ref type="bibr" target="#b3">[4]</ref>, and hierarchical algorithms <ref type="bibr" target="#b12">[12]</ref> typically require hand engineered features to be created for each dataset and task. Further, these features may then be analysed using another process, feature selection, in order to eliminate redundant or poor quality features. This task is even more challenging in the unsupervised setting. Additionally, it is a time-consuming and brittle process, with the choice of features having a large influence over the subsequent performance of the clustering algorithm.</p><p>However, recent advances in deep learning have paved the way for algorithms which can effectively learn from raw data, bypassing the need for manual feature extraction and selection. One such popular method which learns powerful representations of the data automatically is an autoencoder <ref type="bibr" target="#b26">[26]</ref>. Autoencoders effectively seek to learn the intrinsic structure of the data with a deep neural network, and do so by learning to reconstruct the original data, regularized for example, via a bottleneck inducing a compressed representation. This representation learned from the raw data is then typically used in a range of tasks, such as an input to a supervised classifier.</p><p>This line of research has also impacted the unsupervised domain, where deep clustering has become a popular area of study. Deep clustering refers to the process of clustering with deep neural networks, typically with features automatically learned from the raw data by CNNs <ref type="bibr" target="#b31">[30]</ref> or autoencoders <ref type="bibr" target="#b28">[28]</ref> and clustered with a deep neural network.</p><p>These algorithms have reported large performance gains on various benchmark tasks over conventional non-deep clustering algorithms. For example, Guo et al. <ref type="bibr" target="#b7">[7]</ref> pre-train an autoencoder, then initialize the weights of a deep clustering network with k-means. Following this, the autoencoder continues to learn a representation, but jointly with the clustering network which seeks a good clustering.</p><p>In this work, we propose a simple approach, N2D, that effectively replaces the clustering network with a manifold learning technique on top of the autoencoded representation. Specifically, we intend for it to find a distance preserving manifold within this representation. Given this updated embedding, we can then cluster it with conventional nondeep clustering algorithms. By doing so, N2D replaces the complexity of the clustering network with a manifold learning method and straightforward non-deep clustering algorithm, reducing the deepness of the deep clustering, yet achieving superior performance via the extra manifold learning step.</p><p>One important question is which manifold learning technique to apply to the autoencoded representation. There are many possible methods, such as the well-known Principal Component Analysis (PCA) <ref type="bibr" target="#b0">[1]</ref>. PCA seeks to learn a linear transformation of data into a new space, typically via the use of eigendecomposition of the covariance matrix, or by computing the Singular Value Decomposition (SVD) of the data. However, PCA is a linear method and does not perform well in cases where relationships are non-linear. Thankfully, alternative non-linear manifold learning methods exist, and can be categorised by their focus on finding local or global structure. Well known globally focused methods include Isomap <ref type="bibr" target="#b25">[25]</ref>, while t-SNE <ref type="bibr" target="#b18">[18]</ref> is a well known locally focused method. More recently, UMAP <ref type="bibr" target="#b19">[19]</ref> has been proposed, which while also local, has been shown to better preserve global structure. All of these methods seek to utilize the distances between points in order to better learn the underlying structure, and we posit that they will improve the clusterability of an autoencoded embedding. To better understand this, we study the performance of each of these manifold learning methods on both the raw data and the autoencoded embedding.</p><p>Thus, we propose a framework, N2D, where in contrast to recent deep clustering techniques, we replace the deep clustering network with a manifold learning method, and shallow cluster the resulting re-embedded space. We empirically observe that this method is competitive (top-3) with state-ofthe-art deep clustering algorithms across a range of datasets. Further, we observe that it out-performs state-of-the-art algorithms on several others. Code and weights to reproduce the results are available at https://github.com/rymc/n2d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Clustering algorithms can be broadly categorized into two different categories, hierarchical clustering and partitional clustering. Hierarchical clustering <ref type="bibr" target="#b14">[14]</ref> algorithms themselves can be categorized into divisive and agglomerative, where the former repeatedly splits clusters as it moves down the hierarchy, and the latter merges clusters as it moves up the hierarchy. artitional clustering algorithms are an alternative approach which divides a dataset into typically non-overlapping subsets at a single level. Gaussian Mixture Models (GMMs) <ref type="bibr" target="#b23">[23]</ref> and k-means <ref type="bibr" target="#b17">[17]</ref> are two well-known partitional clustering algorithms. k-means divides a dataset into k disjoint clusters by typically minimizing the sum of squared errors between each datapoint and their closest cluster centroid. GMMs are a probabilistic model that can be considered as generalized k-means to utilize covariance structure information and the centers of the latent Gaussians.</p><p>As the performance of various machine learning algorithms, including clustering algorithms, is heavily dependent on the choice of features, much work has occurred in the area of automatically learning these features, or representations of the data. There exist deep learning based methods, such as autoencoders, which seek to autoencode a high dimensional mapping to a lower one, such that the higher dimensional mapping can be reconstructed again. There is also significant work in non-deep methods such as PCA <ref type="bibr" target="#b0">[1]</ref> , ICA <ref type="bibr" target="#b10">[10]</ref>, Local Linear Embedding (LLE) <ref type="bibr" target="#b24">[24]</ref> and Isomap <ref type="bibr" target="#b25">[25]</ref>. Methods such as PCA seek to preserve the important structure of the data, while other methods such as LLE and Isomap, which preserve the geometric and neighbour properties of the data.</p><p>A relatively recent area of study that combines both of these lines of research is deep clustering. Deep clustering methods use deep neural networks to cluster, typically involving two different processes, one where a representation is learned, and one where the actual clustering occurs. This process may occur separately or jointly.</p><p>The deep neural networks used for deep clustering are diverse, and include MLPs <ref type="bibr" target="#b28">[28]</ref>, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b31">[30]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b20">[20]</ref>. When used in the representation learning step these methods will optimize a specific loss, such as the reconstruction loss or generative adversarial loss. However, in addition, a clustering loss is added to guide the algorithm to find more cluster friendly features. These losses may include a k-means loss <ref type="bibr" target="#b30">[29]</ref> or a cluster hardening loss <ref type="bibr" target="#b28">[28]</ref>. These losses are then typically combined in some way, such as with joint training, where the clustering loss is usually given much lower weight than the non-clustering loss <ref type="bibr" target="#b7">[7]</ref>.</p><p>Along these lines, IDEC <ref type="bibr" target="#b7">[7]</ref> and ASPC-DA <ref type="bibr" target="#b6">[6]</ref> both use an autoencoder for their initial pre-training step. Based on this learned representation, these methods initialize the weights of a new clustering network with k-means. IDEC and ASPC-DA then jointly trained this clustering network with the autoencoder. These approaches have been shown to perform well on a number of clustering tasks.</p><p>An alternative to using two different losses is to use a single combined loss, such as DEC <ref type="bibr" target="#b28">[28]</ref> or JULE <ref type="bibr" target="#b31">[30]</ref>. JULE uses CNNs as the representation learning step, integrating the learning of the representation and clustering into the backward and forward passes of a single recurrent model. The downside of their approach is that it is quite inefficient due to the recurrent nature of the model.</p><p>The concept of manifold learning on embeddings has been explored by Hasan and Curry <ref type="bibr" target="#b8">[8]</ref>. In this work they specifically study the setting of applying LLE to existing word embeddings, improving the performance of word embeddings in word similarity tasks. They show how this method has theoretical foundations in metric recovery <ref type="bibr" target="#b9">[9]</ref>. We note that in this work they apply LLE to windows of the embedding and use it to transform test vectors from the original embedding, whereas we are interested in learning the manifold of the entire embedding, optimizing for clusterability.</p><p>Others have studied the integration of local constraints into autoencoder learning. Wei et al. <ref type="bibr" target="#b27">[27]</ref> propose a semisupervised method for document representation which utilizes an autoencoder to jointly learn from both the document itself, and neighbouring documents. They then demonstrate that by incorporating locality they improve performance in both document clustering and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our method relies primarily on the combination of two different manifold learning methods. The first is an autoencoder, which while learning a representation, does not explicitly take local structure into account. We will show that by augmenting the autoencoder with a manifold learning technique which explicitly takes local structure into account, we can increase the quality of the representation learned in terms of clusterability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Autoencoder</head><p>An autoencoder is a deep neural network consisting of two key components. The first is the encoder, which attempts to learn a function which maps the input x to a new feature vector (h = f (x)). The second component is the decoder, which attempts to learn a function which maps the learned feature space back to the original input space (r = g(h). In other words, it is a neural network which attempts to copy its input to its output. This is typically achieved via a form of regularization, for example by forcing the network to compress the input into a lower dimensional space.</p><p>The learning process can be described as minimizing the loss function L(x, g(f (x))), where L is a function which penalizes g(f (x)) for being dissimilar to x. One such loss may be the Mean Squared Error (MSE).</p><p>While autoencoders have been shown to perform well at many feature representation tasks, they do not explicitly preserve the distances of the data in the representation that they learn. We believe that by taking this into account we can improve the quality of the clusters found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Isomap</head><p>There are a multitude of manifold learning techniques that explicitly seek to preserve distances within the data. Isomap <ref type="bibr" target="#b25">[25]</ref> is a nonlinear method which extends multidimensional scaling (MDS) to incorporate geodesic distances imposed by a weighted graph. Geodesic distance is the distance between two points measured over the manifold, and thus by using the geodesic distance Isomap can learn the manifold structure. A k-nearest neighbourhood graph is constructed from the data, where the shortest distance between two nodes is considered the geodesic distance. Isomap constructs a global pairwise geodesic similarity matrix between all points in the data, on which classical scaling is applied. Thus, Isomap can be considered a global manifold learning technique as it seeks to retain the global structure of the data. While Isomap is a global approach and our hypothesis is that a learning a local manifold on the autoencoded embedding will lead to better results, we will investigate the use of Isomap within N2D, specifically to understand how a global method performs and test our hypothesis.</p><p>We consider Isomap to have two key parameters in our setting, the first is the number of components, which is the top n eigenvectors of the geodesic distance matrix which represent the co-ordinates in the new space. The next parameter of importance is the number of neighbours to consider, which is simply the number of k-nearest neighbours to consider as local to a point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. t-SNE</head><p>t-SNE (t-distributed Stochastic Neighbor Embedding) <ref type="bibr" target="#b18">[18]</ref> is a nonlinear method with a specific objective of optimizing local distances when creating the embedding. The first stage of the t-SNE algorithm is to construct a probability distribution over pairs within the data in such away that similar points will have a high probability of being chosen while dissimilar points have an extremely low probability of being chosen. In the second stage t-SNE defines a probability distribution over the mapped points, minimising the KullbackLeibler (KL) divergence between the two distributions.</p><p>As with Isomap, t-SNE can choose the number of components in which to embed the data. It also requires a perplexity value which is related to the number of nearest neighbours used in Isomap. However, t-SNE is typically not very sensitive to this value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UMAP</head><p>A recently proposed manifold learning method is UMAP (Uniform Manifold Approximation and Projection) <ref type="bibr" target="#b19">[19]</ref>, which seeks to accurately represent local structure, but has been shown to also better incorporating global structure. Compared to t-SNE it has a number of benefits to motivate the comparison with t-SNE in our framework. While t-SNE typically struggles with large datasets, UMAP has been shown to scale well. Further, as UMAP better preserves global structure, while remaining focused on preserving distances within local neighbourhoods, it may inherit benefits from both local and global methods.</p><p>UMAP relies on three assumptions, namely that the data is uniformly distributed on a Riemannian manifold, that the Riemannian metric is locally constant and that the manifold is locally connected. From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.</p><p>UMAP is similar to Isomap <ref type="bibr" target="#b25">[25]</ref> in that it uses a k-neighbour based graph algorithm to compute the nearest neighbours of points. At a high level, UMAP first constructs a weighted kneighbour graph, and from this graph a low dimensional layout is computed. This low dimensional layout is optimized to have as close a fuzzy topological representation to the original as possible based on cross entropy.</p><p>It has a number of important hyperparameters that influence performance. The first is the number of neighbours to consider as local. This represents the trade-off between the granularity of how much local structure is preserved and how much of the global structure is captured. As we are primarily concerned with the integration of local structure into our embedding, we will typically choose lower values for the number of neighbours. The second is the dimensionality of the target embedding. In our method we set the dimensionality to be the number of clusters we are seeking to find. UMAP also requires the minimum allowed separation between points in the embedding space. Lower values of this minimum distance will more accurately capture the true manifold structure, but may lead to dense clouds that make visualization difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. N2D</head><p>We posit that by learning the manifold of the autoencoded embedding, specifically learning a manifold with a specific emphasis on locality, we can achieve a more cluster friendly embedding. However, as there is generally no ability to crossvalidate hyperparameters in the unsupervised setting, it is therefore important to choose sensible default parameters for each approach. For all manifold learning methods, we set the number of components or dimensions to be the number of clusters in the data. For Isomap and UMAP we consider the number of neighbours to be an important parameter, and we set it to a sensible default value of 5 for Isomap, and 20 for UMAP. UMAP also has another parameter we believe will be influential, which is the minimum distance between points. We believe that a default minimum distance of 0 is ideal for our method, as our prime motivation is not visualization and thus a more accurate representation of the true manifold is preferred.</p><p>We summarize the high level steps of our proposed method N2D as:</p><p>• Apply an autoencoder to the raw data to learn an initial representation. • We re-embed the autoencoded embedding by searching for a more clusterable manifold with a manifold learning method which preserves local distances. • Finally, given this new, more clusterable embedding, we apply a final shallow clustering algorithm to discover the clusters.</p><p>More concisely, we may also simply represent N2D as</p><formula xml:id="formula_0">C = F C (F M (F A (X)))<label>(1)</label></formula><p>where C is the final clustering, F C is the clustering algorithm, F M is the manifold learner, F A is the autoencoder and X is the original data. We will study three manifold learning methods to understand the effect of the various approaches when applied to both the raw data and the autoencoded embedding, showing how one specific method, UMAP, achieves superior performance when applied to the embedding. On the question of why combine an autoencoder with a manifold learning method, we will demonstrate empirically in Section IV-D the contribution of each step to the overall performance, showing how this step can significantly increase performance. We will also demonstrate in Section IV-D how it is competitive with the state-ofthe-art across a range of datasets, both image and time-series, and itself achieves state-of-the-art results on several.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In order to validate our idea, we conduct experiments on a range of diverse datasets, including standard datasets used to evaluate deep clustering algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We will use two standard evaluation metrics for validating the performance of unsupervised clustering algorithms. In both cases, values range between 0 and 1, where higher values correspond to better clustering performance.</p><p>1) Accuracy: In clustering, accuracy (ACC) is defined as the best match between the ground truth and the predicted clusters.</p><formula xml:id="formula_1">ACC = max m n i=1 1{y i = m(c i )} n<label>(2)</label></formula><p>where y are the ground truth labels, c are the cluster labels, and m enumerates mappings between clusters and labels.</p><p>2) Normalized Mutual Information: The Normalized Mutual Information (NMI) can be viewed as a normalization of the mutual information to scale the results between 0 and 1, where 0 has no mutual information and 1 is perfect correlation. More concretely, NMI is defined as:</p><formula xml:id="formula_2">N M I = 2I(y, c) [H(y) + H(c)]<label>(3)</label></formula><p>where y are the ground truth labels, c are the cluster labels, H measures the entropy, and I is the mutual information between the ground truth labels and the cluster labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Settings</head><p>We base our autoencoder on the architecture described by Xie et al. <ref type="bibr" target="#b28">[28]</ref>, which is a fully connected Multi-Layer Perceptron (MLP). The dimensions are inspired by those chosen by van der Maaten et al. in t-SNE <ref type="bibr" target="#b18">[18]</ref>, which are d-500-500-2000-c, where d is the dimensionality of the data and c is the number of clusters. As typical with autoencoders, the decoder network is a mirror of the encoder. All layers use ReLU activation <ref type="bibr" target="#b21">[21]</ref>. The optimizer is Adam <ref type="bibr" target="#b15">[15]</ref>. We train the autoencoder on for 1000 epochs for all datasets.</p><p>We use UMAP with the following default parameter set across all datasets. The number of neighbours is 20, the number of dimensions is the number of clusters, and the minimum distance between each point in the manifold is 0.</p><p>We use a GMM for the final clustering algorithm, where each component has its own general covariance matrix, and there are c components, where c is the number of clusters. <ref type="figure" target="#fig_0">Figure 1</ref> shows the resulting clusters when using N2D for visualization purposes. However, in order to better understand the effectiveness of our method at clustering we will study each individual component of N2D via measuring the accuracy For visualization purposes we set the number of dimensions to 2 and points plotted to 5000. In contrast, when we use N2D for clustering and not visualization, we cluster in higher dimensions (where the number of dimensions is the number of clusters) and achieve higher clustering performance.  <ref type="table" target="#tab_1">Table I</ref> shows details of the accuracy and NMI of each individual component of N2D. This table shows that the performance of the non-deep clustering algorithm GMM is typically poorest across all datasets. When we introduce manifold learning methods, and cluster those embeddings, we see improvements in cluster accuracy and NMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>As well as the autoencoder, we use 3 different manifold learning methods with different properties. The first is Isomap, which is a globally focused manifold learner. It outperforms t-SNE, the local manifold learning technique, on 2 of the 4 datasets it was able to process. On 2 of the 6 datasets it was unable to complete the learning as it exhausted all memory on our 64GB system. Therefore, on two datasets t-SNE performed better than Isomap, and on two others, Isomap outperformed t-SNE.</p><p>However, when Isomap and t-SNE are each applied to the autoencoded embedding, on only 1 of the 4 datasets does N2D with Isomap outperform N2D with t-SNE. This suggests that on some datasets the clusters are better discovered by a global method, and others by a local method. However, when applied the autoencoded embedding, the more local methods appear to be the better choice.</p><p>This intuitively suggests that a technique which is primarily locally focused but captures global structure better than t-SNE may lead to further improvements. Therefore, when we experiment with UMAP, which meets this criteria, we see that UMAP is the superior approach on 3 of the 6 raw datasets. However, when applied to the autoencoded embedding, N2D with UMAP outperforms both Isomap and t-SNE on all datasets. This supports the hypothesis that a manifold learner, II: A comparison of our method with both shallow clustering algorithms, along with the latest deep-clustering algorithms. Results were retrieved from the literature, or computed by us when not found and possible to compute. The top 3 performing scores are highlighted in bold. Note that our method is consistently in the top-3 across 5 of the 6 datasets, including the best accuracy and NMI for Fashion, pendigits and HAR. Algorithms that are missing scores for datasets are because the paper did not originally test on this dataset and it was not easily possible to get this score. <ref type="table" target="#tab_1">Fashion  pendigits  HAR  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  ACC  NMI  ACC</ref>  which, while locally focused, also captures a degree of the global structure, is best suited for discovering the clusterable manifold of an autoencoded embedding. The largest gains between our approach N2D and the subcomponents is on HAR, where there is a 25 percentage point increase in performance compared to the AE and UMAP, while on MNIST and USPS, where there is an around a 15 percentage point increase in accuracy when using N2D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST-test USPS</head><p>In <ref type="table" target="#tab_1">Table III</ref> we show the amount of time it takes for each stage of the method in minutes, as well as the total time. From this, it is clear that our method is efficient, clustering MNIST and Fashion-MNIST in around 18 minutes, while clustering the remaining 4 datasets in between two and four minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with other methods</head><p>In <ref type="table" target="#tab_1">Table II</ref> we show the accuracy and NMI results for a wide set of clustering algorithms on six different datasets. The clustering algorithms chosen include a number of conventional non-deep methods, such as k-means, spectral clustering (SC) and GMMs. They also include recent deep-clustering based methods, such as ClusterGAN, IDEC, JULE and ASPC-DA. These methods make significant use of deep networks, and typically outperform the non-deep clustering methods.</p><p>The most similar methods to N2D are IDEC and ASPC-DA. Both of these approaches pre-train an autoencoder before jointly training a second deep network with a clustering and non-clustering (reconstruction) loss. The clustering network weights are initialized with a non-deep clustering algorithm such as k-means.</p><p>In contrast, we replace the second deep network with a manifold learning method, UMAP, and then use a nondeep clustering algorithm, a GMM, to cluster the resulting embedding. Hence, our less deep method, N2D, benefits from less complexity, but as can be seen in <ref type="table" target="#tab_1">Table II</ref>, has competitive or superior performance to all other methods.</p><p>On five of the six datasets tested, our approach is in the top 3 for at least one of the metrics. On MNIST-test we are around 1 percentage point lower in accuracy than JULE and DEPICT, and 2 percentage points lower than ASPC-DA which is top. However, on the Fashion dataset, we achieve the highest accuracy, around 5 absolute percentage points higher than ClusterGAN, and 8 absolute percentage points higher than ASPC-DA.</p><p>We also include two non-image datasets, pendigits and HAR, to validate performance on different types of data. Many of the best-performing deep-clustering methods are intended for image clustering (e.g., JULE <ref type="bibr" target="#b31">[30]</ref>, DBC <ref type="bibr" target="#b16">[16]</ref>, DAC <ref type="bibr" target="#b2">[3]</ref>), and thus we were unable to find or easily obtain results on these datasets. However, for the algorithms for which we could obtain or produce results, our method also achieved the best performance. For both datasets we compare our method with some of the most similar deep clustering approaches, DEC and IDEC. On pendigits, we achieve 11 percentage points higher accuracy than the closest approach IDEC and on HAR a 15 percentage point increase in accuracy. In fact, consistently across all datasets, we achieve higher accuracy and NMI scores than these methods.</p><p>We also note that one of the closest competitors, ASPC-DA, which typically slightly outperforms our method on several datasets, achieves this performance due to data augmentation. When data augmentation is removed from ASPC-DA, they typically achieve less competitive performances, e.g. an accuracy of 0.924 (vs 0.988) on MNIST, 0.785 (vs 0.973) on MNIST-test and 0.688 (vs 0.982) on USPS. For future work we would like to evaluate our proposed method with data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we propose a simple deep clustering method, N2D, which reduces the deepness of typical deep clustering algorithms by replacing the clustering network with an alternative framework which seeks to find the manifold within the autoencoder embedding, and clusters this new embedding with a shallow clustering architecture. We studied both global and local manifold learning algorithms, with our results supporting the hypothesis that learning the local manifold of an autoencoded embedding, while also preserving global structure as UMAP does, is better able to discover the most clusterable manifold of an autoencoded embedding. N2D is the resulting combination which is shown to be effective on a range of datasets, including image and time-series datasets. We compare N2D with both conventional shallow clustering algorithms, and the latest state-of-the-art deep clustering algorithms. In the empirical comparison, we show how our proposed method is competitive with the current state-of-theart clustering approaches, achieving top-3 performance in five of the six datasets datasets tested. Further, we outperform the state-of-the-art on several datasets, including surpassing the next best algorithm by around 5 absolute percentage points in accuracy on Fashion-MNIST and 15 percentage points on the activity recognition dataset HAR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Visualization of N2D applied to all six datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MNIST: A traditional benchmark dataset consisting of 70,000 handwritten digits belong to 10 different classes. MNIST-test: A subset of the MNIST dataset, containing only the test set of 10,000 images. HAR: A time series dataset consisting of sensor data from a smart phone. It was collected from 30 people performing various activities of daily living, and contains 6 different activities; walking, walking upstairs, walking downstairs, sitting, standing and laying.</figDesc><table><row><cell>digits are written. Each digit is represented by 8 coordi-</cell></row><row><cell>nates of the stylus when writing a specific digit. There</cell></row><row><cell>are 10992 data points.</cell></row><row><cell>USPS: A dataset of 9298 images belonging to 10 different</cell></row><row><cell>classes. Whereas MNIST images are 28x28, these images</cell></row><row><cell>are 16x16.</cell></row><row><cell>• Fashion: A more challenging alternative to the MNIST</cell></row><row><cell>dataset, consisting of 70,000 images of clothing, for a</cell></row><row><cell>total of 10 classes.</cell></row><row><cell>• pendigits: A time series dataset consisting of sampled</cell></row><row><cell>points from a pressure sensitive tablet as ten different</cell></row></table><note>••••</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Evaluating the performance of each component of the proposed method. AE, UMAP and N2D each have a GMM clustering step post-manifold learning. Results for Isomap were sometimes not available (-) as it exhausted memory on our 64GB machine.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell><cell cols="2">MNIST-test</cell><cell>USPS</cell><cell></cell><cell>Fashion</cell><cell></cell><cell>pendigits</cell><cell></cell><cell>HAR</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell></row><row><cell>GMM</cell><cell>0.389</cell><cell>0.333</cell><cell>0.464</cell><cell cols="2">0.465 0.562</cell><cell>0.541</cell><cell>0.463</cell><cell>0.514</cell><cell>0.674</cell><cell>0.683</cell><cell>0.585</cell><cell>0.648</cell></row><row><cell>AE</cell><cell>0.809</cell><cell>0.835</cell><cell>0.769</cell><cell cols="2">0.745 0.707</cell><cell>0.705</cell><cell>0.569</cell><cell>0.591</cell><cell>0.807</cell><cell>0.757</cell><cell>0.552</cell><cell>0.483</cell></row><row><cell>Isomap</cell><cell>-</cell><cell>-</cell><cell>0.896</cell><cell cols="2">0.755 0.781</cell><cell>0.787</cell><cell>-</cell><cell>-</cell><cell>0.756</cell><cell>0.792</cell><cell>0.608</cell><cell>0.677</cell></row><row><cell>N2D (Isomap)</cell><cell>-</cell><cell>-</cell><cell>0.797</cell><cell>0.77</cell><cell>0.660</cell><cell>0.727</cell><cell>-</cell><cell>-</cell><cell>0.826</cell><cell>0.820</cell><cell>0.632</cell><cell>0.580</cell></row><row><cell>TSNE</cell><cell>0.768</cell><cell>0.810</cell><cell>0.806</cell><cell cols="2">0.823 0.720</cell><cell>0.830</cell><cell>0.608</cell><cell>0.651</cell><cell>0.893</cell><cell>0.872</cell><cell>0.647</cell><cell>0.736</cell></row><row><cell>N2D (TSNE)</cell><cell>0.978</cell><cell>0.940</cell><cell>0.948</cell><cell cols="2">0.883 0.810</cell><cell>0.858</cell><cell>0.588</cell><cell>0.658</cell><cell>0.785</cell><cell>0.826</cell><cell>0.768</cell><cell>0.670</cell></row><row><cell>UMAP</cell><cell>0.825</cell><cell>0.880</cell><cell>0.857</cell><cell cols="2">0.819 0.804</cell><cell>0.845</cell><cell>0.588</cell><cell>0.656</cell><cell>0.819</cell><cell>0.856</cell><cell>0.552</cell><cell>0.696</cell></row><row><cell>N2D (UMAP)</cell><cell>0.979</cell><cell>0.942</cell><cell>0.948</cell><cell cols="2">0.882 0.958</cell><cell>0.901</cell><cell>0.672</cell><cell>0.684</cell><cell>0.885</cell><cell>0.863</cell><cell>0.801</cell><cell>0.683</cell></row><row><cell cols="6">and NMI, as well as how the full N2D algorithm compares</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with a range of other clustering algorithms.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>E. Role of Each Component of N2D</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1002/wics.101</idno>
		<ptr target="https://doi.org/10.1002/wics.101" />
	</analytic>
	<monogr>
		<title level="j">WIREs Comput. Stat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="459" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://dl.acm.org/citation.cfm?id=3001460.3001507" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5736" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive self-paced deep clustering with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/243</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word re-embedding via manifold dimensionality retention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word embeddings as metric recovery in semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/809" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep clustering: On the link between discriminative models and k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04246</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;17</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical clustering schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="254" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminatively boosted image clustering with fully convolutional auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clustergan : Latent space clustering in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3104322.3104425" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ser. ICML&apos;10. USA</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ser. ICML&apos;10. USA</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Biometrics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recent advances in autoencoder-based representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>3rd workshop on Bayesian Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locally embedding autoencoders: A semi-supervised manifold learning approach of document representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0146672</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0146672" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<ptr target="http://proceedings.mlr.press/v48/xieb16.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards kmeans-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
