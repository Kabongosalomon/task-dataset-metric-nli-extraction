<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>yue.zhang@wias.org.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our model outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average. * Email corresponding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Task-oriented dialogue systems <ref type="bibr" target="#b19">(Young et al., 2013)</ref> help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history <ref type="bibr" target="#b12">Madotto et al., 2018;</ref><ref type="bibr" target="#b4">Gangi Reddy et al., 2019;</ref><ref type="bibr" target="#b14">Qin et al., 2019b;</ref><ref type="bibr" target="#b17">Wu et al., 2019a)</ref>. Taking the dialogue in <ref type="figure">Figure 1</ref> as an example, to answer the drivers query about the "gas station", the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset . Words with the same color refers queried entity from the KB. Better viewed in color.</p><p>Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is important to consider methods that can effectively transfer knowledge from a source domain with sufficient labeled data to a target domain with limited or little labeled data.</p><p>Existing work can be classified into two main categories. As shown in <ref type="figure">Figure 2</ref>(a), the first strand of work <ref type="bibr" target="#b12">Madotto et al., 2018;</ref><ref type="bibr" target="#b17">Wu et al., 2019a)</ref> simply combines multi-domain datasets for training. Such methods can implicitly extract the shared features but fail to effectively capture domain-specific knowledge. As shown in <ref type="figure">Figure 2</ref>(b), The second strand of work <ref type="bibr" target="#b14">Qin et al., 2019b)</ref> trains model separately for each domain, which can better capture domain-specific features. However, those methods ignore shared knowledge between different domains (e.g. the location word exists in both schedule domain and navigation domain).</p><p>We consider addressing the limitation of existing work by modeling knowledge connections between domains explicitly. In particular, a simple baseline  <ref type="figure">Figure 2</ref>: Methods for multi-domain dialogue. Previous work either trains a general model on mixed multi-domain mixed datasets (a), or on each domain separately (b). The basic shared-private framework is shown (c). Our proposed extension with dynamic fusion mechanism is shown (d).</p><p>to incorporate domain-shared and domain-private features is shared-private framework <ref type="bibr" target="#b21">Zhong et al., 2018;</ref><ref type="bibr" target="#b18">Wu et al., 2019b)</ref>. Shown in <ref type="figure">Figure 2</ref>(c), it includes a shared module to capture domain-shared feature and a private module for each domain. The method explicitly differentiates shared and private knowledge. However, this framework still has two issues: (1) given a new domain with extremely little data, the private module can fail to effectively extract the corresponding domain knowledge.</p><p>(2) the framework neglects the fine-grained relevance across certain subsets of domains. (e.g. schedule domain is more relevant to the navigation than to the weather domain.) To address the above issues, we further propose a novel Dynamic Fusion Network (DF-Net), which is shown in <ref type="figure">Figure 2</ref>  <ref type="bibr">(d)</ref>. In contrast to the sharedprivate model, a dynamic fusion module (see ยง2.3) is further introduced to explicitly capture the correlation between domains. In particular, a gate is leveraged to automatically find the correlation between a current input and all domain-specific models, so that a weight can be assigned to each domain for extracting knowledge. Such a mechanism is adopted for both the encoder and the decoder, and also a memory module to query knowledge base features. Given a new domain with little or no training data, our model can still make the best use of existing domains, which cannot be achieved by the baseline model.</p><p>We conduct experiments on two public benchmarks, namely SMD  and Multi-WOZ 2.1 <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>. Results show that our framework consistently and significantly outperforms the current state-of-the-art methods. With limited training data, our framework outperforms the prior best methods by 13.9% on average.</p><p>To our best of knowledge, this is the first work to effectively explore shared-private framework in multi-domain end-to-end task-oriented dialog. In addition, when given a new domain which with few or zero shot data, our extended dynamic fusion framework can utilize fine-grained knowledge to obtain desirable accuracies, which makes it more adaptable to new domains.</p><p>All datasets and code are publicly available at:</p><p>https://github.com/LooperXX/DF-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Architecture</head><p>We build our model based on a seq2seq dialogue generation model ( ยง2.1), as shown in <ref type="figure" target="#fig_0">Figure 3</ref>(a).</p><p>To explicitly integrate domain awareness, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>(b) we first propose to use a sharedprivate framework ( ยง2.2) to learn shared and the corresponding domain-specific features. Next, we further use a dynamic fusion network ( ยง2.3) to dynamically exploit the correlation between all domains for fine-grained knowledge transfer, which is shown in <ref type="figure" target="#fig_0">Figure 3</ref>(c). In addition, adversarial training is applied to encourage shared module generate domain-shared feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Seq2Seq Dialogue Generation</head><p>We define the Seq2Seq task-oriented dialogue generation as finding the system response Y according to the input dialogue history X and KB B. Formally, the probability of a response is defined as</p><formula xml:id="formula_0">p(Y | X, B) = n t=1 p(y t | y 1 , ..., y tโ1 , X, B),<label>(1)</label></formula><p>where y t represents an output token. In a vanilla Seq2Seq task-oriented dialogue system , a long short-term Memory network (LSTM, <ref type="bibr" target="#b7">Hochreiter and Schmidhuber (1997)</ref>) is used to encode the dialogue history X = (x 1 , x 2 , .., x T ) (T is the number of tokens in the dialogue history) to produce shared context-sensitive </p><formula xml:id="formula_1">h i = BiLSTM enc ฯ emb (x i ), h iโ1 ,<label>(2)</label></formula><p>where ฯ emb (ยท) represents the word embedding matrix. LSTM is also used to repeatedly predict outputs (y 1 , y 2 , ..., y tโ1 ) by the decoder hidden states (h dec,1 , h dec,2 , ..., h dec,t ). For the generation of y t , the model first calculates an attentive representation h dec,t of the dialogue history over the encoding representation H. Then, the concatenation of h dec,t and h dec,t is projected to the vocabulary space V by U :</p><formula xml:id="formula_2">o t = U [h dec,t , h dec,t ],<label>(3)</label></formula><p>where o t is the score (logit) for the next token generation. The probability of next token y t โ V is finally calculated as:</p><formula xml:id="formula_3">p(y t | y 1 , ..., y tโ1 , X, B) = Softmax(o t ). (4)</formula><p>Different from typical text generation with Seq2seq model, the successful conversations for taskoriented dialogue system heavily depend on accurate knowledge base (KB) queries. We adopt the global-to-local memory pointer mechanism (GLMP) <ref type="bibr" target="#b17">(Wu et al., 2019a)</ref> to query the entities in KB, which has shown the best performance. An external knowledge memory is proposed to store knowledge base (KB) B and dialogue history X. The KB memory is designed for the knowledge source while the dialogue memory is used for directly copying history words. The entities in external knowledge memory are represented in a triple format and stored in the memory module, which can be denoted as M = [B; X] = (m 1 , . . . , m b+T ), where m i is one of the triplet of M , b and T denotes the number of KB and dialog history respectively. For a k-hop memory network, the external knowledge is composed of a set of trainable embedding matrices C = (C 1 , . . . , C k+1 ). We can query knowledge both in encoder and decoder process to enhance model interaction with knowledge module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Knowledge in Encoder</head><p>We adopt the last hidden state as the initial query vector:</p><formula xml:id="formula_4">q 1 enc = h T .<label>(5)</label></formula><p>In addition, it can loop over k hops and compute the attention weights at each hop k using</p><formula xml:id="formula_5">p k i = Softmax((q k enc ) c k i ),<label>(6)</label></formula><p>where c k i is the embedding in i th memory position using the embedding matrix C k . We obtain the global memory pointer G = (g 1 , . . . , g b+T ) by applying g k i = Sigmoid((q k enc ) c k i ), which is used to filter the external knowledge for relevant information for decoding. Finally, the model reads out the memory o k by the weighted sum over c k+1 and updates the query vector q k+1 enc . Formally,</p><formula xml:id="formula_6">o k enc = i p k i c k+1 i , q k+1 enc = q k enc + o k enc . (7)</formula><p>q k+1 enc can be seen as the encoded KB information, and is used to initialized the decoder.</p><p>Query Knowledge in Decoder we use a sketch tag to denote all the possible slot types that start with a special token. (e.g., @address stands for all the Address). When a sketch tag is generated by Eq. 4 at t timestep, we use the concatenation of the hidden states h dec,t and the attentive representation h dec,t to query knowledge, which is similar with the process of querying knowledge in the encoder:</p><formula xml:id="formula_7">q 1 dec = [h dec,t , h dec,t ],<label>(8)</label></formula><formula xml:id="formula_8">p k i = Softmax((q k dec ) c k i g k i ).<label>(9)</label></formula><p>Here, we can treat P t = (p k 1 ,. . . ,p k b+T ) as the probabilities of queried knowledge, and select the word with the highest probability from the query result as the generated word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shared-Private Encoder-Decoder Model</head><p>The model in section 2.1 is trained over mixed multi-domain datasets and the model parameters are shared across all domains. We call such model as shared encoder-decoder model. Here, we propose to use a shared-private framework including a shared encoder-decoder for capturing domainshared feature and a private model for each domain to consider the domain-specific features explicitly. Each instance X goes through both the shared and its corresponding private encoder-decoder.</p><p>Enhancing Encoder Given an instance along with its domain, the shared-private encoderdecoder generates a sequence of encoder vectors denoted as H </p><p>The final shared-specific encoding representation H f enc is a mixture:</p><formula xml:id="formula_10">H f enc = W 2 (LeakyReLU(W 1 [H s enc , H d enc ])). (11)</formula><p>For ease of exposition, we define the sharedspecific fusion function as:</p><formula xml:id="formula_11">shprivate : (H s enc , H d enc ) โ H f enc .<label>(12)</label></formula><p>In addition, self-attention has been shown useful for obtaining context information <ref type="bibr" target="#b21">(Zhong et al., 2018)</ref>. Finally, we follow <ref type="bibr" target="#b21">Zhong et al. (2018)</ref> to use selfattention over H f enc to get context vector c f enc . We replace h T with c f enc in Eq. 5. This makes our query vector combine the domain-shared feature with domain-specific feature.</p><p>Enhancing Decoder At t step of the decoder, the private and shared hidden state is:</p><formula xml:id="formula_12">h {s,d} dec,t = LSTM {s,d} dec,t (X).<label>(13)</label></formula><p>We also apply the shared-specific fusion function to the hidden states and the mixture vector is:</p><formula xml:id="formula_13">shprivate : (h s dec,t , h d dec,t ) โ h f dec,t .<label>(14)</label></formula><p>Similarly, we obtain the fused attentive representation h f dec,t by applying attention from h f dec,t over</p><formula xml:id="formula_14">H f enc . Finally, we replace [h dec,t , h dec,t ] in Eq. 8 with [h f dec,t , h f dec,t ]</formula><p>which incorporates shared and domain-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic Fusion for Querying Knowledge</head><p>The shared-private framework can capture the corresponding specific feature, but neglects the finegrained relevance across certain subsets of domains. We further propose a dynamic fusion layer to explicitly leverage all domain knowledge, which is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Given an instance from any domain, we first put it to multiple private encoderdecoder to obtain domain-specific features from all domains. Next, all domain-specific features are fused by a dynamic domain-specific feature fusion module, followed by a shared-specific feature fusion for obtaining shared-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Domain-Specific Feature Fusion</head><p>Given domain-specific features from all domains, a Mixture-of-Experts mechanism (MoE) <ref type="bibr" target="#b6">(Guo et al., 2018)</ref> is adapted to dynamically incorporate all domain-specific knowledge for the current input in both encoder and decoder. Now, we give a detailed description on how to fuse the timestep t of decoding and the fusion process is the same to encoder. Given all domain feature representations in t decoding steps:</p><formula xml:id="formula_15">{h d i dec,t } |D| i=1</formula><p>, where |D| represents the number of domains, an expert gate E takes {h d i dec,t } as input and outputs a softmax score ฮฑ t,i that represents the degree of correlation between each domain and the current input token. We achieve this by a simple feedforward layer:</p><formula xml:id="formula_16">ฮฑ t = Softmax(W * h d dec,t + b).<label>(15)</label></formula><p>The final domain-specific feature vector is a mixture of all domain outputs, dictated by the expert gate weights ฮฑ t = (ฮฑ t,1 , . . . , ฮฑ t,|D| ), which can be</p><formula xml:id="formula_17">written as h d f dec,t = i ฮฑ t,i h d i dec,t .</formula><p>During training, take the decoder for example, we apply the cross-entropy loss L moe dec as the supervision signal for the expert gate to predict the domain of each token in the response, where the expert gate output ฮฑ t can be treated as the t th token's predicted domain probability distribution by multiple private decoder. Hence, the more accurate the domain prediction is, the more correct expert gets:</p><formula xml:id="formula_18">L moe dec = โ n t=1 |D| i=1 (e i ยท log(ฮฑ t,i |ฮธ s , ฮธ m dec )),<label>(16)</label></formula><p>where ฮธ s represents the parameters of encoderdecoder model, ฮธ m dec represents the parameters of the MoE module (Eq. 15) in the decoder and e i โ {0, 1} represents whether the response with n tokens belongs to the domain d i . Similarly, we can get the L moe enc for the encoder and sum up them as: L moe = L moe enc + L moe dec . L moe is used to encourage samples from a certain source domain to use the correct expert, and each expert learns corresponding domain-specific features. When a new domain has little or no labeled data, the expert gate can automatically calculate the correlation between different domains with the target domain and thus better transfer knowledge from different source domains in both encoder and decoder module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared-Specific Feature Fusion</head><p>We directly apply shprivate operation to fuse shared and final domain-specific feature:</p><formula xml:id="formula_19">shprivate : (h s dec,t , h d f dec,t ) โ h f dec,t .<label>(17)</label></formula><p>Finally, we denote the dynamic fusion function  Adversarial Training To encourage the model to learn domain-shared features, we apply adversarial learning on the shared encoder and decoder module. Following , a gradient reversal layer <ref type="bibr" target="#b5">(Ganin and Lempitsky, 2014)</ref> is introduced after the domain classifier layer. The adversarial training loss is denoted as L adv . We follow <ref type="bibr" target="#b13">Qin et al. (2019a)</ref> and the final loss function of our Dynamic fusion network is defined as:</p><formula xml:id="formula_20">as dynamic(h s dec,t , {h d i dec,t } |D| i=1</formula><formula xml:id="formula_21">L = ฮณ b L basic + ฮณ m L moe + ฮณ a L adv ,<label>(18)</label></formula><p>where L basic keep the same as GLMP <ref type="bibr" target="#b17">(Wu et al., 2019a)</ref>, ฮณ b , ฮณ m and ฮณ a are hyper-parameters. More details about L basic and L adv can be found in appendix.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Two publicly available datasets are used in this paper, which include SMD  and an extension of Multi-WOZ 2.1 <ref type="bibr" target="#b0">(Budzianowski et al., 2018</ref>) that we equip the corresponding KB to every dialogue. <ref type="bibr">1</ref> The detailed statistics are also presented in <ref type="table" target="#tab_2">Table 1</ref>. We follow the same partition as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>The dimensionality of the embedding and LSTM hidden units is 128. The dropout ratio we use in our framework is selected from {0.1, 0.2} and the batch size from {16, 32}. In the framework, we adopt the weight typing trick <ref type="bibr" target="#b17">(Wu et al., 2019a)</ref>. We use Adam (Kingma and Ba, 2015) to optimize the parameters in our model and adopt the suggested hyper-parameters for optimization. All hyper-parameters are selected according to validation set. More details about hyper-parameters can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our model with the following state-ofthe-art baselines.   โข Mem2Seq <ref type="bibr" target="#b12">(Madotto et al., 2018)</ref>: the model takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.</p><p>โข DSR : the model leverages dialogue state representation to retrieve the KB implicitly and applies copying mechanism to retrieve entities from knowledge base while decoding.</p><p>โข KB-retriever <ref type="bibr" target="#b14">(Qin et al., 2019b)</ref>: the model adopts a retriever module to retrieve the most relevant KB row and filter the irrelevant information for the generation process.</p><p>โข GLMP <ref type="bibr" target="#b17">(Wu et al., 2019a)</ref>: the framework adopts the global-to-local pointer mechanism to query the knowledge base during decoding and achieve state-of-the-art performance.</p><p>For Mem2Seq, DSR, KB-retriever 2 , we adopt the reported results from <ref type="bibr" target="#b14">Qin et al. (2019b)</ref> and <ref type="bibr" target="#b17">Wu et al. (2019a)</ref>. For GLMP, we rerun their public code to obtain results on same datasets. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Follow the prior work <ref type="bibr" target="#b12">Madotto et al., 2018;</ref><ref type="bibr" target="#b17">Wu et al., 2019a;</ref><ref type="bibr" target="#b14">Qin et al., 2019b)</ref>, we adopt the BLEU and Micro Entity F1 metrics to evaluate model performance. The results on the two datasets are shown in <ref type="table" target="#tab_4">Table 2</ref>, we can observe that: 1) The basic shared-private framework outperforms the best prior model GLMP in all the datasets. This indicates that the combination of domain-shared and domain-specific features can better enhance each domain performanc compared with only utilizing the implicit domain-shared features. 2) Our framework achieves the state-of-the-art performance on two multi-domain task-oriented dialog datasets, namely SMD and Multi-WOZ 2.1. On SMD dataset, our model has the highest BLEU compared with baselines, which shows that our framework can generate more fluent response. More importantly, our model outperforms GLMP by 2.0% overall, 3.3% in the Navigate domain, 1.1% in the Weather domain and 0.6% in Schedule domain on entity F1 metric, which indicates that considering relevance between target domain input and all domains is effective for enhancing performance of each domain. On Multi-Woz 2.1 dataset, the same trend of improvement has been witnessed, which further shows the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis</head><p>We study the strengths of our model from several perspectives on SMD dataset. We first conduct several ablation experiments to analyze the effect of different components in our framework. Next, we conduct domain adaption experiments to verify the transferability of our framework given a new domain with little or no labeled data. In addition, we provide a visualization of the dynamic fusion layer and case study to better understand how the module affects and contributes to the performance.   <ref type="table">Table 4</ref>: Human evaluation of responses on the randomly selected dialogue history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Ablation</head><p>Several ablation experiments and results are shown in <ref type="table" target="#tab_5">Table 3</ref>. In detail, 1) w/o Domain-shared Knowledge Transfer denotes that we remove domainshared feature and just keep fused domain-specific feature for generation. 2) w/o Domain Fusion mechanism denotes that we simply sum all domainspecific features rather than use the MOE mechanism to dynamically fuse domain-specific features.</p><p>3) w/o Multi-Encoder represents that we remove multi-encoder module and adopt one shared encoder in our framework. 4) w/o Multi-Decoder represents that we remove the multi-decoder module and adopt one shared decoder. 5) w/o Adversarial Training denotes that we remove the adversarial training in experimental setting. Generally, all the proposed components are effective to contribute the final performance. Specifically, we can clearly observe the effectiveness of our dynamic fusion mechanism where w/o domain-specific knowledge fusion causes 1.8% drops and the same trend in removing domain-shared knowledge fusion. This further verifies that domain-shared and domainspecific feature are benefit for each domain performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Domain Adaption</head><p>Low-Resource Setting To simulate lowresource setting, we keep two domains unchanged, and the ratio of the except domain from original data varies from [1%, 5%, 10%, 20%, 30%, 50%]. The results are shown in <ref type="figure">Figure 5</ref>. We can find that: (1) Our framework outperforms the GLMP baseline on all ratios of the original dataset. When the data is only 5% of original dataset, our framework outperforms GLMP by 13.9% on all domains averagely.</p><p>(2) Our framework trained with 5% training dataset can achieve comparable and even better performance compared to GLMP with 50% training dataset on some domains. This implies that our framework effectively transfers knowledge from other domains to achieve better performance for the low-resources new domain.</p><p>Zero-Shot Setting Specially, we further evaluate the performance of domain adaption ability on the zero-shot setting given an unseen domain. We randomly remove one domain from the training set, and other domain data remained unchanged to train the model. During test, the unseen domain input use the MoE to automatically calculate the correlation between other domains and the current input and get the results. Results are shown in Driver: Manhattan, please will it be cloudy on Monday ? Car: Monday will be foggy.</p><p>Driver: Who will be at the meeting Friday? Car: HR will be at the Friday meeting.   <ref type="figure">Figure 6</ref>, we can see our model significantly outperforms GLMP on three domains, which further demonstrate the transferability of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Visualization of Dynamic Fusion Layer</head><p>To better understand what our dynamic fusion layer has learned, we visualize the gate distribution for each domain in low-resource (5%) setting, which fuses domain-specific knowledge among various cases. As shown in the <ref type="figure" target="#fig_5">Figure 7</ref>, for a specific target domain, different examples may have different gate distributions, which indicates that our framework successfully learns how to transfer knowledge between different domains. For example, the navigation column contains 100 examples from its test set and each row show the corresponding expert value. More specifically, in the navigation column, we observe that the expert value in schedule domain is bigger than weather domain, which indicates schedule domain transfers more knowledge to navigation than weather domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Case Study</head><p>Furthermore, we provide one case for navigation domain and their corresponding expert gate distribution. The cases are generated with 5% training data in the navigation domain and other two domain datasets keep the same, which can better show how the other two domains transfer knowledge to the low-resource domain. As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, the expert value of schedule domain is bigger than the weather domain, which indicates the schedule contributes more than weather domain. In further exploration, we find word "location" and "set" appear both in navigation and schedule domain, which shows schedule has closer relation with navigation than weather, which indicates our model successfully transfers knowledge from the closest domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">Human Evaluation</head><p>We provide human evaluation on our framework and other baseline models. We randomly generated 100 responses. These responses are based on distinct dialogue history on the SMD test data. Following  and <ref type="bibr" target="#b14">Qin et al. (2019b)</ref>, We hired human experts and asked them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. Results are illustrated in <ref type="table">Table 4</ref>. We can see that our framework outperforms GLMP on all metrics, which is consistent with the automatic evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Existing end-to-end task-oriented systems can be classified into two main classes. A series of work trains a single model on the mixed multi-domain dataset.  augments the vocabulary distribution by concatenating KB attention to generatge entities. <ref type="bibr" target="#b9">Lei et al. (2018)</ref> first integrates track dialogue believes in end-to-end task-oriented dialog. <ref type="bibr" target="#b12">Madotto et al. (2018)</ref> combines end-toend memory network <ref type="bibr" target="#b15">(Sukhbaatar et al., 2015)</ref> into sequence generation. Gangi <ref type="bibr" target="#b4">Reddy et al. (2019)</ref> proposes a multi-level memory architecture which first addresses queries, followed by results and finally each key-value pair within a result. <ref type="bibr" target="#b17">Wu et al. (2019a)</ref> proposes a global-to-locally pointer mechanism to query the knowledge base. Compared with their models, our framework can not only explicitly utilize domain-specific knowledge but also consider different relevance between each domain. Another series of work trains a model on each domain separately.  leverages dialogue state representation to retrieve the KB implicitly. <ref type="bibr" target="#b14">Qin et al. (2019b)</ref> first adopts the KB-retriever to explicitly query the knowledge base. Their works consider only domain-specific features. In contrast, our framework explicitly leverages domain-shared features across domains.</p><p>The shared-private framework has been explored in many other task-oriented dialog components. Liu and Lane (2017) applies a shared-private LSTM to generate shared and domain-specific features. <ref type="bibr" target="#b21">Zhong et al. (2018)</ref> proposes a global-local architecture to learn shared feature across all slots and specific feature for each slot. More recently, <ref type="bibr" target="#b20">Zhang et al. (2018)</ref> utilizes the shared-private model for text style adaption. In our work, we explore shared-private framework in end-to-end taskoriented dialog to better transfer domain knowledge for querying knowledge base. In addition, we take inspiration from <ref type="bibr" target="#b6">Guo et al. (2018)</ref>, who successfully apply the mix-of-the-experts (MoE) mechanism in multi-sources domain and cross-lingual adaption tasks. Our model not only combines the strengths of MoE to incorporate domain-specific feature, but also applies adversarial training to encourage generating shared feature. To our best of knowledge, we are the first to effectively explore shared-private framework in multi-domain end-toend task oriented dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose to use a shared-private model to investigate explicit modeling domain knowledge for multi-domain dialog. In addition, a dynamic fusion layer is proposed to dynamically capture the correlation between a target domain and all source domains. Experiments on two datasets show the effectiveness of the proposed models. Besides, our model can quickly adapt to a new domain with little annotated data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>A.1 Hyperparameters Setting</p><p>The hyperparameters used for SMD and Multi-WOZ 2.1 dataset are shown in <ref type="table" target="#tab_9">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Basic Loss Function</head><p>The loss L basic used in our Shared-Private Encoder-Decoder Model is the same as GLMP. Different with the standard sequence-to-sequence with attention mechanism model, we use [h f dec,t , h f dec,t ] to replace [h dec,t , h dec,t ] and then get the sketch word probability distribution P vocab t . Based on the gold sketch response Y s = (y s 1 , . . . , y s n ), we calculate the standard cross-entropy loss L v as follows:</p><formula xml:id="formula_22">P vocab t = Softmax(U [h f dec,t , h f dec,t ]),<label>(19)</label></formula><p>L v = n t=1 โlog(P vocab t (y s t )).</p><p>(20)</p><p>Given the system response Y , we get the global memory pointer label sequence G label = (ฤ 1 , . . . ,ฤ b+T ) and local memory pointer label sequence L label = (l 1 , . . . ,l n ) as follows:</p><formula xml:id="formula_23">g i = 1 if Object(m i ) โ Y 0 otherwise ,<label>(21)</label></formula><formula xml:id="formula_24">l t = max(z) if โz s.t. y t = Object(m z ) b + T + 1 otherwise ,<label>(22)</label></formula><p>where m i represents one triplet in the external knowledge M = [B; X] = (m 1 , . . . , m b+T ) and Object(ยท) function is denoted as getting the object word from a triplet. Then, the L g can be written as follows:</p><formula xml:id="formula_25">L g = โ b+T i=1</formula><p>(ฤ i ยท log g i + (1 โฤ i ) ยท log (1 โ g i )) .</p><p>Based on the L label and P t = (p k 1 , . . . , p k b+T ), we can calculate the standard cross-entropy loss L l as follows:</p><formula xml:id="formula_27">L l = n t=1</formula><p>โ log(P t (l t )).</p><p>( <ref type="formula" target="#formula_1">24)</ref> Finally, L basic is the weighted-sum of three losses:</p><formula xml:id="formula_28">L basic = ฮณ g L g + ฮณ v L v + ฮณ l L l ,<label>(25)</label></formula><p>where ฮณ g , ฮณ v and ฮณ l are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Adversarial Training</head><p>We apply a Convolutional Neural Network (CNN) as domain classifier both in the shared encoder and shared decoder to identify the domain of shared representation of dialogue history H s enc and response H s dec . Take the encoder for example, based on the H s enc , we can extract the context representation c s enc by CNN and then ฮฒ enc โ R |D| can be calculated as follows:</p><formula xml:id="formula_29">ฮฒ enc = Sigmoid(LeakyReLU(W enc (c s enc )),<label>(26)</label></formula><p>Then we train the domain classifier by optimizing its parameters ฮธ d to minimize the sequencelevel binary cross-entropy loss L adv enc as follows:</p><formula xml:id="formula_30">max ฮธs min ฮธ d L adv enc = โ |D| i=1</formula><p>(e i ยท log(ฮฒ enc,i |ฮธ s , ฮธ d ) +(1 โ e i ) ยท log(1 โ ฮฒ enc,i |ฮธ s , ฮธ d )),</p><p>where ฮฒ enc,i represents the probability of the input dialogue history belongs to the domain d i . Similarly, we can get the L adv dec and sum up them as: L adv = L adv enc + L adv dec . In order to update the encoder-decoder model parameters ฮธ s underlying the domain classifier, we introduce the gradient reversal layer to reverse the gradient direction which trains our model to extract domain-shared features to confuse the classifier. On the one hand, we train the domain classifier to minimize the domain classification loss. On the other hand, we update the parameters of the network underlying the domain classifier to maximize the domain classification loss, which works adversarially towards the domain classifier. This encourages that our shared encoder and decoder are trained to extract domain-shared features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Workflow of our baseline and our proposed model. hidden states H = (h 1 , h 2 , ..., h T ):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The dynamic fusion layer for fusing domainshared feature and domain-specific feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Eric et al. (2017), Madotto et al. (2018) and Wu et al. (2019a) on SMD and (Budzianowski et al., 2018) on Multi-WOZ 2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Performance of domain adaption on different subsets of original training data. Zero-shot performance (F1 score) on each domain on SMD dataset. The x-axis domain name represents that the domain is unseen and other two domains is the same as original dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Distribution of Mix-of-the-expert mechanism across source domains for randomly selected 100 examples in each domain on SMD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Case of of expert gate distribution in SMD dataset. Text segments with red color represents appearing in both schedule and navigation domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>No Domain-Specific Features</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3">No Domain-Shared Features</cell><cell></cell><cell cols="3">Basic Shared-Specific Fusion Mechanism</cell><cell>Dynamic Fusion Mechanism</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Shared-Specific Fusion</cell><cell></cell><cell>Shared-Specific Fusion</cell></row><row><cell>Shared Model</cell><cell>A Model</cell><cell>B Model</cell><cell>C Model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dynamic Domain-Specific Fusion</cell></row><row><cell>Shared Module</cell><cell>A Module</cell><cell>B Module</cell><cell>C Module</cell><cell>Shared Module</cell><cell>A Module</cell><cell>B Module</cell><cell>C Module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shared Module</cell><cell>A Module</cell><cell>B Module</cell><cell>C Module</cell></row><row><cell>Mixed Data</cell><cell>Domain A</cell><cell>Domain B</cell><cell>Domain C</cell><cell>Mixed Data</cell><cell>Domain A</cell><cell>Domain B</cell><cell>Domain C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixed Data</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Main results. The numbers with * indicate that the improvement of our framework over all baselines is statistically significant with p &lt; 0.05 under t-test.</figDesc><table><row><cell>Model</cell><cell cols="2">Entity F1 (%) Test โ</cell></row><row><cell>Full model</cell><cell>62.7</cell><cell>-</cell></row><row><cell cols="2">w/o Domain-Shared Knowledge Transfer 59.0</cell><cell>3.7</cell></row><row><cell>w/o Dynamic Fusion Mechanism</cell><cell>60.9</cell><cell>1.8</cell></row><row><cell>w/o Multi-Encoder</cell><cell>61.0</cell><cell>1.7</cell></row><row><cell>w/o Multi-Decoder</cell><cell>58.9</cell><cell>3.8</cell></row><row><cell>w/o Adversarial Training</cell><cell>61.6</cell><cell>1.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Ablation tests on the SMD test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Find location and address to home that is nearest me. Your home is at 56 cadwell street. Thanks, set the gps for there.</figDesc><table><row><cell>Weather@5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Schedule@5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dialogue History:</cell><cell></cell><cell>1</cell><cell></cell><cell>0.7762</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>0.1996</cell><cell>0.0242</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Navigation Weather Schedule</cell></row><row><cell>Weather@5%</cell><cell>1</cell><cell></cell><cell></cell><cell>0.8527</cell></row><row><cell>Driver: Manhattan, please will it be cloudy on Monday ? Car: Monday will be foggy.</cell><cell>0.5 0</cell><cell>0.0015</cell><cell>0.1457</cell></row><row><cell></cell><cell></cell><cell cols="3">Navigation Weather Schedule</cell></row></table><note>Response: No problem.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters we use for SMD and Multi-WOZ 2.1 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The constructed datasets will be publicly available for further research.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For Multi-WOZ 2.1 dataset, most dialogs are supported by more than single row, which can not processed by KBretriever, so we compare our framework with it in SMD and Camrest datasets.3 Note that, we find that<ref type="bibr" target="#b17">Wu et al. (2019a)</ref> report macro entity F1 as the micro F1, so we rerun their models (https://github.com/jasonwu0731/GLMP) and obtain results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Min Xu, Jiapeng Li, Jieru Lin and Zhouyang Li for their insightful discussions. We also thank all anonymous reviewers for their constructive comments. This work was supported by the National Natural Science Foundation of China (NSFC) via grant 61976072, 61632011 and 61772153. Besides, this work also faxed the support via Westlake-BrightDreams Robotics research grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MultiWOZ -a large-scale multi-domain wizard-of-Oz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweล</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iรฑigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaลกiฤ</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1547</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5506</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrรผcken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="468" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spain</forename><surname>Valencia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-level memory for task oriented dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Revanth Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1375</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3744" to="3754" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jรผrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-domain adversarial learning for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entityconsistent end-to-end task-oriented dialogue system with KB retriever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning for task-oriented dialogue with dialogue state representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3781" to="3792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global-to-local memory pointer networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shared-private LSTM for multidomain text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32236-6_10</idno>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing -8th CCF International Conference, NLPCC 2019</title>
		<meeting><address><addrLine>Dunhuang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-09" />
			<biblScope unit="page" from="116" to="128" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2012.2225812</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SHAPED: Shared-private encoder-decoder for text style adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1528" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive encoder for dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1458" to="1467" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
