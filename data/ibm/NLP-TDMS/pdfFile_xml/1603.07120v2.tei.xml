<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
							<email>ttng@i2r.a-star.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
							<email>ygong@mail.xjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xián Jiaotong University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>wanggang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single modality action recognition on RGB or depth sequences has been extensively explored recently. It is generally accepted that each of these two modalities has different strengths and limitations for the task of action recognition. Therefore, analysis of the RGB+D videos can help us to better study the complementary properties of these two types of modalities and achieve higher levels of performance. In this paper, we propose a new deep autoencoder based sharedspecific feature factorization network to separate input multimodal signals into a hierarchy of components. Further, based on the structure of the features, a structured sparsity learning machine is proposed which utilizes mixed norms to apply regularization within components and group selection between them for better classification performance. Our experimental results show the effectiveness of our crossmodality feature analysis framework by achieving state-ofthe-art accuracy for action classification on five challenging benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent development of range sensors had an indisputable impact on research and applications of machine vision. Range sensors provide depth information of the scene and objects, which helps in solving problems that are considered hard for RGB inputs <ref type="bibr" target="#b14">[15]</ref>.</p><p>Human activity recognition is one of the active fields in computer vision and has been explored extensively. Recent advances in hand-crafted <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b41">42]</ref> and convnet-based <ref type="bibr" target="#b55">[56]</ref> feature extraction and analysis of RGB action videos <ref type="bibr" target="#b0">1</ref> Gang Wang is the corresponding author. achieved impressive performance. They generally recognize action classes based on appearance and motion patterns in videos. The major limitation of RGB sequences is the absence of 3D structure from the scene. Although some works are done towards this direction <ref type="bibr" target="#b9">[10]</ref>, recovering depth from RGB in general is an underdetermined problem. As a result, depth sequences provide an exclusive modality of information about the 3D structure of the scene, which suits the problem of activity analysis <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b82">83]</ref>. This complements the textural and appearance information from RGB. Our goal in this work is to analyze the multimodal RGB+D signals for identifying the strengths of respective modalities through teasing out their shared and modality-specific components and to utilize them for improving the classification of human actions.</p><p>Having multiple sources of information, one can find a new space of common components which can be more robust than any of the input features. Through linear projections, canonical correlation analysis (CCA) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> gives us the correlated form of input modalities which in essence is a robust representation of multimodal signals. However, the downside of CCA is the linearity limitation. Kernel canonical correlation analysis (KCCA) <ref type="bibr" target="#b24">[25]</ref> extended this idea into nonlinear kernel-based projections, which is still limited to the representation capacity of the kernel's space and is not able to disentangle the high-level nonlinear complexities between the input modalities. Further, the traditional solutions of CCA and KCCA are to solve the maximization of correlation between the projected vectors analytically, which does not scale well with the size of the data.</p><p>To overcome these limitations, a new deep autoencoderbased nonlinear common component analysis network is proposed to discover the shared and informative components of input RGB+D signals.</p><p>Besides the shared components, each input modality has specific features which carry discriminative information for the recognition task. In this respect, we can enhance the representation by incorporating the modality-specific components of respective modalities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51]</ref>. Based on this intuition, at each layer our deep network factorizes the multimodal input features into their shared and modality-specific components. By stacking such layers, we further decode the complex and highly nonlinear representations of the input modalities in a nonlinear fashion.</p><p>Across the layers, our deep multimodality analysis extracts a set of structured features which consist of hierarchically factorized multimodal components. The common components are robust against noise and missing information between the modalities, and the modality-specific components carry the remaining informative features which are irrelevant to the other modality. To effectively perform recognition tasks on our structured features, we design a structured sparsity-based learning framework. With different mixed norms, features of each component can be grouped together and group selection can be applied to learn a better classifier. We also show that the advantage of our learning framework is more significant as network gets deeper.</p><p>The contributions of this work are two-fold: first we introduce a new deep learning network for hierarchical shared-specific factorization of RGB+D features. Second, a structured sparsity learning machine is proposed to explore the structure of hierarchical factorized representations for effective action classification.</p><p>The rest of this paper is organized as follows. Section 2 explores the related work. Section 3 introduces the proposed deep component factorization network. Section 4 describes our classification framework for factorized components. Section 6 provides our experimental results, and section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are other works which applied deep networks to multimodal learning. The work in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b59">60]</ref> used DBM for finding a common space representation for two input modalities, and predict one modality from the other. Andrew et al. <ref type="bibr" target="#b0">[1]</ref> proposed a deep canonical correlation analysis network with two stacks of deep embedding followed by a CCA on top layer. Our method is different from these works in two major aspects. First, the previous work performed the multimodal analysis in just one layer of the deep network, but our proposed method performs the common component analysis in every single layer. Second, we incorporate modality-specific components in each layer to maintain all the informative features, at each layer.</p><p>Jia et al. <ref type="bibr" target="#b20">[21]</ref> factorized the input features to shared and private components by applying structured sparsity, for the task of multi-view learning on human pose estimation, with linearity assumption. Cai et al. <ref type="bibr" target="#b5">[6]</ref> proposed a nonlinear factorization of the features into common and individual components, towards a better representation of features for action recognition. They utilized mixture models to add nonlinearity to linear probabilistic CCA <ref type="bibr" target="#b1">[2]</ref>. Our proposed technique stacks layers of nonlinear shared component analysis to progressively disentangle highly nonlinear correlations between the input features.</p><p>While learning frameworks in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> applied structured sparsity for other similar tasks, our structured sparsity learning machine extends the sparse selection into two levels of concurrent component and layer selection, which is more suited to the hierarchical outputs from our deep factorization network.</p><p>Recent single modality action recognition methods on depth signals can be divided into two major groups: depth map analysis methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b78">79]</ref> and skeleton based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref>.</p><p>The first group extract the action descriptors directly from depth map sequences. The idea of spatio-temporal interest points <ref type="bibr" target="#b25">[26]</ref> was applied in depth videos by <ref type="bibr" target="#b78">[79]</ref>. They also proposed depth cuboid similarity features to represent local patches. HON4D <ref type="bibr" target="#b40">[41]</ref> represents depth sequences as histograms of 4D oriented normals of local patches, quantized on the vertices of a regular polychoron. Rahmani et al. <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b44">45]</ref> achieved higher levels of robustness against viewpoint variations by using histograms of oriented principle components. Lu et al. <ref type="bibr" target="#b34">[35]</ref> proposed binary range-sample descriptors based on τ tests on depth patches. The work of <ref type="bibr" target="#b76">[77]</ref> applied convolutional networks for learning action classes on depth maps. Rahmani and Mian <ref type="bibr" target="#b47">[48]</ref> introduced a nonlinear knowledge transfer model to transform different views of human actions to a canonical view.</p><p>The second group of methods represent actions based on the 3D positions of major body joints, which are available for most of depth based action datasets. Luo et al. <ref type="bibr" target="#b35">[36]</ref> proposed a novel skeleton-based discriminative dictionary learning method, utilizing group sparsity and geometry constraints. Vemulapalli et al. <ref type="bibr" target="#b64">[65]</ref> represented skeletons as points and actions as curves in a Lie group using the 3D relative geometry between body parts. Evangelidis et al. <ref type="bibr" target="#b10">[11]</ref> proposed a compact and view-invariant representation of body poses calculated from joint positions. Wang et al. <ref type="bibr" target="#b77">[78]</ref> introduced a mining technique to find partbased mid-level patterns (frequent local parts) and aggregated the local representations as bag-of-FLPs to be classified by a SVM. Veeriah et al. <ref type="bibr" target="#b63">[64]</ref> extended the structure of the long short-term memory (LSTM) units <ref type="bibr" target="#b17">[18]</ref> to learn differential patterns in skeleton locations. The work of <ref type="bibr" target="#b8">[9]</ref> introduced a hierarchy of recurrent networks to learn partbased motion patterns and combine them for action classification. Zhu et al. <ref type="bibr" target="#b88">[89]</ref> proposed a new regularization term for learning co-occurrences of motion patterns among different joint groups. The work of <ref type="bibr" target="#b52">[53]</ref> introduced a new part-aware LSTM structure to discover the long-term motion patterns of skeleton-based body parts separately and learn the action classes based on these representations. In <ref type="bibr" target="#b72">[73]</ref>, motion and local depth based appearance of each body joint was encoded using Fourier transform over a temporal pyramid. They also proposed a mining method to find the set of most representative body joints for each action class. Shahroudy et al. <ref type="bibr" target="#b53">[54]</ref> formulated the discriminative joint selection by introducing a hierarchical mixed norm. The work of <ref type="bibr" target="#b45">[46]</ref> combined different spatio-temporal depth and skeleton based gradient features and applied a random decision forest for action classification. Meng et al. <ref type="bibr" target="#b36">[37]</ref> proposed a real-time action recognition method by applying random forest classifier on a set of distance values between the body joints and interacted objects. Wang and Wu <ref type="bibr" target="#b73">[74]</ref> applied max-margin time warping to match the descriptors of skeletons over the temporal axis and learn phantom templates for each action class. An extensive review of different approaches and techniques on 3D skeletal data is done by <ref type="bibr" target="#b13">[14]</ref>. The fusion of various depth based features is also studied by <ref type="bibr" target="#b89">[90]</ref>.</p><p>Multimodality analysis of RGB+D action videos is studied by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b33">34]</ref>. Ni et al. <ref type="bibr" target="#b38">[39]</ref> introduced a RGB+D fusion method by concatenating depth descriptors to RGB based representations of STIP points. Liu and Shao <ref type="bibr" target="#b33">[34]</ref> introduced a genetic programming framework to improve the RGB and depth descriptors and their fusion simultaneously through an iterative evolution. The work of <ref type="bibr" target="#b58">[59]</ref> solved the problem of RGB+D action recognition by utilizing RGB information for better tracking of interest point trajectories and describe them by depthbase local surface patches. Hu et al. <ref type="bibr" target="#b19">[20]</ref> proposed a heterogeneous multitask feature learning framework to mine shared and modality-specific RGB+D features. The work of <ref type="bibr" target="#b21">[22]</ref> applied projection matrices to the common and independent spaces between RGB and depth modalities. They learned their model by minimizing the rank of their proposed low-rank bilinear classifier.</p><p>The work of <ref type="bibr" target="#b86">[87]</ref> also extracted STIPs from RGB and combined their HOG and HOF descriptors from RGB channel with local depth patterns (LDP) features from depth channel to fuse the two modalities. Depth-induced multiple channel STIPs <ref type="bibr" target="#b87">[88]</ref>, also added depth distances into GMM-based STIP representations. In <ref type="bibr" target="#b30">[31]</ref> the STIP detection is done separately on RGB and depth and the HOGHOF descriptors are fused by combining the BOW representations of LLC codes of local features. Tsai et al. <ref type="bibr" target="#b62">[63]</ref> used depth channel to segment the human body into known parts. STIPs with descriptors on RGB and depth channels are aggregated for each part by BOF representation over temporal pyramids. They assigned higher weights to non-occluded body parts to achieve a more robust global representations for action recognition. Multistream fused hidden Markov model was utilized to fuse pixel change history feature from RGB with MHI feature from depth channel by <ref type="bibr" target="#b22">[23]</ref>. The work of <ref type="bibr" target="#b54">[55]</ref> proposed a structured sparsity based fusion for RGB+D local descriptors. An evolutionary programming RGB+D fusion method was proposed by <ref type="bibr" target="#b33">[34]</ref>. The proposed RGB+D analysis frameworks, are different from these methods, since our focus is on studying the correlation between the two modalities in the local level features and factorizing them to their correlated and independent components.</p><p>Recent advances of visual recognition in digital images using deep convolutional networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b12">13]</ref> also inspired the research in video analysis. Ng et al. <ref type="bibr" target="#b83">[84]</ref> studied two techniques of feeding videos to convnets for video classification. They proposed temporal pooling of the convnetbased features of frames to aggregate video descriptors from frame features. They also studied the advantages of utilizing a long short-term memory <ref type="bibr" target="#b17">[18]</ref> network stacked over a convnet for video classification. Simonyan and Zisserman <ref type="bibr" target="#b55">[56]</ref> fed a fixed length video sequence and its optical flow to a two-streamed convnet and fused the scores of the two streams in the end to classify the action labels. Wang et al. <ref type="bibr" target="#b75">[76]</ref> combined the advantages of hand-crafted trajectory-based features and deep convnet learning based methods by applying <ref type="bibr" target="#b55">[56]</ref>'s network along the motion trajectories of input videos. A novel deep convoultional framework for video event detection and evidence recounting was proposed by <ref type="bibr" target="#b11">[12]</ref>. They introduced a back pass technique to localize the key evidences of the interested events in spatialtemporal domain. Tran et al. <ref type="bibr" target="#b61">[62]</ref> studied the fully three dimensional convnet based video analysis and evaluated their proposed framework on various video analysis tasks including action recognition.</p><p>The applications of recurrent neural networks for 3D human action recognition were explored very recently. Du et al. <ref type="bibr" target="#b8">[9]</ref> applied a hierarchical RNN to discover common 3D action patterns in a data-driven learning method. They divided the input 3D human skeletal data to five groups of joints and fed them into a separated bidirectional RNN. The output hidden representation of the first layer RNNs were concatenated to form upper-body and lower-body mid-level representation and these were fed to the next layer of bidirectional RNNs. The holistic representation for the entire body was obtained by concatenating the output hidden representations of these second layer RNNs and it was fed to the last RNN layer. The output hidden representation of the final RNN was fed to the softmax classifier for action classification. Differential RNN <ref type="bibr" target="#b63">[64]</ref> added a new gating mechanism to the traditional LSTM to extract the derivatives of internal state (DoS). The derived DoS was fed to the LSTM gates to learn salient dynamic patterns in 3D skele- <ref type="figure">Figure 1</ref>. Illustration of the proposed single layer shared-specific component analysis. Xr and X d are input RGB and depth based features. We factorize each input feature into shared (Y) and specific (Z) components by forcing the Y vectors to be close, and the input features to be reconstructible from derived components.</p><formula xml:id="formula_0">X r X d Y r Y d Z r Z d X r X d W r W d V r V d Q r Q d U r U d</formula><p>ton data. The work of <ref type="bibr" target="#b88">[89]</ref> introduced an internal dropout mechanism applied to LSTM gates for stronger regularization in the RNN-based 3D action learning network. To further regularize the learning, a co-occurrence inducing norm was added to the network's cost function which enforced the learning to discover the groups of co-occurring and discriminative joints for better action recognition. Liu et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> extended the recurrent network based sequence analysis towards sequences of body joints. They added a new dimension dimension to the structure of a deep LSTMbased framework to learn the features over time and over the sequences of joints concurrently. To apply ConvNetbased learning to this domain, <ref type="bibr" target="#b48">[49]</ref> used synthetically generated data and fitted them to real mocap data. Their learning method was able to recognize actions from novel poses and viewpoints. Different from other methods, the proposed framework analyzes the components between the two modalities in a deep network, and factorizes the input RGB+D features into their shared and specific components in a hierarchy of nonlinear layers. Our solution is general and can be applied on any type of multimodal features to analyze their crossmodality components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep shared-specific component analysis</head><p>We have two sets of features extracted from different modalities of data (RGB and depth signals) as our input for the task of action classification. State-of-the-art RGB based features <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b70">71]</ref> include 2D motion patterns and appearance information of objects and scenes. On the other hand, various depth-based features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b78">79]</ref> encode 3D shape and motion information, without appearance and texture details. Consequently, it is beneficial to fuse the complementary RGB and depth-based features for better performance in action analysis.</p><p>There are different techniques for feature fusion. The choice of fusion strategy should rely on dependency of features. When features have very high dependency, descriptor-level fusion gives the best outcome, and when multiple groups of features have very low interdependency, kernel-level fusion performs better <ref type="bibr" target="#b50">[51]</ref>. Since RGB and depth based features encode an entangled combination of common and modality-specific information of the observed action, they are neither independent nor fully correlated. Therefore, it is reasonable to embed the input data into a space of factorized common and modality-specific components. The combination of the shared and specific components in input features can be very complex and highly nonlinear. To disentangle them, we stack layers of nonlinear autoencoder-based component factorization to form a deep shared-specific analysis network.</p><p>In this section, we first introduce our basic framework of shared-specific analysis for multimodal signal factorization, then describe the deep network of stacked layers, where each layer performs factorization analysis and collectively produce a hierarchical set of shared and modality-specific components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single layer shared-specific analysis</head><p>Let us notate input RGB features by X r and depth features by X d . We propose to factorize each input feature pattern into two spaces: first, common component space which corresponds to the highest correlation with the other modality (Y r , Y d ), and second, its modality-specific fea-</p><formula xml:id="formula_1">ture component space (Z r , Z d ):     Y r Y d Z r Z d     = g(X r , X d ; Ω) (1)</formula><p>where Ω is the set of model parameters that will be learned from the training data. We propose a sparse autoencoderbased network as the g(.) function, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>Feature vectors of each modality are factorized into Y and Z which represent shared and individual components of each modality respectively. Each component is derived from a linear projection of the input features followed by a nonlinear activation. Mathematically:</p><formula xml:id="formula_2">Y r = f (W r X r + b Yr 1 n ) (2) Z r = f (V r X r + b Zr 1 n )<label>(3)</label></formula><p>in which f (.) is a nonlinear activation function. We use sigmoid scaling in our implementation. b Yr and b Zr are bias terms. Similarly, for the depth based input, we have:</p><formula xml:id="formula_3">Y d = f (W d X d + b Y d 1 n ) (4) Z d = f (V d X d + b Z d 1 n )<label>(5)</label></formula><p>To prevent output degeneration, we expect the original features to be reconstructible from their factorized compo-nents <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_4">X r = f ( Q r U r Y r Z r + b Xr 1 n ) = f (Q r Y r + U r Z r + b Xr 1 n ) (6) X d = f ( Q d U d Y d Z d + b X d 1 n ) = f (Q d Y d + U d Z d + b X d 1 n )<label>(7)</label></formula><p>Now we can formulate the desired component factorization into an optimization problem with the cost function:</p><formula xml:id="formula_5">Ω * = argmin Ω ∆(Y r , Y d ) + λ Ω 2 + ζ r ∆(X r , X r ) + ζ d ∆(X d , X d ) + α r Ψ(Y r ; ρ Y ) + α d Ψ(Y d ; ρ Y ) + β r Ψ(Z r ; ρ Z ) + β d Ψ(Z d ; ρ Z ) (8)</formula><p>where Ω = {W . , V . , Q . , b . } is the set of all parameters, and [λ, ζ . , α . , β . ] are hyper-parameters of trade-off between terms.</p><p>The first term in <ref type="bibr" target="#b7">(8)</ref> forces the shared components of the two modalities (Y r and Y d ) to be as close as possible. We formulate this term as the Frobenius norm of the difference between two matrices:</p><formula xml:id="formula_6">∆(Y r , Y d ) = Y r − Y d F (9)</formula><p>The second term is the general weight regularization term, applied on the projection weights to prevent networks from overfitting training data.</p><p>The reconstruction costs are represented as ∆(X r , X r ) and ∆(X d , X d ) to prevent the model from degeneration. Here, we use Frobenius norm (the same as <ref type="formula">(9)</ref>) of the reconstruction error for the reconstruction cost term.</p><p>Last four terms of (8) are sparsity penalty terms over Y and Z outputs. It has been shown in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">50]</ref> that applying sparsity on the features of Y and Z will help to improve the learning capability, especially when components are overcomplete. As our sparsity penalty, we use KL-divergence term, applied between Y components and the sparsity parameters ρ Y , as well as Z and ρ Z .</p><p>It is worth pointing out, since the proposed framework is built on a sparse autoencoder-like scheme and has sigmoid scaling nonlinearity, it is necessary to apply PCA whitening on the input matrices X r and X d and scale their elements into the range of [0, 1].</p><p>In this formulation, the disparity between Z d and Z r components is applied implicitly. The similarity inducing norm pushes the common components of the two modalities to move inside Y components. Therefore, we expect the remaining features in each of Z components to be highly different across the modalities. <ref type="figure">Figure 2</ref>. Cascading factorization layers to a deep shared-specific network. To disentangle the highly nonlinear combination of shared-specific components, factorization layers are stacked by feeding the Y components of each layer as inputs of the next layer.</p><formula xml:id="formula_7">X r X d Y 1 r Y 1 d Z 1 r Z 1 d Y 2 r Y 2 d Z 2 r Z 2 d Y l r Y l d Z l r Z l d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep shared-specific component analysis</head><p>State-of-the-art RGB and depth based features for action recognition, are extracted by multiple linear and nonlinear layers of projection, embedding, spatial and temporal pooling, or statistical distribution encodings, e.g. BOvW <ref type="bibr" target="#b57">[58]</ref> and FV <ref type="bibr" target="#b42">[43]</ref> or Fourier temporal pyramids in <ref type="bibr" target="#b72">[73]</ref>. Hence the common components between modalities can lie on highly complex and nonlinear subspaces of input data, and one layer of the proposed shared-specific analysis cannot decode these complexities between the components.</p><p>By cascading multiple shared-specific analysis layers, we build a deep network to further factorize input features based on their higher orders of common and private information between modalities. To do so, we feed Y components of the previous layer as multimodal inputs of the current layer and apply the same method with new learning parameters in order to further factorize the features. As illustrated in <ref type="figure">Figure 2</ref>, each layer extracts modality-specific components of the modalities and passes the shared ones for further factorization in the next layer:</p><formula xml:id="formula_8">     Y (i) r Y (i) d Z (i) r Z (i) d      = g(X r , X d ; Ω (i) ) if i = 1 g(Y (i−1) r , Y (i−1) d ; Ω (i) ) if i &gt; 1<label>(10)</label></formula><p>By applying this hierarchy on nonlinear layers, we expect the network to factorize more complex and higher order components of the inputs as it moves forward through the layers. Our deep network is trained greedily and layerwise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>. In other words, the optimization of each layer is started upon the convergence of the previous layer's training.</p><p>Upon training of the deep network, each input sample will be factorized into a pair of specific components (Z</p><formula xml:id="formula_9">(i) r , Z (i) d ) for each layer i ∈ [1, .., l], plus the concate- nation of last layer's shared components (Y (l) r , Y (l) d ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional shared-specific component analysis</head><p>The input features (X r , X d ) of the proposed deep shared-specific component analysis network are assumed to describe different representations of a multimodal entity. In our application, this entity is a RGB+D human action video and the inputs are RGB-based and depth-based features of it.</p><p>Since every input video can be regarded as a three dimensional cube (in x, y, t), it can be split to sub-cubes along all of its three dimensions, and the proposed multimodal analysis can be done on each of these sub-cubes separately. By limiting our analysis into holistic RGB+D features, we may lose discriminative local information in both modalities, because local features also have dependencies across modalities and their deep shared-specific component analysis (DSSCA) is beneficial. Therefore, as depicted in <ref type="figure">Fig</ref> One can think of this stage as applying the same DSSCA L network on all the sub-cubes of every input RGB+D video. At each step, we have a fixed-sized window over the current sub-cube in both RGB and depth channels of the input video and feed their corresponding sub-video representations to the DSSCA L network as a single training sample. By convolving this window over all of the possible sub-cubes of every input video sample, we train the DSSCA L network.</p><p>The learned DSSCA L is then utilized to decompose the multimodal features of all the convolved sub-cubes. For every input video sample, we concatenate all of the factorized components of its sub-cubes. The resulting representation is then put together with the holistic multimodal features of video sample, to build the input for the holistic DSSCA network (DSSCA H ), similar to <ref type="bibr" target="#b26">[27]</ref>. The inputs of DSSCA H are PCA whitened and scaled into the range of [0, 1].</p><p>Overall, we have L = l 1 + l 2 layers of factorization where l 1 and l 2 are the number of layers in DSSCA L and DSSCA H networks respectively. By applying the trained local-holistic networks into the features of each video sample, we have a set of 2L + 1 independent components:</p><formula xml:id="formula_10">A = {(Z 1 r ) T , (Z 1 d ) T , ..., (Z L r ) T , (Z L d ) T , (Y L ) T } T (11) where Y L = Y L r Y L d</formula><p>is the concatenation of last layer's common components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization algorithm</head><p>The proposed formulation of cost function <ref type="formula">(8)</ref> is not a convex function of training parameters. Therefore, optimization of the learning parameters is not feasible in a single step. We iteratively optimize subsets of the parameters while keeping others fixed to achieve a suboptimal solution which is already shown effective in different applications <ref type="bibr" target="#b79">[80]</ref>.</p><formula xml:id="formula_11">X H r X H d X 1 r PCA X 1 d X 2 r X 2 d X n r X n d DSSCA L DSSCA L DSSCA L ... DSSCA H PCA ... ...</formula><p>Specifically, the learning parameters of each layer can be divided into two subsets. First are the ones defined for projection and reconstruction of the shared components Y . , and second consists of similar parameters for individual component Z . . These two sets are:</p><formula xml:id="formula_12">Ω Y = {W r , W d , Q r , Q d , b Yr , b Y d , b Xr , b X d } (12) Ω Z = {V r , V d , U r , U d , b Zr , b Z d , b Xr , b X d }<label>(13)</label></formula><p>Now, to optimize the overall cost, we first fix Ω Z (except b X. ) and minimize the cost function <ref type="formula">(8)</ref> regarding Ω Y . Then fix parameters of Ω Y (except b X. ) and optimize regarding Ω Z and repeat this iteratively to converge into a suboptimal point.</p><p>In our implementation, all the optimization steps are done by "L-BFGS" algorithm using off-the-shelf "min-Func" software <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structured sparsity learning machine</head><p>Previous shared-specific analysis steps were all unsupervised and applied just based on the mutual characteristics of the two modalities. As a result, the factorized features of each component are not guaranteed to be equally discriminative for the following classification step. Hence we adopt the structured sparsity regularization method of <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b69">70]</ref> aiming to select a number of components/layers sparsely to achieve more robust classification. Since the features of each component are highly correlated, our structured sparsity regularizer bounds the weights of the features inside each component to become activated or deactivated together.</p><p>Mathematically, we want to learn a linear projection matrix B to project our hierarchically factorized features A (see equation <ref type="bibr" target="#b10">11)</ref>, to a class assignment matrix F defined as:</p><formula xml:id="formula_13">f j i = 1 if j th sample belongs to the i th class 0 otherwise<label>(14)</label></formula><p>so that A T B would be as close as possible to F. Each column of A consists of 2L+1 components of features for each training sample. We use the notation A G to denote the rows of A which include the features of component G. Variable G can take values between 1 and 2L + 1 or their corresponding component labels. Correspondingly, columns of B have the same structure, and we denote the G th component's parameters as B G . We refer to the i th column of B as b i which is the projection to our binary classifier for the i th action. Finally b G i refers to the i th column of B G .</p><p>Our classifier is formulated as another optimization problem with the cost function below.</p><formula xml:id="formula_14">B * = argmin B A T B − F 2 F + γ E B G E + γ L B G L + γ W B F<label>(15)</label></formula><p>Component-wise regularizer norm, B G E , groups the weights of each component by applying a 2 norm. Then applies the component selection by a 1 norm over the 2 values of all components. Mathematically:</p><formula xml:id="formula_15">B G E = c i=1 2L+1 G=1 b G i 2 = c i=1 L j=1 b Z j r i 2 + b Z j d i 2 + c i=1 b Y L i 2<label>(16)</label></formula><p>where c is the number of class labels. This mixed norm dictates the component-wise weight learning regarding their discriminative strength for each action class. Since it applies 2 norm inside the components and 1 norm between them, it regularizes the weights within each component, while sparsely selects discriminative components for different classes.</p><p>On the other hand, a layer-wise group selection can also be beneficial, because discriminative features may become factorized in some layers of our hierarchical deep network. Based on this intuition, we apply another group sparsity mixed norm to enforce layer selection. Similar to G E norm, our layer selection norm (G L ) groups the learning parameters corresponding to the components of each layer of the network, and applies 1 sparsity between them:</p><formula xml:id="formula_16">B G L = c i=1 L j=1 b Z j r i b Z j d i 2 + c i=1 b Y L i 2<label>(17)</label></formula><p>The last norm in <ref type="formula" target="#formula_3">(15)</ref> is a general weight decay regularizer to prevent the entire classifier from overfitting.</p><p>Similar to previous section, this optimization is also done using "L-BFGS" algorithm. Upon training the classifier and finding the optimal B * , we classify each testing sample with exemplar features a q as:</p><formula xml:id="formula_17">h(a q ) = argmax c a q , b * c<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CCA-RICA factorization as a baseline method</head><p>As a baseline to the proposed method to perform the shared-specific analysis of the RGB+D inputs, we combined canonical correlation analysis (CCA) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> and reconstruction independent component analysis (RICA) <ref type="bibr" target="#b27">[28]</ref>, to extract correlated and independent components of input features. In this section we describe this baseline method.</p><p>We use the notation X r to represent input local RGB features, and X d for corresponding local depth features. We define the linear projections of the two input features as:</p><formula xml:id="formula_18">Y r = W r,c X r , Y d = W d,c X d<label>(19)</label></formula><p>and to make them maximally correlated we maximize:</p><formula xml:id="formula_19">maximize w j r,c ,w j d,c Corr(Y j r , Y j d ) = Corr(w j r,c X r , w j d,c X d )<label>(20)</label></formula><p>in which superscript j refers to the j th row of the corresponding matrices. Canonical correlation analysis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> solves this analytically as an eigenproblem, in which each eigenvector gives one row of the projection and altogether provides the full projection matrices which lead to the maximum correlation between them.</p><p>Based on our intuition about insufficiency of shared components for recognition tasks, in the second step, we fix correlation projections (W r,c , W d,c ) and apply a reconstruction independent component analysis formulation <ref type="bibr" target="#b27">[28]</ref>, to extract modality-specific components for RGB and depth separately.</p><formula xml:id="formula_20">Z r = W r,i X r , Z d = W d,i X d<label>(21)</label></formula><p>For RGB features we optimize:</p><formula xml:id="formula_21">mininize wr,i λ m X r − X r 2 F + j W j r,i X r 1 where X r = W T r,c , W T r,i W r,c W r,i X r<label>(22)</label></formula><p>Similarly for depth features we optimize:</p><formula xml:id="formula_22">mininize w d,i λ m X d − X d 2 F + j W j d,i X d 1 where X d = W T d,c , W T d,i W d,c W d,i X d<label>(23)</label></formula><p>Upon convergence of <ref type="formula" target="#formula_21">(22)</ref> and <ref type="formula" target="#formula_2">(23)</ref>, the RGB+D features of each trajectory (k) can be represented as a quadruple:</p><formula xml:id="formula_23">{Z r (k), Y r (k), Y d (k), Z d (k)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>This section presents our experimental setup and the results of the proposed methods on three RGB+D action recognition datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental setup</head><p>The proposed methods are evaluated on five RGB+D action recognition datasets. All these datasets are collected using the Microsoft Kinect sensor in an indoor environment <ref type="bibr" target="#b85">[86]</ref>. This sensor captures RGB videos and depth map sequences, and locates the 3D positions of 20 body joints of actors in the scene.</p><p>In our experiments, we try to use features which encode information regarding all the available modalities. From RGB videos, we extract dense trajectories <ref type="bibr" target="#b70">[71]</ref> and use HOG, HOF, MBHX, and MBHY features as trajectory descriptors. To encode the global representation of samples based on their trajectories, we use VQ with 2K codewords, for each descriptor. The final representation of each sample video, is the concatenated max-pooled codes of the four descriptors, over 3 levels of the temporal pyramid. For depth sequences, we use the histograms of oriented 4D normals (HON4D) features <ref type="bibr" target="#b40">[41]</ref>. To explore different setups on each dataset, we extract this feature in different settings. We describe the details in following subsections.</p><p>Since the RGB and depth sequences are not fully aligned and not synced in most of the datasets (all the evaluated ones in this paper, except RGBD-HuDaAct), convolutional cubes have to be large enough so that they mostly cover the same parts of the video between the two modalities. To apply the convolutional network, we consider four temporal quarters of the videos. In this way, each input sample has four temporal segments in our convolutional network and the factorized components of all these segments, together with holistic features of the entire sample are considered as the inputs of the stacked network.</p><p>To cover various aspects of RGB+D motion and appearances of input samples, we used a combination of different features. For depth channel, we extract Fourier coefficients of the joint locations and local occupancy pattern (LOP) features <ref type="bibr" target="#b71">[72]</ref>, histogram of oriented 4D normals (HON4D) <ref type="bibr" target="#b40">[41]</ref>, dynamic skeletons (DS), and dynamic depth patterns (DDP) <ref type="bibr" target="#b19">[20]</ref>. From RGB videos, we extract dynamic color patterns (DCP) <ref type="bibr" target="#b19">[20]</ref> and dense trajectory features <ref type="bibr" target="#b70">[71]</ref>. For depth-based input, we use Fourier coefficients of the joint locations and local occupancy pattern (LOP) features <ref type="bibr" target="#b71">[72]</ref>. The size of X . , Y . , and Z . vectors is fixed as 100 for local features and 200 for holistic and stacked networks in our experiments. On each of the experiments, the optimal values of gammas in SSLM are found via leave-onesample-out cross-validation over training samples.</p><p>To show the effectiveness of our method, we compare it with two baseline methods below:</p><p>Baseline method 1: descriptor level fusion. In this method, we concatenate all the input RGB and depth-based features and train a linear SVM for classification.</p><p>Baseline method 2: kernel level combination. For this baseline method, we calculate the RBF kernel matrices based on all the input RGB and depth-based features and combine them linearly to classify in the form of multikernel SVM. We find the weights of kernels via a brute force search in a cross validation setting using training samples <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the following tables, we report the results of our method in two settings:</p><p>DSSCA Kernel is the kernel combination of the hierarchically factorized components of our shared-specific analysis network.</p><p>DSSCA SSLM: refers to the proposed structured sparsity learning machine based on the hierarchically factorized components described in section 4.</p><p>It is worth mentioning, there are more than 40 datasets specifically for 3D human action recognition. The survey of Zhang et al. <ref type="bibr" target="#b84">[85]</ref> provided a great coverage over the current datasets and discussed their characteristics in different aspects, as well as the best performing methods for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Online RGBD action dataset</head><p>Online RGBD action dataset <ref type="bibr" target="#b82">[83]</ref> is a RGB+D benchmark for action recognition. Unlike most of the other RGB+D benchmarks, this dataset is collected in different locations and provides a cross-environment evaluation setting. It includes samples of 7 daily action classes: drinking, eating, using laptop, reading cellphone, making phone call, reading book, and using remote. For the recognition task, it provides videos of 24 actors. Each actor performs each of  <ref type="table">Table 2</ref>. Performance comparison for holistic network, local network, and stacked local+holistic <ref type="figure" target="#fig_1">(Figure 3</ref>) networks on Online RGBD action datasets. Reported are the results of our method using kernel combination and SSLM. the actions twice. Overall, this dataset include 336 RGB+D video samples. Three different recognition scenarios are defined on this dataset. The first and second scenarios are cross-subject tests. In the first scenario, the first 8 actors are assigned for training and the second 8 actors are for testing. The samples of the second scenario are the same as the first one but training and testing samples are swapped. The third scenario is a cross-environment setting. The videos of the third 8 actors are collected in another location and are considered as test data. The other 16 actors' videos are used for training. The first and second scenarios are cross-subject and the third is a cross-environment evaluation.  <ref type="table">Table 3</ref>. Comparison with a correlation network (without modality-specific components) on the Online RGBD Action dataset, local network, scenario 3. Without Z components, the network is limited to the shared ones and acts similar to CCA.</p><p>select the discriminative components and layers and learns a better classifier. We also compare different structures of our DSSCA network. For each scenario, we report the performance of three structures. "Holistic" refers to the 3-layer deep network applied on holistic features. "Local" is the 2-layer convolutional network applied on local features. "Stacked local+holistic" is the stacked local and holistic networks, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The results are reported in <ref type="table">Table 2</ref>. We conclude that the local and holistic features are complementary and applying stacked local+holistic network can improve the final classification accuracy.</p><p>In our third experiment on this dataset, performance of the proposed networks is compared with a similar network without modality-specific components. The reference network acts similarly to traditional CCA methods. We compare these two networks on the "local" network of third scenario. The result is shown in <ref type="table">Table 3</ref>. We can see including independent components is beneficial and improves the accuracy. Performance of the network with these components is clearly higher. The second observation is our method improves the performance more significantly by having multiple layers. Without having the modality-specific components, the values of common components can not change much, on higher layers. This shows our proposed structure is suitable for cascading more layers and decomposing the features layer by layer. <ref type="table">Table 4</ref> compares our results with the state-of-the-art method on this dataset. Due to the recency of this dataset, only two other works reported results on this dataset. As shown, our method outperforms their results with a large margin, which demonstrates the importance of RGB+D fusion for action recognition as well as the effectiveness of our proposed method for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">MSR-DailyActivity3D dataset</head><p>MSR-DailyActivity dataset <ref type="bibr" target="#b71">[72]</ref> is among the most challenging RGB+D benchmarks for action recognition, which has a high level of intra-class variation and a large number of action classes. It provides 320 RGB+D samples, from 16 classes of daily activities: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lie down on sofa,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Accuracy HOSM <ref type="bibr" target="#b7">[8]</ref> Same environment 49.5% Orderlet <ref type="bibr" target="#b82">[83]</ref> Same env. 71.4% Meng et al. <ref type="bibr" target="#b36">[37]</ref> Same env. <ref type="bibr" target="#b74">75</ref>.8% Proposed DSSCA-SSLM Same env. 94.6% HOSM <ref type="bibr" target="#b7">[8]</ref> Cross env. 50.9% Orderlet <ref type="bibr" target="#b82">[83]</ref> Cross env. 66.1% Proposed DSSCA-SSLM Cross env. 83.8% <ref type="table">Table 4</ref>. Performance comparison of proposed DSSCA with the state-of-the-art results on Online RGBD Action dataset. Same environment setup is the average of S1 and S2 scenarios, and cross environment setup is the same as S3 scenario.   <ref type="table">Table 6</ref>. Performance comparison for holistic network, local network, and stacked local+holistic <ref type="figure" target="#fig_1">(Figure 3</ref>) networks on MSR-DailyActivity3D dataset. Reported are the results of our method using kernel combination and SSLM.</p><p>walk, play guitar, stand up, and sit down. Each action is done by 10 actors, twice by each actor. The standard evaluation on this dataset is defined on a cross-subject setting: first five subjects are used for training and others for testing. Results of the experiments on this benchmark are reported in <ref type="table" target="#tab_3">Tables 5 and 6</ref>. <ref type="table" target="#tab_4">Table 7</ref> also shows the accuracy comparison between the proposed method and the state-of-the-art results reported on this benchmark, in which we reduced the error rate by more than 40% compared to the best reported results so far. This shows our RGB+D analysis method can effectively improve the performance of the action recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">3D action pairs dataset</head><p>3D Action Pairs dataset <ref type="bibr" target="#b40">[41]</ref> is a less challenging RGB+D dataset for action recognition. This dataset provides 6 pairs of action classes: pick up a box/put down a box, lift a box/place a box, push a chair/pull a chair, wear a hat/take off a hat, put on a backpack/take off a backpack, and stick a poster/remove a poster. Each pair of the classes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy HoDG-RDF <ref type="bibr" target="#b45">[46]</ref> 74.5% Bag-of-FLPs <ref type="bibr" target="#b77">[78]</ref> 78.8% HON4D <ref type="bibr" target="#b40">[41]</ref> 80.0% SSFF <ref type="bibr" target="#b54">[55]</ref> 81.9% ToSP <ref type="bibr" target="#b58">[59]</ref> 84.4% RGGP <ref type="bibr" target="#b33">[34]</ref> 85.6% Actionlet <ref type="bibr" target="#b72">[73]</ref> 85.8% SVN <ref type="bibr" target="#b80">[81]</ref> 86.3% BHIM <ref type="bibr" target="#b21">[22]</ref> 86.9% DCSF+Joint <ref type="bibr" target="#b78">[79]</ref> 88.2% MMTW <ref type="bibr" target="#b73">[74]</ref> 88.8% HOPC <ref type="bibr" target="#b44">[45]</ref> 88.8% Depth Fusion <ref type="bibr" target="#b89">[90]</ref> 88.8% MMMP <ref type="bibr" target="#b53">[54]</ref> 91.3% DL-GSGC <ref type="bibr" target="#b35">[36]</ref> 95.0% JOULE-SVM <ref type="bibr" target="#b19">[20]</ref> 95.0% Range-Sample <ref type="bibr" target="#b34">[35]</ref> 95.6% Proposed DSSCA-SSLM 97.5%  <ref type="table">Table 9</ref>. Performance comparison for holistic network, local network, and stacked local+holistic <ref type="figure" target="#fig_1">(Figure 3</ref>) networks on 3D Action Pairs dataset. Reported are the results of our method using kernel combination and SSLM.</p><p>have almost the same set of body motions but in different temporal order. Each action class is captured from 10 subjects, each one 3 times. Overall, this dataset includes 360 RGB+D video samples. The first five subjects are kept for testing and others are for training. <ref type="table" target="#tab_0">Table 10</ref> compares the accuracies between the proposed framework and the state-of-the-art methods reported on this benchmark. Our method ties with two recent works (MMMP <ref type="bibr" target="#b53">[54]</ref>, and BHIM <ref type="bibr" target="#b21">[22]</ref>) in saturating the benchmark by achieving the flawless 100% accuracy on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy DHOG <ref type="bibr" target="#b81">[82]</ref> 66.11% Bag-of-FLPs <ref type="bibr" target="#b77">[78]</ref> 75.56% Actionlet <ref type="bibr" target="#b72">[73]</ref> 82.22% HON4D <ref type="bibr" target="#b40">[41]</ref> 96.67% MMTW <ref type="bibr" target="#b73">[74]</ref> 97.22% HOG3D-LLC <ref type="bibr" target="#b43">[44]</ref> 98.33% HOPC <ref type="bibr" target="#b44">[45]</ref> 98.33% SVN <ref type="bibr" target="#b80">[81]</ref> 98.89% MMMP <ref type="bibr" target="#b53">[54]</ref> 100.0% BHIM <ref type="bibr" target="#b21">[22]</ref> 100.0% Proposed DSSCA-SSLM 100.0% <ref type="table" target="#tab_0">Table 10</ref>. Performance comparison of proposed multimodal correlation-independence analysis with the state-of-the-art methods on 3D Action Pairs dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eval. Baseline DSSCA Dataset</head><p>Method 1 SSLM NTU RGB+D 59.7% 74.9% <ref type="table" target="#tab_0">Table 11</ref>. Comparison of the result of our method with the baseline for the cross-subject evaluation criteria of NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">NTU RGB+D dataset</head><p>NTU RGB+D <ref type="bibr" target="#b52">[53]</ref> is one of the largest scale benchmark dataset for 3D action recognition. It provided 56880 RGB+D video samples of 60 distinct actions. The 60 action classes in NTU RGB+D dataset are: drinking, eating, brushing teeth, brushing hair, dropping, picking up, throwing, sitting down, standing up (from sitting position), clapping, reading, writing, tearing up paper, wearing jacket, taking off jacket, wearing a shoe, taking off a shoe, wearing on glasses, taking off glasses, puting on a hat/cap, taking off a hat/cap, cheering up, hand waving, kicking something, reaching into self pocket, hopping, jumping up, making/answering a phone call, playing with phone, typing, pointing to something, taking selfie, checking time (on watch), rubbing two hands together, bowing, shaking head, wiping face, saluting, putting palms together, crossing hands in front.  <ref type="table" target="#tab_0">Table 12</ref>. Performance comparison for holistic network, local network, and stacked local+holistic <ref type="figure" target="#fig_1">(Figure 3</ref>) networks on the crosssubject evaluation criteria of NTU RGB+D dataset. Reported are the results of our method using SSLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cross-Subject Accuracy HOG 2 <ref type="bibr" target="#b39">[40]</ref> 32.24% Super Normal Vector <ref type="bibr" target="#b80">[81]</ref> 31.82% HON4D <ref type="bibr" target="#b40">[41]</ref> 30.56% Lie Group <ref type="bibr" target="#b64">[65]</ref> 50.08% Skeletal Quads <ref type="bibr" target="#b10">[11]</ref> 38.62% FTP Dynamic Skeletons <ref type="bibr" target="#b19">[20]</ref> 60.23% HBRNN-L <ref type="bibr" target="#b8">[9]</ref> 59.07% P-LSTM <ref type="bibr" target="#b52">[53]</ref> 62.93% ST-LSTM <ref type="bibr" target="#b32">[33]</ref> 69.20% Proposed DSSCA -SSLM 74.86% <ref type="table" target="#tab_0">Table 13</ref>. Performance comparison of proposed multimodal correlation-independence analysis with the state-of-the-art methods on the cross-subject evaluation criteria of NTU RGB+D dataset.</p><p>periments in this section, we limit the depth-based features to Fourier temporal pyramids over skeletons, HON4D and LOP. For RGB-based inputs we use the same set of features used for the other datasets. This dataset suggested two evaluation criteria, crosssubject and cross-view. For the cross-view evaluation, our set of RGB based features perform very poorly and could not contribute powerful enough in the proposed multimodal analysis. Therefore, we evaluate the proposed DSSCA-SSLM framework only on the cross-subject criterion of this dataset.</p><p>Due to the large size of training video samples in this dataset, evaluation of the kernel-based methods (both baseline method 2 and DSSCA-kernel) were not tractable and we only reported the results for baseline method 1 and DSSCA-SSLM frameworks, as provided in <ref type="table" target="#tab_0">Tables 11 and  12</ref>. <ref type="table" target="#tab_0">Table 13</ref> compares the performance of the proposed framework in comparison with other state-of-the-art on this benchmark.  <ref type="table" target="#tab_0">Table 16</ref>. Performance Comparison on RGBD-HuDaAct Dataset classes: exit the room, make a phone call, get up from bed, go to bed, sit down, mop floor, stand up, eat meal, put on jacket, drink water, enter room, take off jacket, and background activity. The standard evaluation on this dataset is defined on a leave-one-subject-out cross-validation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">RGBD-HuDaAct Dataset</head><p>In our experiments we follow the evaluation setup described in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.1">Atomic Local Level Feature Analysis</head><p>Unlike most of the other datasets, this benchmark provides fully synchronized and aligned set of RGB and depth videos. This important characteristic enables us to apply the atomic level of analysis on local RGB and depth features within the video samples. As our atomic local level features, we extract the tracked dense trajectories <ref type="bibr" target="#b70">[71]</ref> in RGB sequences and their HOG, HOF, MBHX, and MBHY descriptors from both modalities.</p><p>To evaluate the effectiveness of the proposed RGB+D analysis, we apply a single layer SSCA to decompose RGB and depth descriptors of the trajectories to their correlated and independent components. For training stage, we sample a set of 40K trajectories from training set. The output of the analysis, which are four factorized components for each trajectory are clustered separately by K-Means with codebook size 1K. LLC coding <ref type="bibr" target="#b74">[75]</ref> and BOF framework are applied on the codes of all the trajectories from each RGB+D video sample to extract their global representations.</p><p>In the final step, a linear SVM is used as the action classifier trained on the extracted global representations of the action video samples.</p><p>We evaluated the performance of canonical correlation analysis (CCA) method also. As a better baseline, we also evaluated added independent components. In our implementation of the CCA-RICA method (section 5) we used the provided codes by the authors of <ref type="bibr" target="#b3">[4]</ref> for CCA and <ref type="bibr" target="#b27">[28]</ref> for RICA.</p><p>All the optimizations in our experiments, are done using "L-BFGS" algorithm. We use the off-the-shelf "minFunc" software released by <ref type="bibr" target="#b51">[52]</ref>. <ref type="table" target="#tab_0">Table 16</ref> shows the results of all the experiments described in this section and compares them with other stateof-the-art methods.</p><p>At first, we evaluated the performance of correlated components of CCA without any modality specific features, which achieves 93.9% outperforming all the reported results on this benchmark. Compared to the accuracy of RGB+D linear coding <ref type="bibr" target="#b30">[31]</ref>, which has the most similar pipeline of action recognition to ours, CCA components shows about two percents improvement. This approves the robustness of shared components and their advantage over using a simple combination of features from the two modalities.</p><p>In the next step, we apply RICA to extract modalityspecific components for RGB and depth local features. Adding specific components improves the accuracy of the classification by 2.5 more percents. This supports our argument about the importance of modality-specific components and their discriminative strengths for action classification. The confusion matrix for this method is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. The majority of the misclassification are caused by the background activity class. This class contains samples</p><formula xml:id="formula_24">A B C D E F G H I J K</formula><p>L M exit the room A 0.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 make a phone call B 0.00 0.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.01 get up from bed C 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 go to bed D 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sit down E 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mop floor F 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 stand up G 0.00 0.04 0.00 0.00 0.06 0.00 0.91 0.00 0.00 0.00 0.00 0.00 0.00 eat meal H 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 put on jacket I 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.98 0.00 0.00 0.02 0.00 drink water J 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.94 0.00 0.00 0.01 enter room K 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 take off jacket L 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.96 0.00 background activity M 0.00 0.13 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.03 0.00 0.00 0.79  L M exit the room A 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 make a phone call B 0.00 0.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 get up from bed C 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 go to bed D 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sit down E 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 mop floor F 0.00 0.00 0.00 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.02 stand up G 0.00 0.02 0.00 0.00 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 eat meal H 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.98 0.00 0.02 0.00 0.00 0.00 put on jacket I 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 drink water J 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.96 0.00 0.00 0.01 enter room K 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 take off jacket L 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.96 0.00 background activity M 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.03 0.00 0.00 0.92 of random motion and other simple activities which are not covered by other 12 classes, like walking around or stay seated without much of motion. Therefore it is inevitable to have some confusion between this class with classes which contain very small amount of clear motion e.g. making a phone call. Similar action classes with reverse temporal order are also mixed up, e.g. sit down and stand up, or put on jacket and take off jacket classes have the same appearance within individual frames, and their only differences are the arrangement of frames over time.</p><p>Next, we evaluate the proposed SSCA method on this atomic local level. SSCA outperforms all other techniques by performing 97.9% of correct classification and achieves the state-of-the-art accuracy on this dataset. Compared to CCA-RICA method, SSCA improves the error rate by more than 40% which is a notable improvement. The confusion matrix of this experiment is also reported in <ref type="figure" target="#fig_4">Figure 5</ref>. Compared to the mixed-up cases of the CCA-RICA method <ref type="figure" target="#fig_2">(Figure 4)</ref>, the confusion patterns are similar but furthered improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.2">Global Level Feature Analysis</head><p>Similar to other datasets reported in the paper, we perform the proposed RGB+D analysis on the global representations extracted from input samples. For RGB signals, the features are HOG, HOF, MBHX, and MBHY descriptors of dense trajectories <ref type="bibr" target="#b70">[71]</ref>, followed by a K-means clustering and locality-constrained linear coding (LLC) <ref type="bibr" target="#b74">[75]</ref> to calculate their global representations as bags-of-features. For depth, we extract HON4D features <ref type="bibr" target="#b40">[41]</ref> for holistic and local depth based features. The results of this experiment are reported in <ref type="table" target="#tab_0">Tables 14 and 15</ref> in a similar evaluation setup to other datasets.</p><p>As can be seen in <ref type="table" target="#tab_0">Table 16</ref>, applying DSSCA analysis in a deep and stacked framework outperforms all the current methods as well as the atomic local level analysis, and achieved the outstanding performance of 99.0% on this benchmark, which shows more than 50% improvement on the error rate compared to the atomic local level SSCA analysis.</p><p>Other reported results are also in accord with our results on other datasets and approve our arguments about the effectiveness of the the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Comparison with single modality</head><p>In <ref type="table" target="#tab_0">Table 17</ref>, we compare our method with baseline method 2, based on single modality features. Since each modality also has holistic and multiple local features, we perform baseline kernel combination to produce the results. For a fair comparison, we use kernel combination for classification based on our factorized components. It is not surprising to observe our method outperforms the baseline, since ours integrates RGB and depth information effectively. <ref type="table" target="#tab_0">Table 18</ref> shows the proportion of the weights assigned by SSLM to the factorized components of the stacked lo-cal+holistic networks. The weights of Y 3 are relatively high, which supports our initial argument about robustness and discriminative properties of the shared factorized components. The Z components of the both modalities in all three layers also gain weights, which shows they also carry informative features and are complementary for the classification. The reported values in this table shows how discriminative are the factorized features inside each of these components. As can be seen, some components achieve very low (close to zero) values. They hold important components in the distribution of the input multimodal data regardless of the action labels. However, regarding the action classification task, they don't have considerable correlation with action labels and cannot contribute very much in classification, so they gain very low weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Analysis of component contributions in the classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper presents a new deep learning framework for a hierarchical shared-specific component factorization (DSSCA), to analyze RGB+D features of human action videos. Each layer of the proposed network is an autoencoder based component factorization unit, which decomposes its multimodal input features into common and modality-specific parts. We further extended our deep factorization framework by applying it in a convolutional setting.</p><p>In addition, we proposed a structured sparsity based classifier (SSLM) which utilizes mixed norms to apply component and layer selection for a proper fusion of decomposed feature components.</p><p>Provided experimental results on five RGB+D action recognition datasets show the strength of our deep sharedspecific component analysis and the proposed structured sparsity learning machine by achieving the state-of-the-art performances on all the reported benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ure 3, we first train the local DSSCA network (DSSCA L ) on RGB+D features of the sub-cubes of training video samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Schema of our convolutional and holistic networks of deep shared-specific component analysis (DSSCA). We divide each video into n local cubes. Local features X i r and X i d are extracted from the i th cube. Convolutional network (denoted as DSSCA L ) is trained and then applied to decompose local features. The factorized components are then combined with holistic features X H r and X H d . This combination undergoes PCA and is fed into the holistic network (denoted as DSSCA H ) as its multimodal input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Confusion matrix for CCA-RICA method on atomic local level features RGBD-HuDaAct dataset. Ground truth action labels are on rows and detections are on columns of the grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Confusion matrix for SSCA method on atomic local level features of RGBD-HuDaAct dataset. Ground truth action labels are on rows and detections are on columns of the grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the results of our methods with the baselines in Online RGBD Action dataset. S1, S2, and S3 refers to the three different scenarios of the Online RGBD Action dataset. First column shows the performance of descriptor concatenation on all RGB+D input features. Second column reports the accuracy of the kernel combination on the same set of features. Third column shows the result of our correlation-independence analysis. It employs a kernel combination for classification. Last column reports the accuracy of proposed structured sparsity learning machine.</figDesc><table><row><cell>Eval.</cell><cell cols="3">Baseline Baseline DSSCA DSSCA</cell></row><row><cell cols="4">Dataset Method 1 Method 2 Kernel SSLM</cell></row><row><cell cols="2">Online S1 86.6%</cell><cell>91.1%</cell><cell>92.9% 95.5%</cell></row><row><cell cols="2">Online S2 85.6%</cell><cell>91.0%</cell><cell>91.9% 93.7%</cell></row><row><cell cols="2">Online S3 73.0%</cell><cell>80.2%</cell><cell>82.0% 83.8%</cell></row><row><cell>Evaluation</cell><cell cols="2">Network</cell><cell>DSSCA DSSCA</cell></row><row><cell>Dataset</cell><cell cols="2">Structure</cell><cell>Kernel SSLM</cell></row><row><cell>Online S1</cell><cell cols="2">Holistic</cell><cell>90.2% 92.0%</cell></row><row><cell>Online S1</cell><cell cols="2">Local</cell><cell>92.9% 93.8%</cell></row><row><cell cols="4">Online S1 Stacked Local+Holistic 92.9% 95.5%</cell></row><row><cell>Online S2</cell><cell cols="2">Holistic</cell><cell>87.4% 91.0%</cell></row><row><cell>Online S2</cell><cell cols="2">Local</cell><cell>88.3% 89.2%</cell></row><row><cell cols="4">Online S2 Stacked Local+Holistic 91.9% 93.7%</cell></row><row><cell>Online S3</cell><cell cols="2">Holistic</cell><cell>79.3% 82.0%</cell></row><row><cell>Online S3</cell><cell cols="2">Local</cell><cell>75.7% 77.5%</cell></row><row><cell cols="4">Online S3 Stacked Local+Holistic 82.0% 83.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>compares the results of the deep shared-specific component analysis (DSSCA) and structure sparsity learning machine (SSLM), with baseline methods on this dataset. The results of this experiment show our DSSCA network successfully decompose input features into a more powerful representation which leads into a clear improvement on the classification performance. They also show our SSLM can</figDesc><table><row><cell>Network</cell><cell cols="2">Layer 1 2 Layers</cell></row><row><cell>Description</cell><cell>SSLM</cell><cell>SSLM</cell></row><row><cell>Local Without Z</cell><cell>73.0%</cell><cell>73.9%</cell></row><row><cell>Local With Z</cell><cell>76.6%</cell><cell>77.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison of the results of our methods with the baselines in MSR-DailyActivity3D dataset.</figDesc><table><row><cell>Evaluation</cell><cell>Network</cell><cell>DSSCA DSSCA</cell></row><row><cell>Dataset</cell><cell>Structure</cell><cell>Kernel SSLM</cell></row><row><cell>Daily</cell><cell>Holistic</cell><cell>95.0% 96.3%</cell></row><row><cell>Daily</cell><cell>Local</cell><cell>95.0% 96.9%</cell></row><row><cell>Daily</cell><cell cols="2">Stacked Local+Holistic 96.3% 97.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison of the proposed multimodal DSSCA with the state-of-the-art methods on MSR-DailyActivity dataset.</figDesc><table><row><cell cols="4">Eval. Baseline Baseline DSSCA DSSCA</cell></row><row><cell cols="4">Dataset Method 1 Method 2 Kernel SSLM</cell></row><row><cell>Pairs</cell><cell>97.7%</cell><cell cols="2">98.3% 100.0% 100.0%</cell></row><row><cell cols="4">Table 8. Comparison of the results of our methods with the base-</cell></row><row><cell cols="3">lines in 3D Action Pairs dataset.</cell></row><row><cell>Evaluation</cell><cell cols="2">Network</cell><cell>DSSCA DSSCA</cell></row><row><cell>Dataset</cell><cell cols="2">Structure</cell><cell>Kernel SSLM</cell></row><row><cell>Pairs</cell><cell cols="2">Holistic</cell><cell>98.9% 99.4%</cell></row><row><cell>Pairs</cell><cell></cell><cell>Local</cell><cell>99.4% 98.9%</cell></row><row><cell>Pairs</cell><cell cols="3">Stacked Local+Holistic 100.0% 100.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 14 .</head><label>14</label><figDesc>Comparison of the results of our methods with the baselines on RGBD-HuDaAct dataset. First column shows the performance of descriptor concatenation on all RGB+D input features.</figDesc><table><row><cell>RGBD-HuDaAct [39] is a large size benchmarks for</cell></row><row><cell>human daily action recognition in RGB+D. This dataset</cell></row><row><cell>includes 1189 RGB+D video sequences from 13 action</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 17 .Table 18 .</head><label>1718</label><figDesc>Comparison between our method and baseline method 2 on single modality RGB and depth based input features, on all the datasets. Proportion of the weights to factorized components in SSLM classifier for Online RGBD, MSR-DailyActivity3D, and 3D action pairs datasets. Reported values are the 2 norms of all the corresponding weights to each of the components, learned by SSLM on the stacked local+holistic networks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="4">MSR Daily 3D Action RGBD</cell><cell>Online</cell><cell>Online</cell><cell>Online</cell></row><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="3">Activity 3D</cell><cell>Pairs</cell><cell>HuDaAct RGBD S1 RGBD S2 RGBD S3</cell></row><row><cell></cell><cell cols="4">Baseline 2 on RGB-based</cell><cell>89.4%</cell><cell></cell><cell>97.7%</cell><cell>95.2%</cell><cell>81.3%</cell><cell>85.6%</cell><cell>75.7%</cell></row><row><cell></cell><cell></cell><cell cols="3">Local+Holistic</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Baseline 2 on depth-based</cell><cell>92.5%</cell><cell></cell><cell>97.7%</cell><cell>79.1%</cell><cell>85.7%</cell><cell>84.7%</cell><cell>66.7%</cell></row><row><cell></cell><cell></cell><cell cols="3">Local+Holistic</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Ours Kernel</cell><cell></cell><cell>96.3%</cell><cell cols="2">100.0%</cell><cell>98.3%</cell><cell>92.9%</cell><cell>91.9%</cell><cell>82.0%</cell></row><row><cell>Dataset</cell><cell>Z 1 r</cell><cell>Z 2 r</cell><cell>Z 3 r</cell><cell>Y 3 Z 3 d</cell><cell>Z 2 d</cell><cell>Z 1 d</cell></row><row><cell cols="7">Online S1 0.12 0.13 0.18 0.20 0.13 0.05 0.18</cell></row><row><cell cols="7">Online S2 0.29 0.06 0.03 0.42 0.06 0.11 0.03</cell></row><row><cell cols="7">Online S3 0.14 0.12 0.06 0.26 0.13 0.00 0.28</cell></row><row><cell>Daily</cell><cell cols="6">0.22 0.05 0.07 0.23 0.08 0.08 0.28</cell></row><row><cell>Pairs</cell><cell cols="6">0.06 0.02 0.16 0.42 0.01 0.03 0.29</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. The ROSE Lab is supported by the National Research Foundation, Singapore, under its Interactive Digital Media (IDM) Strategic Research Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Canonical correlation: a tutorial. Online tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIVR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infinite markov-switching maximum entropy discrimination machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical spatio-temporal pattern for human activity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Recent Advances in Convolutional Neural Networks. ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Space-Time Representation of People Based on 3D Skeletal Data: A Review. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorized latent spaces with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear heterogeneous information machine for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fusion of color and depth video for human behavior recognition in an assistive environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed, Ambient, and Pervasive Interactions</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kernel and nonlinear canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJNS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<title level="m">On space-time interest points. IJCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ica with reconstruction cost for efficient overcomplete feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rgb-d action recognition using linear coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning discriminative representations from rgb-d video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Range-sample depth feature for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Humanobject interaction recognition by learning the distances between the object and the skeleton joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boonaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal deep learning. In ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog 2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. arXiv, abs/1405</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4506</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action classification with locality-constrained linear coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real time action recognition using histograms of depth gradients and random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hopc: Histogram of oriented principal components of 3d pointclouds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Factorized orthogonal latent spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minfunc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCCSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Describing trajectory of surface patch for human action recognition on rgb and depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An efficient partbased approach to action recognition from rgb-d video with bow-pyramid representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Semi-supervised robust dictionary learning via efficient l 2,0 + -norms minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multi-view clustering and feature learning via structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robust and discriminative self-taught learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Heterogeneous visual features fusion via sparse multimodal machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning maximum margin temporal warping for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THMS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Mining mid-level features for action recognition based on effective skeleton representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Discriminative orderlet mining for real-time recognition of human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Rgbd-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Mul-tiMedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Combing rgb and depth map features for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AP-SIPA ASC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Depth induced feature representation for 4d human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhao Runlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Modelling &amp; New Technologies</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Fusing multiple features for depth-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
