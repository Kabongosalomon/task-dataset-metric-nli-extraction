<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Scheidegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Benjaminsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Rosenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Krishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Granström</surname></persName>
							<email>karl.granstrom@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zenuity</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular cameras are one of the most commonly used sensors in the automotive industry for autonomous vehicles. One major drawback using a monocular camera is that it only makes observations in the two dimensional image plane and can not directly measure the distance to objects. In this paper, we aim at filling this gap by developing a multi-object tracking algorithm that takes an image as input and produces trajectories of detected objects in a world coordinate system. We solve this by using a deep neural network trained to detect and estimate the distance to objects from a single input image. The detections from a sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli mixture tracking filter. The combination of the learned detector and the PMBM filter results in an algorithm that achieves 3D tracking using only mono-camera images as input. The performance of the algorithm is evaluated both in 3D world coordinates, and 2D image coordinates, using the publicly available KITTI object tracking dataset. The algorithm shows the ability to accurately track objects, correctly handle data associations, even when there is a big overlap of the objects in the image, and is one of the top performing algorithms on the KITTI object tracking benchmark. Furthermore, the algorithm is efficient, running on average close to 20 frames per second.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>To enable a high level of automation in driving, it is necessary to accurately model the surrounding environment, a problem called environment perception. Data from onboard sensors, such as cameras, radars and lidars, has to be processed to extract information about the environment needed to automatically and safely navigate the vehicle. For example, information about both the static environment, such as road boundaries and lane information, and the dynamic objects, like pedestrians and other vehicles, is of importance. The focus of this paper is the detection and tracking of multiple dynamic objects, specifically vehicles.</p><p>Dynamic objects are often modeled by state vectors, and are estimated over time using a multi-object tracking (MOT) framework. MOT denotes the problem of, given a set of noisy measurements, estimating both the number of dynamic objects, and the state of each dynamic object. Compared to the single object tracking problem, in addition to handling measurement noise and detection uncertainty, the MOT problem also has to resolve problems like object birth and object death 1 ; clutter detections 2 ; and unknown measurement origin.</p><p>A recent family of MOT algorithms are based on random finite sets (RFSs) <ref type="bibr" target="#b0">[1]</ref>. The probability hypothesis density (PHD) <ref type="bibr" target="#b1">[2]</ref> filter, and the cardinalized PHD (CPHD) <ref type="bibr" target="#b3">[3]</ref> filter, are two examples of moment approximations of the multi-object density. The generalized labeled multi-Bernoulli (GLMB) <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref> and the Poisson multi-Bernoulli mixture (PMBM) <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> filters are examples of MOT filters based on multi-object conjugate priors; these filters have been shown to outperform filters based on moment approximation. A recent comparison study published in <ref type="bibr" target="#b8">[8]</ref> has shown that the filters based on the PMBM conjugate prior both achieves greater tracking performance, and has favourable computational cost compared to GLMB, hence we use the PMBM filter in this work.</p><p>All of the aforementioned MOT algorithms takes sets of object estimates, or detections, as their input. This implies that the raw sensor data, e.g., the images, should be preprocessed into detections. The recent development of deep neural networks has lead to big improvement in fields of image processing. Indeed, considerable improvements have be achieved for the object detection problem, see, e.g., <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, which is is crucial to the tracking performance.</p><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b11">[11]</ref> have shown to vastly outperform previous methods in image processing for tasks such as classification, object detection and semantic segmentation. CNNs make use of the spatial relation between neighbouring pixels in images, by processing data in a convolutional manner. Each layer in a CNN consists of a filter bank with a number of convolutional kernels, where each element is a learnable parameter.</p><p>The most common approach for object detection using deep neural networks is region-based CNNs (R-CNNs). R-CNNs are divided into two parts; a region proposal network (RPN), followed by a box regression and classification network. The RPN takes an image as input, and outputs a set of general object proposals, which are fed into the following classification and box regression network. The box regression and classification network will refine the size of the object and classify it into one of the object classes. This type of deep neural network structure is used in, e.g., Fast R-CNN <ref type="bibr" target="#b12">[12]</ref>, and later in the improved Faster R-CNN <ref type="bibr" target="#b9">[9]</ref>. Another approach to the object detection problem is you only look once (YOLO) <ref type="bibr" target="#b10">[10]</ref>. Here, the region proposal step is omitted, and the box regression and classification are applied directly on the entire image.</p><p>In the automotive industry, monocular camera is a well studied and commonly used type of sensors for developing autonomous driving systems. A monocular camera is a mapping between 3D world coordinates and 2D image coordinates <ref type="bibr" target="#b13">[13]</ref> where, in contrary to, e.g., radars and lidars, distance information is lost. However, to achieve a high level of automation, tracking in the image plane is not adequate. Instead, we need to track objects in world coordinates in order to obtain the relative pose between the ego vehicle the detected objects, information that is crucial for automatic decision making and control. We refer to this as 3D tracking.</p><p>Previous work on object tracking using monocular camera data is restricted to tracking in the image-plane, see, e.g., <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, for some recent work. The main contribution of this paper is a multi-vehicle 3D tracking algorithm, that takes as input mono camera data, and outputs vehicle estimates in world coordinates. The proposed MOT algorithm is evaluated using the image sequences from the publicly available KITTI tracking dataset <ref type="bibr" target="#b17">[17]</ref>, and the results show that accurate 3D tracking is achieved.</p><p>The presented 3D tracking filter has two main components: a detector and an object tracking filter. The detector is a deep neural network trained to from an input image not only extract a 2D bounding box for each detected object, but also to estimate the distance from the camera to the object. This is achieved by using object annotations in lidar data during the learning of the network parameters. The object tracking filter is a state-of-the-art PMBM object tracking filter <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> that processes the detections and outputs estimates. The tracking filter is computationally efficient, and handles both false detections and missed detections. For each object, a position, as well as kinematical properties such as velocity, are estimated.</p><p>The paper is structured as follows. In Section II, we give a problem formulation and present an overview of the algorithm. In Section III we present the object detection, and in Section IV we present object tracking. The results of an experimental evaluation using data sequences from the KITTI dataset are presented in Section V, and the paper is concluded in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION AND ALGORITHM OVERVIEW</head><p>The KITTI object tracking dataset <ref type="bibr" target="#b17">[17]</ref> contains data from multiple sensors, e.g., four cameras and a lidar sensor. The data from such sensors can be used for environment perception, i.e., tracking of moving objects and mapping of the stationary environment. In this work we focus on data from a forward looking camera, with the objective to track the other vehicles that are in the environment.</p><p>Each vehicle is represented by a state vector x that contains the relevant information about the object. For 3D object <ref type="figure">Fig. 1</ref>. Algorithm overview. The MOT algorithm has two modules, detection (left, red) and tracking (right, blue/green). The tracking modules consists of a recursive tracking filter (prediction+update) and an object extraction. tracking, the following state vector is used,</p><formula xml:id="formula_0">I k Detection Z k Prediction Update Extraction X k|k f k|k−1 f k|k f k−1|k−1</formula><formula xml:id="formula_1">x = x y z v x v y v z w h T ,<label>(1)</label></formula><p>where (x, y, z) is the 3D position in world coordinates,</p><formula xml:id="formula_2">(v x , v y , v z )</formula><p>is the corresponding velocity, and (w, h) is the width and height of the object's bounding box in the camera image. The position and velocity describes the tracked object's properties of interest; the width and height of the bounding box are used for evaluation analogue to the KITTI object tracking benchmark <ref type="bibr" target="#b17">[17]</ref>. The number of vehicles in the environment is not known, and changes with time, so the task is to estimate both the number of vehicles, as well as each vehicle's state. The vehicles at time step k are represented by a set X k that contains the state vectors of all vehicles that are present in the vicinity of the ego-vehicle. The set of vehicles X k is modeled as a Random Finite Set (RFS) <ref type="bibr" target="#b0">[1]</ref>. That is, the number of objects, or the cardinality of the set, is modeled as a time varying discrete random variable and each object's state is a multivariate random variable.</p><p>The problem addressed in this paper is the processing of the sequence of images I k into a sequence of estimatesX k|k of the set of vehicles,</p><formula xml:id="formula_3">I 0 , I 1 , . . . , I k ⇒X 0|0 ,X 1|1 , . . . ,X k|k ,<label>(2)</label></formula><p>where the sub-indices denote time. In other words, we wish to process the image sequence to gain information at each time step about the number of vehicles (the cardinality of the set X), and the state of each vehicle. The proposed MOT algorithm has two main parts: object detection, and object tracking; an illustration of the algorithm is given in <ref type="figure">fig. 1</ref>.</p><p>In the detection module, each image is processed to output a set of object detections Z k , • Minor non-bold letter, e.g., a, b, γ, denote scalars.</p><formula xml:id="formula_4">I k Detection ====⇒ Z k .<label>(3)</label></formula><p>• Minor bold letters, e.g., x, z, ξ, denote vectors.</p><p>• Capital non-bold letters , e.g., M , F , H, denote matrices.</p><p>• Capital bold letters, e.g., X, Y, Z, denote sets.</p><p>• |X| denotes the cardinality of set X, i.e., the number of elements in X.</p><p>• denotes disjoint set union, i.e.,</p><formula xml:id="formula_5">X Y = Z means X ∪ Y = Z and X ∩ Y = ∅. • [h(·)] X = x∈X h(x) and [h(·)] ∅ = 1 by definition. • a; b = a(x)b(x)dx, the inner product of a(x) and b(x).</formula><p>The set of detections Z k , where each z i k ∈ Z k is an estimated object, is also modeled as a RFS. The detection is based on a CNN, which is presented in detail in Section III.</p><p>The tracking module takes the image detections as input and outputs an object set estimate; it has three parts: prediction, update, and extraction. Together, the prediction and the update constitute a tracking filter that recursively estimates a multiobject set density,</p><formula xml:id="formula_6">Z k PMBM filter = ===== ⇒ f k|k (X k |Z k ),<label>(4)</label></formula><p>where Z k denotes all measurement sets up to time step k, {Z t } t∈(0,k) . Specifically, in this work we estimate a PMBM density <ref type="bibr" target="#b6">[6]</ref>. The Chapman-Kolmogorov prediction</p><formula xml:id="formula_7">f k|k−1 (X k |Z k−1 ) = g(X k |X k−1 )f k−1|k−1 (X k−1 |Z k−1 )δX k−1 , (5a)</formula><p>predicts the PMBM density to the next time step using the multi-object motion model g(X k+1 |X k ). We use the standard multi-object motion model <ref type="bibr" target="#b0">[1]</ref>, meaning that g(·|·) models a Markovian process for objects that remain in the field of view, combined a Poisson point process (PPP) birth process.</p><p>Using the set of detections Z k and the multi-object measurement model h(Z k |X k ), the updated PMBM density is computed using the Bayes update</p><formula xml:id="formula_8">f k|k (X k |Z k ) = h(Z k |X k )f k|k−1 (X k |Z k−1 ) h(Z k |X k )f k|k−1 (X k |Z k−1 )δX k ,<label>(5b)</label></formula><p>We use the standard multi-object measurement model <ref type="bibr" target="#b0">[1]</ref>, in which h(Z k |X k ) models noisy measurement with detection uncertainty, combined with PPP clutter. The final part of the tracking is the object extraction, where object estimates are extracted from the PMBM density,</p><formula xml:id="formula_9">f k|k (X k |Z k ) Extraction = ==== ⇒X k|k<label>(6)</label></formula><p>The tracking is described further in Section IV. The integrals in (5) are set-integrals, defined in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OBJECT DETECTION</head><p>In this section, we describe how deep learning, see, e.g., <ref type="bibr" target="#b18">[18]</ref>, is used to process the images {I t } k t=0 to output sets of detections {Z t } k t=0 . For an image I t with a corresponding set of detections Z t , each detection z ∈ Z t consists of a 2D bounding box and a distance from the camera center to the center of the detected object,</p><formula xml:id="formula_10">z = x min y min x max y max d T ,<label>(7)</label></formula><p>where (x min , y min ) and (x max , y max ) are the pixel positions of the top left and bottom right corner of the bounding box, respectively, and d is the distance from the camera to the object. The bounding box encloses the object in the image. Using this information, the angle from the camera center to the center of the detected object can be inferred. This, together with the camera-to-object-distance d, allows the camera to be transformed into a range/bearing sensor, which is suitable for object tracking in 3D world coordinates. The object detection is implemented using a improved version of the network developed in <ref type="bibr" target="#b19">[19]</ref>. The network can be divided into two parts; the first part can be viewed as a feature extractor, and the second part consists of three parallel output headers. The feature extractor is identical to the DRN-C-26 <ref type="bibr" target="#b20">[20]</ref> network, with the exception that the last two classification layers have been removed. The last two layers are structured for the original classification task of DRN-C-26, which is not suitable in this work.</p><p>To represent objects using a bounding box and its distance, the network has three different types of output: classification score, bounding box and distance. Each header in the network has two 1 × 1 convolutional layers and finally a sub-pixel convolutional layer <ref type="bibr" target="#b21">[21]</ref>, upscaling the output to 1/4th of the input image resolution. The bounding box header has 4 output channels, representing the top left and bottom right corner of the bounding box, the distance header has one output channel, representing the distance to the object, and the classification header has an additional softmax function and represents the different class scores using one-hot encoding, i.e., one output channel for each class, where each channel represents the score for each class, respectively. For each pixel in the output layer there will be an estimated bounding box, i.e., there can be more than one bounding box per object. To address this, Soft-NMS <ref type="bibr" target="#b22">[22]</ref> is applied. In this step, the box with the highest classification score is selected and the score of boxes intersecting the selected box are decayed according to a function of the intersection over union (IOU). This process is repeated until the classification score of all remaining boxes are below a manually chosen threshold.</p><p>The feature extractor is pre-trained on ImageNet <ref type="bibr" target="#b23">[23]</ref> and the full network is fine-tuned using annotated object labels from the KITTI object data set <ref type="bibr" target="#b17">[17]</ref>. The network is tuned using stochastic gradient descent with momentum. The task of classification used a cross entropy loss function while bounding box regression and distance estimation used a smooth L1 loss function <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBJECT TRACKING</head><p>To associate objects between consecutive frames and filter the object detections from the neural network, a PMBM tracking filter is applied. Both the set of objects X k and the set of image detections Z k are modeled as RFSs. The purpose of the tracking module is to process the sequence of detection sets, and output a sequence of estimatesX k|k of the true set of objects. We achieve this by using a PMBM filter to estimate the multi-object density f k|k (X k |Z k ), and to extract estimates from this density.</p><p>In this section, we first present some necessary RFS background, and the standard point object models that are used to model both the object motion, as well as the detection process. Then, we present the PMBM filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RFS background</head><p>In this work, two types of RFSs are important: the PPP and the Bernoulli process. A general introduction to RFS is given in, e.g., <ref type="bibr" target="#b0">[1]</ref>.</p><p>1) Poisson point process: A PPP is a type of RFS where the cardinality is Poisson distributed and all elements are independent and identically distributed (IID). A PPP can be parametrized by an intensity function, D(x), defined as</p><formula xml:id="formula_11">D(x) = µf (x).<label>(8)</label></formula><p>The intensity function has two parameters, the Poisson rate µ &gt; 0 and the spatial distribution f (x). The expected number of set members in a PPP S is x∈S D(x)dx.</p><p>The PPP density is</p><formula xml:id="formula_12">f (X) = e − D(x);1 x∈X D(x) = e −µ x∈X µf (x). (9)</formula><p>The PPPs are used to model object birth, undetected objects and clutter measurements.</p><p>2) Bernoulli process: A Bernoulli RFS is a RFS that with the probability r contains a single element with the probability density function (PDF) f (x), and with the probability 1 − r is empty:</p><formula xml:id="formula_13">f (X) =      1 − r, X = ∅ rf (x), X = {x} 0, |X| &gt; 1 .<label>(10)</label></formula><p>It is suitable to use a Bernoulli RFS to model objects in a MOT problem, since it both models the object's probability of existence r, and uncertainty in its state x. In MOT, the objects are typically assumed to be independent <ref type="bibr" target="#b6">[6]</ref>. The disjoint union of a fixed number of independent Bernoulli RFSs, X = i∈I X i , where I is an index set, is a multi-Bernoulli (MB) RFS. The parameters {r i , f i (·)} i∈I defines the MB distribution.</p><p>A multi-Bernoulli mixture (MBM) density is a normalized, weighted sum of MB densities. The MBM density is entirely defined by {w j , {r j,i , f j,i (·)} i∈I j } j∈J , where J is an index set for the MBs in the MBM, w j is the probability of the jth MB, and I j is the index set for the Bernoulli distributions. In a MOT problem, the different MBs typically corresponds to different data association sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Standard models</head><p>Here we present the details of the standard measurement and motion models, under Gaussian assumptions. 1) Measurement model: Let x i k be the state of the ith vehicle at the kth time step. At time step k, given a set of objects X k = {x i k } i∈I , the set of measurements is Z k = ( i∈I W i k ) K k , where W i k denotes the set of object generated measurements from the ith object, I is an index set and K k denotes the set of clutter measurements. The set K k is modeled as a PPP with the intensity κ(z) = λc(z), where λ is the Poisson rate and the spatial distribution c(z) is assumed to be uniform.</p><p>Assuming an object is correctly detected with probability of detection p D . If the object is detected, the measurement</p><formula xml:id="formula_14">z ∈ W i k has PDF φ z (x i k ) = N (z; a(x i k ), R), where a(x i k )</formula><p>is a camera measurement model. The resulting measurement likelihood is</p><formula xml:id="formula_15">Z (x) = p(Z|x) =      1 − p D , Z = ∅ p D φ z (x), Z = {z} 0, |Z| &gt; 1 .<label>(11)</label></formula><p>As can be seen in eq. (11), if multiple measurements are associated to one object this will have zero likelihood. This is a standard point object assumption, see, e.g., <ref type="bibr" target="#b0">[1]</ref>. Because of the unknown measurement origin 3 , it is necessary to discuss data association. Let the measurements in the set Z be indexed by m ∈ M,</p><formula xml:id="formula_16">Z = {z m } m∈M ,<label>(12)</label></formula><p>and let A j be the space of all data associations A for the jth predicted global hypothesis, i.e., the jth predicted MB. A data association A ∈ A j is an assignment of each measurement in Z to a source, either to the background (clutter or new object) or to one of the existing objects indexed by i ∈ I j . Note that M ∩ I j = ∅ for all j. The space of all data associations for the jth hypothesis is A j = P(M ∪ I j ), i.e., a data association A ∈ A j is a partition of M∪I j into non-empty disjoint subsets C ∈ A, called index cells 4 . Due to the standard MOT assumption that the objects generate measurements independent of each other, an index cell contains at most one object index and at most one measurement index, i.e., |C ∩ I j | ≤ 1 and |C ∩ M| ≤ 1 for all C ∈ A. Any association in which there is at least one cell, with at least two object indices and/or at least two measurement indices, will have zero likelihood because this violates the independence assumption and the point object assumption, respectively. If the index cell C contains an object index, then let i C denote the corresponding object index, and if the index cell C contains a measurement index, then let m C denote the corresponding measurement index.</p><p>2) Standard dynamic model: The existing objects-both the detected and the undetected-survive from time step k to time step k + 1 with probability of survival p S . The objects evolve independently according to a Markov process with Gaussian transition density g(x k+1 |x k ) = N (x k+1 ; b(x k ), Q), where b(·) is a constant velocity (CV) motion model. New objects appear independently of the objects that already exist. The object birth is assumed to be a PPP with intensity D b k+1 (x), defined in eq. (9). C. PMBM filter In this section, the time indexing has been omitted for notational simplicity. The PMBM filter is a combination of two RFSs, a PPP to model the objects that exist at the current time step, but have not yet been detected and a MBM to model the objects that have been detected previously at least once. The set of objects can be divided into two disjoint subsets, X = X d X u , where X d is the set of detected objects and X u is the set of undetected objects. The PMBM density can be expressed as</p><formula xml:id="formula_17">f (X) = X u X d =X f u (X u ) j∈J w j f j (X d ), (13a) f u (X u ) = e − D u (x);1 [D u (·)] X u , (13b) f j (X d ) = i∈I i X i =X d i∈I j f j,i (X i ),<label>(13c)</label></formula><p>where • f u (·) is the PPP density for the set of undetected objects X u , where D u (·) is its intensity. • J is an index set of MBM components. There are |J| MBs, where each MB corresponds to a unique global data association hypothesis. The probability of each component in the MBM is denoted as w j . • For every component j in the MBM, there is an index set I j , where each index i corresponds to a potentially detected object X i . • f j,i (·) are Bernoulli set densities, defined in eq. (10).</p><p>Each MB corresponds to a potentially detected object with a probability of existence and a state PDF. The PMBM density in eq. (13) is defined by the involved parameters,</p><formula xml:id="formula_18">D u , {(w j , {(r j,i , f j,i )} i∈I j )} j∈J .<label>(14)</label></formula><p>Further, the PMBM density is an MOT conjugate prior <ref type="bibr" target="#b6">[6]</ref>, meaning that for the standard point object models (Sections IV-B1 and IV-B2), the prediction and update in eq. (5) both result in PMBM densities. It follows that the PMBM filter propagates the multi-object density by propagating the set of parameters.</p><p>In this work, we assume that the birth intensity D b is a nonnormalized Gaussian mixture. It follows from this assumption that the undetected intensity D u is also a non-normalized Gaussian mixture, and all Bernoulli densities f j,i are Gaussian densities. Below, we present the parameters that result from the prediction and the update, and we present a simple method for extracting target estimates from the set of parameters. To compute the predicted and updated Gaussian parameters, we use the UKF prediction and update, respectively, see, e.g., <ref type="bibr" target="#b24">[24,</ref><ref type="bibr">Ch. 5]</ref>.</p><p>1) Prediction: Given a posterior PMBM density with parameters</p><formula xml:id="formula_19">D u , {(w j , {(r j,i , f j,i )} i∈I j )} j∈J ,<label>(15)</label></formula><p>and the standard dynamic model (Section IV-B2), the predicted density is a PMBM density with parameters</p><formula xml:id="formula_20">D u + , {(w j + , {(r j,i + , f j,i + )} i∈I j )} j∈J ,<label>(16a)</label></formula><p>where</p><formula xml:id="formula_21">D u + (x) = D b (x) + p S D u ; g ,<label>(16b)</label></formula><formula xml:id="formula_22">r j,i + = p S r j,i ,<label>(16c)</label></formula><formula xml:id="formula_23">f j,i + (x) = f j,i ; g ,<label>(16d)</label></formula><p>and w j + = w j . For Gaussian mixture intensity D u , and Gaussian densities f j,i , the predictions ·; g in eq. (16) are easily computed using the UKF prediction, see, e.g., <ref type="bibr" target="#b24">[24,</ref><ref type="bibr">Ch. 5]</ref>.</p><p>2) Update: Given a prior PMBM density with parameters</p><formula xml:id="formula_24">D u + , {(w j + , {(r j,i + , f j,i + )} i∈I j + )} j∈J+ ,<label>(17)</label></formula><p>a set of measurements Z, and the standard measurement model (Section IV-B1), the updated density is a PMBM density</p><formula xml:id="formula_25">f (X|Z) = X u X d =X f u (X u ) j∈J+ A∈A j w j A f j A (X d ), (18a) f u (X u ) = e − D u ;1 x∈X u D u (x),<label>(18b)</label></formula><formula xml:id="formula_26">f j A (X d ) = C∈A X C =X C∈A f j C (X C ),<label>(18c)</label></formula><p>where the weights are</p><formula xml:id="formula_27">w j A = w j + C∈A L C j ∈J A ∈A j w j + C ∈A L C ,<label>(18d)</label></formula><formula xml:id="formula_28">L C =    κ + p D D u + ; φ z m C if C ∩ I j = ∅, C ∩ M = ∅, 1 − r j,i C + p D if C ∩ I j = ∅, C ∩ M = ∅, r j,i C + p D f j,i C + ; φ z m C if C ∩ I j = ∅, C ∩ M = ∅,<label>(18e)</label></formula><p>the densities f j C (X) are Bernoulli densities with parameters</p><formula xml:id="formula_29">r j C =          pD D u + ;φ z m C κ+pD D u + ;φ z m C if C ∩ I j = ∅, C ∩ M = ∅, r j,i C + (1−pD) 1−r j,i C + pD if C ∩ I j = ∅, C ∩ M = ∅, 1 if C ∩ I j = ∅, C ∩ M = ∅,<label>(18f)</label></formula><formula xml:id="formula_30">f j C (x) =          φ z m C (x)D u + (x) D u + ;φ z m C if C ∩ I j = ∅, C ∩ M = ∅, f j,i C + (x) if C ∩ I j = ∅, C ∩ M = ∅, φ z m C (x)f j,i C + (x) f j,i C + ;φ z m C if C ∩ I j = ∅, C ∩ M = ∅,<label>(18g)</label></formula><p>and the updated PPP intensity is D u (x) = (1−p D )D u + (x). For Gaussian mixture intensity D u + , and Gaussian densities f j,i + , the updates ·; φ in (18) are easily computed using the UKF update, see, e.g., <ref type="bibr" target="#b24">[24,</ref><ref type="bibr">Ch. 5]</ref>.</p><p>3) Extraction: Let the set of updated PMBM parameters be</p><formula xml:id="formula_31">D u , {(w j , {(r j,i , f j,i )} i∈I j )} j∈J .<label>(19)</label></formula><p>To extract a set of object estimates, the hypothesis with highest probability is chosen,</p><formula xml:id="formula_32">j = arg max j∈J w j .<label>(20)</label></formula><p>From the corresponding MB, with parameters</p><formula xml:id="formula_33">{(r j ,i , f j ,i )} i∈I j ,<label>(21)</label></formula><p>all Bernoulli components with probability of existence r j ,i larger than a threshold τ are selected, and the expected value of the object state is included in the set of object estimates,</p><formula xml:id="formula_34">X = x j ,i i∈I j :r j ,i &gt;τ ,<label>(22a)</label></formula><formula xml:id="formula_35">x j ,i = E f j ,i x j ,i = xf j ,i (x)dx.<label>(22b)</label></formula><p>V. EXPERIMENTAL RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>For evaluation, the KITTI object tracking dataset <ref type="bibr" target="#b17">[17]</ref> is used. The datasets consists of 21 training sequences and 29 testing sequences that were collected using sensors mounted on a moving car. Each sequence has been manually annotated with ground truth information, e.g., in the images, objects from the classes Car, Pedestrian and Cyclist have been marked by bounding boxes. In this work, the training dataset was split into two parts; one for training the CNN, and one for validation. The sequences used for training are 0, 2, <ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b17">17</ref> and 20, and the remaining ones are used for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>In this work we are primarily interested in the 3D tracking results. However, the KITTI testing sequences evaluate the tracking in 2D, hence we present results in both 2D and 3D. Performance is evaluated using IOU of the image plane bounding boxes and Euclidean distance as distance measurements, respectively. For a valid correspondence between a ground truth (GT) object and an estimated object, the 2D IOU has to be at least 50 %, and the 3D Euclidean distance has to be within 3 m, for the 2D and 3D evaluation, respectively. The performance is evaluated using the CLEAR MOT performance measures <ref type="bibr" target="#b25">[25]</ref>, including MOT accuracy (MOTA), MOT precision (MOTP), with addition of mostly tracked (MT), mostly lost (ML), identity switches (IDS) and fragmentations (FR) from <ref type="bibr" target="#b26">[26]</ref>, and F1 score (F1), precision (PRE), recall (REC) and false alarm rate (FAR). The F1 score is the weighted harmonic mean of the precision and recall. Note that, for the 2D IOU measure, a larger value is better, whereas for the 3D Euclidean distance, lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>Examples of the 3D tracking results are shown in <ref type="figure" target="#fig_0">fig. 2</ref>. The three examples show that the tracking algorithm successfully estimates the states of vehicles moving in the same direction as the ego-vehicle, vehicles moving in the opposite direction, as well as vehicles making sharp turns in intersections. In dense scenarios, such as in <ref type="figure" target="#fig_0">fig. 2b</ref>, there are big overlaps between the bounding boxes; this is handled without problem by the data association. Noteworthy is that the distance estimates are quite noisy. Sometimes this leads to incorrect initial estimates of the velocity vector, as can be seen at the beginning of the track of the oncoming vehicle labelled purple in <ref type="figure" target="#fig_0">fig. 2c</ref>. However, the tracking filter quickly converges to a correct estimate. Videos of these, and of additional sequences, can be seen at https://goo.gl/AoydgW.</p><p>Quantitative results from the evaluation on the validation sequences are shown in table II. Noteworthy is the low amount of identity switches, in both 2D and in 3D. Comparing the raw CNN detections and the MOT algorithm, the MOT precision is lower, and the MOT recall is higher, leading to an F1 score that is higher for the MOT than for the CNN; in other words, the overall object detection performance is slightly improved.</p><p>The runtime of the algorithm is on average in total 52 ms, 38 ms for the detection network and 14 ms for the tracking algorithm, on a Nvidia Tesla V100 SXM2 and a single thread on a 2.7 GHz Intel Core i7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. KITTI MOT benchmark</head><p>The MOT algorithm was also evaluated in 2D using the test sequences on the KITTI evaluation server. For these results, the full training set was used for training the detection CNN. The results are presented in table III; at the time of submission our algorithm was ranked 3rd in terms of MOTA among the published algorithms. Note that, even if not reaching the same MOTA performance, the runtime of our algorithm (frames per second (FPS)) is one magnitude faster and has a significantly lower number of identity switches than the two algorithms with higher MOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented an image based MOT algorithm using deep learning detections and PMBM filtering. It was shown that a CNN and a subsequent PMBM filter can be used to detect and track objects. The algorithm successfully can track multiple objects in 3D from a single camera image, which can provide valuable information for decision making and control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was partially supported by the Wallenberg Autonomous Systems and Software Program (WASP).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Object estimates shown both projected to the image plane (top) and in top-view (bottom). The dotted tracks illustrate the GT and tracked object estimates at previous time steps. The color of the tracks corresponds to the identity of the tracked object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TABLE</head><label>I</label><figDesc></figDesc><table /><note>OF NOTATION</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF THE EVALUATION OF TRACKING PERFORMANCE IN 2D AND 3D ON THE Car CLASS. ↑ AND ↓ REPRESENTS THAT HIGH VALUES AND LOW VALUES ARE BETTER, RESPECTIVELY. THE BEST VALUES ARE MARKED WITH BOLD FONT.TABLE III KITTI MOT BENCHMARK [17] RESULTS FOR Car CLASS. ↑ AND ↓ REPRESENTS THAT HIGH VALUES AND LOW VALUES ARE BETTER, RESPECTIVELY. THE BEST VALUES ARE MARKED WITH BOLD FONT. [14] IS TUSIMPLE, [16] IS IMMDP AND [27] IS MCMOT-CPD. ONLY RESULTS OF PUBLISHED METHODS ARE REPORTED.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell>MOTA↑</cell><cell></cell><cell>MOTP↑</cell><cell></cell><cell>MT↑</cell><cell>ML↓</cell><cell>IDS↓</cell><cell>FR↓</cell><cell>F1↑</cell><cell>PRE↑</cell><cell>REC↑</cell><cell>FAR↓</cell></row><row><cell>2D</cell><cell>CNN MOT</cell><cell></cell><cell>-81.23 %</cell><cell></cell><cell cols="3">82.04 % 81.63 % 76.22 % 74.59 %</cell><cell>3.78 % 3.78 %</cell><cell>-19</cell><cell>-107</cell><cell>91.16 % 95.72 % 91.26 % 94.76 % 88.02 % 87.02 %</cell><cell>9.08 % 11.46 %</cell></row><row><cell></cell><cell cols="2">Method</cell><cell>MOTA↑</cell><cell></cell><cell>MOTP↓</cell><cell></cell><cell>MT↑</cell><cell>ML↓</cell><cell>IDS↓</cell><cell>FR↓</cell><cell>F1↑</cell><cell>PRE↑</cell><cell>REC↑</cell><cell>FAR↓</cell></row><row><cell>3D</cell><cell>CNN MOT</cell><cell></cell><cell cols="6">-47.20 % 110.73 cm 48.65 % 111.39 cm 45.95 % 10.27 % 11.35 %</cell><cell>-20</cell><cell>-166</cell><cell>73.53 % 78.74 % 73.86 % 78.18 % 70.00 % 68.97 % 41.90 % 44.32 %</cell></row><row><cell cols="2">MOTA↑</cell><cell cols="2">MOTP↑</cell><cell>MT↑</cell><cell>ML↓</cell><cell>IDS↓</cell><cell>FR↓</cell><cell>FPS↑ a</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Object birth and object death is when an object first appears within, and departs from, the ego-vehicle's surveillance area, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Clutter detections are false detections, i.e., detections not corresponding to an actual object.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">An inherent property of MOT is that it is unknown which measurements are from object and which are clutter, and among the object generated measurements, it is unknown which object generated which measurement. Hence, the update must handle this uncertainty.<ref type="bibr" target="#b4">4</ref> For example, let M = (m 1 , m 2 , m 3 ) and I = (i 1 , i 2 ), i.e., three measurements and two objects. One valid partition of M∩I, i.e., one of the possible associations, has the following four cells {m 1 }, {m 2 , i 1 }, {m 3 }, {i 2 }. The meaning of this is that measurement m 2 is associated to object i 1 , object i 2 is not detected, and measurements m 1 and m 3 are not associated to any previously detected object, i.e., measurements m 1 and m 3 are either clutter or from new objects.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahler</surname></persName>
		</author>
		<title level="m">Statistical Multisource-Multitarget Information Fusion. Norwood</title>
		<meeting><address><addrLine>MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Artech House, Inc</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitarget Bayes filtering via first-order multitarget moments</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1152" to="1178" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">a The time for object detection is not included in the specified runtime</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PHD filters of higher order in target number</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1523" to="1543" />
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Labeled Random Finite Sets and Multi-Object Conjugate Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3460" to="3475" />
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Labeled Multi-Bernoulli Filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3246" to="3260" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Marginal multi-bernoulli filters: RFS derivation of MHT, JIPDA, and association-based member</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1664" to="1687" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Poisson multi-Bernoulli mixture filter: direct derivation and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Á</forename><forename type="middle">F</forename><surname>García-Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.04264" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance Evaluation of Multi-Bernoulli Conjugate Priors for Multi-Target Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F G</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 20th International Conference on Information Fusion (Fusion)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2323" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple View Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Track: Online Multiobject Tracking by Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vehicle detection and road scene segmentation using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dilated Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.09914" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Soft-NMS -Improving Object Detection with One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bayesian Filtering and Smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The CLEAR MOT metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurasip Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to associate: HybridBoosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="2953" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Class Multi-Object Tracking using Changing Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">9914</biblScope>
			<biblScope unit="issue">Mcmc</biblScope>
			<biblScope unit="page" from="68" to="83" />
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
