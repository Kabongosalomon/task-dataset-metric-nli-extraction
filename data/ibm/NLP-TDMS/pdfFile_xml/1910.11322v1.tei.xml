<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TexturePose: Supervising Human Mesh Estimation with Texture Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TexturePose: Supervising Human Mesh Estimation with Texture Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https: //seas.upenn.edu/˜pavlakos/projects/texturepose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the area of human pose estimation has experienced significant successes for tasks with an increasing level of difficulty; 2D joint detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref>, dense * equal contribution <ref type="figure">Figure 1</ref>: For a short video, or multi-view images of a person, a specific patch on the body surface has constant texture. This consistency can be formulated as an auxiliary loss in the training of a network for model-based pose estimation, and allows us to leverage information directly from raw pixels of natural images. Images and texture come from the People-Snapshot dataset <ref type="bibr" target="#b2">[3]</ref>. correspondence estimation <ref type="bibr" target="#b3">[4]</ref> or even 3D skeleton reconstruction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>. Typically, as we ascend the pyramid of human understanding, we target more and more challenging tasks. As expected, the emergence of sophisticated parametric models of the human body, like SCAPE <ref type="bibr" target="#b5">[6]</ref>, SMPL(-X) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>, and Adam <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51]</ref>, has really paved the way for full 3D pose and shape estimation from image data. And while this step has been well explored for video or multi-view data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>, the ultimate goal is to reach the same level of analysis from a single image.</p><p>Traditional optimization-based approaches, e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>, have performed very reliably for model-based pose estimation. However, more recently, the interest has moved towards data-driven approaches regressing the parameters of the human body model, directly from images. Considering the lack of images with 3D shape ground truth for training, the main challenge is to identify reliable sources of supervision. Proposed methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref> have focused on leveraging all the available sources of 2D an-  <ref type="figure">Figure 2</ref>: Overview of the proposed texture consistency supervision. Here, for simplicity, the input during training consists of two images i, j of the same person. The main assumption is that the appearance of the person does not change dramatically across the input images, (i.e., the frames come from a monocular video as in <ref type="figure">Figure 1</ref>, or from time-synchronized multi-view cameras). We apply our deep network on both images and estimate the shape of the person. Subsequently, we project the predicted shape on the image, and after inferring visibility for each point on the surface, we build the texture maps Ai and Aj. The crucial observation, that the appearance of the person remains constant, translates to a texture consistency loss, forcing the two texture maps to be equal for all surface points Vij that are visible in both images. This loss acts as supervision for the network and complements other weak losses that are typically used in the training.</p><formula xml:id="formula_0">∼ ∼ V ij A i − A j</formula><p>notations like 2D keypoints, silhouettes, or semantic parts. Simultaneously, external sources of 3D data (e.g., MoCap and body scans) can also be useful, by applying learned priors <ref type="bibr" target="#b17">[18]</ref>, or decomposing the task in different architectural components <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref>. In this work, instead of focusing on the available 2D annotations, or the appropriate way to employ external 3D data, the questions we ask are different. Can natural images alone provide us a useful cue for this task? Is there a form of supervision we can leverage without further annotations? Here, we argue, and demonstrate, that the answer to these questions is positive.</p><p>We present TexturePose, a way to leverage complementary supervision directly from natural images ( <ref type="figure">Figure 2</ref>). The main observation is that the appearance of a person does not change significantly over small periods of time (e.g., during a short video). Our insight is that this appearance constancy enforces strong constraints in the estimated pose of each frame, which naturally translates to a powerful supervision signal that is useful for cases of monocular video or multi-view images. A critical component is the incorporation of a parametric model of the human body, SMPL <ref type="bibr" target="#b24">[25]</ref>, within our pipeline, allowing us to map the texture of the image to a generic texture map, which is independent of the shape and pose. Considering a network estimating the model parameters, during training, we generate the mesh and project it on the image. Through efficient computation, we are able to infer a (partial) texture map for each frame. Our novel supervision signal, based on texture consistency, enforces that the texture of each point of the texture map remains constant for all the frames of the same subject. This seemingly unimportant piece of in-formation goes a long way and proves itself to be a crucial form of auxiliary supervision. We validate its importance in settings involving multiple views of the same subject, or monocular video with very weak annotations. In every case, we compare with approaches that have access to the same level of annotations (or potentially even more), and we consistently outperform them. Ultimately, this supervision allows us to outperform state-of-the-art approaches for model-based pose estimation from a single image.</p><p>Our contributions can be summarized as follows:</p><p>• We propose TexturePose, a novel approach to leverage complementary supervision from natural images through appearance constancy of each human across different frames.</p><p>• We demonstrate the effectiveness of our texture consistency supervision in cases of monocular video and multi-view capture, consistently outperforming approaches with access to the same or more annotations than we do.</p><p>• We achieve state-of-the-art results among model-based 3D human pose estimation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this Section, we summarize the approaches that are more relevant to ours.</p><p>Model-based human pose estimation: Differently from skeleton-based 3D pose estimation, model-based human pose estimation involves a parametric model of the human body, e.g., SCAPE <ref type="bibr" target="#b5">[6]</ref> or SMPL <ref type="bibr" target="#b24">[25]</ref>. The goal is to estimate the model parameters that give rise to a 3D shape which is consistent with image evidence. The initial works in this area <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> as well some more recent approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">52]</ref> were mainly optimization-based. Recently, the trend has shifted to directly regressing the model parameters from a single image using deep networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref>. Given the lack of images with 3D shape ground truth, these approaches typically rely on 2D annotations, like 2D keypoints, silhouettes and semantic parts, as well as external 3D data. Although, we believe there is great merit into using the bulk of already annotated data, in this paper we aspire to get beyond this data and explore complementary forms of supervision which are available also in unlabeled or weakly labeled data.</p><p>Multi-view pose estimation: Our goal in this work is not explicitly to estimate human pose from multiple views (in fact the work of Huang et al. <ref type="bibr" target="#b12">[13]</ref> addressed this nicely in a model-based way). However, our approach is relevant to recent approaches leveraging multi-view consistency as a form of supervision to train deep networks. Pavlakos et al. <ref type="bibr" target="#b33">[34]</ref> estimate 3D poses combining reliable 2D pose estimates, and treats them as pseudo ground truth to train a network for 3D human pose. Simon et al. <ref type="bibr" target="#b43">[44]</ref> propose a similar approach to improve a hand keypoint detector given multi-view data. Rhodin et al. <ref type="bibr" target="#b38">[39]</ref> learn 3D pose estimation by enforcing the pose consistency in all views. On the other hand, follow-up work from Rhodin et al. <ref type="bibr" target="#b37">[38]</ref>, uses multiple views to learn a representation of 3D human pose in an unsupervised manner. In contrast to the above works, we believe that our approach offers much greater opportunities to leverage multi-view consistency. The incorporation of a parametric model allows us to go beyond body joint consistency, by leveraging shape and texture consistency. Simultaneously, instead of learning a new representation from multi-view data, we choose to maintain the SMPL representation, and only leverage the collective power of data to better regress the parameters of this representation.</p><p>Supervision signals: While we have already discussed some aspects of the supervision typically employed for 3D human pose estimation, here we attempt to extend the discussion particularly to the varying levels of supervision used by different works. Full body pose and shape supervision is typically available only in synthetic images <ref type="bibr" target="#b47">[48]</ref>, or images with successful body fits <ref type="bibr" target="#b22">[23]</ref>. Weak supervision provided by 2D annotations is typical, with different works employing 2D keypoints, silhouettes and semantic parts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>. Non-parametric approaches typically use extra supervision from 2D keypoint annotation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55]</ref>, while some recent works leverage ordinal depth relations of the joints <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>. Multi-view consistency is also well explored as discussed earlier <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. In terms of pose priors, Zhou et al. <ref type="bibr" target="#b54">[55]</ref> use weak symmetry constraints, while Kanazawa et al. <ref type="bibr" target="#b17">[18]</ref> incorporates a learning-based prior on pose and shape parameters using adversarial networks. In contrast to the above, instead of using additional annotations or exploiting external information, our goal is to leverage all the information that is available in natural images. This of course does not exclude the use of other supervision forms. In fact, we demonstrate that our approach can properly complement typical supervision signals (e.g., 2D keypoints, pose priors), and improve performance only by additionally enforcing texture consistency.</p><p>Texture-based approaches: The idea of using texture to guide pose estimation goes back at least to the work of Sidenbladh et al. <ref type="bibr" target="#b41">[42]</ref>, where texture consistency was used for tracking. More recently, Bogo et al. <ref type="bibr" target="#b8">[9]</ref> use high resolution texture information to improve registration alignments. Guo et al. <ref type="bibr" target="#b10">[11]</ref> also enforce photometric consistency to recover accurate human geometry over time. Alldieck et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> focus on estimating the texture for human models. In the work of Kanazawa et al. <ref type="bibr" target="#b18">[19]</ref>, texture is employed to learn a parametric model of bird shapes. While we share similar intuitions with the above works, here we propose to use texture as a supervisory signal to guide and improve learning for 3D human pose and shape estimation.</p><p>Finally, to put our work in a greater context, the idea of appearance constancy is popular also beyond human pose estimation, e.g., in approaches for unsupervised learning of depth, ego-motion and optical flow <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">54]</ref>. A key difference is that while they estimate the structure of the world in a non-parametric form (depth map), we instead inject some domain knowledge (i.e., assuming a human pose estimation task) and we leverage a model, SMPL, that helps us explain the image observations. A similar motivation is shared with the work of Tung et al. <ref type="bibr" target="#b46">[47]</ref>. However, our approach is more flexible, since they require keypoints as input to their network, frames should be continuous to allow for motion extraction, while they eventually rely on a separate network for optical flow computation. Simultaneously, we present a more generic framework, which can be applied for monocular video or multi-view images alike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical approach</head><p>In this Section, we start with a short introduction about the representation we use and the basic notation (Subsection 3.1). Then, we describe the regression architecture (Subsection 3.2). We continue with the formulation of texture consistency, and the corresponding loss (Subsection 3.3). Next, we describe the additional losses we can incorporate when we process images from monocular or multi-view input (Subsection 3.4). Finally, we provide an overview of the complete pipeline (Subsection 3.5), and discuss potential weaknesses of our approach (Subsection 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation</head><p>SMPL: The SMPL model <ref type="bibr" target="#b24">[25]</ref> is a parametric model of the human body. Given the input parameters for pose θ, and shape β, the model defines a function M(θ, β) which outputs the body mesh M ∈ R 3×N , with N = 6890 vertices. The body joints X are expressed as a linear combination of the mesh vertices, so using a pre-trained linear regressor W , we can map from the mesh to k joints of interest</p><formula xml:id="formula_1">X ∈ R 3×k = W M .</formula><p>Texture map: The meshes produced by SMPL are deformations of an original template T . A corresponding UV map un-warps the template surface onto an image, A, which is the texture map. Each pixel t of this texture map is also called texel. By construction, the mapping between texels and mesh surface coordinates is fixed and independent of changes in 3D surface geometry.</p><p>Camera: The camera we use follows the weak perspective camera model. The parameters of interest are denoted with π and include the global orientation R ∈ R 3×3 , scale s ∈ R, and translation t ∈ R 2 . Given these parameters, the 2D projection x of the 3D joints X is expressed as:</p><formula xml:id="formula_2">x = π(X) = sΠ(RX) + t,<label>(1)</label></formula><p>where Π stands for the orthographic projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regression model</head><p>Our goal is to learn a predictor f , here realized by a deep network, that given a single image I, it maps it to the pose and shape parameters of the person on the image. More concretely, the output of the network consists of a set of pa-</p><formula xml:id="formula_3">rameters Θ = f (I), where Θ = [θ, β, π].</formula><p>Here, θ and β indicate the SMPL pose and shape parameters, and π are the camera parameters. Our deep network follows the architecture of Kanazawa et al. <ref type="bibr" target="#b17">[18]</ref>, with the exception of the output, which in our case regresses 3D rotations using the representation proposed by Zhou et al. <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TexturePose</head><p>Given θ and β we can generate a mesh M and the corresponding 3D joints X. The mesh can be projected to the image using the estimated camera parameters π. Through efficient computation <ref type="bibr" target="#b28">[29]</ref>, we can infer the visibility for each point on the surface, and as a result, for every texel t of the texture map A. Let us denote with v t the inferred visibility of texel t on the texture map A. The collection of all visibility indices v t can be arranged in a binary mask V , the visibility mask. Considering each point P t on the mesh surface, we can estimate its image projection using the camera parameters, p t = π(P t ). For the visible points, we can estimate their texture via bilinear sampling from the image I, so a t = G(I; p t ), where G is a bilinear sampling kernel. The collection of all values a t , i.e., texture values for every texel t, constitute the texture map A.  <ref type="figure">Figure 3</ref>: With our formulation, training with images from a multicamera system is similar to training with images from monocular video ( <ref type="figure">Figure 2</ref>). The main additional consistency constraint is that the subject has the same 3D shape (same body mesh), which means that we can apply a per-vertex loss between the two mesh predictions. Before applying the predicted global orientation, the mesh predictions are in the same canonical orientation, so we can apply our loss directly on the mesh predictions. In case the extrinsics are provided, we can transform the second mesh to the frame of the first view, and then apply the same loss.</p><p>Let us now assume that we have access to two images i, j of the same person. Using the procedure above, we can estimate the two texture maps A i , A j , along with the corresponding visibility masks V i , V j . Let us denote with V ij = V i V j the mask of the surface points that are visible in both views. Then the texture consistency loss can be simply defined as:</p><formula xml:id="formula_4">L texture cons = ||V ij (A i − A j )||.<label>(2)</label></formula><p>This loss enforces that the texture should be the same for texels (or equivalently, points on the surface) that are visible in both images. Since visibility masks are used only to mask-out the texels that should not contribute to the loss, visibility computation does not have to be differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Beyond texture</head><p>Monocular: In the monocular case, the texture consistency loss is applied between pairs of frames for the same subject. Beyond the texture consistency, we can also enforce that the shape parameters of the subject remain the same for all pairs of frames. This shape consistency can be enforced with the following loss function:</p><formula xml:id="formula_5">L shape cons = ||β i − β j ||.<label>(3)</label></formula><p>Furthermore, we want to guarantee that we get a valid 3D shape, i.e., the estimated pose and shape parameters of the parametric model lie in the space of valid poses and shapes respectively. To enforce this, we use the adversarial prior of Kanazawa et al. <ref type="bibr" target="#b17">[18]</ref>, which factorizes the model parameters into: (i) pose parameters θ, (ii) shape parameters β, and (iii) per-part relative rotations θ i , that is one 3D rotation for each of the 23 joints of SMPL. In the end, we train a discriminator D k for each factor of the body model. The generator loss can then be expressed as:</p><formula xml:id="formula_6">L adv prior = k (D k (Θ) − 1) 2 .<label>(4)</label></formula><p>Depending on the availability of additional 2D keypoint annotations, we can also enforce that the 3D joints project close to the annotated 2D keypoints. We get the projection of the 3D joints X to the 2D locations x, based on Eq. 1. Then, the 2D-3D consistency can be expressed as:</p><formula xml:id="formula_7">L 2D = ||x − x gt ||,<label>(5)</label></formula><p>where x gt are the ground truth 2D joints. Finally, adding smoothness on the pose parameters is also possible, but we avoid it, to keep our approach more generic and applicable even in settings where the frames are not consecutive.</p><p>Multiple views: When we have access to multiple views i and j of a subject at the same time instance, then all the above losses remain relevant. The main additional constraint we need to enforce is that the pose of the person is the same across all viewpoints. This could be incorporated, by simply forcing all the pose parameters to have the same value. In contrast to that, we observed that a loss applied directly on the mesh vertices behaves much better <ref type="figure">(Figure 3</ref>). This can be formulated as a simple per-vertex loss:</p><formula xml:id="formula_8">L mesh cons = ||M i − M j ||.<label>(6)</label></formula><p>Remember that M i , M j do not include the global orientation estimates R i , R j , so both meshes are in the canonical orientation, meaning that we can compare them directly. This loss effectively reflects the more generic case, where no knowledge of the camera extrinsics is available for the multi-view system. If extrinsic calibration is also known, then we simply need to apply the global pose estimates R i , R j , transform the second mesh to the coordinate system of the first mesh and then use the same per-vertex loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complete pipeline</head><p>Our network is trained using batches of images. When we want to use a short sequence in training, or a few timesynchronized viewpoints, we include all the frames of interest in the batch. Typically, for monocular video, we include five consecutive frames, while for multi-view images, we use as many viewpoints are available at a specific time instance (typically four for Human3.6M). Conveniently, during testing, we can process each frame independently, without the need for video or multi-view input.</p><p>Depending on the setting, and making sure that we are compatible with prior work, we can also augment our batches with images that have stronger supervision (e.g., full 3D pose is known). Since the texture consistency assumption alone keeps the problem pretty underconstrained, similar to prior works (e.g., <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>), we found that it was useful to have stronger supervision in at least a few examples. For fair comparisons, in the empirical evaluation, we make sure that we use the same, or strictly less annotations than what prior work is using.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Shortcomings</head><p>Although we empirically demonstrate the significant value of TexturePose (Section 4), it is fair to also identify some of the shortcomings of our approach. For example the constant appearance assumption can easily be violated (e.g., due to illumination or viewpoint changes). Moreover, motion blur is common and can also decrease the level of "clean" pixels we can benefit from. Finally, our approach makes an assumption that no object occludes the person. Since we do not account for the potential occlusions, we can easily fill the texture map with the texture of the occluding object. Although occlusions are not very typical in most of the images for the datasets we use, this can be a source of potential error given a new video for training. The work of Ranjan et al. <ref type="bibr" target="#b36">[37]</ref> addresses a similar problem in the context of Structure from Motion, and we believe that a similar approach should be applicable in our setting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical evaluation</head><p>In this Section, we summarize the empirical evaluation of our approach. First, we provide more details about the datasets we employ for training and evaluation (Subsection 4.1), and then we present quantitative (Subsection 4.2) and qualitative results (Subsection 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For the majority of our ablation studies, we used the Human3.6M dataset <ref type="bibr" target="#b13">[14]</ref>. Additionally, we used training data from the MPII 2D human pose dataset <ref type="bibr" target="#b4">[5]</ref>, while LSP dataset <ref type="bibr" target="#b15">[16]</ref> was employed only to evaluate our approach. In the Sup.Mat. we present more extensive experiments leveraging the recently introduced VLOG-People and In-staVariety datasets <ref type="bibr" target="#b19">[20]</ref> for training, as well as the 3DPW dataset <ref type="bibr" target="#b48">[49]</ref> for evaluation. Human3.6M: It is an indoor benchmark for 3D human pose estimation. It includes multiple subjects performing daily actions like Eating, Smoking and Walking. It provides videos from four calibrated, time-synchronized cameras, making it easy to evaluate the different aspects of our approach both in the monocular and the multi-view setting. For training, we used subjects S1, S5, S6, S7 and S8, unless otherwise stated. Being consistent with prior work <ref type="bibr" target="#b17">[18]</ref>, the evaluation is done on subjects S9 and S11, considering Protocol 1 <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and Protocol 2 <ref type="bibr" target="#b17">[18]</ref>. MPII: It is an in-the-wild dataset for 2D human pose estimation, providing only the 2D joint locations for each person. Previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref> typically employ this dataset because of the large number of 2D keypoint annotations. One typically unexplored advantage of this dataset is the fact that it also provides the neighboring frames of the video that includes the annotated frame. We see this as a large pool of unlabeled data, that we can leverage for free, and we demonstrate their effectiveness in training our models. We call this set "MPII video" and consists of the annotated frames for each video, along with four more frames (two before, two after), which come with no labels. LSP: It is also an in-the-wild dataset for 2D human pose estimation, but of much smaller scale compared to MPII. We employ LSP only for evaluation, where we make use of its test set. Particularly, given our shape prediction, we project it back to the image and we evaluate silhouette and part segmentation accuracy. For this evaluation, we use the segmentation labels provided by <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative evaluation</head><p>Ablative studies: We start with Human3.6M where we initially treat all the images as frames of monocular sequences. One strong baseline, inspired from the "unpaired" setting of <ref type="bibr" target="#b17">[18]</ref>, assumes that the network has access to the 2D joints for each image, and an independent dataset of 3D poses, but no image with corresponding 3D ground truth is available. We train the network with 2D reprojection loss, while we also enforce an adversarial prior for pose and shape parameters such that the predicted poses/shapes are close to the poses/shapes in the dataset. As we can see, in <ref type="table">Table 1</ref> (first row), this gives us decent performance. If we also apply our texture consistency loss, over the frames of the short clips, then we get significant improvement (second row). Finally, to put these results into context, we also train the same architecture providing full 3D pose and shape ground truth for each image (third row). As expected, this ideal version is performing better, but our texture consistency loss managed to close the gap between the weaklyand the fully-supervised setting.</p><p>A similar experiment attempts to investigate the effect of leveraging texture consistency, but this time from in-thewild videos. To this end, we use the frames of MPII video applying our texture consistency. The results for our experiments are presented in <ref type="table" target="#tab_3">Table 2</ref>. The initial baseline (first row) is the same as in <ref type="table">Table 1</ref>, and uses full 3D ground truth from Human3.6M for training. The next thing we want to investigate is whether adding purely unlabeled video can improve performance. So, for the next baseline (second row), we provide no labels for the in-the-wild frames, but enforce texture consistency. Interestingly, the model does  <ref type="figure">2)</ref>. The numbers are mean reconstruction errors in mm. Using only 2D annotations on each frame and an adversarial pose/shape prior, we get reasonable performance. Simply providing more video frames instead of single frames without any additional annotation, we are able to get an important performance improvement, because of the texture consistency loss. As a lower limit, we present results when the ground truth 3D pose and shape parameters are available for each image during training. Although this last version uses explicitly stronger annotations, we are able to shrink the gap between the baselines that train with 2D and 3D annotations respectively. get improved just by seeing more unlabeled data simply by enforcing texture consistency. Unfortunately, we observed that although the performance improves for Human3.6M, when we apply the same model to in-the-wild images, it achieves mediocre results qualitatively. We believe that at least a few labels should be necessary to make the model generalize better. To this end, we conduct two more experiments, adding annotations for one frame of the MPII video sequences. In the first experiment (third row), we add the annotation for the frame, but no texture consistency loss is enforced, while for the second one (fourth row), we both add the annotation for the frame, and we activate the texture consistency loss. As we can see, adding the unlabeled frames helps by default when combined with a texture consistency loss, and gives a solid performance improvement, making the model appropriate both for Human3.6M and for in-the-wild images, as we will present later.</p><p>The same findings extend also to the case that we add more video data that only contain automatic pseudo-annotations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7]</ref>. We present our results using two recently published datasets for training, i.e., VLOG-People and InstaVariety <ref type="bibr" target="#b19">[20]</ref> in the Sup.Mat., along with the 3DPW dataset <ref type="bibr" target="#b48">[49]</ref> for additional evaluation.</p><p>Comparison with the state-of-the-art: For the comparison with the state-of-the-art, we use our best model from the previous experiment (last row of <ref type="table" target="#tab_3">Table 2</ref>). The results are presented in <ref type="table">Table 3</ref>. Our method outperform the previous baselines. Of course, we use MPII video for training which is not used by the other approaches, but making it possible to leverage the unlabeled frames is possible due to the texture consistency loss, which is one of our contributions. At the same time, other approaches also employ explicitly more annotations than we do, e.g., <ref type="bibr" target="#b17">[18]</ref> has access to more images with 2D annotations (COCO <ref type="bibr" target="#b23">[24]</ref>) and 3D annotations (MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref>), which we do not use.   <ref type="figure">2)</ref>, indicating the effect of TexturePose, when we incorporate in-thewild videos (MPII) in our training. Adding unlabeled video frames and enforcing texture consistency (row 2) improves Human3.6M evaluation, but the qualitative performance for in-the-wild images is mediocre. If we add a sparse set of 2D keypoint annotations (row 3), the performance can improve. However, the most interesting aspect is that by simply adding the sparse 2D keypoints labels, the unlabeled video frames and enforcing texture consistency (row 4), we can improve performance even more, meaning that extra unlabeled data can always be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rec. Error</head><p>Lassner et al. <ref type="bibr" target="#b22">[23]</ref> 93.9 Pavlakos et al. <ref type="bibr" target="#b34">[35]</ref> 75.9 NBF <ref type="bibr" target="#b30">[31]</ref> 59.9 HMR <ref type="bibr" target="#b17">[18]</ref> 56.8 Kanazawa et al. <ref type="bibr" target="#b19">[20]</ref> 56.9 Arnab et al. <ref type="bibr" target="#b6">[7]</ref> 54.3 Kolotouros et al. <ref type="bibr" target="#b21">[22]</ref> 50.1 Ours 49.7 <ref type="table">Table 3</ref>: Evaluation on the Human3.6M dataset (Protocol 2). The numbers are mean reconstruction errors in mm. We compare with regression approaches that output a mesh of the human body. Our approach achieves state-of-the-art results.</p><p>Moreover, we evaluate our approach on the LSP dataset using the same model (which has never been trained with images from LSP). Although in-the-wild, this dataset gives us access to segmentation annotations, so that we can evaluate shape estimation implicitly through mesh reprojection. The complete results are presented in <ref type="table" target="#tab_5">Table 4</ref>. Here, we outperform the regression-based baseline of <ref type="bibr" target="#b17">[18]</ref> which is more relevant to ours and we are also very competitive to the optimization-based approaches, which explicitly optimize for the image-model alignment, so they tend to perform better under these metrics.</p><p>Finally, we also compare with baselines trained with multiple-views. We follow the Protocol of Rhodin et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, training with full 3D supervision for S1 and employ the other training users without any further annotations, other than extrinsic calibration. The results are presented in <ref type="table">Table 5</ref>. We successfully outperform both baselines. It is interesting that both <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> are non-parametric approaches, and we are still able to outperform them, con-   <ref type="table">Table 5</ref>: Evaluation on Human3.6M (Protocol 1) for methods trained on multiple views. The numbers are mm in various metrics. We follow the protocol of <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b38">39]</ref>, using full 3D ground truth for S1, and leveraging the other subjects as unlabeled data, where only the camera calibration is known.</p><p>sidering that strong non-parametric baselines <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> typically perform better than parametric approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref> (at least under the 3D joints metrics). We believe that this is exactly because we are able to leverage cues that are not an option for 3D skeleton baselines, e.g., they cannot map texture to a skeleton figure, as we can do with a mesh surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative evaluation</head><p>A variety of qualitative results of our results are provided in <ref type="figure" target="#fig_1">Figure 4</ref> as well as in the Sup.Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this paper, we presented TexturePose, an approach to train neural networks for model-based human pose estimation by leveraging supervision directly from natural images. Effectively, we capitalize on the observation that the appearance of a person does not change dramatically within a short video or for images from multiple views. This allows us to apply a texture consistency loss which acts as a form of auxiliary supervision. This generic formulation makes our approach particularly flexible and applicable in monocular video and multi-view images alike. We compare TexturePose with different baselines requiring the same (or larger) amount of annotations and we consistently outperform them, achieving state-of-the-art results across modelbased pose estimation approaches. Going forwards, we believe that these weak supervision signals could really help us scale our training by leveraging weakly annotated or purely unlabeled data. Having already identified the short-comings of our approach (Subsection 3.6), it is a great challenge to identify ways to go beyond them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative Results. Rows 1-5: LSP dataset. Rows 6-7: H36M dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MPII videos (+texture) + MPII 2D 51.3 49.7</figDesc><table><row><cell></cell><cell>P1 P2</cell></row><row><cell>H36M</cell><cell>64.8 63.9</cell></row><row><cell>H36M + MPII videos (+texture)</cell><cell>60.1 58.6</cell></row><row><cell>H36M</cell><cell>+ MPII 2D 54.1 51.6</cell></row><row><cell>H36M +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the Human3.6M dataset (Protocols 1 &amp;</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation on foreground-background and six-part segmentation on the LSP test set. The numbers are accuracies and f1 scores. Our approach outperforms the strong regression-based baseline of<ref type="bibr" target="#b17">[18]</ref> across theTable,and it is very competitive to the optimization baselines based on SMPLify (which typically have advantage for tasks involving image-model alignment like this). The numbers for the first two rows are taken from<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell cols="3">MPJPE NMPJPE Rec. Error</cell></row><row><cell>Rhodin et al. [39] n/a</cell><cell>153.3</cell><cell>128.6</cell></row><row><cell>Rhodin et al. [38] 131.7 Ours 110.7</cell><cell>122.6 97.6</cell><cell>98.2 74.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully appreciate support through the following grants: NSF-IIP-1439681 (I/UCRC), NSF-IIS-1703319, NSF MRI 1626008, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, ARL DCIST CRA W911NF-17-2-0181, the DARPA-SRC C-BRIC, by Honda Research Institute and a Google Daydream Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detailed human avatars from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3D people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Rıza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic FAUST: Registering human bodies in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time geometry, albedo, and motion reconstruction using a single rgb-d camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2D features and intermediate 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Yu Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3D human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mpi-Is</surname></persName>
		</author>
		<ptr target="https://github.com/MPI-IS/mesh.4" />
		<imprint/>
	</monogr>
	<note>Mesh processing library</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning human optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning monocular 3D human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3D human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
