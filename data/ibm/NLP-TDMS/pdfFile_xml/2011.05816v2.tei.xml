<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
							<email>jycai@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<email>jiewangx@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tensor factorization based models have shown great power in knowledge graph completion (KGC). However, their performance usually suffers from the overfitting problem seriously. This motivates various regularizers-such as the squared Frobenius norm and tensor nuclear norm regularizers-while the limited applicability significantly limits their practical usage. To address this challenge, we propose a novel regularizer-namely, DUality-induced RegulArizer (DURA)-which is not only effective in improving the performance of existing models but widely applicable to various methods. The major novelty of DURA is based on the observation that, for an existing tensor factorization based KGC model (primal), there is often another distance based KGC model (dual) closely associated with it. Experiments show that DURA yields consistent and significant improvements on benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs contain quantities of factual triplets, which represent structured human knowledge. In the past few years, knowledge graphs have made great achievements in many areas, such as natural language processing <ref type="bibr" target="#b36">[37]</ref>, question answering <ref type="bibr" target="#b13">[14]</ref>, recommendation systems <ref type="bibr" target="#b30">[31]</ref>, and computer vision <ref type="bibr" target="#b18">[19]</ref>. Although commonly used knowledge graphs usually contain billions of triplets, they still suffer from the incompleteness problem that a lot of factual triplets are missing. Due to the large scale of knowledge graphs, it is impractical to find all valid triplets manually. Therefore, knowledge graph completion (KGC)-which aims to predict missing links between entities based on known links automatically-has attracted much attention recently.</p><p>Distance based (DB) models and tensor factorization based (TFB) models are two important categories of KGC models. DB models use the Minkowski distance to measure the plausibility of a triplet. Although they can achieve state-of-the-art performance, many of them still have difficulty in modeling complex relation patterns, such as one-to-many and many-to-one relations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>. TFB models treat knowledge graphs as partially observed third-order binary tensors and formulate KGC as a tensor completion problem. Theoretically, these models are highly expressive and can well handle complex relations. However, their performance usually suffers from the overfitting problem seriously and consequently cannot achieve state-of-the-art.</p><p>To tackle the overfitting problem of TFB models, researchers propose various regularizers. The squared Frobenius norm regularizer is a popular one that applies to various models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29]</ref>. However, experiments show that it may decrease performance for some models (e.g., RESCAL) <ref type="bibr" target="#b23">[24]</ref>. More recently, motivated by the great success of the matrix trace norm in the matrix completion problem <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>, Lacroix et al. <ref type="bibr" target="#b15">[16]</ref> propose a tensor nuclear p-norm regularizer. It gains significant improvements against the squared Frobenius norm regularizer. However, it is only suitable for canonical polyadic (CP) decomposition <ref type="bibr" target="#b12">[13]</ref> based models, such as CP and ComplEx <ref type="bibr" target="#b28">[29]</ref>, but not appropriate for a more general class of models, such as RESCAL <ref type="bibr" target="#b22">[23]</ref>. Therefore, it is still challenging to find a regularizer that is both widely applicable and effective.</p><p>In this paper, we propose a novel regularizer for tensor factorization based KGC models-namely, DUality-induced RegulArizer (DURA). The major novelty of DURA is based on the observation called duality-for an existing tensor factorization based KGC model (primal), there is often another distance based KGC model closely associated with it (dual). The duality can be derived by expanding the squared score functions of the associated distance based models. Then, the cross-term in the expansion is exactly a tensor factorization based KGC model, and the squared terms in it give us a regularizer. Using DURA, we can preserve the expressiveness of tensor factorization based KGC models and prevent them from the overfitting problem. DURA is widely applicable to various tensor factorization based models, including CP, ComplEx, and RESCAL. Experiments show that, DURA yields consistent and significant improvements on datasets for the knowledge graph completion task. It is worth noting that, when incorporated with DURA, RESCAL <ref type="bibr" target="#b22">[23]</ref>-which is one of the first knowledge graph completion models-performs comparably to state-of-the-art methods and even beats them on several benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we review the background of this paper in Section 2.1 and introduce the notations used throughout this paper in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Knowledge Graph Given a set E of entities and a set R of relations, a knowledge graph K = {(e i , r j , e k )} ⊂ E × R × E is a set of triplets, where e i and r j are the i-th entity and j-th relation, respectively. Usually, e i and e k are also called the head entity and the tail entity, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Graph Completion (KGC)</head><p>The goal of KGC is to predict valid but unobserved triplets based on the known triplets in K. KGC models contain two important categories: distance based models and tensor factorization based models, both of which are knowledge graph embedding (KGE) methods. KGE models associate each entity e i ∈ E and relation r j ∈ R with an embedding (may be real or complex vectors, matrices, and tensors) e i and r j . Generally, they define a score function s : E ×R×E → R to associate a score s(e i , r j , e k ) with each potential triplet (e i , r j , e k ) ∈ E ×R×E. The scores measure the plausibility of triplets. For a query (e i , r j , ?), KGE models first fill the blank with each entity in the knowledge graphs and then score the resulted triplets. Valid triplets are expected to have higher scores than invalid triplets.</p><p>Distance Based (DB) KGC Models DB models define the score function s with the Minkowski distance. That is, the score functions have the formulation of s(e i , r j , e k ) = − Γ(e i , r j , e k ) . p , where Γ is a model-specific function. Equivalently, we can also use a squared score function s(e i , r j , e k ) = − Γ(e i , r j , e k ) 2 p . Tensor Factorization Based (TFB) KGC Models TFB models regard a knowledge graph as a third-order binary tensor X ∈ {0, 1} |E|×|R|×|E| . The (i, j, k) entry X ijk = 1 if (e i , r j , e k ) is valid otherwise X ijk = 0. Suppose that X j denotes the j-th frontal slice of X , i.e., the adjacency matrix of the j-th relation. Usually, a TFB KGC model factorizes X j as X j ≈ Re (HR j T ), where the i-th (k-th) row of H (T) is e i (e k ), R j is a matrix representing relation r j , Re (·) and · are the real part and the conjugate of a complex matrix, respectively. That is, the score functions are defined as s(e i , r j , e k ) = Re (ē i R j e k ). Note that the real part and the conjugate of a real matrix are itself. Then, the aim of TFB models is to seek matrices H, R 1 , . . . , R |R| , T, such that Re (HR j T ) can approximate X j . LetX j = Re (HR j T ) andX be a tensor of which the j-th frontal slice isX j . The regularized formulation of a tensor factorization based model can be written as</p><formula xml:id="formula_0">min X1,...,X |R| |R| j=1 L(X j ,X j ) + λg(X ),<label>(1)</label></formula><p>where λ &gt; 0 is a fixed parameter, L(X j ,X j ) measures the discrepancy between X j andX j , and g is the regularization function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Notations</head><p>We use h i ∈ E and t k ∈ E to distinguish head and tail entities. Let · 1 , · 2 , and · F denote the L 1 norm, the L 2 norm, and the Frobenius norm of matrices or vectors. We use ·, · to represent the inner products of two real or complex vectors. Specifically, if u, v ∈ C 1×n are two row vectors in the complex space, then the inner product is defined as u, v =ūv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Knowledge graph completion (KGC) models include rule-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>, KGE methods, and hybrid methods <ref type="bibr" target="#b10">[11]</ref>. This work is related to KGE methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. More specifically, it is related to distance based KGE models and tensor factorization based KGE models.</p><p>Distance based models describe relations as relational maps between head and tail entities. Then, they use the Minkowski distance to measure the plausibility of a given triplet. For example, TransE <ref type="bibr" target="#b3">[4]</ref> and its variants <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17]</ref> represent relations as translations in vector spaces. They assume that a valid triplet (h i , r j , t k ) satisfies h i,rj + r j ≈ t k,rj , where h i,rj and t k,rj mean that entity embeddings may be relation-specific. Structured embedding (SE) <ref type="bibr" target="#b2">[3]</ref> uses linear maps to represent relations. Its score function is defined as <ref type="bibr" target="#b26">[27]</ref> defines each relation as a rotation in a complex vector space and the score function is defined as <ref type="bibr" target="#b37">[38]</ref> assumes that R 1 j is diagonal and R 2 j is an identity matrix. It shares a similar score function s</p><formula xml:id="formula_1">s(h i , r j , t k ) = − R 1 j h i −R 2 j t k 1 . RotatE</formula><formula xml:id="formula_2">s(h i , r j , t k ) = − h i • r j − t k 1 , where h i , r j , t k ∈ C k and |[r] i | = 1. ModE</formula><formula xml:id="formula_3">(h i , r j , t k ) = − h i • r j − t k 1 with RotatE but h i , r j , t k ∈ R k .</formula><p>Tensor factorization based models formulate the KGC task as a third-order binary tensor completion problem. RESCAL <ref type="bibr" target="#b22">[23]</ref> factorizes the j-th frontal slice of X as X j ≈ AR j A , in which embeddings of head and tail entities are from the same space. As the relation specific matrices contain lots of parameters, RESCAL is prone to be overfitting. DistMult <ref type="bibr" target="#b33">[34]</ref> simplifies the matrix R j in RESCAL to be diagonal, while it sacrifices the expressiveness of models and can only handle symmetric relations. In order to model asymmetric relations, ComplEx <ref type="bibr" target="#b28">[29]</ref> extends DistMult to complex embeddings. Both DistMult and ComplEx can be regarded as variants of CP decomposition <ref type="bibr" target="#b12">[13]</ref>, which are in real and complex vector spaces, respectively.</p><p>Tensor factorization based (TFB) KGC models usually suffer from overfitting problem seriously, which motivates various regularizers. In the original papers of TFB models, the authors usually use the squared Frobenius norm (L 2 norm) regularizer <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29]</ref>. This regularizer cannot bring satisfying improvements. Consequently, TFB models do not gain comparable performance to distance based models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. More recently, Lacroix et al. <ref type="bibr" target="#b15">[16]</ref> propose to use the tensor nuclear 3-norm <ref type="bibr" target="#b8">[9]</ref> (N3) as a regularizer, which brings more significant improvements than the squared Frobenius norm regularizer. However, it is designed for the CP-like models, such as CP and ComplEx, and not suitable for more general models such as RESCAL. Moreover, some regularization methods aim to leverage external background knowledge <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. For example, to model equivalence and inversion axioms, Minervini et al. <ref type="bibr" target="#b19">[20]</ref> impose a set of model-dependent soft constraints on the predicate embeddings. Ding et al. <ref type="bibr" target="#b6">[7]</ref> use non-negativity constraints on entity embeddings and approximate entailment constraints on relation embeddings to impose prior beliefs upon the structure of the embeddings space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section, we introduce a novel regularizer-DUality-induced RegulArizer (DURA)-for tensor factorization based knowledge graph completion. We first introduce basic DURA in Section 4.1 and explain why it is effective in Section 4.2. Then, we introduce DURA in Section 4.3. Finally, we give a theoretical analysis for DURA under some special cases in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic DURA</head><p>Consider the knowledge graph completion problem (h i , r j , ?). That is, we are given the head entity and the relation, aiming to predict the tail entity. Suppose that f j (i, k) measures the plausibility of a   <ref type="figure" target="#fig_1">Figure 1b</ref> shows that TFB models without regularization can get the same score even though the embeddings of t k are dissimilar. (c) <ref type="figure" target="#fig_1">Figure 1c</ref> shows that with DURA, embeddings of t k are encouraged to locate in a small region.</p><formula xml:id="formula_4">given triplet (h i , r j , t k ), i.e., f j (i, k) = s(h i , r j , t k ).</formula><p>Then the score function of a TFB model is</p><formula xml:id="formula_5">f j (i, k) = Re (h i R j t k ) = Re ( h i R j , t k ).<label>(2)</label></formula><p>It first maps the entity embeddings h i by a linear transformation R j , and then uses the real part of an inner product to measure the similarity between h i R j and t k . Notice that another commonly used similarity measure-the squared Euclidean distance-can replace the inner product similarity in Equation <ref type="formula" target="#formula_5">(2)</ref>. We can obtain an associated distance based model formulated as</p><formula xml:id="formula_6">f E j (i, k) = − h i R j − t k 2 2 .<label>(3)</label></formula><p>Therefore, there exists a duality: for an existing tensor factorization based KGC model (primal), there is often another distance based KGC model (dual) closely associated with it.</p><p>Specifically, the relationship between the primal and the dual can be formulated as</p><formula xml:id="formula_7">f E j (i, k) = − h i R j − t k 2 2 = − h i R j 2 2 − t k 2 2 + 2Re ( h i R j , t k ) = 2f j (i, k) − h i R j 2 2 − t k 2 2 .<label>(4)</label></formula><p>Usually, we expect f E j (i, k) and f j (i, k) to be higher for all valid triplets (h i , r j , t k ) than those for invalid triplets. Suppose that S is the set that contains all valid triplets. Then, for triplets in S, we have that</p><formula xml:id="formula_8">max f E j (i, k) = min −f E j (i, k) = min −2f j (i, k) + h i R j 2 2 + t k 2 2 .<label>(5)</label></formula><p>By noticing that min −2f j (i, k) = max 2f j (i, k) is exactly the aim of a TFB model, the duality induces a regularizer for tensor factorization based KGC models, i.e.,</p><formula xml:id="formula_9">(hi,rj ,t k )∈S h i R j 2 2 + t k 2 2 ,<label>(6)</label></formula><p>which is called basic DURA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Why Basic DURA Helps</head><p>In this section, we demonstrate that basic DURA encourages tail entities connected to a head entity through the same relation to have similar embeddings, which accounts for its effectiveness in improving performance of TFB models.</p><p>First, we claim that tail entities connected to a head entity through the same relation should have similar embeddings. Suppose that we know a head entity h i and a relation r j , and our aim is to predict the tail entity. If r j is a one-to-many relation, i.e., there exist two entities t 1 and t 2 such that both (h i , r j , t 1 ) and (h i , r j , t 2 ) are valid, then we expect that t 1 and t 2 have similar semantics. For example, if two triplets (felid, include, tigers) and (felid, include, lions) are valid, then tigers and lions should have similar semantics. Further, we expect that entities with similar semantics have similar embeddings. In this way, if we have known that (tigers, is, mammals) is valid, then we can predict that (lions, is, mammals) is also valid. See <ref type="figure" target="#fig_1">Figure 1a</ref> for an illustration of the prediction process.</p><p>However, TFB models fail to achieve the above goal. As shown in <ref type="figure" target="#fig_1">Figure 1b</ref>, suppose that we have known h iRj when the embedding dimension is 2. Then, we can get the same score s(h i , r j , t k ) for k = 1, 2, . . . , n so long as t k lies on the same line perpendicular to h iRj . Generally, the entities t 1 and t 2 have similar semantics. However, their embeddings t 1 and t 2 can even be orthogonal, which means that the two embeddings are dissimilar. Therefore, the performance of TFB models for knowledge graph completion is usually unsatisfying.</p><p>By Equation <ref type="formula" target="#formula_8">(5)</ref>, we know that basic DURA constraints the distance between h iRj and t k . When h i andR j are known, t k lies in a small region (see <ref type="figure" target="#fig_1">Figure 1c</ref> and we verify this claim in Section 5.4). Therefore, tail entities connected to a head entity through the same relation will have similar embeddings, which is beneficial to the prediction of unknown triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DURA</head><p>Basic DURA encourages tail entities with similar semantics to have similar embeddings. However, it cannot handle the case that head entities have similar semantics.</p><p>Suppose that two triplets (tigers, is, mammals) and (lions, is, mammals) are valid. Similar to the discussion in Section 4.2, we expect that tigers and lions have similar semantics and thus have similar embeddings. If we further know that (felid, include, tigers) is valid, we can predict that (felid, include, lions) is valid. However, basic DURA cannot handle the case. Let h 1 , h 2 , t 1 , and R 1 be the embeddings of tigers, lions, mammals, and is, respectively. Then, Re (h 1 R 1 t 1 ) and Re (h 2 R 1 t 1 ) can be equal even if h 1 and h 2 are orthogonal, as long as</p><formula xml:id="formula_10">h 1 R 1 = h 2 R 1 .</formula><p>To tackle the above issue, noticing that Re</p><formula xml:id="formula_11">(h i R j t k ) = Re (t k R j h i ), we define another dual distance based KGC modelf E j (i, k) = − t k R j − h i 2 2</formula><p>, Then, similar to the derivation in Equation <ref type="formula" target="#formula_8">(5)</ref>, the duality induces a regularizer given by</p><formula xml:id="formula_12">(hi,rj ,t k )∈S t k R j 2 + h i 2 .<label>(7)</label></formula><p>When a TFB model are incorporated with regularizer <ref type="formula" target="#formula_12">(7)</ref>, head entities with similar semantics will have similar embeddings.</p><p>Finally, combining the regularizer <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_12">(7)</ref>, DURA has the form of</p><formula xml:id="formula_13">(hi,rj ,t k )∈S h i R j 2 2 + t k 2 2 + t k R j 2 2 + h i 2 2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Theoretic Analysis for Diagonal Relation Matrices</head><p>If we further relax the summation condition in the regularizer (8) to all possible entities and relations, we can write DURA as:</p><formula xml:id="formula_14">|E| |R| j=1 ( HR j 2 F + T 2 F + TR j 2 F + H 2 F ),<label>(9)</label></formula><p>where |E| and |R| are the number of entities and relations, respectively.</p><p>In the rest of this section, we use the same definitions ofX j andX as in the problem <ref type="bibr" target="#b0">(1)</ref>. When the relation embedding matrices R j are diagonal in R or C as in CP or ComplEx, the formulation <ref type="bibr" target="#b8">(9)</ref> gives an upper bound to the tensor nuclear 2-norm ofX , which is an extension of trace norm regularizers in matrix completion. To simplify the notations, we take CP as an example, in which all involved embeddings are real. The conclusion in complex space can be analogized accordingly.</p><formula xml:id="formula_15">Definition 1 (Friedland &amp; Lim [9]). The nuclear 2-norm of a 3D tensor A ∈ R n1 ⊗ R n2 ⊗ R n3 is A * = min r i=1 u 1,i 2 u 2,i 2 u 3,i 2 : A = r i=1 u 1,i ⊗ u 2,i ⊗ u 3,i , r ∈ N ,</formula><p>where u k,i ∈ R n k for k = 1, ..., 3, i = 1, ..., r, and ⊗ denotes the outer product.</p><p>For notation convenience, we define a relation matrix R ∈ R |R|×D , of which the j-th row consists of the diagonal entries of R j . That is, R(j, d) = R j <ref type="figure">(d, d)</ref>, where R(i, j) represents the entry in the i-th row and j-th column of the matrix R.</p><p>In the knowledge graph completion problem, the tensor nuclear 2-norm ofX is</p><formula xml:id="formula_16">X * = min D d=1 h :d 2 r :d 2 t :d 2 :X = D d=1 h :d ⊗ r :d ⊗ t :d ,</formula><p>where D is the embedding dimension, h :d , r :d , and t :d are the d-th columns of H, R, and T.</p><p>For DURA in <ref type="formula" target="#formula_14">(9)</ref>, we have the following theorem.</p><p>Theorem 1. Suppose thatX j = HR j T for j = 1, 2, . . . , |R|, where H, T, R j are real matrices and R j is diagonal. Then, the following equation holds</p><formula xml:id="formula_17">min Xj =HRj T 1 |R| |R| j=1 ( HR j 2 F + T 2 F + TR j 2 F + H 2 F ) = X * .</formula><p>The minimization attains if and only if h Proof. See the supplementary material.</p><p>Therefore, DURA in <ref type="bibr" target="#b8">(9)</ref> gives an upper bound to the tensor nuclear 2-norm, which is a tensor analog to the matrix trace norm.</p><p>Remark DURA in <ref type="formula" target="#formula_13">(8)</ref> is actually a weighted version of the one in <ref type="bibr" target="#b8">(9)</ref>, in which the regularization terms corresponding to the sampled valid triplets. As shown in Srebro &amp; Salakhutdinov <ref type="bibr" target="#b24">[25]</ref> and Lacroix et al. <ref type="bibr" target="#b15">[16]</ref>, the weighted versions of regularizers usually outperform the unweighted regularizer when entries of the matrix or tensor are sampled non-uniformly. Therefore, in the experiments, we implement DURA in a weighted way as in <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we introduce the experimental settings in Section 5.1 and show the effectiveness of DURA in Section 5.2. We compare DURA to other regularizers in Section 5.3 and visualize the entity embeddings in Section 5.4. Finally, we analyze the sparsity induced by DURA in Section 5.5. The code of DURA is available on GitHub at https://github.com/MIRALab-USTC/KGE-DURA. We consider three public knowledge graph datasets-WN18RR <ref type="bibr" target="#b27">[28]</ref>, FB15k-237 <ref type="bibr" target="#b5">[6]</ref>, and YAGO3-10 <ref type="bibr" target="#b17">[18]</ref> for the knowledge graph completion task, which have been divided into training, validation, and testing set in previous works. The statistics of these datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. WN18RR, FB15k-237, and YAGO3-10 are extracted from WN18 <ref type="bibr" target="#b3">[4]</ref>, FB15k <ref type="bibr" target="#b3">[4]</ref>, and YAGO3 <ref type="bibr" target="#b17">[18]</ref>, respectively. Toutanova &amp; Chen <ref type="bibr" target="#b27">[28]</ref> and Dettmers et al. <ref type="bibr" target="#b5">[6]</ref> indicated the test set leakage problem in WN18 and FB15k, where some test triplets may appear in the training dataset in the form of reciprocal relations. They created WN18RR and FB15k-237 to avoid the test set leakage problem, and we use them as the benchmark datasets. We use MRR and Hits@N (H@N) as evaluation metrics. For more details of training and evaluation protocols, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Moreover, we find it better to assign different weights for the parts involved with relations. That is, the optimization problem has the form of min (ei,rj ,e k )∈S</p><formula xml:id="formula_18">[ ijk (H, R 1 , . . . , R J , T) +λ(λ 1 ( h i 2 2 + t k 2 2 ) + λ 2 ( h i R j 2 2 + t k R j 2 2 ))],</formula><p>where λ, λ 1 , λ 2 &gt; 0 are fixed hyperparameters. We search λ in {0.005, 0.01, 0.05, 0.1, 0.5} and λ 1 , λ 2 in {0.5, 1.0, 1.5, 2.0}. <ref type="table">Table 2</ref>: Evaluation results on WN18RR, FB15k-237 and YAGO3-10 datasets. We reimplement CP, DistMult, ComplEx, and RESCAL using the "reciprocal" setting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>, which leads to better results than the reported results in the original paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>In this section, we compare the performance of DURA against several state-of-the-art KGC models, including CP <ref type="bibr" target="#b12">[13]</ref>, RESCAL <ref type="bibr" target="#b22">[23]</ref>, ComplEx <ref type="bibr" target="#b28">[29]</ref>, TuckER <ref type="bibr" target="#b1">[2]</ref> and some DB models: RotatE <ref type="bibr" target="#b26">[27]</ref>, MuRP <ref type="bibr" target="#b0">[1]</ref>, and HAKE <ref type="bibr" target="#b37">[38]</ref>. <ref type="table">Table 2</ref> shows the effectiveness of DURA. RESCAL-DURA and ComplEx-DURA perform competitively with the SOTA DB models. RESCAL-DURA outperforms all the compared DB models in terms of MRR and H@1. Note that we reimplement CP, ComplEx, and RESCAL under the "reciprocal" setting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, and obtain better results than the reported performance in the original papers. Overall, TFB models with DURA significantly outperform those without DURA, which shows its effectiveness in preventing models from overfitting.</p><p>Generally, models with more parameters and datasets with smaller sizes imply a larger risk of overfitting. Among the three datasets, WN18RR has the smallest size of only 11 kinds of relations and around 80k training samples. Therefore, the improvements brought by DURA on WN18RR are expected to be larger compared with other datasets, which is consistent with the experiments. As stated in Wang et al. <ref type="bibr" target="#b31">[32]</ref>, RESCAL is a more expressive model, but it is prone to overfit on small-and medium-sized datasets because it represents relations with much more parameters. For example, on WN18RR dataset, RESCAL gets an H@10 score of 0.493, which is lower than ComplEx (0.522). The advantage of its expressiveness does not show up at all. However, incorporated with DURA, RESCAL gets an 8.4% improvement on H@10 and finally attains 0.577, which outperforms all compared models. On larger datasets such as YAGO3-10, overfitting also exists but may be non-significant. Nonetheless, DURA still leads to consistent improvement, demonstrating the ability of DURA to prevent models from overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to Other Regularizers</head><p>In this section, we compare DURA to the popular squared Frobenius norm regularizer and the recent tensor nuclear 3-norm (N3) regularizer <ref type="bibr" target="#b15">[16]</ref>. The squared Frobenius norm regularizer is given by</p><formula xml:id="formula_19">g(X ) = H 2 F + T 2 F + |R| j=1 R j 2 F . N3 regularizer is given by g(X ) = D d=1 ( h :d 3 3 + r :d 3 3 + t :d 3</formula><p>3 ), where · 3 denotes L 3 norm of vectors. We implement both the squared Frobenius norm (FRO) and N3 regularizers in the weighted way as stated in Lacroix et al. <ref type="bibr" target="#b15">[16]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the performance of the three regularizers on three popular models: CP, ComplEx, and RESCAL. Note that when the TFB model is RESCAL, we only compare DURA to the squared Frobenius norm regularization as N3 does not apply to it.</p><p>For CP and ComplEx, DURA brings consistent improvements compared to FRO and N3 on all datasets. Specifically, on FB15k-237, compared to CP-N3, CP-DURA gets an improvement of 0.013 in terms of MRR. Even for the previous state-of-the-art TFB model ComplEx, DURA brings further improvements against the N3 regularizer. Incorporated with FRO, RESCAL performs worse than the vanilla model, which is consistent with the results in Ruffinelli et al. <ref type="bibr" target="#b23">[24]</ref>. However, RESCAL-DURA brings significant improvements against RESCAL. All the results demonstrate that DURA is more widely applicable than N3 and more effective than the squared Frobenius norm regularizer. In this section, we visualize the tail entity embeddings using T-SNE <ref type="bibr" target="#b29">[30]</ref> to show that DURA encourages tail entities with similar semantics to have similar embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization</head><p>Suppose that (h i , r j ) is a query, where h i and r j are head entities and relations, respectively. An entity t k is an answer to a query (h i , r j ) if (h i , r j , t k ) is valid. We randomly selected 10 queries in FB15k-237, each of which has more than 50 answers. <ref type="bibr" target="#b0">1</ref> Then, we use T-SNE to visualize the answers' embeddings generated by CP and CP-DURA. <ref type="figure" target="#fig_2">Figure  2</ref> shows the visualization results. Each entity is represented by a 2D point and points in the same color represent tail entities with the same (h i , r j ) context (i.e. query). <ref type="figure" target="#fig_2">Figure 2</ref> shows that, with DURA, entities with the same (h i , r j ) contexts are indeed being assigned more similar representations, which verifies the claims in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sparsity Analysis</head><p>As real-world knowledge graphs usually contain billions of entities, the storage of entity embeddings faces severe challenges. Intuitively, if embeddings are sparse, that is, most of the entries are zero, we can store them with less storage. Therefore, the sparsity of the generated entity embeddings becomes crucial for real-world applications. In this part, we analyze the sparsity of embeddings induced by different regularizers. Generally, there are few entries of entity embeddings that are exactly equal to 0 after training, which means that it is hard to obtain sparse entity embeddings directly. However, when we score triplets using the trained model, the embedding entries with values close to 0 will have minor contributions to the score of a triplet. If we set the embedding entries close to 0 to be exactly 0, we can transform embeddings into sparse ones. Thus, there is a trade-off between sparsity and performance decrement.</p><p>We define the following λ-sparsity to indicate the proportion of entries that are close to zero:</p><formula xml:id="formula_20">s λ = I i=1 D d=1 1 {|x|&lt;λ} (E id ) I × D ,<label>(10)</label></formula><p>where E ∈ R I×D is the entity embedding matrix, E id is the entry in the i-th row and d-th column of E, I is the number of entities, D is the embedding dimension, and 1 C (x) is the indicator function that takes value of 1 if x ∈ C or otherwise the value of 0.</p><p>To generate sparse version entity embeddings, following Equation <ref type="formula" target="#formula_0">(10)</ref>, we select all the entries of entity embeddings-of which the absolute value are less than a threshold λ-and set them to be 0. Note that for any given s λ , we can always find a proper threshold λ to approximate it, as the formula is increasing with respect to λ. Then, we evaluate the quality of sparse version entity embeddings on the knowledge graph completion task. <ref type="figure" target="#fig_4">Figure 3</ref> shows the effect of entity embeddings' λ-sparsity on MRR. Results in the figure show that DURA causes much gentler performance decrement as the embedding sparsity increases. In <ref type="figure" target="#fig_4">Figure 3a</ref>, incorporated with DURA, CP maintains MRR of 0.366 unchanged even when 60% entries are set to 0. More surprisingly, when the sparsity reaches 70%, CP-DURA can still outperform CP-N3 with zero sparsity. For RESCAL, when setting 80% entries to be 0, RESCAL-DURA still has the MRR of 0.341, which significantly outperforms vanilla RESCAL, whose MRR has decreased from 0.352 to 0.286. In a word, incorporating with DURA regularizer, the performance of CP and RESCAL remains comparable to the state-of-the-art models, even when 70% of entity embeddings' entries are set to 0.</p><p>Following Han et al. <ref type="bibr" target="#b11">[12]</ref>, we store the sparse version embedding matrices using compressed sparse row (CSR) format or compressed sparse column (CSC) format, which requires 2a + n + 1 numbers, where a is the number of non-zero elements and n is the number of rows or columns. Experiments show that DURA brings about 65% fewer storage costs for entity embeddings when 70% of the entries are set to 0. Therefore, DURA can significantly reduce the storage usage while maintaining satisfying performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a widely applicable and effective regularizer-namely, DURA-for tensor factorization based knowledge graph completion models. DURA is based on the observation that, for an existing tensor factorization based KGC model (primal), there is often another distance based KGC model (dual) closely associated with it. Experiments show that DURA brings consistent and significant improvements to TFB models on benchmark datasets. Moreover, visualization resultls show that DURA can encourage entities with similar semantics to have similar embeddings, which is beneficial to the prediction of unknown triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Proof for Theorem 1 Theorem 2. Suppose thatX j = HR j T for j = 1, 2, . . . , |R|, where H, T, R j are real matrices and R j is diagonal. Then, the following equation holds Proof. We have that In the same manner, we know that</p><formula xml:id="formula_21">min Xj =HRj T 1 |R| |R| j=1 ( HR j 2 F + T 2 F + TR j 2 F + H 2 F ) = X * .</formula><formula xml:id="formula_22">|R| j=1 HR j 2 F + T 2 F = |R| j=1 I i=1 h i • r j 2 F + D d=1 t :d 2 F = |R| j=1 D d=1 t :d 2 F + I i=1 D d=1 h 2 id r 2 jd = |R| j=1 D d=1 t :d</formula><formula xml:id="formula_23">1 2 |R| min Xj =HRj T |R| j=1 ( TR j 2 F + H 2 F ) = X * .</formula><p>The equality holds if and only if t :d 2 r :d 2 = |R| h :d 2 .</p><p>Therefore, the conclusion holds if and only if h :d 2 r :d 2 = |R| t :d 2 and t :d 2 r :d 2 = |R| h :d 2 , ∀ d ∈ {1, 2, . . . , D}.</p><p>Therefore, for DURA, we know that</p><formula xml:id="formula_24">min Xj =HRj T 1 |R| g(X ) = X * ,</formula><p>which completes the proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The optimal value of p</head><p>In DB models, the commonly used p is either 1 or 2. When p = 2, DURA takes the form as the one in Equation <ref type="formula" target="#formula_13">(8)</ref> in the main text. If p = 1, we cannot expand the squared score function of the associated DB models as in Equation <ref type="formula" target="#formula_7">(4)</ref>. Thus, the induced regularizer takes the form of (hi,rj ,t k )∈S h iRj − t k 1 + t k R j − h i 1 . The above regularizer with p = 1 (Reg_p1) does not gives an upper bound on the tensor nuclear-2 norm as in Theorem 1. <ref type="table" target="#tab_3">Table 4</ref> shows that, DURA significantly outperforms Reg_p1 on WN18RR and FB15k-237. Therefore, we choose p = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Complexity</head><p>Suppose that k is the number of triplets known to be true in the knowledge graph, n is the embedding dimension of entities. Then, for CP and ComplEx, the complexity of DURA is O(kn); for RESCAL, the complexity of DURA is O(kn 2 ). That is to say, the computational complexity of weighted DURA is the same as the weighted squared Frobenius norm regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Details About Experiments</head><p>In this section, we introduce the training protocol and the evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Training Protocol</head><p>We adopt the cross entropy loss function for all considered models as suggested in Ruffinelli et al. <ref type="bibr" target="#b23">[24]</ref>. We adopt the "reciprocal" setting that creates a new triplet (e k , r −1 j , e i ) for each triplet (e i , r j , e k ) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>. We use Adagrad <ref type="bibr" target="#b7">[8]</ref> as the optimizer, and use grid search to find the best hyperparameters based on the performance on the validation datasets. Specifically, we search learning rates in {0.1, 0.01}, regularization coefficients in {0, 1 × 10 −3 , 5 × 10 −3 , 1 × 10 −2 , 5 × 10 −2 , 1 × 10 −1 , 5 × 10 −1 }. On WN18RR and FB15k-237, we search batch sizes in {100, 500, 1000} and embedding sizes <ref type="table">Table 5</ref>: Hyperparameters found by grid search. k is the embedding size, b is the batch size, λ is the regularization coefficients, and λ 1 and λ 2 are weights for different parts of the regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k-237 YAGO3-10 We implement DURA in PyTorch and run on all experiments with a single NVIDIA GeForce RTX 2080Ti graphics card.</p><formula xml:id="formula_25">k b λ λ 1 λ 2 k b λ λ 1 λ 2 k b λ λ 1 λ</formula><p>As we regard the link prediction as a multi-class classification problem and adopt the cross entropy loss, we can assign different weights for different classes (i.e., tail entities) based on their frequency of occurrence in the training dataset. Specifically, suppose that the loss of a given query (h, r, ?) is ((h, r, ?), t), where t is the true tail entity, then the weighted loss is w(t) ((h, r, ?), t), where w(t) = w 0 #t max{#t i : t i ∈ training set} + (1 − w 0 ), w 0 is a fixed number, #t denotes the frequency of occurrence in the training set of the entity t. For all models on WN18RR and RESCAL on YAGO3-10, we choose w 0 = 0.1 and for all the other cases we choose w 0 = 0.</p><p>We choose a learning rate of 0.1 after grid search. <ref type="table">Table 5</ref> shows the other best hyperparameters for DURA found by grid search. Please refer to the Experiments part in the main text for the search range of the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Evaluation Protocol</head><p>Following Bordes et al. <ref type="bibr" target="#b3">[4]</ref>, we use entity ranking as the evaluation task. For each triplet (h i , r j , t k ) in the test dataset, the model is asked to answer (h i , r j , ?) and (t k , r −1 j , ?). To do this, we fill the positions of missing entities with candidate entities to create a set of candidate triplets, and then rank the triplets in descending order by their scores. Following the "Filtered" setting in Bordes et al. <ref type="bibr" target="#b3">[4]</ref>, we then filter out all existing triplets known to be true at ranking. We choose Mean Reciprocal Rank (MRR) and Hits at N (H@N) as the evaluation metrics. Higher MRR or H@N indicates better performance. Detailed definitions are as follows.</p><p>• The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:</p><formula xml:id="formula_26">MRR = 1 |Q| |Q| i=1 1 rank i .</formula><p>• The Hits@N is the ratio of ranks that no more than N :</p><formula xml:id="formula_27">Hits@N = 1 |Q| |Q| i=1 1 x≤N (rank i ),</formula><p>where 1 x≤N (rank i ) = 1 if rank i ≤ N or otherwise 1 x≤N (rank i ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 The queries in T-SNE visualization</head><p>In <ref type="table" target="#tab_4">Table 6</ref>, we list the ten queries used in the T-SNE visualization (Section 5.4 in the main text). Note that a query is represented as (h, r, ?), where h denotes the head entity and r denotes the relation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of why basic DURA can improve the performance of TFB models when the embedding dimensions are 2. Suppose that triplets (h i , r j , t k ) (k = 1, 2, . . . , n) are valid. (a)Figure 1ademonstrates that tail entities connected to a head entity through the same relation should have similar embeddings. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>d 2 r :d 2 =</head><label>2</label><figDesc>|R| t :d 2 and t :d 2 r :d 2 = |R| h :d 2 , ∀ d ∈ {1, 2, . . . , D}, where h :d , r :d , and t :d are the d-th columns of H, R, and T, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the embeddings of tail entities using T-SNE. A point represents a tail entity. Points in the same color represent tail entities that have the same (h r , r j ) context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The effect of entity embeddings' λ-sparsity on MRR. The used dataset is FB15k-237.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The equation holds if and only if h :d 2 r :d 2 = |R| t :d 2 and t :d 2 r :d 2 = |R| h :d 2 , for all d ∈ {1, 2, . . . , D}, where h :d , r :d , and t :d are the d-th columns of H, R, and T, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 2 2 r :d 2 2= 2 2d 2 √ 2 t :d 2 √t :d such that h :d 2 r :d 2 = 2 F + T 2 F</head><label>222222222</label><figDesc>|R| h :d 2 r :d 2 t :d 2 =2 |R| D d=1 h :d 2 r :d 2 t :d 2 . The equality holds if and only if h :d |R| t :d , i.e., h :d 2 r :d 2 = |R| t :d 2 . For all CP decompositionX = D d=1 h :d ⊗ r :d ⊗ t :d , we can always let h :d = h :d , r :d = t |R| h :d 2 r :d 2 r :d and t :d = h :d 2 r :d |R| |R| t :d 2 , and meanwhile ensure thatX = D d=1 h :d ⊗ r :d ⊗ t :d . Therefore, we know that ) = min X = D d=1 h :d ⊗r :d ⊗t :d D d=1 h :d 2 r :d 2 t :d 2 = X * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1e-1 0.5 1.5 2000 100 5e-2 0.5 1.5 1000 1000 5e-2 0.5 1.5 RESCAL 512 1024 1e-1 1.0 1.0 512 512 1e-1 2.0 1.5 512 1024 5e-2 1.0 1.0 in {500, 1000, 2000}. On YAGO3-10, we search batch sizes in {256, 512, 1024} and embedding sizes in {500, 1000}. We search both λ 1 and λ 2 in {0.5, 1.0, 1.5, 2.0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of three benchmark datasets.</figDesc><table><row><cell></cell><cell cols="3">WN18RR FB15k-237 YAGO3-10</cell></row><row><cell>#Entity</cell><cell>40,943</cell><cell>14,541</cell><cell>123,182</cell></row><row><cell>#Relation</cell><cell>11</cell><cell>237</cell><cell>37</cell></row><row><cell>#Train</cell><cell>86,835</cell><cell>272,115</cell><cell>1,079,040</cell></row><row><cell>#Valid</cell><cell>3,034</cell><cell>17,535</cell><cell>5,000</cell></row><row><cell>#Test</cell><cell>3,134</cell><cell>20,466</cell><cell>5,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between DURA, the squared Frobenius norm (FRO), and the nuclear 3-norm (N3) regularizers. Results of * are taken from Lacroix et al.<ref type="bibr" target="#b15">[16]</ref>. CP-N3 and ComplEx-N3 are re-implemented and their performances are better than the reported results in Lacroix et al.<ref type="bibr" target="#b15">[16]</ref>. The best performance on each model are marked in bold.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell cols="2">YAGO3-10</cell></row><row><cell></cell><cell cols="9">MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10</cell></row><row><cell>CP-FRO*</cell><cell>.460</cell><cell>-</cell><cell>.480</cell><cell>.340</cell><cell>-</cell><cell>.510</cell><cell>.540</cell><cell>-</cell><cell>.680</cell></row><row><cell>CP-N3</cell><cell cols="3">.470 .430 .544</cell><cell cols="3">.354 .261 .544</cell><cell cols="3">.577 .505 .705</cell></row><row><cell>CP-DURA</cell><cell cols="3">.478 .441 .552</cell><cell cols="3">.367 .272 .555</cell><cell cols="3">.579 .506 .709</cell></row><row><cell>ComplEx-FRO*</cell><cell>.470</cell><cell>-</cell><cell>.540</cell><cell>.350</cell><cell>-</cell><cell>.530</cell><cell>.570</cell><cell>-</cell><cell>.710</cell></row><row><cell>ComplEx-N3</cell><cell cols="3">.489 .443 .580</cell><cell cols="3">.366 .271 .558</cell><cell cols="3">.577 .502 .711</cell></row><row><cell cols="4">ComplEx-DURA .491 .449 .571</cell><cell cols="3">.371 .276 .560</cell><cell cols="3">.584 .511 .713</cell></row><row><cell>RESCAL-FRO</cell><cell cols="3">.397 .363 .452</cell><cell cols="3">.323 .235 .501</cell><cell cols="3">.474 .392 .628</cell></row><row><cell cols="4">RESCAL-DURA .498 .455 .577</cell><cell cols="3">.368 .276 .550</cell><cell cols="3">.579 .505 .712</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison to Reg_p1. "R": RESCAL. "C": ComplEx.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15k-237</cell><cell></cell></row><row><cell></cell><cell cols="6">MRR H@1 H@10 MRR H@1 H@10</cell></row><row><cell cols="2">R-Reg_p1 .281</cell><cell>.220</cell><cell>.394</cell><cell>.310</cell><cell>.228</cell><cell>.338</cell></row><row><cell cols="2">C-Reg_p1 .409</cell><cell>.393</cell><cell>.439</cell><cell>.316</cell><cell>.229</cell><cell>.487</cell></row><row><cell>R-DURA</cell><cell>.498</cell><cell>.455</cell><cell>.577</cell><cell>.368</cell><cell>.276</cell><cell>.550</cell></row><row><cell>C-DURA</cell><cell>.491</cell><cell>.449</cell><cell>.571</cell><cell>.371</cell><cell>.276</cell><cell>.560</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>The queries in T-SNE visualizations. Academy Award for Best Original Song, /award/award_category/winners./award/award_honor/ceremony,</figDesc><table><row><cell cols="2">Index Query</cell></row><row><cell>1</cell><cell>(political drama, /media_common/netflix_genre/titles, ?)</cell></row><row><cell>2</cell><cell>(?)</cell></row><row><cell>3</cell><cell>(Germany, /location/location/contains,?)</cell></row><row><cell>4</cell><cell>(doctoral degree , /education/educational_degree/people_with_this_degree./education/education/major_field_of_study,?)</cell></row><row><cell>5</cell><cell>(broccoli, /food/food/nutrients./food/nutrition_fact/nutrient,?)</cell></row><row><cell>6</cell><cell>(shooting sport, /olympics/olympic_sport/athletes./olympics/olympic_athlete_affiliation/country,?)</cell></row><row><cell>7</cell><cell>(synthpop, /music/genre/artists, ?)</cell></row><row><cell>8</cell><cell>(Italian American, /people/ethnicity/people,?)</cell></row><row><cell>9</cell><cell>(organ, /music/performance_role/track_performances./music/track_contribution/role, ?)</cell></row><row><cell>10</cell><cell>(funk, /music/genre/artists, ?)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For more details about the 10 queries, please refer to the supplementary material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-relational poincaré graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using simple constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Nuclear norm of higher-order tensors. Mathematics of Computation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1255" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on World Wide Web</title>
		<meeting>the 22nd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Biennial Conference on Innovative Data Systems Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularizing knowledge graph embeddings via equivalence and inversion axioms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nováček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Vandenbussche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>Ceci, M., Hollmén, J., Todorovski, L., Vens, C., and Džeroski, S.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial sets for regularising neural link predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017-08-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2056" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Saul, L. K., Weiss, Y., and Bottou, L.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on Continuous Vector Space Models and Their Compositionality</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Propagating user preferences on the knowledge graph for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ripplenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2735" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
