<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Region of Interest Extraction Layer for Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Rossi</surname></persName>
							<email>leonardo.rossi@unipr.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IMP Lab -D.I.A</orgName>
								<orgName type="institution">University of Parma Parma</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akbar</forename><surname>Karimi</surname></persName>
							<email>akbar.karimi@unipr.it</email>
							<affiliation key="aff1">
								<orgName type="laboratory">IMP Lab -D.I.A. University of Parma Parma</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Prati</surname></persName>
							<email>andrea.prati@unipr.it</email>
							<affiliation key="aff2">
								<orgName type="laboratory">IMP Lab -D.I.A. University of Parma Parma</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Region of Interest Extraction Layer for Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone. This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful information. Therefore, the proposed layer (called Generic RoI Extractor -GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance.</p><p>A comprehensive ablation study at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.</p><p>The code is publicly available on GitHub repository at https: //github.com/IMPLabUniPr/mmdetection/tree/groie dev</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Nowadays, instance segmentation is one of the most studied topics in the computer vision community. It differs from both object detection, where the final output is the set of rectangular bounding boxes which localize and classify any object instance, and semantic segmentation, where the goal is to classify any image pixel without considering if it is part of a specific instance. In instance segmentation, the final goal is to be able to cut the single instances of objects from the original image. Its characteristics make this task very useful for several advanced applications, such as object relationship detection, automatic image captioning, content-based image retrieval, and many others.</p><p>In the recent literature, many studies have addressed the instance segmentation problem. The proposed architectures can be grouped into two main categories: one-step and twostep architectures. The one-step architectures obtain the results with a single pass, making a direct prediction from the input image. On the contrary, an architecture belonging to the second category (two-step) is usually composed of a Region Proposal Network (RPN) <ref type="bibr" target="#b0">[1]</ref>, which returns a list of Regions of Interest (RoI) that are likely to contain the searched object, followed by a more specialized network with the purpose of detecting or segmenting the object/instance within each of the bounding boxes found. These networks descend from their ancestor network called R-CNN <ref type="bibr" target="#b0">[1]</ref>.</p><p>The typical components of a two-step architecture are shown in <ref type="figure">Fig. 1</ref>. As it can be seen in the diagram, the layer (highlighted in red) connecting the two steps is usually represented by the RoI extractor, which is the main focus of this paper. Since this layer plays a crucial role in terms of final results, it should be carefully designed to minimize the loss of information.</p><p>The main objective of this layer is to perform pooling in order to transform the input region, which can be of any size, to a fixed-size feature map. Several previous papers have tackled this problem using different RoI pooling algorithms such as RoI Align <ref type="bibr" target="#b1">[2]</ref>, RoI Warp <ref type="bibr" target="#b2">[3]</ref> and Precise RoI Pooling <ref type="bibr" target="#b3">[4]</ref>. Since instances of objects can appear in the image with different scales, the existing architectures (as shown in <ref type="figure">Fig. 1</ref>) exploit a Feature Pyramid Network (FPN) <ref type="bibr" target="#b4">[5]</ref> combined with an RPN (e.g. Fast R-CNN <ref type="bibr" target="#b5">[6]</ref>, Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> and Mask R-CNN <ref type="bibr" target="#b1">[2]</ref>), to generate multi-scale feature maps. An FPN is composed of a bottom-up pathway, where final convolutional layers from the backbone are often chosen, followed by a top-down pathway to reconstruct spatial resolution from the upper layers of the pyramid that have a higher semantic value. With the introduction of a FPN, the fundamental issue is the selection of a FPN layer to which the RoI pooler will be applied.</p><p>Traditional methods make the selection based on the RoI obtained by the RPN. They use the formula proposed by <ref type="bibr" target="#b4">[5]</ref> to discover the best k-th layer to sample from, which is based on the width w and height h of the RoI as follows:</p><formula xml:id="formula_0">k = k 0 + log 2 âˆš wh/244<label>(1)</label></formula><p>where k 0 represents the highest level feature map and 224 is the typical image size used to pre-train the backbone with ImageNet dataset. This hard selection of a single layer of FPN might limit the power of the network's description and our intuition (supported by previous works, such as <ref type="bibr" target="#b7">[8]</ref>) is that if all scale-specific features are retained, better object detection and segmentation results can be achieved.</p><p>The main contributions of this paper are the following: 1) A novel RoI extraction layer called GRoIE is proposed, with the aim of a more generic, configurable and interchangeable framework for RoI extraction in two-step architectures for instance segmentation. 2) Exhaustive ablation study on different components of the proposed layer is conducted in order to evaluate how the performance changes depending on the various choices. 3) GRoIE is introduced to the major state-of-the-art architectures to demonstrate its superior performance with respect to traditional RoI extraction layers. The paper is organized as follows. Section II describes the state of the art. In Section III, the proposed architecture is described in detail. Section IV describes the experimental methodology as well as our in-depth ablation study on component selection. Additionally, in this section, we show how the inclusion of GRoIE layer in state-of-the-art architectures can lead to significant improvements in the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As mentioned in the introduction, modern detectors employ a RoI extraction layer to select the features produced by the backbone network according to the candidate bounding boxes coming from a RPN. This layer was first introduced in R-CNN network. Since then, many architectures derived from R-CNN (e.g., Mask R-CNN, Grid R-CNN <ref type="bibr" target="#b8">[9]</ref>, Cascade R-CNN <ref type="bibr" target="#b9">[10]</ref>, HTC <ref type="bibr" target="#b10">[11]</ref> and GC-net <ref type="bibr" target="#b11">[12]</ref>) have used this layer as well. Usually, to be more invariant to object scale, the layer is not directly applied to the backbone features, but instead to an FPN attached on top of the backbone.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, a RoI pooling action is applied to a single heuristically-selected FPN output layer. This approach suffers from a problem related to untapped information. In <ref type="bibr" target="#b12">[13]</ref>, the authors propose to extract mask proposals from each scale separately, rescale them and include the resulting scales in a unique multi-scale ranked list. Eventually, only the best proposals are selected. In <ref type="bibr" target="#b13">[14]</ref>, the authors propose to fuse features belonging to different scales by max function, using an independent backbone for each image scale. In our work, on the contrary, we utilize a feature pyramid to simplify the network and avoid doubling the number of parameters for each scale. In SharpMask <ref type="bibr" target="#b14">[15]</ref>, the authors make a coarse mask prediction after which they fuse feature layer back in a top-down fashion until reaching the same size of the input image. In PANet <ref type="bibr" target="#b7">[8]</ref>, the authors highlight that the information is not strictly connected with a single layer of the FPN. They propagate low-level features, building another FPN-like structure coupled with the original FPN, where the RoI-pooled images are combined. Our proposed GRoIE layer is inspired by this approach with the difference that it is more lightweight because of not using any extra FPN-coupled stack and proposes a novel way to aggregate data from the RoI-pooled features. Auto-FPN <ref type="bibr" target="#b15">[16]</ref> extends PANet model by applying the Neural Architecture Search (NAS) concept. Also, AugFPN <ref type="bibr" target="#b16">[17]</ref> can be considered an extension of PANet model. The module we directly compare our module with is the Soft RoI Selector, which performs a RoI pooling on each FPN layer for concatenating the results. Subsequently, through the Adaptive Spatial Fusion, they are combined to create a weight map which passes through 1x1 and 3x3 convolutions sequentially. In our case, we first apply a distinct convolutional operation on each layer of the FPN output which very effectively helps the network to automatically focus on the best scales. Next, we apply a sum instead of concatenation because we have proven it has a greater learning potential for the network. Finally, an attention layer is applied that combines fully-connected layers and convolutions to further filter the multi-scale context.</p><p>In Multi-Scale Subnet <ref type="bibr" target="#b17">[18]</ref>, authors propose an alternative method to RoI Align which uses crop-resized branches to extract the RoI at different scales. They use convolution with 1x1 kernel to simply maintain the same number of outputs for each branch without the purpose of helping the network to process data. Then, before summing all branches, they apply an average pooling to reduce each branch to the same size. Finally, a convolutional layer with 3x3 kernel is used as postprocessing stage. In our ablation study, we demonstrate that these convolutional configurations for pre-and post-processing are not the best ones possible to achieve better performance.</p><p>IONet <ref type="bibr" target="#b18">[19]</ref> proposes not to use any FPN network but Method AP AP 50 AP 75 APs APm AP l baseline <ref type="bibr" target="#b4">[5]</ref> 36.5 58.  <ref type="table" target="#tab_4">TABLE I  COMPARISON OF DIFFERENT METHODS FOR SELECTING FPN LAYERS.  TRAINING AND TESTING ARE PERFORMED ON COCO MINIVAL DATASET  WITH 12 TRAINING EPOCHS. FOR EXPLANATION OF THE DIFFERENT   EVALUATION METRICS IN THE TABLE COLUMNS, PLEASE REFER TO   SECTION IV-A</ref> concatenated, re-scaled and dimension-reduced features directly from the backbone before performing classification and bounding box regression. Finally, Hypercolumn <ref type="bibr" target="#b19">[20]</ref> employs a hypercolumn representation to classify a pixel, using convolutions with 1x1 kernel and up-sampling the results to a common size to be able to sum them all. In this case, the absence of a optimized RoI pooling solution and an FPN can negatively affect the final performance. Moreover, simply processing columns of pixels taken from different stages of the backbone can be a limitation. In fact, in our ablation study we will demonstrate that adjacent pixels are important for optimally extracting information within the various features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERIC ROI EXTRACTION LAYER</head><p>The FPN is an architecture commonly used to extract features from different image resolutions. It has been demonstrated to have an effective power to maintain spatial information avoiding the expensive computation caused by a separate elaboration of each scale. Inside a two-stage detection framework, one FPN output layer is heuristically selected as unique source of RoI Pooling action. Although the formula is well thought out, it is clear that the layer selection is the result of an arbitrary choice.</p><p>In order to demonstrate this statement, we have compared this heuristic (proposed by <ref type="bibr" target="#b4">[5]</ref>) as baseline with a random selection of the FPN layer to sample from. <ref type="table">Table I</ref> shows the average precision (AP) with different metrics (detailed in Section IV-A). Comparing the first two rows of the table, it is evident that the difference between the randomly-selected and the heuristic choice is not enormous. As a further proof, <ref type="figure" target="#fig_0">Fig.  2</ref> shows the progress with training epochs and demonstrates that the progress is similar. This is understandable considering that each FPN layer is derived from the previous one. It means that information is existent in the FPN layers, but in a more or less tangled way to be classified by the following modules of the network.</p><p>These results highlight that the network is capable of extracting information with good enough quality to discriminate classes from any available scale. To corroborate this finding, we have also tried to sum the FPN layers, obtaining an improvement of 0.3% in average precision (see <ref type="table">Table I</ref> and <ref type="figure" target="#fig_0">Fig. 2</ref>). This enhancement suggests that if all the layers are aggregated appropriately, it is more likely to produce higher quality features.</p><p>Based on these preliminary ideas, we propose a novel RoI extraction layer called Generic RoI Extractor (GRoIE) whose architecture can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>GRoIE is composed of the following modules:</p><p>1) RoI pooler module: it is a module that performs a max pooling on non-uniform region of interest to obtain a fixed-size representation. Currently, many pooling techniques such as RoI Pooling <ref type="bibr" target="#b5">[6]</ref> and RoI Align <ref type="bibr" target="#b1">[2]</ref> are available. Among the existing RoI pooling techniques, we found RoI Align <ref type="bibr" target="#b1">[2]</ref> as the most appropriate since it reduces a rectangular feature map region by dividing the original RoI in equal boxes and applying bilinear interpolation inside each of them. This helps to avoid pixel quantization. 2) Pre-processing module: its objective is to apply a preliminary elaboration to the pooled regions. This gives the network an additional degree of freedom which is specific for each image scale. This module is devoted to pre-processing the feature maps and it is usually obtained by means of a convolutional layer associated with each image scale. As will be shown in the ablation analysis reported in Section IV-C, the optimal configuration consists of a single 5x5 convolutional layer per scale. Our experiments suggest that it is not convenient to process the features individually which can be explained by acknowledging that each feature is semantically connected with adjacent features. This is particularly true, remembering that the final objective is object detection/segmentation and, usually, objects are spread over a consistent region of the image. 3) Aggregation module: it defines how to aggregate the single RoIs coming from each branch. The most frequent operations are concatenation and summation. There are multiple ways of merging different branches. After our  ablation analysis, we found that the sum is able to minimize the number of features to be computed for the next layer, and this requires less effort from the network to converge to a stable training. 4) Post-processing module: it is an extra elaboration step applied to the merged features before eventually returning them. It permits the network to learn global features, jointly considering all the scales. To strengthen informative power of the final RoI, three module types have been considered for post-processing: a convolutional layer, a non-local layer <ref type="bibr" target="#b20">[21]</ref> and an attention layer <ref type="bibr" target="#b21">[22]</ref>. Although the attention module is more complex because it requires also a fully-connected layer, our ablation analysis demonstrates that it is the best performing choice. The reason is that unlike the pre-processing module, the main objective of this layer is to eliminate useless information. In particular, the "query content and relative position" configuration, called Îµ 2 in <ref type="bibr" target="#b21">[22]</ref>, attention factor is used. This is more sensitive to the query content and have the higher impact on image contents. Summarizing, starting from a region produced by the RPN, for each scale, a fixed-size RoI is pooled from the region. The resulting n feature maps are, first, separately pre-processed and, then, merged into a single feature map. Finally, postprocessing is applied to extract global information. This architecture grants an equal contribution of each scale and benefits from the information embodied in all FPN layers by overcoming the limitations inherent in the arbitrary choice of a single FPN layer. It is worth noting that this procedure is valid for both object detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section two sets of experiments are reported. The first set is a module-wise ablation analysis of the proposed GRoIE layer with the aim of finding the best combination of choices for each of the modules described in the previous section. As was mentioned above, GRoIE can be plugged into architectures for both object detection (bounding box) and instance segmentation.</p><p>In the first set of experiments, we focus on object detection task only and employ the well-known Faster R-CNN as baseline. In the second set, we apply GRoIE, with the best configuration found, to different architectures with the aim of showing the improvement in average precision for both object detection and instance segmentation. This will allow us to show that the improvement produced by GRoIE is independent from both tasks as well as the utilized architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Evaluation Metrics</head><p>Datasets. In order to evaluate our proposal, we performed experiments on MS COCO dataset 2017 <ref type="bibr" target="#b22">[23]</ref> which is the de facto standard dataset for large-scale object detection and instance segmentation tasks. It is composed of 80 object categories and contains more than 116 thousand images in its training set. Evaluation Metrics. To extract the metrics, we used the official COCO python package. The validation dataset, referred to as minival, includes 5000 images.</p><p>The package calculates the Average Precision (AP) with different IoU (Intersection over the Union) thresholds for both bounding box and segmentation tasks. The primary metric, indicated simply as AP , is calculated with IoU thresholds from 0.5 to 0.95. Other metrics include AP 50 with the IoU threshold of 0.5 and AP 75 with 0.75. In addition, separate metrics are calculated for small (AP s ), medium (AP m ) and large (AP l ) objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>All the results with which we compare ours are not taken from the original papers, but they were obtained by training on the same hardware, with the same configuration (apart the RoI extractor) and by using the original authors' code when available. These precautions are taken in order not to have the comparison affected by any small changes in either the configuration or the code. We used MMDetection <ref type="bibr" target="#b23">[24]</ref> as base framework to develop our code.</p><p>The following base configuration was used for every experiment. Experiments were conducted on 6 GPUs (Nvidia Tesla P100 with 12 GB of memory) for 12 epochs with an initial learning rate of 0.015, with a weight decay of 0.0001 after 9 and 11 epochs, a batch size of 2 images per GPU, and a random seed always equals to the number zero. Since in most of the experiments reported in the literature, reference hardware is composed of 8 GPUs with batch size 2 and learning rate equal to 0.02, we followed the Linear Scaling Rule proposed in <ref type="bibr" target="#b24">[25]</ref> to have a fair comparison. The long edge and short edge of the images were resized to 1333 and  800, but the aspect ratio was maintained. ResNet50 <ref type="bibr" target="#b25">[26]</ref> was used as backbone and RoI Align was selected for the RoI Pooling module (no ablation analysis was conducted on this module).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Module-wise ablation analysis</head><p>In this section, we investigate how the choices of the GRoIE modules influence its final performance. We compare our RoI Extractor architectures with the baseline represented by the single-layer RoI extractor proposed as part of the Faster R-CNN on <ref type="bibr" target="#b6">[7]</ref> paper. Aggregation module analysis. We start from aggregation module because choosing how to merge the data technically has a significant importance on the architecture of the module itself. In order to evaluate the effects of different choices separately, neither pre-processing nor post-processing are applied in this experiment. Each FPN output layer is RoI pooled to create a 256 dimensional feature map and subsequently merged to form a single RoI.</p><p>There are mainly two choices for aggregating different branches: concatenation and summation. In the first case, we need to reduce the feature maps from 1024 to 256 dimensions because we have 4 FPN layers, each one composed by 256 dimensions feature maps. This can be easily done using a convolutional layer with 1x1 kernel. A sum-based aggregation is simpler, but a fair comparison with concatenation is needed. Therefore, in addition to a naive sum operator, we included a variant of sum aggregation followed by a convolutional layer with 1x1 kernel as post-processing. We call this sum+.   <ref type="table">Table II</ref> shows the comparison between the proposed choices and a single-layer RoI extractor module (indicated as "baseline"). To better justify our final choice, we show in <ref type="figure" target="#fig_2">Fig. 4</ref> the trend in average precision when the training epochs progress. Looking at the results of sum+ and concatenation, one might argue that the integration of different FPN layers, the basis of our work, is not always beneficial. This can be attributed to the added complexity which can be, in some cases, counterproductive and generate side effects. In the case of sum, while at the beginning the trend is very similar, later in the training this operator achieves better accuracy with a stable trend, suggesting that this gap could potentially increase with more training epochs. Therefore, we selected sum operator for the aggregation module of GRoIE. Pre-processing module analysis. For this ablation analysis, as mentioned above and based on the findings of the previous module, we chose the sum operator for the aggregation module and did not apply any post-processing. With regard to pre-processing, we consider three possible choices: using a convolutional layer with different kernel sizes, using a non-local module or using an attention module which was described in the previous section. <ref type="table">Table III</ref> shows the comparison of these choices with the baseline as in the case of the aggregation module. Regarding the convolutional layer, it can be noticed that by increasing the kernel size, the results are consistently improved. This confirms the close correlation between neighboring features. We should mention that the processed feature maps are only 7x7 in size. This stopped us from increasing the kernel furthermore. Post-processing module analysis. Finally, we analyze the post-processing module, by keeping the sum operator as aggregation strategy and not applying pre-processing.</p><p>Comparing <ref type="table" target="#tab_4">Tables III and IV which contain results</ref> for pre-and post-processing modules reveals a major difference. While in the former, convolutional layers with different kernel sizes improve the results but non-local/attention modules do  not, in the latter table the outcomes are opposite; that is, improvement of convolutional layers is negligible, while nonlocal and attention methods bring about noticeable enhancement. This can be explained by the fact that while in preprocessing there is the need to extract spatial contributions of the different layers where convolution acts correctly, in the post-processing phase the layers have already been merged by the aggregation module. Therefore, convolution does not add significant information. On the contrary, in post-processing, non-local and attention methods are able to remove useless information by focusing only on the significant parts of the image with attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application of GRoIE to different architectures</head><p>As stated at the beginning of this section, the second set of experiments starts from the choices made on GRoIE modules based on the ablation analysis and integrates our proposed layer within several state-of-the-art architectures, with the aim of evaluating its benefits for both object detection and instance segmentation. We have considered, first of all, the networks that best represent the two-stage networks: Faster R-CNN and Mask R-CNN. Furthermore, we have taken into consideration the networks that have shown the best results in the recent years: Grid R-CNN <ref type="bibr" target="#b8">[9]</ref> for object detection and GC-net <ref type="bibr" target="#b11">[12]</ref> for instance segmentation too. For the latter network, there are two RoI extractors. The first one is used for the detection part to extract the RoIs provided by the RPN; the second one is used by the segmentation part to extract the RoIs provided by the detection.</p><p>For this experiment, we have thus replaced only the standard RoI extraction modules with GRoIE in its most performing configuration: sum as aggregation function, 5x5 convolution for pre-processing and attention module for post-processing. <ref type="table">Table V</ref> shows the achieved results for both object detection (bounding boxes) and instance segmentation. It is rather evident that the introduction of GRoIE as RoI extraction layer strongly contributes to an improvement in precision in all the tested architectures. As expected, the amount of this improvement is not always the same and varies from a minimum of 0.7% AP to a maximum of 1.1% AP for bounding boxes, and from a minimum of 1.3% AP to a maximum of 1.7% AP for instance segmentation. Looking at the other evaluation metrics, the gain is even more noticeable, with a maximum of 2.2% for AP l in GC-net.</p><p>This improvement is even more evident from Figs. 5 and 6, where the average precision is illustrated with the progress of training epochs. In these graphs, it can be seen that in later epochs the positive effect of GRoIE increases, suggesting that it can arguably be even higher with more training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a novel RoI extraction layer for two-step architectures designed for object detection and instance segmentation. The intuition underlying our proposal is that all the feature scales obtained by an FPN are potentially equally-useful for obtaining good final results. The proposed layer, called GRoIE (Generic RoI Extractor), builds upon this intuition by first pre-processing each single layer, then aggregating them together, and finally applying attentive mechanisms as post-processing in order to remove useless (global) information.</p><p>Experiments are conducted on COCO dataset and a comprehensive ablation study has been conducted in order to select the best configuration of modules. Furthermore, the addition of GRoIE to state-of-the-art two-step architectures for both object detection and instance segmentation has shown a consistent improvement in average precision in all the experiments.</p><p>While preliminary, the results reported in this paper are quite promising and seem to indicate the potentiality of GRoIE as novel extraction layer. As a consequence, our future works will concentrate on exploiting the modularity of GRoIE to further enhance the quality of the output features to improve the overall accuracy of different computer vision applications. In addition, neural networks are now increasingly heavy to perform. For this reason, an important field of exploration also for GRoIE regards precisely adopting every possible stratagem to lighten the workload while keeping performance unchanged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Average precision trend for different FPN layer selection strategies. Training and testing are performed on COCO minival dataset with 12 training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Generic RoI Extraction framework. (1) RoI Pooler. (2) Preprocessing phase. (3) Aggregation function. (4) Post-processing phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Aggregation module analysis of average precisions trend on training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Object detection average precision on minival COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Instance segmentation average precision on minival COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>ANALYSIS ON POST-PROCESSING MODULE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>TABLE V AVERAGE PRECISION W/ AND W/O OUR GROIE MODULE. IN THE CASE OF OBJECT DETECTION NETWORKS, SINCE THEY DO NOT MAKE IMAGE SEGMENTATION, AN N/A HAS BEEN INSERTED.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Instance segmentation</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>APs</cell><cell>APm</cell><cell>AP l</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>APs</cell><cell>APm</cell><cell>AP l</cell></row><row><cell>Faster R-CNN</cell><cell>r50-FPN</cell><cell>36.5</cell><cell>58.4</cell><cell>39.1</cell><cell>21.9</cell><cell>40.4</cell><cell>46.8</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>+GRoIE (ours)</cell><cell>r50-FPN</cell><cell>37.5</cell><cell>59.2</cell><cell>40.6</cell><cell>22.3</cell><cell>41.5</cell><cell>47.8</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Grid R-CNN</cell><cell>r50-FPN</cell><cell>39.1</cell><cell>57.2</cell><cell>42.2</cell><cell>22.1</cell><cell>43.0</cell><cell>50.6</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>+GRoIE (ours)</cell><cell>r50-FPN</cell><cell>39.8</cell><cell>58.1</cell><cell>42.9</cell><cell>23.6</cell><cell>43.9</cell><cell>51.5</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Mask R-CNN</cell><cell>r50-FPN</cell><cell>37.3</cell><cell>58.9</cell><cell>40.4</cell><cell>21.7</cell><cell>41.1</cell><cell>48.2</cell><cell>34.1</cell><cell>55.5</cell><cell>36.1</cell><cell>18.0</cell><cell>37.6</cell><cell>46.7</cell></row><row><cell>+GRoIE (ours)</cell><cell>r50-FPN</cell><cell>38.4</cell><cell>59.9</cell><cell>41.7</cell><cell>22.9</cell><cell>42.1</cell><cell>49.7</cell><cell>35.8</cell><cell>57.1</cell><cell>38.0</cell><cell>19.1</cell><cell>39.0</cell><cell>48.7</cell></row><row><cell>GC-net</cell><cell>r50-FPN</cell><cell>39.5</cell><cell>62.0</cell><cell>42.7</cell><cell>24.6</cell><cell>43.2</cell><cell>51.6</cell><cell>35.9</cell><cell>58.5</cell><cell>38.0</cell><cell>20.4</cell><cell>39.4</cell><cell>49.0</cell></row><row><cell>+GRoIE (ours)</cell><cell>r50-FPN</cell><cell>40.3</cell><cell>62.4</cell><cell>44.0</cell><cell>24.2</cell><cell>44.4</cell><cell>52.5</cell><cell>37.2</cell><cell>59.3</cell><cell>39.8</cell><cell>20.2</cell><cell>41.0</cell><cell>51.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research benefits from the HPC (High Performance Computing) facility of the University of Parma, Italy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1476" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Augfpn: Improving multi-scale feature learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05384</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale subnetwork for roi pooling for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Linh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Theory and Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
