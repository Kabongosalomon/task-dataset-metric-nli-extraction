<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
							<email>debesh@simula.no</email>
							<affiliation key="aff0">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Arctic University of Norway</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Ali</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krister</forename><surname>Emanuelsen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Simula Research Laboratory</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Garcia-Ceja</surname></persName>
							<affiliation key="aff9">
								<orgName type="institution">Sintef Digital</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>De Lange</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Augere Medical AS</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Medical Department</orgName>
								<orgName type="institution">Sahlgrenska University Hospital-Mölndal</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Medical Research</orgName>
								<orgName type="institution">Baerum Hospital</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Karolinska University Hospital</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Håvard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Arctic University of Norway</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Arctic University of Norway</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Gastrointestinal endoscopy · Tool segmentation · Endo- scopic instrument · Convolutional Neural Network · Benchmarking</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gastrointestinal (GI) pathologies are periodically screened, biopsied, and resected using surgical tools. Usually the procedures and the treated or resected areas are not specifically tracked or analysed during or after colonoscopies. Information regarding disease borders, development and amount and size of the resected area get lost. This can lead to poor follow-up and bothersome reassessment difficulties post-treatment. To improve the current standard and also to foster more research on the topic we have released the "Kvasir-Instrument" dataset which consists of 590 annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists. Additionally, we provide a baseline for the segmentation of the GI tools to promote research and algorithm development. We obtained a dice coefficient score of 0.9158 and a Jaccard index of 0.8578 using a classical U-Net architecture. A similar dice coefficient score was observed for DoubleUNet. The qualitative results showed that the model did not work for the images with specularity and the frames with multiple instruments, while the best result for both methods was observed on all other types of images. Both, qualitative and quantitative results show that the model performs reasonably good, but there is a large potential for further improvements. Benchmarking using the dataset provides an opportunity for researchers to contribute to the field of automatic endoscopic diagnostic and therapeutic tool segmentation for gastrointestinal (GI) endoscopy. arXiv:2011.08065v1 [physics.med-ph] 23 Oct 2020 2 Jha et al.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Gastrointestinal (GI) pathologies are periodically screened, biopsied, and resected using surgical tools. Usually the procedures and the treated or resected areas are not specifically tracked or analysed during or after colonoscopies. Information regarding disease borders, development and amount and size of the resected area get lost. This can lead to poor follow-up and bothersome reassessment difficulties post-treatment. To improve the current standard and also to foster more research on the topic we have released the "Kvasir-Instrument" dataset which consists of 590 annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists. Additionally, we provide a baseline for the segmentation of the GI tools to promote research and algorithm development. We obtained a dice coefficient score of 0.9158 and a Jaccard index of 0.8578 using a classical U-Net architecture. A similar dice coefficient score was observed for DoubleUNet. The qualitative results showed that the model did not work for the images with specularity and the frames with multiple instruments, while the best result for both methods was observed on all other types of images. Both, qualitative and quantitative results show that the model performs reasonably good, but there is a large potential for further improvements. Benchmarking using the dataset provides an opportunity for researchers to contribute to the field of automatic endoscopic diagnostic and therapeutic tool segmentation for gastrointestinal (GI) endoscopy.</p><p>Keywords: Gastrointestinal endoscopy · Tool segmentation · Endoscopic instrument · Convolutional Neural Network · Benchmarking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Minimally Invasive Surgery (MIS) is a commonly used technique in surgical procedures. The advantage of MIS is that small surgical incisions are made in the patient for endoscopy that causes less pain, reduced time of the hospital stay, fast recovery, reduced blood loss, and less scaring process as compared to the traditional open surgery. The nature of the operation is complex, and the surgeons have to precisely tackle hand-eye coordination, which may lead to restricted mobility and a narrow field of view <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, unlike the treatment of accessory organs such as liver and pancreas, no incision is required for GI tract organs (oesophagus, stomach, duodenum, colon, and rectum). GI procedures also includes both, minimally invasive surveillance and treatment (including surgery) procedures. A varied number of tools are used as per the requirement of these procedures. For example, balloon dilatation to help open the GI surface, biopsy forceps for tissue sample collection, polyp removal with snares and submucosal injections.</p><p>A computer and robotic-assisted surgical system can enhance the capability of the surgeons <ref type="bibr" target="#b8">[9]</ref>. It can provide the opportunity to gain additional information about the patient, which can be useful for decision making during surgery <ref type="bibr" target="#b5">[6]</ref>. However, it is difficult to understand the spatial relationship between surgical instruments, cameras, and anatomy for the patient <ref type="bibr" target="#b11">[12]</ref>. In GI tract endoscopy, it is vital to track and guide surgeons during tumor resection or biopsy collection from a defined site, and help to correlate the biopsied samples and treatment locations post-diagnostic and therapeutic or surgical procedures. While most datasets and automated-algorithm developments for instrument segmentation are mostly focused on laparoscopy-based surgical removal, automatic guidance of tools for GI tract surgery has not been addressed before.</p><p>New developments in the area of robot-assisted systems show that there is potential for developing a fully automated robotic surgeon <ref type="bibr" target="#b14">[15]</ref>. The da Vinci robot is a surgical system that is considered the de-facto standard-of-care for certain urological, gynecological, and general procedures <ref type="bibr" target="#b3">[4]</ref>. Thus, it is critical to have information regarding the intra-operative guidance, which plays an essential role in decision making. However, there are specific challenges, such as limited field of view and difficulties with the surgeons handling the instruments during surgery <ref type="bibr" target="#b13">[14]</ref>. Therefore, image-based instrument segmentation and tracking are gaining more and more attention in both robotic and non-robotic minimally invasive surgery. Previous work targeting instrument segmentation, detection, and tracking on endoscopic video images failed on challenging images such as images with blood, smoke, and motion artifacts <ref type="bibr" target="#b13">[14]</ref>. Other reasons that make semantic segmentation of surgical instruments a challenging task are the presence of images containing shadows, specular reflections, blood, camera lens fogging, and the complex background tissue <ref type="bibr" target="#b14">[15]</ref>. The segmentation masks of these images can be useful for instrument detection and tracking. Similarly, in the GI tract procedures, from tissue sample collection to surgical removal of pathologies is performed in low field-of-view areas. Visual clutter such as artifacts, moving objects, and fluid, hinders the localisation of the target site during surgical procedures. Additionally, currently, there is no way of correlating the tissue sample collection with biopsied location and assessing surgical procedure effectiveness or even post-treatment recovery analysis. Automated localisation and tracking of instruments can help guide the endoscopists and surgeons to perform their tasks more effectively. Also, post-procedure video analysis can be done using these automated methods to track such tools, thus enabling improved surgical procedures or surveillance and their post-assessment. Currently, this is an open problem in the research community, where most procedures are not automated in GI tract endoscopy.</p><p>While there is an open research question for the automated tool detection and guidance in GI procedures, there is a lack of available public datasets. We aim to initiate the development of automated systems for the segmentation of GI tract diagnostic and therapeutic endoscopy tools. This research direction will enable tracking and localisation of essential tools used in endoscopy and help to improve targeted biopsies and surgeries in complex GI tract organs. To accomplish this, and to address the lack of publicly available labeled datasets, we have publicly released 590 pixel-level annotated frames that comprise of tools such as balloon dilation for facilitating opening of GI organs, biopsy forceps for tissue sample collection, polyp removal with snares, submucosal injections, radio-frequency ablation of dysplastic mucosa using probes and some other related surgical/diagnostic procedures. The released video frames will allow for building automated machine learning algorithms that can be applied during clinical procedures or post-analyses. To commence this effort, we provide a baseline benchmark on this dataset. U-Net <ref type="bibr" target="#b12">[13]</ref> is a common semantic segmentation based architecture for medical image segmentation tasks. In this paper, we thus present results utilising two U-Net based architectures. The provided dataset is open and can be used for research and development, and we invite multimedia researchers to improve over the provided baseline methods. The main contributions of this paper are:</p><p>-Release of 590 annotated bounding box and segmentation masks of GI diagnostic and surgical tool dataset. To the best of our knowledge, this is the first dataset of segmented tools in the GI tract. -Benchmark of the provided dataset using U-Net and DoubleUNet architectures for semantic segmentation. Standard computer vision metrics are used for a fair comparison of methods and possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Surgical vision is evolving as a promising technique to segment and track instruments using endoscopic images <ref type="bibr" target="#b5">[6]</ref>. To gather researchers on a single platform, Bodenstedt et al. <ref type="bibr" target="#b5">[6]</ref> organized "EndoVis 2015 Instrument sub-challenge" for developing new techniques and benchmarking the Machine Learning (ML) algorithm for segmentation and tracking of the instruments on a common dataset. The organizers challenged on two different tasks, (1) Segmentation, (2) Tracking. The goal of the challenge was to address the problem related to segmentation and tracking of articulated instruments in both laparoscopic and robotic surgery 2 . A comprehensive evaluation of the methods used in instrument segmentation and tracking task for minimally invasive surgery is summarized in this work <ref type="bibr" target="#b5">[6]</ref>. The extensive evaluation showed that deep learning works well for instrument segmentation and tracking tasks.</p><p>In 2017, a follow up to the previous 2015 challenge was organized called "Robotic Instrument Segmentation Sub-Challenge" 3 . The challenge was part of the Endoscopic vision challenge that was organized at MICCAI 2017. This challenge offered three tasks: (1) Binary segmentation task, (2) Parts based segmentation task, and (3) Instrument type segmentation task. The goal of the binary segmentation task was to separate the image into an instrument and background. Parts segmentation challenged the participants to divide the binary instrument into a shaft, wrist, and jaws. Type segmentation challenged the participants to identify different instrument types. A detailed description of the challenge tasks, dataset, methodologies used by ten participating teams in different tasks, challenge design, and limitation of the challenge can be found in the challenge summary paper <ref type="bibr" target="#b3">[4]</ref>.</p><p>In 2019, a similar challenge called "Robust Medical Instrument Segmentation Challenge 2019" 4 was organized by Roß et al. <ref type="bibr" target="#b13">[14]</ref>. This challenge offered three tasks (1) Binary segmentation, (2) Multiple instance detection, and <ref type="formula">(3)</ref> Multiple instance segmentation. The challenge was focused on addressing two key issues in surgical instruments, Robustness and Generalization, and benchmark medical instrument segmentation and detection on the provided surgical instrument dataset. EAD2019 challenge focused on endoscopic artefact detection primarily, but also included instrument class in their detection, segmentation and "out-of-sample" generalisation tasks. The challenge outcome revealed that most methods performed well for instrument detection and segmentation class <ref type="bibr" target="#b1">[2]</ref>. However, this dataset mostly consisted of large biopsy forceps.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we present available instrument datasets in the field. All of the datasets were designed for hosting challenges. The training dataset is released for all the datasets (except ROBUST-MIS); however, the test dataset is not provided by the challenge organizers. Thus, it makes it difficult to calculate and compare the results on the test dataset. However, experiments are still possible by splitting the training dataset into train, validation, and testing sets. The Robust Medical instrument segmentation dataset is yet not public. However, the participants who have participated in the challenge have the opportunity to download the training dataset. Usually, there are certain practicalities to download the dataset, such as signing the agreement and, getting permission from the owner, which takes time, and it is inconvenient. Moreover, to participate in the challenge, the participants have to signup in the particular year, and usually, the organizers do not make the dataset public unless they make a publication out of it, meaning it may take up to years. Thus, the significance of the datasets becomes less as the technology is changing rapidly. More information on available instrument datasets, contents, and offered tasks by the organizers and about the availability can be found from <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The literature review shows that there are only a few open-access datasets for MIS instrument segmentation. However, to the best of our knowledge, GI tract organ tools have never been explored. This is the first attempt to identify this avenue and provide the community with a curated and annotated public dataset that comprises of diagnostic and therapeutic tools in the GI tract. We believe that the presented dataset and the widely used U-Net based algorithm benchmark will encourage the multimedia researchers to develop a robust and efficient algorithm on the provided dataset that can help clinical procedures in endoscopy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Kvasir-Instrument dataset</head><p>In this section, we introduce the Kvasir-Instrument dataset with details on how the data was collected, the annotation protocol, and the dataset's structure. The dataset was collected from endoscopic examinations performed at the Baerum Hospital in Norway. The unlabelled images' frames are selected from the Hy-perKvasir dataset <ref type="bibr" target="#b6">[7]</ref>. HyperKvasir provides frame-level annotations for 10,662 frames for 23 different classes. However, the majority of the images (99,417 frames) are not labeled. We trained a model using the labeled samples of this dataset and tried to predict the classes of the unlabeled samples. Although our algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> could not classify all the images correctly; however, we were able to classify the instrument class out of hundreds of thousands of provided image frames. Additionally, some images were extracted manually from the polyp class of the Kvasir-SEG <ref type="bibr" target="#b10">[11]</ref> dataset. Below, we present the acquisition and annotation protocols used in the data preparation:</p><p>Data acquisition: The images and videos were collected using standard endoscopy equipment from Olympus (Olympus Europe, Germany) and Pentax (Pentax Medical Europe, Germany) at Vestre Viken Hospital Trust, Norway. All the data used in this study were obtained from videos for procedures that had followed the patient consenting protocol of Baerum Hospital. Additionally, <ref type="figure">Fig. 2</ref>: Kvasir-Instrument dataset: First two rows represent frames with biopsy forceps, the middle row consist of metallic clip, the fourth row is a radiofrequency ablation probe and the last row depicts the crescent and hexagonal shaped snares for polyp removal. no patient information was used for archiving. We have performed a random naming for each publicly released frame for effective annonymisation.</p><p>Annotation strategy: We have uploaded the Kvasir-Instrument dataset to labelbox <ref type="bibr" target="#b4">5</ref> and labeled the Region of Interest (ROI) in the image frames, i.e., the ROI of diagnostic and therapeutic tools in our cases and generated all the ground truth masks. <ref type="figure">Figure 2</ref> shows the example images, bounding box, image annotation, and generated masks for the Kvasir-Instrument dataset. All annotations were then exported in a JSON format which was used to generate masks for each of the annotations. Related codes and more information about the dataset can be found here <ref type="bibr" target="#b5">6</ref> .</p><p>The exported file contained the information of the images along with the coordinate points that were used for mask and bounding box generation. All annotations were performed using a three-step strategy:</p><p>-First, the selected samples were labeled by two experienced research assistants. -The annotated samples where cross-validated for their delineation quality by two experienced GI experts (more than 10 years of work experience in colonoscopy). -Finally, the suggested changes were incorporated using the comments from experts and were validated for only those samples.</p><p>The Kvasir-Instrument dataset includes 590 frames consisting of various GI endoscopy tools used during both, endoscopic surveillance and therapeutic or surgical procedures. A thorough annotation strategy (detailed above) was used to create bounding boxes and segmentation masks. The dataset consists of variable tool size with respect to image height and width as presented in <ref type="figure" target="#fig_0">Figure 1</ref>. The majority of the tools are small and medium-sized. The sample bounding box annotation, precise area delineation and extracted masks, are shown in <ref type="figure">Figure 2</ref>.</p><p>Our dataset is publicly available, and can be accessed at: https://datasets. simula.no/kvasir-instrument/. It consists of original image samples (in JPEG format), their corresponding masks (in PNG format), and bounding box information (in JSON format). A sample python script to help researchers visualise the data is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarking, results and discussion</head><p>In this section, we explore encoder-decoder based classical models for baseline algorithm benchmarking, their implementation details for reproducibility, details on evaluation metric used for quantitative analysis, and results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline methods</head><p>U-Net has been explored in the past through many biomedical segmentation challenges and has shown strength towards an effective supervised segmentation model. In this paper, we, therefore, use U-Net based architectures on our Kvasir-Instrument dataset to provide a baseline result for future comparisons. U-Net uses an encoder-decoder architecture, that is, a contractive feature extraction path and expansive path with a classifier to perform binary classification of each image pixel in an upsampled feature map. In our previous work, we have shown that the strength of supervised classification can be amplified by using the output mask from one U-Net <ref type="bibr" target="#b12">[13]</ref> architecture to the other by proposing Dou-bleUNet <ref type="bibr" target="#b9">[10]</ref>. In addition, the DoubleUNet architecture uses VGG-19 pretrained on ImageNet as one of the encoder block, squeeze and excite block and Atrous spatial pyramid pooling (ASPP) block. All other components in the network remain the same as the U-Net. For both networks, dice loss gives an 1 − DSC, where DSC is the dice similarity coefficient (see Eq. 1 below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We have implemented the U-Net-based and DoubleUNet based architectures using the Keras framework <ref type="bibr" target="#b7">[8]</ref> with TensorFlow <ref type="bibr" target="#b0">[1]</ref> as backend running on the Experimental Infrastructure for Exploration of Exascale Computing (eX3), NVIDIA DGX-2 machine. We have resized the training dataset into 512×512. We set the batch size of 8 for training. Both architectures are optimized by using the Adam optimizer. We have made use of dice loss as the loss function. We split the dataset using 80% of the dataset for training and the remaining 20% for the testing (evaluation). We performed basic augmentation, such as horizontal flip, vertical flip, and random rotation. Moreover, we have also provided the train-test split so that others can improve the methods on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>In this medical image segmentation approach, each pixel of the diagnostic and therapeutic tool either belongs to a tool or non-tool region. Dice similarity coefficient (DSC) is the main evaluation metric used to evaluate this task. Additionally, we calculate other standard metrics such as Jaccard similarity coefficient (JC) or intersection over union (IoU), precision, recall, overall accuracy, F2, and frames per second (FPS) as it is a commonly used metric in biomedical image segmentation tasks. The mathematical expressions for them are as follows:   Overall accuracy (Acc.) = tp + tn tp + tn + f p + f n</p><formula xml:id="formula_0">DSC = 2 · tp 2 · tp + f p + f n<label>(1)</label></formula><p>Frame Per Second (F P S) = 1 sec/f rame <ref type="bibr" target="#b6">(7)</ref> Here, tp, fp, tn, fn are the true positives, false positive, true negative, and false negative, respectively. <ref type="table" target="#tab_2">Table 2</ref> shows the results of the baseline methods for the tool segmentation on the Kvasir-Instrument dataset. From the table, we can observe that the UNet achieved a high JC of 0.8578 and DSC of 0.9158, which is slightly above than the DoubleUNet that yielded JC of 0.8430 and DSC of 0.9038. Also, UNet achieved a speed of 20.4636 FPS, whereas computational time is double for DoubleUNet with only 10 FPS. Similarly, both the recall and precision scores are very comparable for both U-Net (p = 0.8998, r = 0.9487) and DoubleUNet (p = 0.8966, r = 0.9275). <ref type="figure" target="#fig_1">Figure 3</ref> shows the qualitative result on the challenging images. It can be observed that that both UNet and DoubleUNet are under-segmenting the cap region (top) and over-segmenting the small clip area (bottom). Some parts of these images are confused because of the presence of saturation areas. However, both models was able to segment well with most endoscopic tool samples in the dataset. This is also evident from the quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative and Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>From the experimental results in <ref type="table" target="#tab_2">Table 2</ref>, we can validate that the classiccal U-Net architecture outperforms DoubleUNet model. Additionally, U-Net is 2× faster than the DoubleUNet. This is because U-Net uses basic convolution blocks, whereas DoubleUNet uses pre-trained encoders, ASPP, squeeze and excite blocks, all of which increases the inference latency. Here, the UNet is optimized by dice loss instead of binary cross-entropy loss, which showed improved performance during our experiments.</p><p>Further, fine-tuning on other similar datasets, rigorous data augmentation and applying more advanced Deep learning (DL) techniques can improve the baseline results -eventually achieving the detection, localisation, and segmentation performance needed to make the technology useful in a clinical environment. Additionally, use of DL networks with less parameters could increase the computational efficiency thereby enabling real-time systems that can be used in clinical settings effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have curated, annotated, and publicly released a dataset that incorporates tools used in GI endoscopy screening and surgical procedures. The dataset consists of images, bounding boxes and segmentation masks of endoscopy tools used during different procedures in the GI tract. Additionally, we provided baseline segmentation methods for the automatic delineation of these tools and have compared them using standard computer vision metrics. In the future, we plan to continuously increase the amount of data and also call for multi-media challenges on using the presented dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Distribution of Kvasir-Instrument dataset. On left: Small (green), medium (blue) and large (pink) sized tool clusters. On right: sample images with variable tool size in images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Failed cases: Cap region (top) is under-segmented and small clip area is over-segmented and consist of large number of false positives (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Available instrument datasets Endoscopic vision (EndoVis) challenge is being organized since 2015 at Medical Image Computing and Computer Assisted Intervention Society (MICCAI) with an exception in 2016. The Endovis challenge hosts different sub-challenges. The year-wise information about the hosted sub-challenge can be found on the challenge website 1 .</figDesc><table><row><cell>Dataset</cell><cell>Content</cell><cell>Task type</cell><cell>Procedure</cell></row><row><cell>Instrument segmentation and tracking (2015) [6]</cell><cell>Rigid and robotic instruments</cell><cell>Segmentation and tracking</cell><cell>Laparoscopy</cell></row><row><cell></cell><cell></cell><cell>Binary segmentation,</cell><cell></cell></row><row><cell>Robotic Instrument Segmentation (2017) [4]</cell><cell>Robotic surgical instruments</cell><cell>part based segmentation, instrument</cell><cell>Abdominal porcine</cell></row><row><cell></cell><cell></cell><cell>segmentation</cell><cell></cell></row><row><cell>Robotic Scene</cell><cell>Surgical instruments</cell><cell>Multi-instance</cell><cell>Robotic</cell></row><row><cell>Segmentation (2018) [3]</cell><cell>and other</cell><cell>segmentation</cell><cell>nephrectomy</cell></row><row><cell>Robust Medical instrument segmentation (2019) [14]</cell><cell>laparoscopic instrument</cell><cell>Binary segmentation, instance segmentation multiple instance detection, multiple</cell><cell>Laparoscopy</cell></row><row><cell>Kvasir-Instrument</cell><cell>Diagnostic and therapeutic tools in endoscopic images</cell><cell>Binary segmentation Detection and localization</cell><cell>Gastroscopy &amp; colonoscopy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Baseline results for tool segmentation</figDesc><table><row><cell>Method</cell><cell>JC</cell><cell cols="2">DSC F2-score Precision Recall Acc. FPS</cell></row><row><cell>U-Net [13]</cell><cell cols="2">0.8578 0.9158 0.9320</cell><cell>0.8998 0.9487 0.9864 20.4636</cell></row><row><cell cols="3">DoubleUNet [10] 0.8430 0.9038 0.9147</cell><cell>0.8966</cell><cell>0.9275 0.9838 10.0000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://endovis.grand-challenge.org/ 2 https://endovissub-instrument.grand-challenge.org/ EndoVisSub-Instrument/ 3 https://endovissub2017-roboticinstrumentsegmentation.grand-challenge. org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://robustmis2019.grand-challenge.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.labelbox.com/ 6 https://github.com/DebeshJha/Kvasir-Instrument</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is funded in part by the Research Council of Norway, project number 263248 (Privaton) and project number 282315 (AutoCap). We performed all computations in this paper on equipment provided by the Experimental Infrastructure for Exploration of Exascale Computing (eX 3 ), which is financially supported by the Research Council of Norway under contract 270053.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI})</title>
		<meeting>eeding of {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI})</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Robotic scene segmentation sub-challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azizian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06426</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06426</idno>
		<title level="m">robotic instrument segmentation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The status of augmented reality in laparoscopic surgery as of 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="66" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agustinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Müller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pakhomov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02475</idno>
		<title level="m">Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borgli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer Nature Scientific Data</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Keras</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-guided interventions: technology review and clinical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="119" to="142" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04868</idno>
		<title level="m">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Multimedia Modeling</title>
		<meeting>of International Conference on Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for instrument segmentation in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>of International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting>of International Conference on Medical image computing and computer-assisted intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10299</idno>
		<title level="m">Robust medical instrument segmentation challenge 2019</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic instrument segmentation in robot-assisted surgery using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning and Applications (ICMLA)</title>
		<meeting>of International Conference on Machine Learning and Applications (ICMLA)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13278</idno>
		<title level="m">The medico-task 2018: Disease detection in the gastrointestinal tract using global features and deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An extensive study on cross-dataset bias and evaluation metrics interpretation for machine learning applied to gastrointestinal tract abnormality classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03912</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
