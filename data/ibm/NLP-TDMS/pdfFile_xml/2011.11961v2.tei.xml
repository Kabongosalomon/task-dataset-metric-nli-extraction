<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is a Green Screen Really Necessary for Real-Time Portrait Matting?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurou</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhua</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is a Green Screen Really Necessary for Real-Time Portrait Matting?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For portrait matting without the green screen 1 , existing works either require auxiliary inputs that are costly to obtain or use multiple models that are computationally expensive. Consequently, they are unavailable in real-time applications. In contrast, we present a light-weight matting objective decomposition network (MODNet), which can process portrait matting from a single input image in real time. The design of MODNet benefits from optimizing a series of correlated sub-objectives simultaneously via explicit constraints. Moreover, since trimap-free methods usually suffer from the domain shift problem in practice, we introduce (1) a self-supervised strategy based on sub-objectives consistency to adapt MODNet to real-world data and (2) a one-frame delay trick to smooth the results when applying MODNet to portrait video sequence.</p><p>MODNet is easy to be trained in an end-to-end style. It is much faster than contemporaneous matting methods and runs at 63 frames per second. On a carefully designed portrait matting benchmark newly proposed in this work, MODNet greatly outperforms prior trimap-free methods. More importantly, our method achieves remarkable results in daily photos and videos. Now, do you really need a green screen for real-time portrait matting? Our code, pre-trained models, and validation benchmark will be made available at: https://github.com/ZHKKKe/MODNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Portrait matting aims to predict a precise alpha matte that can be used to extract people from a given image or video. It has a wide variety of applications, such as photo editing and movie re-creation. Currently, a green screen is required to obtain a high quality alpha matte in real time.</p><p>When a green screen is not available, most existing matting methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref> use a pre-defined trimap as a priori. However, the trimap is costly for humans to annotate, or suffer from low precision if captured via a depth * kezhanghan@outlook.com <ref type="bibr" target="#b0">1</ref> Also known as the blue screen technology. camera. Therefore, some latest works attempt to eliminate the model dependence on the trimap, i.e., trimap-free methods. For example, background matting <ref type="bibr" target="#b36">[37]</ref> replaces the trimap by a separate background image. Others <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref> apply multiple models to firstly generate a pseudo trimap or semantic mask, which is then served as the priori for alpha matte prediction. Nonetheless, using the background image as input has to take and align two photos while using multiple models significantly increases the inference time. These drawbacks make all aforementioned matting methods not suitable for real-time applications, such as preview in a camera. Besides, limited by insufficient amount of labeled training data, trimap-free methods often suffer from domain shift <ref type="bibr" target="#b39">[40]</ref> in practice, i.e., the models cannot well generalize to real-world data, which has also been discussed in <ref type="bibr" target="#b36">[37]</ref>.</p><p>To predict an accurate alpha matte from only one RGB image by using a single model, we propose MODNet, a light-weight network that decomposes the portrait matting task into three correlated sub-tasks and optimizes them simultaneously through specific constraints. There are two insights behind MODNet. First, neural networks are better at learning a set of simple objectives rather than a complex one. Therefore, addressing a series of matting subobjectives can achieve better performance. Second, applying explicit supervisions for each sub-objective can make different parts of the model to learn decoupled knowledge, which allows all the sub-objectives to be solved within one model. To overcome the domain shift problem, we introduce a self-supervised strategy based on sub-objective consistency (SOC) for MODNet. This strategy utilizes the consistency among the sub-objectives to reduce artifacts in the predicted alpha matte. Moreover, we suggest a one-frame delay (OFD) trick as post-processing to obtain smoother outputs in the application of video matting. <ref type="figure">Fig. 1</ref> summarizes our framework.</p><p>MODNet has several advantages over previous trimapfree methods. First, MODNet is much faster. It is designed for real-time applications, running at 63 frames per second (f ps) on an Nvidia GTX 1080Ti GPU with an input size of 512 × 512. Second, MODNet achieves state-of-the-art results, benefitted from (1) objective decomposition and con- current optimization; and (2) specific supervisions for each of the sub-objectives. Third, MODNet can be easily optimized end-to-end since it is a single well-designed model instead of a complex pipeline. Finally, MODNet has better generalization ability thanks to our SOC strategy. Although our results are not able to surpass those of the trimap-based methods on the portrait matting benchmarks with trimaps, our experiments show that MODNet is more stable in practical applications due to the removal of the trimap input. We believe that our method is challenging the necessity of using a green screen for real-time portrait matting. Since open-source portrait matting datasets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> have limited scale or precision, prior works train and validate their models on private datasets of diverse quality and difficulty levels. As a result, it is not easy to compare these methods fairly. In this work, we evaluate existing trimapfree methods under a unified standard: all models are trained on the same dataset and validated on the portrait images from Adobe Matting Dataset <ref type="bibr" target="#b48">[49]</ref> and our newly proposed benchmark. Our new benchmark is labelled in high quality, and it is more diverse than those used in previous works. Hence, it can reflect the matting performance more comprehensively. More on this is discussed in Sec. 5.1.</p><p>In summary, we present a novel network architecture, named MODNet, for trimap-free portrait matting in real time. Moreover, we introduce two techniques, SOC and OFD, to generalize MODNet to new data domains and smooth the matting results on videos. Another contribution of this work is a carefully designed validation benchmark for portrait matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Matting</head><p>The purpose of image matting is to extract the desired foreground F from a given image I. Unlike the binary mask output from image segmentation <ref type="bibr" target="#b31">[32]</ref> and saliency detection <ref type="bibr" target="#b46">[47]</ref>, matting predicts an alpha matte with preccise foreground probability for each pixel, which is represented by α in the following formula:</p><formula xml:id="formula_0">I i = α i F i + (1 − α i ) B i ,<label>(1)</label></formula><p>where i is the pixel index, and B is the background of I. When the background is not a green screen, this problem is ill-posed since all variables on the right hand side are unknown. Most existing matting methods take a pre-defined trimap as an auxiliary input, which is a mask containing three regions: absolute foreground (α = 1), absolute background (α = 0), and unknown area (α = 0.5). In this way, the matting algorithms only have to estimate the foreground probability inside the unknown area based on the priori from the other two regions.</p><p>Traditional matting algorithms heavily rely on low-level features, e.g., color cues, to determine the alpha matte through sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> or propagation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref>, which often fail in complex scenes. With the tremendous progress of deep learning, many methods based on convolutional neural networks (CNN) have been proposed, and they improve matting results significantly. Cho et al. <ref type="bibr" target="#b7">[8]</ref> and Shen et al. <ref type="bibr" target="#b37">[38]</ref> combined the classic algorithms with CNN for alpha matte refinement. Xu et al. <ref type="bibr" target="#b48">[49]</ref> proposed an auto-encoder architecture to predict alpha matte from a RGB image and a trimap. Some works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> argued that the attention mechanism could help improve matting performance. Lutz et al. <ref type="bibr" target="#b30">[31]</ref> demonstrated the effectiveness of generative adversarial networks <ref type="bibr" target="#b12">[13]</ref> in matting. Cai et al. <ref type="bibr" target="#b3">[4]</ref> suggested a trimap refinement process before matting and showed the advantages of an elaborate trimap. Since obtaining a trimap requires user effort, some recent methods (including our MODNet) attempt to avoid it, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Trimap-free Portrait Matting</head><p>Image matting is extremely difficult when trimaps are unavailable as semantic estimation will be necessary (to locate the foreground) before predicting a precise alpha matte.</p><p>Currently, trimap-free methods always focus on a specific type of foreground objects, such as humans. Nonetheless, feeding RGB images into a single neural network still yields unsatisfactory alpha mattes. Sengupta et al. <ref type="bibr" target="#b36">[37]</ref> proposed to capture a less expensive background image as a pseudo green screen to alleviate this issue. Other works designed their pipelines that contained multiple models. For example, Shen et al. <ref type="bibr" target="#b5">[6]</ref> assembled a trimap generation network before the matting network. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> applied a fusion network to combine the predicted foreground and background. Liu et al. <ref type="bibr" target="#b28">[29]</ref> concatenated three networks to utilize coarse labeled data in matting. The main problem of all these methods is that they cannot be used in interactive applications since: (1) the background images may change frame to frame, and (2) using multiple models is computationally expensive. Compared with them, our MODNet is light-weight in terms of both input and pipeline complexity. It takes one RGB image as input and uses a single model to process portrait matting in real time with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Other Techniques</head><p>We briefly discuss some other techniques related to the design and optimization of our method.</p><p>High-Resolution Representations. Popular CNN architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref> generally contain an encoder, i.e., a low-resolution branch, to reduce the resolution of the input. Such a process will discard image details that are essential in many tasks, including image matting. Wang et al. <ref type="bibr" target="#b45">[46]</ref> proposed to keep high-resolution representations throughout the model and exchange features between different resolutions, which induces huge computational overheads. Instead, MODNet only applies an independent highresolution branch to handle foreground boundaries.</p><p>Attention Mechanisms. Attention <ref type="bibr" target="#b4">[5]</ref> for deep neural networks has been widely explored and proved to boost the performance notably. In computer vision, we can divide these mechanisms into spatial-based or channel-based according to their operating dimension. To obtain better results, some matting models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> combined spatial-based attentions that are time-consuming. In MODNet, we integrate the channel-based attention so as to balance between performance and efficiency.</p><p>Consistency Constraint. Consistency is one of the most important assumptions behind many semi-/self-supervised <ref type="bibr" target="#b35">[36]</ref> and domain adaptation <ref type="bibr" target="#b47">[48]</ref> algorithms. For example, Ke et al. <ref type="bibr" target="#b23">[24]</ref> designed a consistency-based framework that could be used for semi-supervised matting. Toldo et al. <ref type="bibr" target="#b44">[45]</ref> presented a consistency-based domain adaptation strategy for semantic segmentation. However, these methods consist of multiple models and constrain the consistency among their predictions. In contrast, our MODNet imposes consistency among various sub-objectives within a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODNet</head><p>In this section, we elaborate the architecture of MODNet and the constraints used to optimize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Methods that are based on multiple models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref> have shown that regarding trimap-free matting as a trimap prediction (or segmentation) step plus a trimap-based matting step can achieve better performances. This demonstrates that neural networks are benefited from breaking down a complex objective. In MODNet, we extend this idea by dividing the trimap-free matting objective into semantic estimation, detail prediction, and semantic-detail fusion. Intuitively, semantic estimation outputs a coarse foreground mask while detail prediction produces fine foreground boundaries, and semantic-detail fusion aims to blend the features from the first two sub-objectives.</p><p>As shown in <ref type="figure">Fig. 2</ref>, MODNet consists of three branches, which learn different sub-objectives through specific constraints. Specifically, MODNet has a low-resolution branch (supervised by the thumbnail of the ground truth matte) to estimate human semantics. Based on it, a high-resolution branch (supervised by the transition region (α ∈ (0, 1)) in the ground truth matte) is introduced to focus on the portrait boundaries. At the end of MODNet, a fusion branch (supervised by the whole ground truth matte) is added to predict the final alpha matte. In the following subsections, we will delve into the branches and the supervisions used to solve each sub-objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Estimation</head><p>Similar to existing multiple-model approaches, the first step of MODNet is to locate the human in the input image I. The difference is that we extract the high-level semantics only through an encoder, i.e., the low-resolution branch S of MODNet, which has two main advantages. First, semantic estimation becomes more efficient since it is no longer done by a separate model that contains the decoder. Second, the high-level representation S(I) is helpful for subsequent branches and joint optimization. We can apply arbitrary CNN backbone to S. To facilitate real-time interaction, we adopt the MobileNetV2 <ref type="bibr" target="#b34">[35]</ref> architecture, an ingenious model developed for mobile devices, as our S.</p><p>When analysing the feature maps in S(I), we notice that some channels have more accurate semantics than others. Besides, the indices of these channels vary in different images. However, the subsequent branches process all S(I) in the same way, which may cause the feature maps with false semantics to dominate the predicted alpha mattes in some images. Our experiments show that channel-wise attention mechanisms can encourage using the right knowledge and discourage those that are wrong. Therefore, we append a SE-Block <ref type="bibr" target="#b18">[19]</ref> after S to reweight the channels of S(I).  <ref type="figure">Figure 2</ref>. Architecture of MODNet. Given an input image I, MODNet predicts human semantics sp, boundary details dp, and final alpha matte αp through three interdependent branches, S, D, and F , which are constrained by specific supervisions generated from the ground truth matte αg. Since the decomposed sub-objectives are correlated and help strengthen each other, we can optimize MODNet end-to-end.</p><p>To predict coarse semantic mask s p , we feed S(I) into a convolutional layer activated by the Sigmoid function to reduce its channel number to 1. We supervise s p by a thumbnail of the ground truth matte α g . Since s p is supposed to be smooth, we use L2 loss here, as:</p><formula xml:id="formula_1">L s = 1 2 s p − G(α g ) 2 ,<label>(2)</label></formula><p>where G stands for 16× downsampling followed by Gaussian blur. It removes the fine structures (such as hair) that are not essential to human semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detail Prediction</head><p>We process the transition region around the foreground portrait with a high-resolution branch D, which takes I, S(I), and the low-level features from S as inputs. The purpose of reusing the low-level features is to reduce the computational overheads of D. In addition, we further simplify D in the following three aspects: (1) D consists of fewer convolutional layers than S; (2) a small channel number is chosen for the convolutional layers in D; (3) we do not maintain the original input resolution throughout D. In practice, D consists of 12 convolutional layers, and its maximum channel number is 64. The feature map resolution is downsampled to 1/4 of I in the first layer and restored in the last two layers. The impact of this setup on detail prediction is negligible since D contains a skip link.</p><p>We denote the outputs of D as D(I, S(I)), which implies the dependency between sub-objectives -high-level human semantics S(I) is a priori for detail prediction. We calculate the boundary detail matte d p from D(I, S(I)) and learn it through L1 loss, as:</p><formula xml:id="formula_2">L d = m d d p − α g 1 ,<label>(3)</label></formula><p>where m d is a binary mask to let L d focus on the portrait boundaries. m d is generated through dilation and erosion on α g . Its values are 1 if the pixels are inside the transition region, and 0 otherwise. In fact, the pixels with m d = 1 are the ones in the unknown area of the trimap. Although d p may contain inaccurate values for the pixels with m d = 0, it has high precision for the pixels with m d = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic-Detail Fusion</head><p>The fusion branch F in MODNet is a straightforward CNN module, combining semantics and details. We first upsample S(I) to match its shape with D(I, S(I)). We then concatenate S(I) and D(I, S(I)) to predict the final alpha matte α p , constrained by:</p><formula xml:id="formula_3">L α = α p − α g 1 + L c ,<label>(4)</label></formula><p>where L c is the compositional loss from <ref type="bibr" target="#b48">[49]</ref>. It measures the absolute difference between the input image I and the composited image obtained from α p , the ground truth foreground, and the ground truth background.</p><p>MODNet is trained end-to-end through the sum of L s , L d , and L α , as:</p><formula xml:id="formula_4">L = λ s L s + λ d L d + λ α L α ,<label>(5)</label></formula><p>where λ s , λ d , and λ α are hyper-parameters balancing the three losses. The training process is robust to these hyperparameters. We set λ s = λ α = 1 and λ d = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adaptation to Real-World Data</head><p>The training data for portrait matting requires excellent labeling in the hair area, which is almost impossible for natural images with complex backgrounds. Currently, most annotated data comes from photography websites. Although these images have monochromatic or blurred backgrounds, the labeling process still needs to be completed by experienced annotators with considerable amount of time and the help of professional tools. As a consequence, the labeled datasets for portrait matting are usually small. Xu et al. <ref type="bibr" target="#b48">[49]</ref> suggested using background replacement as a data augmentation to enlarge the training set, and it has become a typical setting in image matting. However, the training samples obtained in such a way exhibit different properties from those of the daily life images for two reasons. First, unlike natural images of which foreground and background fit seamlessly together, images generated by replacing backgrounds are usually unnatural. Second, professional photography is often carried out under controlled conditions, like special lighting that is usually different from those observed in our daily life. Therefore, existing trimap-free models always tend to overfit the training set and perform poorly on realworld data.</p><p>To address the domain shift problem, we utilize the consistency among the sub-objectives to adapt MODNet to unseen data distributions (Sec. 4.1). Moreover, to alleviate the flicker between video frames, we apply a one-frame delay trick as post-processing (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sub-Objectives Consistency (SOC)</head><p>For unlabeled images from a new domain, the three subobjectives in MODNet may have inconsistent outputs. For example, the foreground probability of a certain pixel belonging to the background may be wrong in the predicted alpha matte α p but is correct in the predicted coarse semantic mask s p . Intuitively, this pixel should have close values in α p and s p . Motivated by this, our self-supervised SOC strategy imposes the consistency constraints between the predictions of the sub-objectives ( <ref type="figure">Fig. 1 (b)</ref>) to improve the performance of MODNet in the new domain.</p><p>Formally, we use M to denote MODNet. As described in Sec. 3, M has three outputs for an unlabeled imageĨ, as:</p><formula xml:id="formula_5">s p ,d p ,α p = M (Ĩ) .<label>(6)</label></formula><p>We force the semantics inα p to be consistent withs p and the details inα p to be consistent withd p by:</p><formula xml:id="formula_6">L cons = 1 2 G(α p ) −s p 2 +m d α p −d p 1 ,<label>(7)</label></formula><p>wherem d indicates the transition region inα p , and G has the same meaning as the one in Eq. 2. However, adding the L2 loss on blurred G(α p ) will smooth the boundaries in average alpha matte αt-1 alpha matte αt alpha matte αt+1 <ref type="figure">Figure 3</ref>. Flickering Pixels Judged by OFD. The foreground moves slightly to the left in three consecutive frames. We focus on three pixels: (1) the pixel marked in green does not satisfy the 1st condition in C; (2) the pixel marked in blue does not satisfy the 2nd condition in C; (3) the pixel marked in red flickers at frame t.</p><p>the optimizedα p . Hence, the consistency betweenα p and d p will remove the details predicted by the high-resolution branch. To prevent this problem, we duplicate M to M and fix the weights of M before performing SOC. Since the fine boundaries are preserved ind p output by M , we append an extra constraint to maintain the details in M as:</p><formula xml:id="formula_7">L dd =m d d p −d p 1 .<label>(8)</label></formula><p>We generalize MODNet to the target domain by optimizing L cons and L dd simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">One-Frame Delay (OFD)</head><p>Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the outputs. In matting, this phenomenon usually appears as flickers in the predicted matte sequence. Since the flickering pixels in a frame are likely to be correct in adjacent frames, we may utilize the preceding and the following frames to fix these pixels. If the f ps is greater than 30, the delay caused by waiting for the next frame is negligible.</p><p>Suppose that we have three consecutive frames, and their corresponding alpha mattes are α t−1 , α t , and α t+1 , where t is the frame index. We regard α i t as a flickering pixel if it satisfies the following conditions C (illustrated in <ref type="figure">Fig. 3</ref>):</p><formula xml:id="formula_8">1. |α i t−1 − α i t+1 | ≤ ξ , 2. |α i t − α i t−1 | &gt; ξ and |α i t − α i t+1 | &gt; ξ .</formula><p>In practice, we set ξ = 0.1 to measure the similarity of pixel values. C indicates that if the values of α i t−1 and α i t+1 are close, and α i t is very different from the values of both α i t−1 and α i t+1 , a flicker appears in α i t . We replace the value of α i t by averaging α i t−1 and α i t+1 , as:</p><formula xml:id="formula_9">α i t = α i t−1 + α i t+1 / 2, if C, α i t , otherwise.<label>(9)</label></formula><p>Note that OFD is only suitable for smooth movement. It may fail in fast motion videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce the PPM-100 benchmark for portrait matting. We then compare MODNet with existing matting methods on PPM-100. We further conduct ablation experiments to evaluate various aspects of MOD-Net. Finally, we demonstrate the effectiveness of SOC and OFD in adapting MODNet to real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Photographic Portrait Matting Benchmark</head><p>Existing works constructed their validation benchmarks from a small amount of labeled data through image synthesis. Their benchmarks are relatively easy due to unnatural fusion or mismatched semantics between the foreground and the background ( <ref type="figure" target="#fig_1">Fig. 4 (a)</ref>). Therefore, trimapfree models may be comparable to trimap-based models on these benchmarks but have unsatisfactory results in natural images, i.e., the images without background replacement, which indicates that the performance of trimap-free methods has not been accurately assessed. We prove this standpoint by the matting results on Adobe Matting Dataset 2 .</p><p>In contrast, we propose a Photographic Portrait Matting benchmark (PPM-100), which contains 100 finely annotated portrait images with various backgrounds. To guarantee sample diversity, we define several classifying rules to balance the sample types in PPM-100. For example, (1) whether the whole human body is included; (2) whether the image background is blurred; and (3) whether the person holds additional objects. We regard small objects held by people as a part of the foreground since this is more in line with the practical applications. As exhibited in <ref type="figure" target="#fig_1">Fig. 4(b)(c)(d)</ref>, the samples in PPM-100 have more natural backgrounds and richer postures. So, we argue that PPM-100 is a more comprehensive benchmark. <ref type="bibr" target="#b1">2</ref> Refer to Appendix B for the results of portrait images (with synthetic backgrounds) from Adobe Matting Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on PPM-100</head><p>We compare MODNet with FDMPA <ref type="bibr" target="#b50">[51]</ref>, LFM <ref type="bibr" target="#b49">[50]</ref>, SHM <ref type="bibr" target="#b5">[6]</ref>, BSHM <ref type="bibr" target="#b28">[29]</ref>, and HAtt <ref type="bibr" target="#b32">[33]</ref>. We follow the original papers to reproduce the methods that have no publicly available codes. We use DIM <ref type="bibr" target="#b48">[49]</ref> as trimap-based baseline.</p><p>For a fair comparison, we train all models on the same dataset, which contains nearly 3000 annotated foregrounds. The background replacement <ref type="bibr" target="#b48">[49]</ref> is applied to extend our training set. For each foreground, we generate 5 samples by random cropping and 10 samples by compositing the backgrounds from the OpenImage dataset <ref type="bibr" target="#b24">[25]</ref>. We use MobileNetV2 pre-trained on the Supervisely Person Segmentation (SPS) <ref type="bibr" target="#b41">[42]</ref> dataset as the backbone of all trimapfree models. For previous methods, we explore the optimal hyper-parameters through grid search. For MODNet, we train it by SGD for 40 epochs. With a batch size of 16, the initial learning rate is 0.01 and is multiplied by 0.1 after every 10 epochs. We use Mean Square Error (MSE) and Mean Absolute Difference (MAD) as quantitative metrics. <ref type="table">Table 1</ref> shows the results on PPM-100, MODNet surpasses other trimap-free methods in both MSE and MAD. However, it still performs inferior to trimap-based DIM, since PPM-100 contains samples with challenging poses or costumes. When modifying our MODNet to a trimap-based method, i.e., taking a trimap as input, it outperforms trimapbased DIM, which reveals the superiority of our network architecture. <ref type="figure">Fig. 5</ref> visualizes some samples <ref type="bibr" target="#b2">3</ref> .</p><p>We further demonstrate the advantages of MODNet in terms of model size and execution efficiency. A small model facilitates deployment on mobile devices, while high execution efficiency is necessary for real-time applications. We measure the model size by the total number of parameters, and we reflect the execution efficiency by the average in-</p><formula xml:id="formula_10">Input SHM FDMPA LFM DIM</formula><p>HAtt BSHM Our GT <ref type="figure">Figure 5</ref>. Visual Comparisons of Trimap-free Methods on PPM-100. MODNet performs better in hollow structures (the 1st row) and hair details (the 2nd row). However, it may still make mistakes in challenging poses or costumes (the 3rd row). DIM <ref type="bibr" target="#b48">[49]</ref> here does not take trimaps as the input but is pre-trained on the SPS <ref type="bibr" target="#b41">[42]</ref> dataset. Zoom in for the best visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Trimap MSE ↓ MAD ↓ DIM <ref type="bibr" target="#b48">[49]</ref> 0.0016 0.0063 MODNet (Our) 0.0013 0.0056 DIM <ref type="bibr" target="#b48">[49]</ref> 0.0221 0.0327 DIM † <ref type="bibr" target="#b48">[49]</ref> 0.0115 0.0178 FDMPA † <ref type="bibr" target="#b50">[51]</ref> 0.0101 0.0160 LFM † <ref type="bibr" target="#b49">[50]</ref> 0.0094 0.0158 SHM † <ref type="bibr" target="#b5">[6]</ref> 0.0072 0.0152 HAtt † <ref type="bibr" target="#b32">[33]</ref> 0.0067 0.0137 BSHM † <ref type="bibr" target="#b28">[29]</ref> 0.0063 0.0114 MODNet † (Our) 0.0046 0.0097  . Although MODNet has a slightly higher number of parameters than FDMPA, our performance is significantly better. We also conduct ablation experiments for MODNet on PPM-100 ( <ref type="table">Table 2</ref>). Applying L s and L d to constrain human semantics and boundary details brings considerable improvement. The result of assembling SE-Block proves the effectiveness of reweighting the feature maps. Although the SPS pre-training is optional to MODNet, it plays a vital role in other trimap-free methods. For example, in <ref type="table">Table 1</ref>, the performance of trimap-free DIM without pre-training is far worse than the one with pre-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Real-World Data</head><p>Real-world data can be divided into multiple domains according to different device types or diverse imaging methods. By assuming that the images captured by the same kind of device (such as smartphones) belong to the same domain, we capture several video clips as the unlabeled data for selfsupervised SOC domain adaptation. In this stage, we freeze the BatchNorm <ref type="bibr" target="#b20">[21]</ref> layers within MODNet and finetune the convolutional layers by Adam with a learning rate of 0.0001. Here we only provide visual results 4 because no ground truth mattes are available. In <ref type="figure" target="#fig_3">Fig. 7</ref>, we composite the foreground over a green screen to emphasize that SOC is vital for generalizing MODNet to real-world data. In addition, OFD further removes flickers on the boundaries.</p><p>Applying trimap-based methods in practice requires an additional step to obtain the trimap, which is commonly implemented by a depth camera, e.g., ToF <ref type="bibr" target="#b10">[11]</ref>. Specifically, the pixel values in a depth map indicate the distance from the 3D locations to the camera, and the locations closer to the camera have smaller pixel values. We can first define a threshold to split the reversed depth map into foreground and background. Then, we can generate the trimap through dilation and erosion. However, this scheme will identify all objects in front of the human, i.e., objects closer to the cam- In this case, an incorrect trimap generated from the depth map causes the trimap-based DIM <ref type="bibr" target="#b48">[49]</ref> to fail. For comparsion, MOD-Net handles this case correctly, as it inputs only an RGB image.</p><p>BM Our Input <ref type="figure">Figure 9</ref>. MODNet versus BM under Fixed Camera Position. MODNet outperforms BM <ref type="bibr" target="#b36">[37]</ref> when a car is entering the background (marked in red). era, as the foreground, leading to an erroneous trimap for matte prediction in some scenarios. In contrast, MODNet avoids such a problem by decoupling from the trimap input. We give an example in <ref type="figure" target="#fig_4">Fig. 8</ref>.</p><p>We also compare MODNet against the background matting (BM) proposed by <ref type="bibr" target="#b36">[37]</ref>. Since BM does not support dynamic backgrounds, we conduct validations 4 in the fixedcamera scenes from <ref type="bibr" target="#b36">[37]</ref>. BM relies on a static background image, which implicitly assumes that all pixels whose value changes in the input image sequence belong to the foreground. As shown in <ref type="figure">Fig. 9</ref>, when a moving object suddenly appears in the background, the result of BM will be affected, but MODNet is robust to such disturbances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper has presented a simple, fast, and effective MODNet to avoid using a green screen in real-time portrait matting. By taking only RGB images as input, our method enables the prediction of alpha mattes under changing scenes. Moreover, MODNet suffers less from the domain shift problem in practice due to the proposed SOC and OFD. MODNet is shown to have good performances on the carefully designed PPM-100 benchmark and a variety of real-world data. Unfortunately, our method is not able to handle strange costumes and strong motion blurs that are not covered by the training set. One possible future work is to address video matting under motion blurs through additional sub-objectives, e.g., optical flow estimation.</p><p>Input SHM FDMPA LFM DIM HAtt BSHM Our GT <ref type="figure">Figure 10</ref>. More Visual Comparisons of Trimap-free Methods on PPM-100. We compare our MODNet with DIM <ref type="bibr" target="#b48">[49]</ref>, FDMPA <ref type="bibr" target="#b50">[51]</ref>, LFM <ref type="bibr" target="#b49">[50]</ref>, SHM <ref type="bibr" target="#b5">[6]</ref>, HAtt <ref type="bibr" target="#b32">[33]</ref>, and BSHM <ref type="bibr" target="#b28">[29]</ref>. Note that DIM here does not take trimaps as the input but is pre-trained on the SPS <ref type="bibr" target="#b41">[42]</ref> dataset. Zoom in for the best visualization.</p><p>Appendix A <ref type="figure">Fig. 10</ref> provides more visual comparisons of MODNet and the existing trimap-free methods on PPM-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head><p>We argue that trimap-free models can obtain results comparable to trimap-based models in the previous benchmarks because of unnatural fusion or mismatched semantics between synthetic foreground and background. To demonstrate this, we conduct experiments on the open-source Adobe Matting Dataset (AMD) <ref type="bibr" target="#b48">[49]</ref>. We first pick the portrait foregrounds from AMD. We then composite 10 samples for each foreground with diverse backgrounds. We finally validate all models on this synthetic benchmark. <ref type="table">Table 3</ref> shows the quantitative results on the aforementioned benchmark. Unlike the results on PPM-100, the performance gap between trimap-free and trimap-based models is much smaller. For example, MSE and MAD between trimap-free MODNet and trimap-based DIM is only about 0.001. We provide some visual comparison in <ref type="figure">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Trimap-based DIM Trimap-free MODNet Trimap GT <ref type="figure">Figure 11</ref>. Visual Results on AMD. In the first row, the foreground and background lights come from opposite directions (unnatural fusion). In the second row, the portrait is placed on a huge meal (mismatched semantics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Trimap MSE ↓ MAD ↓ DIM <ref type="bibr" target="#b48">[49]</ref> 0.0014 0.0069 MODNet (Our) 0.0011 0.0061 DIM <ref type="bibr" target="#b48">[49]</ref> 0.0075 0.0159 DIM † <ref type="bibr" target="#b48">[49]</ref> 0.0048 0.0116 FDMPA † <ref type="bibr" target="#b50">[51]</ref> 0.0047 0.0115 LFM † <ref type="bibr" target="#b49">[50]</ref> 0.0043 0.0101 SHM † <ref type="bibr" target="#b5">[6]</ref> 0.0031 0.0092 HAtt † <ref type="bibr" target="#b32">[33]</ref> 0.0034 0.0094 BSHM † <ref type="bibr" target="#b28">[29]</ref> 0.0029 0.0088 MODNet † (Our) 0.0024 0.0081 <ref type="table">Table 3</ref>. Quantitative Results on AMD. We pick the portrait foregrounds from AMD for validation. ' †' indicates the models pretrained on the SPS <ref type="bibr" target="#b41">[42]</ref> dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>XFigure 4 .</head><label>4</label><figDesc>Benchmark Comparison. (a) Validation benchmarks used in<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50]</ref> synthesize samples by replacing the background. Instead, our PPM-100 contains original image backgrounds and has higher diversity in the foregrounds. We show samples (b) with fine hair, (c) with additional objects, and (d) without bokeh or with full-body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Comparisons of Model Size and Execution Efficiency.Shorter inference time is better, and fewer model parameters is better. We can divide 1000 by the inference time to obtain f ps.two indicators. The inference time of MODNet is 15.8 ms (63 f ps), which is twice the f ps of previous fastest FDMPA (31 f ps)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Results of SOC and OFD on a Real-World Video. We show three consecutive video frames from left to right. From top to bottom: (a) Input, (b) MODNet, (c) MODNet + SOC, and (d) MODNet + SOC + OFD. The blue marks in frame t − 1 demonstrate the effectiveness of SOC while the red marks in frame t highlight the flickers eliminated by OFD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Advantages of MODNet over Trimap-based Method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Quantitative Results on PPM-100. ' †' indicates the models pre-trained on the SPS dataset. '↓' means lower is better. L s L d SEB SPS MSE ↓ MAD ↓ Ablation of MODNet. SEB: SE-Block in MODNet lowresolution branch. SPS: Pre-training on the SPS dataset.ference time over PPM-100 on an NVIDIA GTX 1080Ti GPU (input images are cropped to 512 × 512). Note that fewer parameters do not imply faster inference speed due to large feature maps or time-consuming mechanisms, e.g., attention, that the model may have.Fig. 6illustrates these</figDesc><table><row><cell>0.0162 0.0235</cell></row><row><cell>0.0097 0.0158</cell></row><row><cell>0.0083 0.0142</cell></row><row><cell>0.0068 0.0128</cell></row><row><cell>0.0046 0.0097</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Refer to Appendix A for more visual comparisons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Refer to our online supplementary video for more results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An attentive survey of attention models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gungor Polatkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mithal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lock-in time-of-flight (tof) cameras: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Sergi Foix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carme</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors Journal</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Shmuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudiger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIIP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhaemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laurens van der Maatena, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse coding for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jubin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehsan Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Varnousfaderani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image matting with kl-divergence based sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting semantic human matting with coarse annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Alphagan: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<idno>abs/1807.10088</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
		<idno>abs/2001.05566</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey on semi-, self-and unsupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmarje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monty</forename><surname>Santarossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon-Martin</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
		<idno>abs/2002.08721, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Background matting: The world is your green screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<title level="m">Chi-Keung Tang, and Heung-Yeung Shum. Poisson matting. TOG</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Supervisely person dataset. supervise.ly</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunc Ozan</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMAVIS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1904.09146</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A survey of unsupervised deep domain adaptation. TIST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast deep matting for portrait animation on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
