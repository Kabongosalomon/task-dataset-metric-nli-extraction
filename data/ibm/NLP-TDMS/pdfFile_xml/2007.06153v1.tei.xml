<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mousavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<postCode>30303</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aashis</forename><surname>Khanal</surname></persName>
							<email>akhanal1@student.gsu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<postCode>30303</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Estrada</surname></persName>
							<email>restrada1@gsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<postCode>30303</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Synthetic data · Deep learning · Virtual environment</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning requires data, but acquiring and labeling real-world data is challenging, expensive, and time-consuming. More importantly, it is nearly impossible to alter real data post-acquisition (e.g., change the illumination of a room), making it very difficult to measure how specific properties of the data affect performance. In this paper, we present AI Playground (AIP), an open-source, Unreal Engine-based tool for generating and labeling virtual image data. With AIP, it is trivial to capture the same image under different conditions (e.g., fidelity, lighting, etc.) and with different ground truths (e.g., depth or surface normal values). AIP is easily extendable and can be used with or without code. To validate our proposed tool, we generated eight datasets of otherwise identical but varying lighting and fidelity conditions. We then trained deep neural networks to predict (1) depth values, (2) surface normals, or (3) object labels and assessed each network's intra-and cross-dataset performance. Among other insights, we verified that sensitivity to different settings is problem-dependent. We confirmed the findings of other studies that segmentation models are very sensitive to fidelity, but we also found that they are just as sensitive to lighting. In contrast, depth and normal estimation models seem to be less sensitive to fidelity or lighting and more sensitive to the structure of the image. Finally, we tested our trained depth-estimation networks on two real-world datasets and obtained results comparable to training on real data alone, confirming that our virtual environments are realistic enough for real-world tasks. Training Scenario / Fidelity Test / Fidelity Goal δ1 Ò δ2 Ò δ3 Ò RELÓ RMSÓ log10Ó Brown / Day / High Brown/ Day / High SC 0.7992 0.9113 0.9474 0.1426 0.0278 0.0740 Blue / Day / Low Blue / Day / Low SC 0.7609 0.8980 0.9278 0.1643 0.0366 0.0858 Brown / Night / High Brown / Night / High SC 0.8333 0.9248 0.9509 0.1327 0.0278 0.0689 Brown/ Day / Low Brown/ Day / Low SC 0.7719 0.8945 0.9388 0.1544 0.0289 0.0798 Brown/ Day / High Brown/ Night / High L 0.7616 0.8928 0.9315 0.1711 0.0398 0.0875 Brown/ Night / High Brown/ Day / High L 0.7366 0.8942 0.9420 0.1904 0.0351 0.0939 Blue / Day / Low Blue / Day / High F 0.7817 0.9062 0.9329 0.1587 0.0370 0.0822 Brown/ Day / Low Brown/ Day / High F 0.8010 0.9113 0.9475 0.1426 0.0273 0.0731 Brown/ Night / High Blue / Night / High M 0.5959 0.8632 0.9079 0.3415 0.0671 0.1193 Brown / Day / High Blue Day / High M 0.6420 0.8528 0.9223 0.2220 0.0433 0.1067</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The remarkable success of deep learning in recent years would not have been possible without large, high-quality datasets <ref type="bibr" target="#b7">[8]</ref>. Deep neural networks have thousands or even millions of parameters, which require vast numbers of training examples to tune. However, producing a high-quality dataset of real data is very challenging. First, one has to acquire the raw data, an often laborious task. Second, the training data must either be labeled manually-which is slow, subjective Real data also has an additional, more subtle limitation: it is very difficult to control before acquisition and nearly impossible to change afterwards. For instance, once an image has been taken, one cannot change its illumination from day to night or replace one object for another <ref type="bibr" target="#b0">1</ref> . The only way to achieve these effects is by manipulating the source of the data before acquisition; however, this approach requires a controlled environment and precise measurements. For example, to change the color of a couch one would need to swap out two otherwise identical couches and place them in the same, exact location. Aside from its difficulty, this approach is not feasible for natural scenes or crowd-sourced data.</p><p>The above limitation makes it is very difficult to isolate the impact of individual features on a system's performance. For example, imagine that we want to assess how an object's texture affects our system's ability to segment it. In this case, we would need to compare our system's output across different objects and hope that the impact of other features, e.g., lighting or shape, cancels out across the samples. As such, data ablation studies are rare in machine learning. Most ablation analyses add/remove either (1) components of the model <ref type="bibr" target="#b9">[10]</ref> or (2) secondary features computed from the data <ref type="bibr" target="#b8">[9]</ref>. The latter is close in spirit to data ablation but is more limited, since secondary features are dependent on the raw, unchangeable data.</p><p>To address this gap, we developed AI Playground, a user-friendly tool based on the Unreal Engine (Epic Games, USA) <ref type="bibr" target="#b3">[4]</ref> that supports data ablation studies in computer vision. <ref type="bibr" target="#b1">2</ref> Our system allows researchers to easily capture synthetic data from fully customizable virtual environments; this data can then be used to train or test an AI system. Virtual data is free from acquisition errors or labeling bias and is ideal for the data ablation studies discussed above, e.g., capturing the same image under multiple lighting conditions. More importantly, as 1 Photo-manipulation techniques can be used to alter images, but their effects are either non-specific (e.g., reducing brightness) or introduce unwanted artifacts. They also require significant human effort. <ref type="bibr" target="#b1">2</ref>  (2) multiple ground-truth annotations (e.g., depth, surface normals, etc.); (3) built-in tools for data ablation (e.g., for adjusting lighting, polygon resolution, etc.); and (4) a user-friendly, graphical interface. Users can either run our system as a pre-built application or import it as a regular UE project. In the latter case, users can extend their local version of AIPlayground or copy parts of it (e.g., scripts) for use in their own projects. It is easy to add custom environments or ground-truth annotations without writing any code. And we provide sample code and the necessary documentation to add new forms of data ablation to AIPlayground. <ref type="figure">Figure 2</ref> provides a flowchart of our tool.</p><p>To validate its usefulness, we used AIP to carry out a series of data ablation studies. As detailed in Sec. 4, we trained and tested deep networks on (1) monocular depth estimation, (2) surface normal estimation, and (3) semantic segmentation. AIP allowed us to draw novel insights about feature importance (Sec. 5), and we also confirmed that networks trained on depth estimation via AI Playground achieve good performance on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Data-hungry models like DCNN (Deep Convolutional Neural Networks) have generated newfound interest in virtual data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. One popular approach is to use modded old video games (e.g., Atari games <ref type="bibr" target="#b10">[11]</ref>). However, this approach lacks customizability and photo-realism. This data cannot be customized to fit a more specific problem and using old video games introduces a lack of photorealism that has been proven beneficial for virtual data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast, Veeravasarapu et al. <ref type="bibr" target="#b17">[18]</ref> used probabilistic generative models to create random environments in Blender (Blender Foundation, The Netherlands) <ref type="bibr" target="#b1">[2]</ref>. However, these probabilistic models need to be manually adapted for each type of desired environment. For example, the probabilistic model of an outdoor street scene varies significantly from one of an interior environment. Also, while randomness is useful for quickly creating novel environments, these environments may not be faithful to reality. For example, a random probabilistic model might decide to put a couch on a table, which never happens in the real world. Furthermore, depending on hardware, rendering an image in Blender using ray-tracing can take up to a minute or more; the same level of fidelity can be achieved in game engines in real-time. As mentioned in <ref type="bibr" target="#b17">[18]</ref>, generating a Path-traced image in Blender takes up to 9 minutes (547 seconds), and ray-tracing based rendering for a single image can take 20 seconds or more.</p><p>In another study, researchers used 3D reconstruction to generate a photorealistic 3D scene that allows limited interaction such as walking around <ref type="bibr" target="#b14">[15]</ref>. This method requires expensive equipment and complex calculations to generate the pixel-wise ground-truth for tasks like depth estimation and surface normal estimation. The generated ground-truth and 3D environment are subject to artifacts and estimation errors appearing as black spots in the images. Also, these environments are extremely hard to expand as they require costly specialized equipment for measurement.</p><p>The work most similar to our proposed system is UnrealCV-an Unreal Engine 4 (UE4) plugin that has been used in a number of research projects. Unre-alCV provides an interface to communicate with the Unreal Engine for computer vision and robotics research <ref type="bibr" target="#b18">[19]</ref>. However, UnrealCV requires command-linebased interaction and C++ coding. As such, it has a high barrier of entry and can be discouraging for computer vision researchers who are unfamiliar with game engines. It also lacks intuitive dials and knobs for dynamic interaction with the environment. More importantly, it is not built for data ablation; any systematic changes in fidelity, lighting, etc. have to be coded from scratch by the researcher.</p><p>In contrast, our goal is to reduce the skill level need to obtain virtual pixelperfect data. Our approach is accessible, user-friendly, and has many intuitive ways to interact with the environment. We use the high quality renderer integrated in Unreal Engine to produce lifelike synthetic images, and AIP does not require any knowledge of UE4 programming. As we detail in the following section, our companion Python module (Probe) communicates with the UE4 application to control the environment and take samples while keeping a record of every step for image re-creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AIPlayground</head><p>AIPlayground is a UE4-based tool for data ablation studies in computer vision. Unreal Engine is the engine of choice for video games with high-resolution, realtime 3D graphics. It is free for both commercial and non-commercial use and its source code is publicly available (though not fully open source). As illustrated in <ref type="figure">Fig. 2</ref>, our system has four components: (1) high-resolution 3D environments; (2) multiple ground-truth annotations; (3) data ablation controls; and (4) a user-friendly, graphical interface. As we discuss further below, we use Blueprint, Unreal's visual scripting language, for the ground-truth annotations and data ablation controls. We use a separate Python interaction module-Probe-for data collection, which is also publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Three-dimensional environments</head><p>In addition to being a game engine, Unreal Engine provides powerful tools for realistic architectural visualization. As such, we developed two environments based on UE4's built-in "Realistic Rendering" scene, dubbed Brown Room and Blue Room in our experiments. Each environment has two general lighting profiles, Day and Night, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. To mimic existing real-world datasets, the environments are static (i.e., no movement of the components aside from the probe character). AIP currently uses static (i.e., baked) lighting to illuminate the scene. Baking light-maps is a commonly used method to simulate high-fidelity lighting on lower-capacity hardware. It uses ray-tracing to determine dark and light spots in the scene and paints the textures on those areas to look accordingly. The result is a very realistic environment that is rendered rapidly with little to no extra computation required at run-time. This means AIP supports very high frame-rates, which allow for fast data acquisition. We can switch between different ground-truth annotations in fractions of a second without causing artifacts such as blur, fuzziness on the edges, or motion-blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ground-truth annotations</head><p>One of the main advantages of virtual environments is that obtaining groundtruth annotations is trivial relative to real-world environments. Specifically, we use Unreal Blueprint (an internal scripting language) to calculate the groundtruth properties listed below. AIP includes Blueprint scripts for estimating depth, surface normals, and object classes, and can be readily extended by adding additional scripts. We use post-processing shaders, called materials in UE4, to overlay these properties over the image, enabling pixel-perfect alignment between the data and the ground-truth labels (see <ref type="figure" target="#fig_0">Fig. 1</ref> for examples). Depth estimation: We calculate the normalized distance between each pixel that belongs to a specific object and the camera. We set the real-life range of depth to 10 meters, which covers the entire environment and does not clip between any corners of the room. We define the depth using perspective projection relative to the viewer's POV, which is significantly more accurate than orthographic methods. In perspective depth, each light ray is traced to the exact pixel from the object its coming from; in orthographic depth, on the other hand, lightrays are assumed to be coming from infinity (see <ref type="figure" target="#fig_2">Fig. 4</ref>). In real-world datasets, e.g., NYUv2 <ref type="bibr" target="#b11">[12]</ref> and DIODE <ref type="bibr" target="#b15">[16]</ref>, depth is registered based on orthographic projection because of physical limitations in the sensor.</p><p>Surface normals: We estimate the normal vector w.r.t to each 3D surface, then color each pixel to indicate the vector's direction. We use 6 main colors to show 6 axis of direction (positive and negative xyz, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation:</head><p>In UE4, it is easy to map visible pixels to their corresponding 3D objects. Our Blueprint script uses this mapping to overlay pixel-perfect semantic labels on the various objects in the scene (e.g., couch, table, lamp, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data ablation controls</head><p>Similar to the ground-truth, we use Blueprint to dynamically alter properties of the environment. We can access and isolate specific properties in different objects. For example, we can isolate metallic objects or rough surfaces with a pixel-perfect binary ground truth. We can also change the fidelity of reflections, lighting, mesh level of detail (LOD), render resolution (either localized to an object or globally), anti-aliasing algorithms (or toggle on and off), or render scaling. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates the same scene rendered under different fidelity settings. Our scripts are reusable, in the sense that they do not require adaptation to other environments and are also easily portable to other UE projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">User interface</head><p>The AIP Core can be opened as a project in UE4, giving access to all its assets and scripts. Alternatively, we provide a pre-compiled version which can be run as an independent program. AIP has intuitive user menus and keyboard shortcuts. Our Python Probe script uses the latter to collect data (see Sec. 4 for details). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We carried out multiple experiments to validate the usefulness of our proposed system. Specifically, we tested AIP in two ways. First, we verified its viability as a data ablation tool. As we detail below, we captured the same images under different fidelity and lighting settings (which we refer to as a scenario), then trained deep neural networks on each scenario to assess the impact of the various environmental features. We carried out both same-and cross-scenario testing (e.g., a Brown/Day/High network on Brown/Night/High). <ref type="table" target="#tab_1">Table 1</ref> summarizes the scenarios used. For each scenario, we tested our networks on (1) monocular depth and (2) surface normal estimation, as well as (3) semantic segmentation.</p><p>Second, to validate that our virtual data is realistic enough, we tested networks trained with AIP on real-world depth-estimation datasets, achieving results comparable to training on real data alone. Below, we first detail our experimental setup, then discuss each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Hardware: We conducted all our experiments in a Dell Precision 7920R server with two Intel Xeon Silver 4110 CPUs, two GeForce GTX 1080 Ti graphics cards, and 128 GBs of RAM.</p><p>Image acquisition: Our Probe script can control the viewpoint by simulating keystrokes. It can move and look freely (yaw and pitch) in the environment. Probe can also send specific commands and can gather images with high overlap (in groups) or low overlap (completely random). Probe's step size, look sensitivity, randomness of image acquisition (group capture), and number of images to gather are all customizable and can be saved for reproduction across all different scenarios. For our depth estimation experiments, we randomly collected 8265, 640ˆ480 synthetic color images. We collected the same images, by replicating the same camera positions and rotations, across different lighting and fidelity scenarios (Tbl. 2). We split these images into 80% for training, and 20% for testing. Similarly, for semantic segmentation and surface normal estimation, we gathered 3000 images for each scenario and split in the same ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural networks:</head><p>We used the encoder-decoder architecture, and loss function from <ref type="bibr" target="#b11">[12]</ref> for depth estimation, and an implementation of U-net <ref type="bibr" target="#b13">[14]</ref> from <ref type="bibr" target="#b6">[7]</ref> for surface normal estimation and semantic segmentation. We use smooth L1 loss function for Surface Normal Estimation, and Cross-Entropy loss for segmentation task. We use a mini-batch size of 16, learning rate of 0.001, and trained for 51 epochs for all experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monocular depth estimation experiments</head><p>Data ablation: <ref type="table" target="#tab_2">Table 2</ref> shows a representative sample of the data ablation experiments we conducted using our depth ground truth. For these experiments, we initialized our deep networks using the weights from a network trained on NYUv2. For evaluation, we used the same metrics as those used in <ref type="bibr" target="#b2">[3]</ref>: average relative error (REL), root mean squared error (RMS), average log10 error, and threshold accuracy (δ i ă 1.25 i for i " r1, 2, 3s). As we discuss further in Sec. 5, models trained in higher fidelity data generally tend to yield higher scores, even on lower-fidelity scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world validation:</head><p>To demonstrate the transferability of learned features from a synthetic dataset, we tested our best-performing models on the real-world DIODE and NYUv2 datasets. In addition to the full test set, we also evaluated our networks on a filtered subset that only contained scenes structurally similar to our virtual environments, i.e., indoor scenes of a living room, with objects such as couches, beds, TVs, etc. As Tbl. 3 shows, our high-fidelity trained model had better threshold accuracy on DIODE than a model trained only on NYUv2 <ref type="bibr" target="#b16">[17]</ref>, <ref type="table">Table 4</ref>: Surface normal estimation: Metrics are percentage of pixels that differ by 11.5˝, 22.5˝, and 30˝from the true normal, and mean and median errors. Mean and median are higher than <ref type="bibr" target="#b19">[20]</ref> because our loss function did not implement hybrid measures to reduce them. This wasn't necessary since our ground-truth data does not suffer from the problem mentioned in <ref type="bibr" target="#b19">[20]</ref>. confirming that the features learned on our environments are transferable to real-world data. In addition, our model trained on Night lighting, high-fidelity settings achieved 31% δ 1 vs 28% δ 1 of NYUv2 model -59% δ 2 vs 50% δ 2 of NYUv2 model -79.4% δ 3 vs 67.3% of δ 3 of NYUv2 model. These results further confirm that our photo-realistic data can match and even exceed real-life training. Furthermore, these models achieved a much higher score in our filtered test set, suggesting that depth estimation is more sensitive to the structure of the input image than to lighting or fidelity. We also believe our models would have performed even better had DIODE used perspective depth <ref type="figure" target="#fig_2">(Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Surface normal estimation experiments</head><p>We carried out a similar set of data ablation experiments as above, but using surface normal data as the ground truth. Here, we trained each model from scratch, i.e., without pre-trained weights, and used the same evaluation metrics as in <ref type="bibr" target="#b19">[20]</ref>: mean (average L1 loss), median (average L2 loss), and percentage of pixels that differ by 11.5˝, 22.5˝, and 30˝relative to the true surface normal. Surface normal estimation is a promising use case for AIP because it is very challenging to capture surface normal information for real scenes. One needs expensive equipment to measure the angles, and these sensors are extremely hard to calibrate. As Tbl. 4 shows, we can successfully train deep networks using AIP (see <ref type="figure" target="#fig_4">Fig. 6</ref>). Overall, surface normal models seem to be less sensitive to photorealistic features and higher fidelity settings compared to depth estimation or segmentation. Models trained on high fidelity settings perform 2% better than ones trained on low fidelity, a point we discuss further in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic segmentation experiments</head><p>Semantic segmentation involves assigning a class label to every pixel on the image. The built-in environments in AIP have fifteen classes, all of which corre- sponds to regular household objects, e.g., wall, couch, table, TV, plant, etc. We use a label of other for miscellaneous items. As with the surface normals, we trained different networks from scratch on each scenario. We used mean intersection over union (IOU) of all classes as our evaluation metric. As we can see in Tbl. 5, model performance is directly linked to a scenario's fidelity (see <ref type="figure" target="#fig_4">Fig. 6</ref>). Semantic segmentation seems to depend heavily on the render scaling and resolution. At lower settings, borders of the objects are blurry, as is their texture. This causes the model to label them as other since it cannot surely ascertain their object class, thus lowering the global IOU (see <ref type="figure" target="#fig_5">Fig. 7</ref> for an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Below, we discuss some insights from our data ablation experiments that serve as examples of the kind of analyses that AIP makes possible.</p><p>Sensitivity to lighting: Changes in lighting are a result of the environment, so they cannot be "fixed" by a better acquisition device. As such, a general-purpose model should be robust to them. However, objects can appear in drastically different ways under different lighting conditions, which did affect performance across all experiments. More specifically, segmentation models are particularly sensitive to differences in lighting. In <ref type="figure" target="#fig_5">Fig. 7</ref> both models labeled the top part of the TV as Wall since they have almost the same color. However, the model trained on a Day setting was much less accurate on the Night image than its counterpart, presumably because the Night setting is darker overall and has more pronounced reflections. The opposite effect is visible in the reverse case (bottom <ref type="figure" target="#fig_5">Fig. 7)</ref>, where the reflection in the lamp confused the model because that level of reflection from sunlight does not exist in the Night lighting. Our surface normal models are also sensitive to changes in lighting. However, for depth estimation, performance drops only slightly when the lighting is changed, suggesting that local contrast is less important for this problem.</p><p>The impact of fidelity on surface normals vs. segmentation: Semantic segmentation is very sensitive to changes in fidelity. When objects are blurred due to lower rendering resolution and lower texture clarity, the model appears to be indecisive about picking an object's class in its border regions. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we see that the model incorrectly classified border regions as Other.</p><p>In contrast, surface normal estimation is more robust to these kinds of changes. This difference between these two problems highlights the importance of using data ablation tools. Previous studies, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>, mainly focus on the effects of fidelity on their segmentation experiments. Our findings with surface normals, on the other hand, suggest that fidelity as a general feature of the image might not be enough to draw conclusions about the quality of the data. AIP's tools allow us to study other aspects of data, such as texture, structure complexity, lighting and more.</p><p>Perspective vs orthographic depth: Orthographic depth projection is when light-rays coming to the camera are assumed to be coming from infinity. In calculating the depth ground-truth, this simplification introduces errors to the measurement. We have seen the effects of this assumption on the NYUv2 and DIODE dataset ( <ref type="figure" target="#fig_2">Fig. 4)</ref>. Specifically, our models' performance on DIODE was lower in part due to them being trained on perspective depth, which is different from the GT used in DIODE. Although orthographic measurements are currently widely used, we argue that perspective depth, which AIP supports, is the correct way to measure depth.</p><p>Impact of fidelity on depth estimation: Generally, the performance of models trained on higher fidelity settings are better than those trained in lower fidelity settings <ref type="table" target="#tab_2">(Table 2</ref>). However, one exception is when the lower fidelity setting in training better matches the features of the target domain. In Tbl. 3, our low fidelity model does slightly better on NYUv2 than the high-fidelity one. We argue this is due to the blur present in NYUv2, which is also present in our low fidelity settings training set due to its lower render settings, making them visually similar. The DIODE dataset, on the other hand, is much more recent, so the depth ground truth was measured with a more accurate sensor. Due to the lack of blur and fuzz on the ground-truth, we did not observe the same kind of performance gain on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future work</head><p>In this work, we presented AI Playground, a data creation and ablation tool for machine learning. Using AIP, we generated different image datasets and conducted experiments that are nearly impossible with real data, thus demonstrating that AIP is a viable tool for data ablation studies in computer vision. We also verified that our high-fidelity trained models can match or exceed the scores achieved by training with real-data. As suggested by other studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>, we found that higher-fidelity data is linked to better performance in segmentation, but we also found that sensitivity to scene structure, fidelity and lighting scenario of training data varies from task to task. For example, our surface normal and depth estimation models were not as sensitive to fidelity as our segmentation models were. AIP enables us to change individual features, e.g., quality of shadows, quality of reflections, quality of lighting or resolution of textures, and assess their impact on different models based on the current task. More generally, AIP can help researchers find sensitive points in their models and aid them in creating high-quality data for training neural networks for a specific computer vision task.</p><p>We are currently working to add more environments to AIP to widen its usage range. These environments include more indoor scenes, outdoor scenes and fully interactive environments allowing individual interaction with objects. Additionally, we'll be providing support for reinforcement learning studies and real-time ray-tracing. There are still many other possible experiments that remain to be explored. For example, UE4 allows the fast change of lighting profile by using HDRI maps. This opens the possibility of adding more specific lighting scenarios like rainy, overcast and foggy. In our future updates, we'll be adding support to introduce intentional camera artifacts such as chromatic aberration, penumbra, lens flares and distortions to help study the effects of using small sensors in capturing data. This is especially useful in robotics since consumer-grade robots rarely come with expensive capture equipment; fine-tuning training to the exact specifications of the camera is a very exciting avenue for future work. Furthermore, we are refining our ground-truth options, including removing texture and changing colors and properties of shaders. These enhancements will enable us to manipulate the scene even further, e.g., changing the pattern in a fabric or changing smoothness of a stone. We believe that AIP will open new and exciting avenues in synthetic data and machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Virtual environments: Sample screenshots from our annotated virtual environments. From left to right: depth, surface normals, and semantic labels and may require significant expertise-or with expensive, specialized equipment. Finally, errors can occur in both the acquisition and labeling phases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Sample images captured by Probe. Left to right: Brown Room Day, Brown Room Night, Blue Room Day, Blue Room Night (All high settings)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Depth estimation: AIP uses perspective projection (first row), which is more accurate than orthographic projection (second row). The third column uses color banding to highlight the differences between these two approaches. The bottom rows show examples from the DIODE and NYUv2 datasets. Note the lack of artifacts in the virtual ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fidelity Comparison: Left: Day(High Fidelity), Right: Night(High Fidelity). Each image snippet of Low Fidelity indicates the difference in Texture resolution, Reflections quality, Render Scaling and Shadow quality. The amount of change in each of these settings is customizable through AIP's Core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Sample results: Sample images, ground truth, and predictions for semantic segmentation (first three columns), depth estimation (middle columns), and surface normal estimation (last three columns). Figure best viewed onscreen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Semantic segmentation: (Image, Ground Truth, Prediction). Top: A model trained on Brown Day High (DH) images segmenting a Brown Night High (NH) image. Bottom: a model trained on Brown Night High tested on Brown Day High. Note the impact of lighting on the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Source code, documentation, supplementary images, and high-definition figures are available on our GitHub page: https://git.io/JJkhQ</figDesc><table /><note>Fig. 2: AI Playground: Our tool has two main modules: the AIP Core within UE4 and Probe, a Python module that communicates with the Core. Probe receives instructions generated by the Command module, and saves its state in its own dedicated memory. This allows changing settings inside the engine while AIP is running. Manually changing components is also possible via the GUI.our experiments confirm, today's high-resolution computer graphics are realistic enough to be used for training deep neural networks on real-world tasks. As we detail in Sec. 3, AI Playground is an open-source UE project with four main components: (1) a set of high-resolution environments;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Scenarios used in experiments a</figDesc><table><row><cell>Default</cell><cell></cell><cell></cell><cell>Settings</cell></row><row><cell>Maps</cell><cell cols="2">Lighting Fidelity</cell><cell>Anti-Aliasing</cell></row><row><cell>Brown Room</cell><cell>Day</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell>Brown Room</cell><cell>Night</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell>Brown Room</cell><cell>Day</cell><cell>Low</cell><cell>Temporal AA</cell></row><row><cell>Brown Room</cell><cell>Night</cell><cell>Low</cell><cell>Temporal AA</cell></row><row><cell>Blue Room</cell><cell>Day</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell>Blue Room</cell><cell>Night</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell>Blue Room</cell><cell>Day</cell><cell>Low</cell><cell>Temporal AA</cell></row><row><cell>Blue Room</cell><cell>Night</cell><cell>Low</cell><cell>Temporal AA</cell></row><row><cell>Abstract Shapes</cell><cell>Day</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell cols="2">Unlit b Brown Room N/A</cell><cell>High</cell><cell>Temporal AA</cell></row><row><cell>Unlit Blue Room</cell><cell>N/A</cell><cell>High</cell><cell>Temporal AA</cell></row></table><note>a shows settings used, not indicative of all settings available.b diffuse shading.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Depth estimation: Data ablation test results. Metrics are threshold accuracy (δ i ă 1.25 i ), average relative error (REL), root mean squared error (RMS), and average (log10) error. Arrows indicate if higher or lower values are better. For space, we included only some of the conducted experiments; results shown are indicative of the behavior of the trained models in other scenarios. SC: Sanity Check. L: Change in Lighting. M: Change in Maps. F: Positive Change in Fidelity</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Depth estimation: Results on real-world datasets.</figDesc><table><row><cell>Train / Fidelity</cell><cell>Test</cell><cell>δ1 Ò δ2 Ò δ3 Ò RELÓ RMSÓ log10Ó</cell></row><row><cell cols="2">Brown / Day / High NYUv2</cell><cell>0.3666 0.6012 0.7586 0.5044 0.2014 0.1938</cell></row><row><cell cols="2">Brown / Day / Low NYUv2</cell><cell>0.3720 0.6062 0.7627 0.5010 0.2010 0.1921</cell></row><row><cell cols="2">Brown / Night / High DIODE</cell><cell>0.3563 0.5948 0.7945 0.7659 3.6897 0.2148</cell></row><row><cell cols="2">Brown / Night / Low DIODE</cell><cell>0.3163 0.5647 0.7345 0.7743 3.7898 0.2149</cell></row><row><cell cols="3">Brown / Night / High DIODE -Filtered 0.6546 0.7725 0.8371 0.6608 2.9765 0.1458</cell></row><row><cell cols="3">Brown / Day / High NYUv2 -Filtered 0.5996 0.8405 0.9308 0.2835 0.1232 0.1054</cell></row><row><cell cols="3">DIODE/Indoor [17] DIODE/Indoor 0.4919 0.7159 0.8256 0.3306 1.6948 0.1775</cell></row><row><cell>NYUv2 [1]</cell><cell>NYUv2</cell><cell>0.895 0.980 0.9960 0.1030 0.390 0.0430</cell></row><row><cell>NYUv2 [1]</cell><cell cols="2">DIODE/Indoor 0.2869 0.5097 0.6730 0.6599 2.8854 0.2573</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SC: Sanity Check. L: Change in Lighting. M: Change in Maps. F: Positive Change in Fidelity</figDesc><table><row><cell cols="2">Scenario / Fidelity Test / Fidelity</cell><cell cols="2">Goal 11.5˝Ò 22.5˝Ò 30˝Ò MeanÓ MedianÓ</cell></row><row><cell cols="2">Brown / Day / High Brown / Day / High</cell><cell cols="2">SC 0.9014 0.9566 0.9727 24.4575 88.2878</cell></row><row><cell>Blue / Day / Low</cell><cell>Blue / Day / Low</cell><cell cols="2">SC 0.9274 0.9746 0.989 30.5607 94.9516</cell></row><row><cell cols="2">Blue / Night / High Blue / Night / High</cell><cell>SC</cell><cell>0.865 0.9224 0.9401 28.2409 69.2181</cell></row><row><cell cols="2">Brown / Day / Low Brown / Day / Low</cell><cell cols="2">SC 0.8883 0.9443 0.961 25.3718 81.4871</cell></row><row><cell cols="4">Brown / Day / High Brown / Night / High L 0.052145 0.2238 0.3464 106.70 121.26</cell></row><row><cell cols="2">Brown / Night / High Brown / Day / High</cell><cell cols="2">L 0.050291 0.2135 0.4253 115.82 119.86</cell></row><row><cell>Blue / Day / Low</cell><cell>Blue / Day / High</cell><cell cols="2">F 0.195269 0.2683 0.3015 97.832 113.57</cell></row><row><cell cols="2">Brown / Day / Low Brown / Day / High</cell><cell cols="2">F 0.028247 0.2102 0.368 109.14 118.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Semantic segmentation: Mean intersection over union (IOU) of all classes for different scenarios. Higher values are better. SC: Sanity Check. L: Change in Lighting. M: Change in Maps. F: Positive Change in Fidelity</figDesc><table><row><cell cols="2">Scenario / Fidelity Test / Fidelity</cell><cell cols="2">Goal Global IOUÒ</cell></row><row><cell cols="2">Brown / Day / High Brown / Day / High</cell><cell>SC</cell><cell>0.8984</cell></row><row><cell>Blue / Day / Low</cell><cell>Blue / Day / Low</cell><cell>SC</cell><cell>0.4119</cell></row><row><cell cols="2">Blue / Night / High Blue / Night / High</cell><cell>SC</cell><cell>0.8335</cell></row><row><cell cols="2">Brown / Day / Low Brown / Day / Low</cell><cell>SC</cell><cell>0.4714</cell></row><row><cell cols="3">Brown / Day / High Brown / Night / High L</cell><cell>0.6932</cell></row><row><cell cols="2">Brown / Night / High Brown / Day / High</cell><cell>L</cell><cell>0.6418</cell></row><row><cell>Blue / Day / Low</cell><cell>Blue / Day / High</cell><cell>F</cell><cell>0.3862</cell></row><row><cell cols="2">Brown / Day / Low Brown / Day / High</cell><cell>F</cell><cell>0.4188</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<ptr target="https://arxiv.org/abs/1812.11941" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>Blender -a 3D modelling and rendering package</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1406.2283</idno>
		<ptr target="http://arxiv.org/abs/1406.2283" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Epic Games: Unreal engine</title>
		<ptr target="https://www.unrealengine.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<title level="m">Virtual worlds as proxy for multi-object tracking analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Framework for generation of synthetic ground truth data for driver assistance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haltakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic deep networks for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<idno>abs/1903.07803</idno>
		<ptr target="http://arxiv.org/abs/1903.07803" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Merrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00174</idno>
		<title level="m">Randomized Ablation Feature Importance</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Waubert De Puiseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meisen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08644</idno>
		<title level="m">Ablation Studies in Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Playing Atari with Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Habitat: A Platform for Embodied AI Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DIODE: A Dense Indoor and Outdoor DEpth Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1908.00463</idno>
		<ptr target="http://arxiv.org/abs/1908.00463" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<title level="m">Diode: A dense indoor and outdoor depth dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model-driven simulations for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeravasarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rothkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Visvanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1063" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unrealcv: Virtual worlds for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Y.Z.S.Q.Z.X.T.S.K.Y.W.A.Y.</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangwei</forename><surname>Zhong</surname></persName>
			<affiliation>
				<orgName type="collaboration">Y.Z.S.Q.Z.X.T.S.K.Y.W.A.Y.</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM Multimedia Open Source Software Competition</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep surface normal estimation with hierarchical RGB-D fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1904.03405</idno>
		<ptr target="http://arxiv.org/abs/1904.03405" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
