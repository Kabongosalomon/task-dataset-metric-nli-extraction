<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaBins: Depth Estimation using Adaptive Bins</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shariq</forename><surname>Farooq</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ibraheem Alhashim KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhat</forename><surname>Kaust</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ibraheem Alhashim KAUST</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
							<email>pwonka@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Ibraheem Alhashim KAUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaBins: Depth Estimation using Adaptive Bins</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformerbased architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-ofthe-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper tackles the problem of estimating a high quality dense depth map from a single RGB input image. This is a classical problem in computer vision that is essential for many applications <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref>. In this work, we propose a new architecture building block, called AdaBins that leads to a new state-of-the-art architecture for depth estimation on the two most popular indoor and outdoor datasets, NYU <ref type="bibr" target="#b36">[37]</ref> and KITTI <ref type="bibr" target="#b13">[14]</ref>.</p><p>The motivation for our work is the conjecture that current architectures do not perform enough global analysis of the output values. A drawback of convolutional layers is that they only process global information once the tensors reach a very low spatial resolution at or near the bottleneck. However, we believe that global processing is a lot more powerful when done at high resolution. Our general idea is to perform a global statistical analysis of the output of a traditional encoder-decoder architecture and to refine the output with a learned post-processing building block that operates at the highest resolution. As a particular realization of this idea, we propose to analyze and modify the distribution of the depth values.</p><p>Depth distribution corresponding to different RGB inputs can vary to a large extent (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Some images have most of the objects located over a very small range of depth values. Closeup images of furniture will, for example, contain pixels most of which are close to the camera while other images may have depth values distributed over a much broader range, e.g. a corridor, where depth values range from a small value to the maximum depth supported by the network. Along with the ill-posed nature of the problem, such a variation in depth distribution makes depth regression in an end-to-end manner an even more difficult task. Recent works have proposed to exploit assumptions about indoor environments such as planarity constraints <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> to guide the network, which may or may not hold for a real-  <ref type="figure">Figure 2</ref>: Overview of our proposed network architecture. Our architecture consists of two major components: an encoderdecoder block and our proposed adaptive bin-width estimator block called AdaBins. The input to our network is an RGB image of spatial dimensions H and W , and the output is a single channel h × w depth image (e.g., half the spatial resolution). world environment, especially for outdoors scenes.</p><p>Instead of imposing such assumptions, we investigate an approach where the network learns to adaptively focus on regions of the depth range which are more probable to occur in the scene of the input image.</p><p>Our main contributions are the following:</p><p>• We propose an architecture building block that performs global processing of the scene's information.</p><p>We propose to divide the predicted depth range into bins where the bin widths change per image. The final depth estimation is a linear combination of the bin center values.</p><p>• We show a decisive improvement for supervised single image depth estimation across all metrics for the two most popular datasets, NYU <ref type="bibr" target="#b36">[37]</ref> and KITTI <ref type="bibr" target="#b13">[14]</ref>.</p><p>• We analyze our findings and investigate different modifications on the proposed AdaBins block and study their effect on the accuracy of the depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The problem of 3D scene reconstruction from RGB images is an ill-posed problem. Issues such as lack of scene coverage, scale ambiguities, translucent or reflective materials all contribute to ambiguous cases where geometry cannot be derived from appearance. Recently, methods that rely on convolutional neural networks (CNNs) are able to produce reasonable depth maps from a single RGB input image at real-time speeds.</p><p>Monocular depth estimation has been considered by many CNN methods as a regression of a dense depth map from a single RGB image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>As the two most important competitors, we consider BTS <ref type="bibr" target="#b25">[26]</ref> and DAV <ref type="bibr" target="#b21">[22]</ref>. BTS uses local planar guidance layers to guide the features to full resolution instead of standard upsampling layers during the decoding phase. DAV uses a standard encoder-decoder scheme and proposes to exploit co-planarity of objects in the scene via attention at the bottleneck. Our results section compares to these (and many other) methods.</p><p>Encoder-decoder networks have made significant contributions in many vision related problems such as image segmentation <ref type="bibr" target="#b34">[35]</ref>, optical flow estimation <ref type="bibr" target="#b9">[10]</ref>, and image restoration <ref type="bibr" target="#b27">[28]</ref>. In recent years, the use of such architectures have shown great success both in the supervised and the unsupervised setting of the depth estimation problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b0">1]</ref>. Such methods typically use one or more encoder-decoder networks as a sub part of their larger network. In this paper we adapted the baseline encoderdecoder network architecture used by <ref type="bibr" target="#b0">[1]</ref>. This allows us to more explicitly study the performance attribution of our proposed extension on the pipeline which is typically a difficult task.</p><p>Transformer networks are gaining greater attention as a viable building block outside of their traditional use in NLP tasks and into computer vision tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. Following the success of recent trends that combine CNNs with Transformers <ref type="bibr" target="#b1">[2]</ref>, we propose to leverage a Transformer encoder as a building block for non-local processing on the output of a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we present the motivation for this work, provide details of the AdaBins architecture, and describe the corresponding loss functions used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Our idea could be seen as a generalization of depth estimation via an ordinal regression network as proposed by Fu et al. <ref type="bibr" target="#b10">[11]</ref>. Fu et al. observed that a performance improvement could be achieved if the depth regression task is transformed into a classification task. They proposed to divide the depth range into a fixed number of bins of predetermined width. Our generalization solves multiple limitations of the initial approach. First, we propose to compute adaptive bins that dynamically change depending on the features of the input scene. Second, a classification approach leads to a discretization of depth values which results in poor visual quality with obvious sharp depth discontinuities. This might still lead to good results with regard to the standard evaluation metrics, but it can present a challenge for downstream applications, e.g. computational photography or 3D reconstruction. Therefore, we propose to predict the final depth values as a linear combination of bin centers. This allows us to combine the advantages of classification with the advantages of depth-map regression. Finally, compared to other architectures, e.g. DAV <ref type="bibr" target="#b21">[22]</ref>, we compute information globally at a high resolution and not primarily in the bottleneck part at a low resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">AdaBins design</head><p>Here, we discuss four design choices of our proposed architecture that are most important for the obtained results.</p><p>First, we employ an adaptive binning strategy to discretize the depth interval D = (d min , d max ) into N bins. This interval is fixed for a given dataset and is determined by dataset specification or manually set to a reasonable range. To illustrate our idea of dividing a depth interval into bins, we would like to contrast our final solution with three other possible design choices we evaluated:</p><p>• Fixed bins with a uniform bin width: the depth interval D is divided into N bins of equal size.</p><p>• Fixed bins with a log scale bin width: the depth interval D is divided into bins of equal size in log scale.</p><p>• Trained bin widths: the bin widths are adaptive and can be learned for a particular dataset. While the bin widths are general, all images finally share the same bin subdivision of the depth interval D.</p><p>• AdaBins: the bin widths b are adaptively computed for each image.</p><p>We recommend the strategy of AdaBins as the best option and our ablation study validates this choice by showing the superiority of this design over its alternatives. An illustration of the four design choices for bin widths can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Head</head><p>Conv 3×3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv p×p</head><p>Pixel-wise dot product R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bin widths: b Features Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1×1 kernels</head><p>Misc.</p><p>Patch embeddings <ref type="figure">Figure 4</ref>: An overview of the mini-ViT block. The input to the block is a multi-channel feature map of the input image. The block includes a Transformer encoder that is applied on patch embeddings of the input for the purpose of learning to estimate bin widths b and a set of convolutional kernels needed to compute our Range-Attention-Maps R.</p><formula xml:id="formula_0">C d ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Conv</head><p>Second, discretizing the depth interval D into bins and assigning each pixel to a single bin leads to depth discretization artifacts. We therefore predict the final depth as a linear combination of bin centers enabling the model to estimate smoothly varying depth values.</p><p>Third, several previous architectures propose performing global processing using attention blocks to process information after an encoder block in the architecture (e.g., image captioning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> or object detection <ref type="bibr" target="#b1">[2]</ref>). Also, the current state-of-the-art in depth estimation uses this strategy <ref type="bibr" target="#b21">[22]</ref>. Such an architecture consists of three blocks ordered as such: encoder, attention, followed by a decoder. We initially followed this approach but noticed that better results can be achieved when using attention at the spatially higher resolution tensors. We therefore propose an architecture that also has these three blocks, but ordered as follows: encoder, decoder, and finally attention.</p><p>Fourth, we would like to build on the simplest possible architecture to isolate the effects of our newly proposed Ad-aBins concept. We therefore build on a modern encoderdecoder <ref type="bibr" target="#b0">[1]</ref> using EfficientNet B5 <ref type="bibr" target="#b39">[40]</ref> as the backbone for the encoder.</p><p>In the next subsection, we provide a description of the entire architecture. <ref type="figure">Fig. 2</ref> shows an overview of our proposed depth estimating architecture. Our architecture consists of two major components: 1) an encoder-decoder block built on a pretrained EfficientNet B5 <ref type="bibr" target="#b39">[40]</ref> encoder and a standard feature upsampling decoder; 2) our proposed adaptive binwidth estimator block called AdaBins. The first component is primarily based on the simple depth regression network of Alhashim and Wonka <ref type="bibr" target="#b0">[1]</ref>  The first block in the AdaBins module is called mini-ViT. An overview of this block is shown in <ref type="figure">Fig. 4</ref>. It is a simplified version of a recently proposed technique of using transformers for image recognition <ref type="bibr" target="#b5">[6]</ref> with minor modifications. The details of mini-ViT are explained in the next paragraph. There are two outputs of mini-ViT: 1) a vector b of bin-widths, which defines how the depth interval D is to be divided for the input image, and 2) Range-Attention-Maps R of size h × w × C, that contain useful information for pixel-level depth computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture description</head><p>Mini-ViT. Estimating sub-intervals within the depth range D which are more probable to occur for a given image would require a combination of local structural information and global distributional information at the same time. We propose to use global attention in order to calculate a binwidths vector b for each input image. Global attention is expensive both in terms of memory and computational complexity, especially at higher resolutions. However, recent rapid advances in transformers provide some efficient alternatives. We take inspiration from the Vision Transformer ViT <ref type="bibr" target="#b5">[6]</ref> in designing our AdaBins module with transformers. We also use a much smaller version of the transformer proposed as our dataset is smaller and refer to this transformer as mini-ViT or mViT in the following description.</p><p>Bin-widths. We first describe how the bin-widths vector b is obtained using mViT. The input to the mViT block is a tensor of decoded features x d ∈ R h×w×C d . However, a transformer takes a sequence of fixed size vectors as input. We first pass the decoded features through a convolutional block, named as Embedding Conv (see <ref type="figure">Fig 4)</ref>, with kernel size p × p, stride p and number of output channels E. Thus, the result of this convolution is a tensor of size h/p × w/p × E (assuming both h and w are divisible by p). The result is reshaped into a spatially flattened tensor x p ∈ R S×E , where S = hw p 2 serves as the effective sequence length for the transformer. We refer to this sequence of E-dimensional vectors as patch embeddings.</p><p>Following common practice <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, we add learned positional encodings to the patch embeddings before feeding them to the transformer. Our transformer is a small transformer encoder (see <ref type="table">Table.</ref> 1 for details) and outputs a sequence of output embeddings x o ∈ R S×E . We use an MLP head over the first output embedding (we also experimented with a version that has an additional special token as first input, but did not see an improvement). The MLP head uses a ReLU activation and outputs an N-dimensional vector b . Finally, we normalize the vector b such that it sums up to 1, to obtain the bin-widths vector b as follows:</p><formula xml:id="formula_1">b i = b i + N j=1 (b j + ) ,<label>(1)</label></formula><p>where = 10 −3 . The small positive ensures each binwidth is strictly positive. The normalization introduces a competition among the bin-widths and conceptually forces the network to focus on sub-intervals within D by predicting smaller bin-widths at interesting regions of D.</p><p>In the next subsection, we describe how the Range-Attention-Maps R are obtained from the decoded features and the transformer output embeddings.</p><p>Range attention maps. At this point, the decoded features represent a high-resolution and local pixel-level information while the transformer output embeddings effectively contain more global information. As shown in <ref type="figure">Fig. 4</ref>, output embeddings 2 through C + 1 from the transformer are used as a set of 1 × 1 convolutional kernels and are convolved with the decoded features (following a 3 × 3 convolutional layer) to obtain the Range-Attention Maps R. This is equivalent to calculating the Dot-Product attention weights between pixel-wise features treated as 'keys' and transformer output embeddings as 'queries'. This simple design of using output embeddings as convolutional kernels lets the network integrate adaptive global information from the transformer into the local information of the decoded features. R and b are used together to obtain the final depth map.</p><p>Hybrid regression. Range-Attention Maps R are passed through a 1 × 1 convolutional layer to obtain N -channels which is followed by a Softmax activation. We interpret the N Softmax scores p k , k = 1, ..., N , at each pixel as probabilities over b as follows:</p><formula xml:id="formula_2">c(b i ) = d min + (d max − d min )(b i /2 + i−1 j=1 b j ) (2)</formula><p>Finally, at each pixel, the final depth valued is calculated from the linear combination of Softmax scores at that pixel and the depth-bin-centers c(b) as follows:</p><formula xml:id="formula_3">d = N k=1 c(b k )p k<label>(3)</label></formula><p>Compared to Fu et al. <ref type="bibr" target="#b10">[11]</ref> we do not predict the depth as the bin center of the most likely bin. This enables us to predict smooth depth maps without the discretization artifacts as can bee seen in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>Pixel-wise depth loss. Inspired by <ref type="bibr" target="#b25">[26]</ref>, we use a scaled version of the Scale-Invariant loss (SI) introduced by Eigen et al. <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_4">L pixel = α 1 T i g 2 i − λ T 2 ( i g i ) 2<label>(4)</label></formula><p>where g i = logd i − log d i and the ground truth depth d i and T denotes the number of pixels having valid ground truth values. We use λ = 0.85 and α = 10 for all our experiments.</p><p>Bin-center density loss. This loss term encourages the distribution of bin centers to follow the distribution of depth values in the ground truth. We would like to encourage the bin centers to be close to the actual ground truth depth values and the other way around. We denote the set of bin centers as c(b) and the set of all depth values in the ground truth image as X and use the bi-directional Chamfer Loss <ref type="bibr" target="#b8">[9]</ref> as a regularizer:</p><p>L bins = chamf er(X, c(b)) + chamf er(c(b), X) (5) Finally, we define the total loss as:</p><formula xml:id="formula_5">L total = L pixel + βL bins<label>(6)</label></formula><p>We set β = 0.1 for all our experiments. We experimented with different loss functions including the RMSE loss, and the combined SSIM <ref type="bibr" target="#b41">[42]</ref> plus L 1 loss suggested by <ref type="bibr" target="#b0">[1]</ref>. However, we were able to achieve the best results with our proposed loss. We offer a comparison of the different loss functions and their performance in our ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted an extensive set of experiments on the standard depth estimation from a single image datasets for both indoor and outdoor scenes. In the following, we first briefly describe the datasets and the evaluation metrics, and then present quantitative comparisons to the state-of-the-art in supervised monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head><p>NYU Depth v2 is a dataset that provides images and depth maps for different indoor scenes captured at a pixel resolution of 640 × 480 <ref type="bibr" target="#b36">[37]</ref>. The dataset contains 120K training samples and 654 testing samples <ref type="bibr" target="#b7">[8]</ref>. We train our network on a 50K subset. The depth maps have an upper bound of 10 meters. Our network outputs depth prediction having a resolution of 320 × 240 which we then upsample by 2× to match the ground truth resolution during both training and testing. We evaluate on the pre-defined center cropping by Eigen et al. <ref type="bibr" target="#b7">[8]</ref>. At test time, we compute the final output by taking the average of an image's prediction and the prediction of its mirror image which is commonly used in previous work.</p><p>KITTI is a dataset that provides stereo images and corresponding 3D laser scans of outdoor scenes captured using equipment mounted on a moving vehicle <ref type="bibr" target="#b13">[14]</ref>. The RGB images have a resolution of around 1241 × 376 while the corresponding depth maps are of very low density with lots of missing data. We train our network on a subset of around 26K images, from the left view, corresponding to scenes not included in the 697 test set specified by <ref type="bibr" target="#b7">[8]</ref>. The depth maps have an upper bound of 80 meters. We train our network on a random crop of size 704 × 352. For evaluation, we use the crop as defined by Garg et al. <ref type="bibr" target="#b12">[13]</ref> and bilinearly upsample the prediction to match the ground truth resolution. The final output is computed by taking the average of an image's prediction and the prediction of its mirror image.</p><p>SUN RGB-D is an indoor dataset consisting of around 10K images with high scene diversity collected with four different sensors <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23]</ref>. We use this dataset only for Method    cross-evaluating pre-trained models on the official test set of 5050 images. We do not use it for training.</p><formula xml:id="formula_6">δ 1 ↑ δ 2 ↑ δ 3 ↑ REL ↓ RMS ↓<label>log</label></formula><p>Evaluation metrics. We use the standard six metrics used in prior work <ref type="bibr" target="#b7">[8]</ref> to compare our method against stateof-the-art. These error metrics are defined as: average relative error (REL): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We implement the proposed network in PyTorch <ref type="bibr" target="#b32">[33]</ref>. For training, we use the AdamW optimizer <ref type="bibr" target="#b29">[30]</ref> with weight-decay 10 −2 . We use the 1-cycle policy <ref type="bibr" target="#b37">[38]</ref> for the learning rate with max lr = 3.5 × 10 −4 , linear warm-up </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the state-of-the-art</head><p>We consider the following two methods to be our main competitors: BTS <ref type="bibr" target="#b25">[26]</ref> and DAV <ref type="bibr" target="#b21">[22]</ref>. For completeness, we also include selected previous related methods in the comparison tables. For BTS and DAV we report the corresponding evaluation numbers from their papers. For BTS we also verified these numbers by retraining their network using the authors code. DAV did not have code available by the deadline, but the authors sent us the resulting depth images used in our figures. In our tables we report the numbers given by the authors in their paper 2 .</p><p>NYU-Depth-v2: See <ref type="table" target="#tab_3">Table 2</ref> for the comparison of the performance on the official NYU-Depth-v2 test set. While the state of the art performance on NYU has been saturated for quite some time, we were able to significantly outperform the state of the art in all metrics. The large gap to the previous state of the art emphasises that our proposed architecture addition makes an important contribution to improving the results.</p><p>KITTI: <ref type="table" target="#tab_4">Table 3</ref> lists the performance metrics on the KITTI dataset. Our proposed architecture significantly outperforms previous state-of-the-art across all metrics. In particular, our method improves the RMS score by about 13.5% and Squared Relative Difference by 22.4% over the previous state-of-the-art.</p><p>SUN RGB-D: To compare the generalisation performance, we perform a cross-dataset evaluation by training <ref type="bibr" target="#b1">2</ref> The authors of DAV clarified in an email that they compute the depth maps at 1/4th the resolution and then downsample the ground truth for evaluation. However, we believe that all other methods, including ours, evaluate at the full resolution.   our network on the NYU-Depth-v2 dataset and evaluate it on the test set of the SUN RGB-D dataset without any finetuning. For comparison, we also used the same strategy for competing methods for which pretrained models are available <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b3">4]</ref> and report results in <ref type="table">Table.</ref> 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>For our ablation study, we evaluate the influence of the following design choices on our results:</p><p>AdaBins: We first evaluate the importance of our Ad-aBins module. We remove the AdaBins block from the architecture and use the encoder-decoder to directly predict the depth map by setting C d = 1. We then use the loss given by Eq. 4 to train the network. We call this design standard regression and compare it against variants of our AdaBins module. <ref type="table" target="#tab_8">Table. 6</ref> shows that the architecture without AdaBins (Row 1) performs worse than all other variants (Rows 2-5).</p><p>Bin types: In this set of experiments we examine the performance of adaptive bins over other choices as stated in Sec. 3.2. <ref type="table">Table.</ref> 6 lists results for all the discussed variants. The Trained-but-Fixed variant performs worst among all choices and our final choice employing adaptive bins significantly improves the performance and outperforms all other variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of bins (N ):</head><p>To study the influence of the number of bins, we train our network for various values of N and measure the performance in terms of Absolute Relative Error metric. Results are plotted in <ref type="figure" target="#fig_4">Fig. 6</ref>. Interestingly, starting from N = 20, the error first increases with increasing N and then decreases significantly. As we keep increasing N above 256, and with higher values the gain in performance starts to diminish. We use N = 256 for our final model.</p><p>Loss function: <ref type="table">Table.</ref> 4 lists performance corresponding to the three choices of loss function. Firstly, the L 1 /SSIM combination does not lead to the state-of-the-art performance in our case. Secondly, we trained our network with and without the proposed Chamfer loss (Eq. 5). Introducing the Chamfer loss clearly gives a boost to the performance. For example, introducing the Chamfer loss reduces the Ab-solute Relative Error from 10.6% to 10.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a new architecture block, called AdaBins for depth estimation from a single RGB image. AdaBins leads to a decisive improvement in the state of the art for the two most popular datasets, NYU and KITTI. In future work, we would like to investigate if global processing of information at a high resolution can also improve performance on other tasks, such as segmentation, normal estimation, and 3D reconstruction from multiple images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>DAV <ref type="bibr" target="#b21">[22]</ref> BTS <ref type="bibr" target="#b25">[26]</ref> Ours </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of AdaBins: Top: input RGB images. Middle: depth predicted by our model. Bottom: histogram of depth values of the ground truth (blue) and histogram of the predicted adaptive depth-bin-centers (red) with depth values increasing from left to right. Note that the predicted bin-centers are focused near smaller depth values for closeup images but are widely distributed for images with a wider range of depth values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Choices for bin widths. Uniform and Loguniform bins are pre-determined. 'Trained bins' vary from one dataset to another. Adaptive bins vary for each input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>N depth-bin-centers c(b) := {c(b 1 ), c(b 2 ), ..., c(b N )} calculated from bin-widths vector RGB Fu et al. [11]Ours Demonstration of artifacts introduced by the discretization of the depth interval. Our hybrid regression results in smoother depth maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p</head><label></label><figDesc>p −ŷ p ) 2 ); average (log 10 ) error: 1 n n p |log 10 (y p ) − log 10 (ŷ p )|; threshold accuracy (δ i ): % of y p s.t. max( yp yp ,ŷ p yp ) = δ &lt; thr for thr = 1.25, 1.25 2 , 1.25 3 ; where y p is a pixel in depth image y, y p is a pixel in the predicted depth imageŷ, and n is the total number of pixels for each depth image. Additionally for KITTI, we use the two standard metrics: Squared Relative Difference (Sq. Rel): log y p − logŷ p 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Effect of number of bins (N) on performance as measured by Absolute Relative Error metric. we can observe interesting behaviour for lower values of N. As N increases, performance starts to saturate. from max lr/25 to max lr for the first 30% of iterations followed by cosine annealing to max lr/75.Total number of epochs is set to 25 with batch size 16. Training our model takes 20 min per epoch on a single node with four NVIDIA V100 32GB GPUs. For all results presented we train for 25 epochs. Our main model has about 78M parameters: 28M for the CNN encoder, 44M for the CNN decoder, and 5.8M for the new AdaBins module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Qualitative comparison with the state-of-the-art on the NYU-Depth-v2 dataset. Qualitative comparison with the state-of-the-art on the KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Visualization of surface normals extracted from predicted depth maps. Qualitative comparison of generalization from NYU-Depth-v2 to SUN RGB-D dataset. Darker pixels are farther. Missing ground truth values are shown in white.RGBBTS<ref type="bibr" target="#b25">[26]</ref> Ours Qualitative comparison on KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>with some modifications. The two basic modifications are switching the encoder from Mini-ViT architecture details.DenseNet<ref type="bibr" target="#b19">[20]</ref> to EfficientNet B5 and using a different appropriate loss function for the new architecture. In addition, the output of the decoder is a tensor x d ∈ R h×w×C d , not a single channel image representing the final depth values. We refer to this tensor as the "decoded features". The second component is a key contribution in this paper, the AdaBins module. The input to the AdaBins module are decoded features of size h×w ×C d and the output tensor is of size h × w × 1. Due to memory limitations of current GPU</figDesc><table><row><cell>Patch size (p)</cell><cell>E</cell><cell>Layers</cell><cell>num heads</cell><cell>C</cell><cell>MLP Size</cell><cell>Params</cell></row><row><cell>16</cell><cell cols="2">128 4</cell><cell>4</cell><cell cols="3">128 1024 5.8 M</cell></row></table><note>hardware, we use h = H/2 and w = W/2 to facilitate bet- ter learning with larger batch sizes. The final depth map is computed by simply bilinearly upsampling to H × W × 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>10 ↓</figDesc><table><row><cell>Eigen et al. [8]</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell><cell>0.158</cell><cell>0.641</cell><cell>-</cell></row><row><cell>Laina et al. [25]</cell><cell>0.811</cell><cell>0.953</cell><cell>0.988</cell><cell>0.127</cell><cell>0.573</cell><cell>0.055</cell></row><row><cell>Hao et al. [16]</cell><cell>0.841</cell><cell>0.966</cell><cell>0.991</cell><cell>0.127</cell><cell>0.555</cell><cell>0.053</cell></row><row><cell>Lee et al. [27]</cell><cell>0.837</cell><cell>0.971</cell><cell>0.994</cell><cell>0.131</cell><cell>0.538</cell><cell>-</cell></row><row><cell>Fu et al. [11]</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell><cell>0.115</cell><cell>0.509</cell><cell>0.051</cell></row><row><cell>SharpNet [34]</cell><cell>0.836</cell><cell>0.966</cell><cell>0.993</cell><cell>0.139</cell><cell>0.502</cell><cell>0.047</cell></row><row><cell>Hu et al. [19]</cell><cell>0.866</cell><cell>0.975</cell><cell>0.993</cell><cell>0.115</cell><cell>0.530</cell><cell>0.050</cell></row><row><cell>Chen et al. [4]</cell><cell>0.878</cell><cell>0.977</cell><cell>0.994</cell><cell>0.111</cell><cell>0.514</cell><cell>0.048</cell></row><row><cell>Yin et al. [47]</cell><cell>0.875</cell><cell>0.976</cell><cell>0.994</cell><cell>0.108</cell><cell>0.416</cell><cell>0.048</cell></row><row><cell>BTS [26]</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell><cell>0.110</cell><cell>0.392</cell><cell>0.047</cell></row><row><cell>DAV [22]</cell><cell>0.882</cell><cell>0.980</cell><cell>0.996</cell><cell>0.108</cell><cell>0.412</cell><cell>-</cell></row><row><cell>AdaBins (Ours)</cell><cell>0.903</cell><cell>0.984</cell><cell>0.997</cell><cell>0.103</cell><cell>0.364</cell><cell>0.044</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of performances on the NYU-Depth-v2 dataset. The reported numbers are from the corresponding original papers. Best results are in bold, second best are underlined.</figDesc><table><row><cell>Method</cell><cell>δ 1 ↑</cell><cell>δ 2 ↑</cell><cell>δ 3 ↑</cell><cell>REL ↓</cell><cell>Sq Rel ↓</cell><cell>RMS ↓</cell><cell>RMS log ↓</cell></row><row><cell>Saxena et al. [36]</cell><cell>0.601</cell><cell>0.820</cell><cell>0.926</cell><cell>0.280</cell><cell>3.012</cell><cell>8.734</cell><cell>0.361</cell></row><row><cell>Eigen et al. [8]</cell><cell>0.702</cell><cell>0.898</cell><cell>0.967</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell></row><row><cell>Liu et al. [29]</cell><cell>0.680</cell><cell>0.898</cell><cell>0.967</cell><cell>0.201</cell><cell>1.584</cell><cell>6.471</cell><cell>0.273</cell></row><row><cell>Godard et al. [15]</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell></row><row><cell>Kuznietsov et al. [24]</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell></row><row><cell>Gan et al. [12]</cell><cell>0.890</cell><cell>0.964</cell><cell>0.985</cell><cell>0.098</cell><cell>0.666</cell><cell>3.933</cell><cell>0.173</cell></row><row><cell>Fu et al. [11]</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell></row><row><cell>Yin et al. [47]</cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell></row><row><cell>BTS[26]</cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell><cell>0.059</cell><cell>0.245</cell><cell>2.756</cell><cell>0.096</cell></row><row><cell>AdaBins (Ours)</cell><cell>0.964</cell><cell>0.995</cell><cell>0.999</cell><cell>0.058</cell><cell>0.190</cell><cell>2.360</cell><cell>0.088</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of performances on the KITTI dataset. We compare our network against the state-of-the-art on this dataset. The reported numbers are from the corresponding original papers. Measurements are made for the depth range from 0m to 80m. Best results are in bold, second best are underlined.</figDesc><table><row><cell>Loss</cell><cell>δ 1 ↑</cell><cell>δ 2 ↑</cell><cell>δ 3 ↑</cell><cell cols="2">REL↓ RMS↓ log 10 ↓</cell></row><row><cell cols="5">L 1 /SSIM 0.888 0.980 0.995 0.107 0.384</cell><cell>0.046</cell></row><row><cell>SI</cell><cell cols="4">0.897 0.984 0.997 0.106 0.368</cell><cell>0.044</cell></row><row><cell>SI+Bins</cell><cell cols="4">0.903 0.984 0.997 0.103 0.364</cell><cell>0.044</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of performance with respect to the choice of loss function.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of models trained on the NYU-Depth-v2 dataset and tested on the SUN RGB-D dataset<ref type="bibr" target="#b38">[39]</ref> without fine-tuning.</figDesc><table><row><cell>Variant</cell><cell>δ 1 ↑</cell><cell>δ 2 ↑</cell><cell>δ 3 ↑</cell><cell cols="2">REL ↓ RMS ↓</cell></row><row><cell>Base + R</cell><cell cols="4">0.881 0.980 0.996 0.111</cell><cell>0.419</cell></row><row><cell cols="5">Base + Uniform-Fix-HR 0.892 0.981 0.995 0.107</cell><cell>0.383</cell></row><row><cell>Base + Log-Fix-HR</cell><cell cols="4">0.896 0.981 0.995 0.108</cell><cell>0.379</cell></row><row><cell>Base + Train-Fix-HR</cell><cell cols="4">0.893 0.981 0.995 0.109</cell><cell>0.381</cell></row><row><cell>Base + AdaBins-HR</cell><cell cols="4">0.903 0.984 0.997 0.103</cell><cell>0.364</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Comparison of different design choices for bin-</cell></row><row><cell>widths and regression. AdaBins module results in a signif-</cell></row><row><cell>icant boost in performance. Base: encoder-decoder with an</cell></row><row><cell>EfficientNet B5 encoder. R: standard regression. HR: Hy-</cell></row><row><cell>brid Regression. (Log)Uniform-Fix: Fixed (log) uniform</cell></row><row><cell>bin-widths. Train-Fix: Trained bin-widths but Fixed for</cell></row><row><cell>each dataset.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/shariqfarooq123/AdaBins</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix A. <ref type="bibr" target="#b0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Geometric Consistency</head><p>We provide a qualitative evaluation of the geometric consistency of depth maps predicted by our model. Surface normal maps provide a good way to visualize the orientation and texture details of surfaces present in the scene. <ref type="figure">Fig 9</ref> shows the visualization of the normals extracted from the depth maps for our model and for DAV <ref type="bibr" target="#b21">[22]</ref> and BTS <ref type="bibr" target="#b25">[26]</ref>. Although the orientations predicted by DAV seems to be consistent, the texture details are almost completely lost. BTS, on the other hand, preserves the texture but sometimes results in erroneous orientation details. Our method exhibits detailed texture and consistent orientations without explicitly imposing geometric constraints, such as co-planarity, used by other methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Generlization Analysis</head><p>Here we qualitatively analyze the capability of our method to generalise to unseen data. We use the models (AdaBins and BTS <ref type="bibr" target="#b25">[26]</ref>) trained on NYU-Depth-v2 <ref type="bibr" target="#b36">[37]</ref> but show predictions on SUN RGB-D <ref type="bibr" target="#b38">[39]</ref> dataset in <ref type="figure">Fig 10.</ref> Depth maps predicted by BTS have conspicuous artifacts whereas our method provides consistent results on the unseen data.  <ref type="bibr" target="#b25">[26]</ref> and our method on the KITTI dataset. For better visualization, we have removed the sky regions from the visualized depth maps using segmentation masks predicted by a pretrained segmentation model <ref type="bibr" target="#b2">[3]</ref>. We can observe that our method demonstrates superior performance particularly in predicting extents and edges of the on-road vehicles, sign-boards and thin poles. Additionally, BTS tends to blend the farther away objects with background whereas our method preserves the structure with clear separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Results on KITTI dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. MLP Head Details</head><p>We use a three-layer MLP on the first output embedding of the transformer in the mini-ViT module. The architecture details with parameters are given in <ref type="table">Table 7</ref>.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno>abs/1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structureaware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meshed-memory transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depthlab: Real-time 3d interaction with depth maps for mobile augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Lee</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Dzitsiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Prasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dourgarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Pascoal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Gladstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Moura E Silva Cruces</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantine Nicholas John</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Nematollah Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="232" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. J. Robotics Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detail preserving depth estimation from a single image using attention guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image captioning: Transforming objects into words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02760</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depthassisted real-time 3d object detection for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nohyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woontack</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICAT&apos;</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="126" to="132" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Noise2Noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active refocusing of images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Jennifer Dy and Andreas Krause</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05</title>
		<meeting>the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of residual networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<idno>abs/1708.07120</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5622" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeptam: Deep tracking and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="822" to="838" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
