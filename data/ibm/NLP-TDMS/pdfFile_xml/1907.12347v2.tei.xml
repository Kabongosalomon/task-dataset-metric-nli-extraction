<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yau</forename><forename type="middle">Pun</forename><surname>Chen</surname></persName>
							<email>ypchen@connect.ust.hk</email>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<email>yuwingtai@tencent.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<email>cktang@cs.ust.hk</email>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hkust</forename></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/HKUSTCV/FSS-1000</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, we have witnessed the success of deep learning in image recognition thanks to the availability of large-scale human-annotated datasets such as PAS-CAL VOC, ImageNet, and COCO. Although these datasets have covered a wide range of object categories, there are still a significant number of objects that are not included. Can we perform the same task without a lot of human annotations? In this paper, we are interested in few-shot object segmentation where the number of annotated training examples are limited to 5 only. To evaluate and validate the performance of our approach, we have built a few-shot segmentation dataset, FSS-1000, which consists of 1000 object classes with pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.</p><p>We build our baseline model using standard backbone networks such as VGG-16, ResNet-101, and Inception. To our surprise, we found that training our model from scratch using FSS-1000 achieves comparable and even better results than training with weights pre-trained by ImageNet which is more than 100 times larger than FSS-1000. Both our approach and dataset are simple, effective, and easily extensible to learn segmentation of new object classes given very few annotated training examples. Dataset is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although unprecedented in the number of object categories when first released, contemporary image datasets for training deep neural networks such as PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> (19,740 images, 20 classes), ILSVRC <ref type="bibr" target="#b28">[29]</ref> (1,281,167 images, 1,000 classes), and COCO <ref type="bibr" target="#b20">[21]</ref> (204,721 images, 80 classes) are actually quite limited for visual recognition tasks in the real world: a rough estimate of the number of different objects on the Earth falls in the range of 500,000</p><p>This research is supported in part by Tencent and the Research Grant Council of the Hong Kong SAR under grant no. 1620818. to 700,000, following the total number of nouns in the English language. While the exact total number of visual object categories is smaller than these numbers, these largescale datasets contribute less than 1% in total. Extending a new object category to existing datasets is a major undertaking because a lot of human annotation effort is required: in ImageNet, the mean number of images in a given class is 650. More importantly, observe that the number of images within each object category in ImageNet for instance can vary significantly, ranging from 1 to 3,047. This inevitably introduces undesirable biases which may have a detrimental effect on important tasks solely relying on pre-trained weights obtained using a dataset that is biased in both the choice of object classes (small number) and images within a given class (uneven distribution). Biases in existing datasets have also been recently reported <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Thus, Few-Shot Learning has emerged as an attractive alternative for important computer vision tasks, especially when the given new dataset is very small and dissimilar so relying on the aforementioned pre-trained weights may not work well. Particularly relevant is image segmentation which requires extremely labor-intensive, pixelwise labeling for supervised learning. In few-shot segmentation, given an input consisting of a small support image set with labels (5 in this paper) and a query image set without labels, the learned model should properly segment the query images, even the pertinent objects belong to an object class unseen before.</p><p>There is no large-scale object dataset for few-shot segmentation. Previous research on few-shot segmentation relies on a manual split of the PASCAL VOC dataset to train and evaluate a new model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>, but only 20 and 80 classes in the PASCAL VOC and COCO datasets respectively contain pixelwise segmentation information. Thus, building a large-scale object segmentation dataset is necessary to extensively and objectively evaluate the performance of our and future few-shot models.</p><p>FSS-1000 is the first large-scale dataset for few-shot segmentation with built-in object category hierarchy which emphasizes the number of object classes rather than the number of images. FSS-1000 is highly scalable: 10 new images with ground-truth segmentation are all it takes for new object class extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images</head><p>Classes Classification Detection Segmentation Mean Stddev SUN <ref type="bibr" target="#b36">[37]</ref> 131,067 3,819 <ref type="bibr" target="#b38">39</ref>  <ref type="figure">Figure 1</ref>. Normalized image distribution. To make these datasets comparable, we normalize each dataset respectively in the total number of images (y-axis) and in the total number of object supercategories (x-axis) such that the area under each curve is 1 to make them comparable. All existing datasets are biased toward a number of object categories except FSS-1000 (red).</p><p>Our baseline network architecture is constructed by appending a decoder module to the relation network <ref type="bibr" target="#b32">[33]</ref>, which is a simple and elegant deep model effective and originally designed for few-shot image classification only. Reshaping the relation network into a fully-convolutional U-Net architecture <ref type="bibr" target="#b23">[24]</ref>, our extensive experimental results show that this baseline model trained from scratch on FSS-1000, which is less than 1% of the size of contemporary large-scale datasets, outperforms the model fine-tuned from weights pre-trained on ImageNet/COCO dataset. In addition, without any fine-tuning / re-training, our trained baseline network can be applied to any unseen classes directly with decent performance. With its excellent segmentation performance as well as extensibility, FSS-1000 and our baseline model are expected to make a lasting contribution to few-shot image segmentation. Please also refer to the supplemental materials for our extensive experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first review the relationship and difference between FSS-1000 and modern datasets aiming to solve image segmentation and few-shot classification. Then we review contemporary research on few-shot learning and semantic segmentation and discuss how we relate the few-shot segmentation to previous research.</p><p>Large-Scale Datasets When deep learning had started to become a dominating tool for computer vision, the importance of building large-scale datasets was emphasized for training deep networks. The PASCAL VOC <ref type="bibr" target="#b4">[5]</ref> was the first to provide a challenging image dataset for object class recognition and semantic segmentation. The latest version VOC2012 contains 20 object classes and 9,993 images with segmentation annotations. Despite the absence of segmen-tation labels, the Imagenet <ref type="bibr" target="#b3">[4]</ref> is built upon the backbone of WordNet and provides image-level labels for 5,247 classes for training, out of which a subset of 1,000 categories are split out to form the ILSVRC <ref type="bibr" target="#b28">[29]</ref> dataset. This challenge has made a significant impact on the rapid progress in visual recognition task and computer vision in recent years. The latest Open Image dataset <ref type="bibr" target="#b16">[17]</ref> contains 7,186 trainable distinct object classes for classification and 600 classes for detection, making it the largest existing dataset with object classes and location annotations. Following the PASCAL VOC and ImageNet, the COCO segmentation dataset <ref type="bibr" target="#b20">[21]</ref> includes more than 200,000 images with instance-wise semantic segmentation labels. There are 80 object classes and over 1.5 million object instances in COCO dataset.</p><p>In this paper, we instead focus on broadening the number of object classes in a segmentation dataset rather than increasing dataset size. Our FSS-1000 consists of 1,000 object classes, wherein each class we label 10 images with binary segmentation annotation. So in total, our dataset contains 10,000 images with pixelwise segmentation labels. We are particularly interested in segmentation due to its obvious benefits: segmentation captures the essential feature of an object without background; instance level segmentation can be ready from segmentation. The structure of our dataset is similar to widely-used datasets for few-shot visual recognition. For example, the Omniglot dataset <ref type="bibr" target="#b17">[18]</ref> consists of 1,623 different handwritten characters of 50 different alphabets, which is equivalent to 1,623 object classes with 50 images in each class. The MiniImageNet, first proposed in <ref type="bibr" target="#b34">[35]</ref>, consists of 60,000 images with 100 classes each having 600 examples. But none of these few-shot learning datasets incorporate dense pixelwise segmentation labels, which is essential in training a deep network model for semantic segmentation.</p><p>Few-Shot Learning Recent research in few-shot classification can be classified into 1) learn a good initial condition for the network to be fine-tuned on extremely small training set, as proposed in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>; 2) rely on memory properties of RNN, introduced in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>; 3) learn a metric between few-shot samples and queries, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. We choose to extend the relation network <ref type="bibr" target="#b32">[33]</ref> for few-shot segmentation because it is a simple, general and working framework. By concatenating the CNN feature maps between support images and query images, the relation module can consider the hidden relationship between these two sets of images guided by the loss function. In the original relation network, it uses the MSE loss to compare the fi-  nal probability vector to the ground truth. In this paper, we simply modify the loss to calculate pixelwise differences between the segmentation ground truth and heatmap. In OSLSM <ref type="bibr" target="#b30">[31]</ref>, the authors proposed a two-branch network to solve few-shot segmentation. The network is quite complex, and their training set was limited to the PASCAL VOC dataset with only 20 object classes. Consequently, their feature extractor may suffer severe bias making it hard to be generalized to other objects. The guided network <ref type="bibr" target="#b25">[26]</ref> can also suffer the same limitation on their dataset choice. Though point annotation can be used to guide the training of few-shot segmentation, the sparse annotation can seriously hamper accuracy.</p><p>Semantic Image Segmentation Previous research exploiting CNN to make dense prediction often relied on patchwise training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> and pre-and post-processing of superpixels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. In <ref type="bibr" target="#b21">[22]</ref> the authors first proposed a simple and elegant fully convolutional network (FCN) to solve semantic segmentation. Notably, this is the first work which was trained end-to-end on a fully convolutional network for dense pixel prediction, which showed that the last layer feature maps from a good backbone network such as VGG-16 contain sufficient foreground features which can be decoded by the upsampling network to produce segmentation results. Intuitively, that is also the guiding principle behind our modification on relation network architecture. Though modern network architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> achieve high accuracy in the COCO challenge by adding complex network modules and branches, these models cannot be adapted easily to segment new classes with few training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FSS-1000</head><p>Recent few-shot datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref> support few-shot classification but there is no large-scale few-shot segmentation dataset. In this section, we first introduce the details of data collection and annotation, then discuss the properties of FSS-1000. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 1</ref> compare FSS-1000 with existing popular datasets. FSS-1000 targets at solving general objects few-shot segmentation problem. So datasets only focusing on sub-domain object categories in the world (e.g. handwritten characters, human faces and road scenes) are not included in the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>Object Classes We first referred to the classes in ILSVRC <ref type="bibr" target="#b28">[29]</ref> in our choice of object categories for FSS-1000. Consequently, FSS-1000 has 584 classes out of its 1,000 classes overlap with the classes in the ILSVRC dataset. We find ILSVRC dataset heavily biases toward animals, both in terms of the distribution of categories and number of images. Therefore, we fill in the other 486 by new classes unseen in any existing datasets. Specifically, we include more daily objects so that network models trained on FSS-1000 can learn from diverse artificial and manmade objects/features in addition to natural and organic objects/features where the latter was emphasized by existing large-scale datasets. Our diverse 1,000 object classes are further arranged in a hierarchy to be detailed in section 3.2.</p><p>Raw Images To avoid bias, the raw images were retrieved by querying object keywords on three different Internet search engines, namely, Google, Bing and Yahoo. We downloaded the first 100 results returned (or less if less than 100 images were returned) from a given search engine. No special criteria or assumption was used to select the candidates, however, due to the bias of Internet search engines, a large number of the images returned contain a single object photographed with sharp focus. In the final step, we intentionally included some images with a relatively small object, multiple objects or other objects in the background to balance the easy and hard examples of the dataset.</p><p>Images with aspect ratio larger than 2 or smaller than 0.5 were excluded. Since all images and their segmentation maps were to be resized to 224×224, bad aspect ratio would destroy important geometric properties after the resize operation. For the same reason, images with height or width less than 224 pixels were discarded because they would trigger upsampling which would affect the image quality after resizing. Pixelwise Segmentation Annotation We used Photoshop's "quick selection" tool which allows users to loosely select an object automatically, and refined or corrected the selected area to produce the desired segmentation. <ref type="figure" target="#fig_0">Figure 2</ref> shows example images overlaid with their corresponding segmentation maps in FSS-1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties</head><p>This section summarizes the three desirable properties of FSS-1000:   Scalability To extend FSS-1000 to include a new class, all it takes are 10 images with pixelwise binary segmentation labels for the new class. This is significantly easier than other datasets such as PASCAL VOC and COCO. First, the mean number of images in a given class is much larger than 10 in these datasets. Second, in these large-scale datasets the object classes need to be first pre-defined. Thus we believe binary annotation is a better annotation strategy in few-shot learning datasets, since it allows easy expansion of new object classes without concerning old object classes that have already been annotated.</p><p>Hierarchy <ref type="figure" target="#fig_1">Figure 3</ref> shows examples of one sub-category for each given super-category in the dataset to illustrate the hierarchical structure of FSS-1000. The object classes are arranged hierarchically following a 3-level structure, while not every bottom-level subclass has a middle-level superclass. The top of the object hierarchy consists of 12 supercategories while the bottom contains the 1,000 classes as the leaf nodes. Note that this is strictly not a tree structure because a given class may belong to more than one superclass (e.g., an apple is both "fruit" and "food").</p><p>Instance FSS-1000 dataset supports instance-level segmentation with instance segmentation labels in 758 out of the 1,000 classes in the dataset, which are significantly more classes than PASCAL VOC and MS COCO. One major difference between our dataset and PASCAL VOC / MS COCO instance level segmentation is that our dataset only annotates one type of objects in one image, despite there may be other object categories appearing in the background. We annotate at most 10 instances in a single image, which follows the same instance annotation principle adopted by COCO. <ref type="figure" target="#fig_2">Figure 4</ref> shows examples of instance annotations in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Formulation</head><p>In few-shot learning, the train-test split is on object categories, thus, all testing categories are unseen during training. In both training and testing, the input is divided into two sets, namely, the support set and the query set. The support set consists of samples with annotation, while the query set contains samples without annotation. In few-shot classification, the support set usually includes C classes and K training examples. This setting is defined as C-way-K-shot classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>. In few-shot segmentation, we adopt this notation but extend the query output to be per-pixel classification of the query image, rather than a single class label. Specifically, in few-shot segmentation, the input-output pair is given by (X, Y ), where L = l (i,j) ; l ∈ {1, 2, ..., C}</p><formula xml:id="formula_0">X = {(I s , L s , I q ); s ∈ {1, 2, ..., K}} Y = y (i,j) |I q ; y ∈ {1, 2, ..., C} l (i,j)</formula><p>is the ground-truth class label and y (i,j) represents the predicted class label for pixel (i, j) in a given image. I s is the 3-channel RGB support image. For each support input X with image and label pair (I s , L s ), the model predicts a pixelwise classification map over query image I q . Following the annotation strategy of FSS-1000, we set C = 2 and only focus on few-shot binary segmentation problem in this paper. However, a general C-way-K-shot segmentation could be solved by a union of C binary segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Pipeline Our network consists of three sub-modules: an encoder module E θ , a relation module R φ and a decoder module D ω . For a given input X to the network, the encoder E θ encodes the support and query images respectively into feature maps E θ (I s ) and E θ (I q ). For K-shot forwarding, we perform element-wise averaging over the depth channels of support feature maps, so that the encoder module always produces support feature maps of the same depth regardless of the size of the support set. The support and query feature maps are then combined in the relation module R φ . We choose channel-wise concatenation as the combination operation, while other choices such as parameter regression and nearest neighbors are possible and discussed in <ref type="bibr" target="#b25">[26]</ref>. The relation module generates coarse segmentation results in low-resolution based on the concatenated feature maps. Finally, the coarse result is fed into the decoder module to restore the prediction map to the same resolution of the input. <ref type="figure" target="#fig_3">Figure 5</ref> shows the entire workflow. In summary, the output is defined by</p><formula xml:id="formula_1">Y = D ω (R φ ( K s=1 E θ (I s ), E θ (I q ))).</formula><p>Loss function We use the cross entropy loss between the query prediction output and the ground-truth annotation to train our model. Specifically, under our binary few-shot segmentation setting, binary cross entropy (BCE) loss is adopted to optimize the parameters in the network:</p><formula xml:id="formula_2">θ * , φ * , ω * = argmin θ,φ,ω i j − L (i,j) log y (i,j) + (1 − L (i,j) ) log(1 − y (i,j) )</formula><p>Mean square error (MSE) is also a widely used objective function for semantic segmentation task. Different from BCE loss, MSE models the problem as regression to the target output. Our experiments show that BCE and MSE loss achieve similar performance under our network setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Module Details</head><p>One can design his/her own or choose any popular feature extraction backbone such as VGG-16 <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref> and Inception <ref type="bibr" target="#b33">[34]</ref> as the encoder module inside the network. The support and query features compose the combined feature map whose depth is twice the channel number of the last-layer output of the encoder. The relation module utilizes two 1 × 1 convolutional layers on the combined feature map to embed the relationship between the support features and query features. The decoder module is designed according to the number of downscale operations in the encoder module, which applies equivalent upsample blocks to restore the resolution back to the original input. In each upsample block stands a nearest neighbor upsampling layer and a convolutional layer. Skip connection is adopted between encoder and decoder feature maps, following the scheme proposed by U-Net <ref type="bibr" target="#b23">[24]</ref>. We find it helpful to produce fine details in segmentation when information in the encoder feature maps are fused to the decoder module by channel-wise concatenation. ReLU activation is applied throughout the deep network except for the last layer's activation where Sigmoid is used in order to scale the output to a suitable range to calculate cross-entropy loss. More detail parameters of our architecture are provided in the supplemental materials.  <ref type="table">Table 2</ref>. Different network settings to explore the best settings for our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MeanIoU OSLSM-1shot <ref type="bibr" target="#b30">[31]</ref> 70.29% OSLSM-5shot 73.02% Guided Network-1shot <ref type="bibr" target="#b25">[26]</ref> 71.94% Guided Network-5shot 74.27% Ours-1shot 73.47% Ours-5shot 80.12%  <ref type="table">Table 4</ref>. Comparison of different models on PASCAL-5 i . GN is Guided Network and Ours* is our model trained on FSS-1000. All models are using 5-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct experiments to evaluate the practicability of FSS-1000 and the performance of our method under fewshot learning settings. We evaluate models with the same network architecture but trained on different datasets to show that FSS-1000 is effective for few-shot segmentation task. Different support sets and their influence on query results will be discussed. Finally we illustrate that models trained on FSS-1000 are capable to generalize the few-shot segmentation knowledge to new unseen classes. The metric we use is the intersection-over-union (IoU) of positive labels in a binary segmentation map. IoU is a standard metric and widely adopted in evaluating image segmentation methods. All the networks are implemented in PyTorch. We use Adam solver <ref type="bibr" target="#b14">[15]</ref> to optimize the parameters. The learning rate is initially set to 10 −3 (10 −4 for fine-tuning) and halved for every 50, 000 episodes. We train all the networks for 500, 000 episodes. Network setting To explore the best settings for our network, we train different models using a combination of different backbones and loss functions on FSS-1000. <ref type="table">Table 2</ref> tabulates the respective performance on VGG-16, ResNet-101 and InceptionNet as backbone, and BCE and MSE as loss function. Based on the result, we choose VGG-16 as feature extractor and use BCE loss in our model throughout the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">FSS-1000</head><p>We train OSLSM and Guided Network on FSS-1000 to provide benchmarks and justify our dataset.   our adapted relation network achieves the best results on FSS-1000. Moreover, ours is the only model whose 5-shot training boosts the accuracy by over 10% compared to the 1-shot case. We believe that embedding multiple support images at the input end of the network and encouraging the feature extractor to consider correlation between multiple support images and the query image is the appropriate way to design k-shot (k &gt; 1) segmentation network, rather than simply combining 1-shot prediction <ref type="bibr" target="#b30">[31]</ref> or merging highlevel features of multiple supports <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">PASCAL-5 i</head><p>To compare with previous few-shot methods, we train and test our network on PASCAL-5 i <ref type="bibr" target="#b30">[31]</ref>. <ref type="table">Table 4</ref> shows that our simple baseline model (Ours) marginally outperforms OSLSM and Guided Network. More importantly, our model trained only on FSS-1000 without fine-tuning on PASCAL-5 i (Ours*) achieves much better results compared to models trained on PASCAL-5 i (Ours), exceeding the state-of-the-art performance of the very recent <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Pre-training</head><p>We compare our network model trained on different datasets to demonstrate the effectiveness of FSS-1000 in few-shot segmentation. Since there are no publicly available few-shot image segmentation datasets, we convert PASCAL VOC 2012 and COCO datasets by setting the desired foreground class label as positive and all others as negative, followed by the identical clean-up stage described in section 3.1 to the binarized labels. Two new datasets  test classes. The generation of these datasets are in line with the settings in <ref type="bibr" target="#b38">[39]</ref>. For FSS-1000, we build the validation/test set by randomly sampling 20 distinct sub-categories from the 12 super-categories; the other images and labels are used in training. The train/validation/test split used in the experiments consists of 5,200/2,400/2,400 image and label pairs. Each test set of fsPASCAL, fsCOCO and FSS are designed to be disjoint with all the training sets in terms of classes for fair comparison. <ref type="table" target="#tab_6">Table 5</ref> tabulates the performance of different models. For each model (row), the marks in sequence indicate the dataset(s) used in pre-training stages with the last mark indicating the dataset used in fine-tuning. Model III has only one indicating that it is exclusively trained on the dataset.</p><p>Using the pre-trained weights from ImageNet, Model II trained on FSS-1000 outperforms the fsCOCO-trained model I on both test sets by a large margin of 8% and 5.8%, which is due to the FSS training set containing the COCO <ref type="figure">Figure 9</ref>. Effect of different support sets. The leftmost support of each row is used to generate 1-shot results. For each class, we show the result of a good support set followed by a bad support set in the next row.  <ref type="table">Table 6</ref>. 500 test images are randomly sampled from FSS-1000 to compare time and accuracy performance of labeling segmentation data between humans and few-shot model.</p><p>training classes, but with more variety. Notably, without using any pre-trained weights Model III achieves slightly better results compared to Model II, which substantiate our claim that bias in feature extractor does exist in models pretrained and/or trained on a dataset unevenly distributed in object categories and images within each class. Interestingly, Model IV pre-trained on FSS-1000 and fine-tuned on fsCOCO achieves the best result on both test sets, outperforming Model III exclusively trained on FSS and the model I pre-trained on ILSRVC fine-tuned on fs-COCO. We believe the former is due to the addition of more data, and the latter is due to the difference in requirement of feature maps ideal for classification and segmentation task. Intuitively, semantic segmentation requires more accurate low-level features to produce fine details in segmentation map, while classification focuses on high-level features for image understanding. Therefore, we argue that pre-training with FSS-1000 serves as a good alternative for ImageNet pre-training in few-shot semantic segmentation.</p><p>Overall, models trained on fsCOCO produce quite good results in test classes that are similar to COCO training classes. For these classes, sometimes their segmentation results are better in local details compared to the results produced by models trained on FSS-1000 due to more variations in the training set. However, it failed in classes significantly different from the 60 COCO training classes. The somewhat limited variation in object categories in existing datasets makes it hard for models trained on them to generalize to more unseen classes under the few-shot setting.</p><p>On the other hand, models trained on FSS-1000 classes can handle these cases. Quantitative results and qualitative results are shown in <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> respectively. Results on fsPASCAL and further comparisons are provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of Support Set</head><p>We train four different models, using 1, 3, 5 and 7 support images respectively, to study how different number of support images influence the accuracy of few-shot segmentation. Two important observations can be summarized from <ref type="figure" target="#fig_6">Figure 8</ref>.</p><p>First, more support images generally boost the segmentation accuracy because more variations of color, pose, and scale of the object are included. However, the performance increase becomes negligible when more than 5 support images are given. Due to this bottleneck effect, we set up most of the experiments under the 5-shot setting.</p><p>Second, the accuracy boost is different among different classes. For easy cases (e.g. rigid objects), the improvement is not obvious because a single support image is enough for the deep network to capture and distinguish strong features of the object. For hard cases (e.g. deformable objects), more support images are essential for the network to learn the complex shapes to make correct segmentation. <ref type="figure">Figure 9</ref> demonstrates the effect of support set, which shows that scale and pose of the object to be segmented are the most important characteristics to guide few-shot semantic segmentation on FSS-1000. Since FSS-1000 does not explicitly consider scale variations (future work), a tiny or oversized object in the support set is not a good reference for segmentation. Significant differences in scales can mislead the network to capture wrong feature contents in the query. Besides, significantly different poses in support and query sets can result in bad segmentation results, due to the intrinsic fragility to rotation in CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Auto-Labeling on Novel and Unseen Classes</head><p>Traditionally a large number of human-annotated images are required to train a deep network for segmenting a new class. <ref type="table">Table 6</ref> tabulates the tradeoff in time and accuracy for annotating 500 test images in FSS-1000 by humans (using Photoshop and GrabCut <ref type="bibr" target="#b27">[28]</ref> algorithm) and our few-shot segmentation.</p><p>With its good accuracy and time tradeoff, despite the current limitations in scale invariance aforementioned, FSS-1000 allows us to automatically segment a novel object category by just providing a few support examples without retraining or fine-tuning a given model. We pick a number of very novel classes unseen by FSS-1000, and label 5 images of each class serving as the support set. <ref type="figure" target="#fig_8">Figure 10</ref> shows the test results which demonstrates that our model trained on FSS-1000 is capable of generalizing to these unseen classes. More extensive results on novel classes are included in supplementary materials.  For example, android robot is an unreal object unseen in FSS-1000. In cartography from satellite images which often come in overlapping image tiles, cartographers need to label only 5 images or tiles and our system can automatically segment the rest, such as recognizing river in our example where saliency detection does not work in general. The cell example shows the good potential of FSS-1000 in instance segmentation which significantly contributes to cell counting in medical image analysis where, for instance, a patient's health directly correlates to his or her red blood cell count. With the advance of whole slide images (WSI) in which the width and height often exceed 100,000 pixels (and thus many cells to count), using our few-shot segmentation trained on FSS-1000, pathologists only need to label 5 image relevant regions and then the rest of the WSI will be automatically labeled. Although manual corrections for missed or wrong cells may still be necessary given the current accuracy, comparing with exhaustive labeling which requires hours or even days to complete, the potential contribution of FSS-1000 is substantial. Similarly, the related Support 1: IoU 72.87% Support 2: IoU 78.17% <ref type="figure">Figure 11</ref>. Iterative few-shot segmentation. Left and right show respectively the support sets and results before and after including corrected failure cases in the support set. Complete testing set of Eiffel Tower is available in the supplemental material.</p><p>animal examples of sheep, penguin and wild goose show FSS-1000's potential for large-scale instance segmentation. Finally, our baseline backbone network is not very robust to scale variance, occlusion and background noises (future work). In sunflower, the segmentation results for instances too big or too small (especially for images with depth of field where faraway sunflowers are out of focus) become incomplete or even totally omitted. Despite that, FSS-1000 still reports limited success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Iterative Few-Shot Segmentation</head><p>Our few-shot segmentation successively benefits from support sets improved easily by including failure cases after correction in each pass. Consider the Eiffel Tower unseen by FSS-1000 in <ref type="figure">Figure 11</ref> where we manually label 200 images for quantitative evaluation (IoU). The first support set (left) did not have sufficient view and scale variations and did not see clearly the bottom part of the tower which resulted in its incomplete segmentation in some test cases. After mining a few of such hard cases, correcting and including them in the second support set (right), the previous hard cases could now be correctly segmented. We believe that few-shot segmentation performed in stages can offer an immediate performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Few-shot learning/segmentation is an emerging attractive alternative where only a few training examples are required. However, there is no existing large-scale dataset for few-shot segmentation. In this paper, we address the limitation of existing large-scale datasets in their biases and lack of scalability, and build the first few-shot segmentation dataset FSS-1000 emphasizing class diversity rather than dataset size. We adapt the relation network architecture to few-shot segmentation. This baseline few-shot segmentation model, trained exclusively on FSS-1000 without using pre-trained weights, achieves higher accuracy than previous methods including on test sets unseen by FSS-1000. We further demonstrated the efficacy and potential of FSS-1000 in large-scale segmentation on totally unseen classes without re-training or fine-tuning, and showed its promise on fewshot instance segmentation and iterative few-shot recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Example images and their corresponding segmentation in FSS-1000. For the 12 super-categories here, 5 examples are shown, where the ground-truth segmentation map is overlaid in red in the corresponding image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Hierarchy of FSS-1000. Arrow represents "is a subclass of" relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Example of instance annotation in the FSS-1000 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Our baseline network architecture using VGG-16 as backbone. The relation module is adapted from<ref type="bibr" target="#b32">[33]</ref> where a decoder module is appended to produce the segmentation map. Both support and query features are concatenated to the decoder module via skip connection. More details of this standard architecture are available in supplemental materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>MeanIoU of superclasses in FSS-1000 tested with models trained on fsPASCAL, fsCOCO and FSS-1000. Bars at the bottom indicate the percentage of the number of categories overlapping with FSS-1000 in the corresponding dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Image results of our baseline model respectively trained on fsPASCAL, fsCOCO and FSS-1000. Support labels and predicted segmentation are overlaid in red in corresponding support images and query images. Ground truth labels for query images are in green. The classes in the first two rows are present in fs-PASCAL and fsCOCO whereas the rest are unique in FSS-1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>MeanIoU of superclasses in FSS-1000 tested with k-shot models (k = 1,3,5,7). are thus produced: fsPASCAL and fsCOCO. There are respectively 4,318 image and label pairs in 20 object classes in fsPASCAL, which consists of 15 training classes and 5 test classes, and 48,015 image and label pairs in 80 object classes in fsCOCO, containing 60 training classes and 20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Test results for unseen classes. From top to bottom: android robot; the river from UC Merced Land Use Dataset<ref type="bibr" target="#b37">[38]</ref>; a large cell image cropped into patches; herds of sheep; penguin from Oxford penguin counting dataset<ref type="bibr" target="#b0">[1]</ref>; flock of wild goose; different images of fields of sunflower depict various scales in the presence of occlusion and perspective distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Large-scale datasets comparison. Mean and standard deviation are based on the expected number of images in each class.</figDesc><table><row><cell>.22</cell><cell>717.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Air animal Electronic device Daily object Fruit &amp; Plant Food Land animal</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Outdoor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Music</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>object</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&amp; Sport</cell></row><row><cell>chickadee</cell><cell>dragonfly</cell><cell>eagle</cell><cell>sulphur_butterfly</cell><cell>woodpecker</cell><cell>gampad</cell><cell>hair_drier</cell><cell>modem</cell><cell>ipod</cell><cell>psp</cell><cell>yurt</cell><cell>torii</cell><cell>igloo</cell><cell>triumphal_arch</cell><cell>totem_pole</cell><cell>tennis_racket</cell><cell>dumbbell</cell><cell>violin</cell><cell>punching_bag</cell><cell>cornet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transport ation</cell></row><row><cell>comb</cell><cell>teapot</cell><cell>toilet</cell><cell>pen</cell><cell>pillow</cell><cell>avocado</cell><cell>groud</cell><cell>pumpkin</cell><cell>tulip</cell><cell>celery</cell><cell>lock</cell><cell>paintbrush</cell><cell>sponge</cell><cell>stapler</cell><cell>syringe</cell><cell>airliner</cell><cell>muscle_car</cell><cell>pickup</cell><cell>speedboat</cell><cell>trolleybus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Misc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Water animal</cell></row><row><cell>cheese</cell><cell>pizza</cell><cell>waffle</cell><cell>mooncake</cell><cell>ice_lolly</cell><cell>newt</cell><cell>impala</cell><cell>egyptian_cat</cell><cell>african_crocodile</cell><cell>indian_elephant</cell><cell>cradle</cell><cell>mcdonald_logo</cell><cell>paper_crane</cell><cell>pokemon_ball</cell><cell>super_mario</cell><cell>starfish</cell><cell>garfish</cell><cell>lobster</cell><cell>sturgeon</cell><cell>mud_turtle</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Different few-shot segmentation networks trained and tested on FSS-1000.</figDesc><table><row><cell>Method</cell><cell cols="5">PASCAL-5 0 PASCAL-5 1 PASCAL-5 2 PASCAL-5 3 Mean</cell></row><row><cell>OSLSM [31]</cell><cell>34.2%</cell><cell>57.9%</cell><cell>43.2%</cell><cell>37.8%</cell><cell>43.3%</cell></row><row><cell>GN [26]</cell><cell>33.1%</cell><cell>58.9%</cell><cell>44.3%</cell><cell>39.9%</cell><cell>44.1%</cell></row><row><cell>Ours</cell><cell>37.4%</cell><cell>60.9%</cell><cell>46.6%</cell><cell>42.2%</cell><cell>46.8%</cell></row><row><cell>PANet [36]</cell><cell>51.8%</cell><cell>64.6%</cell><cell>59.8%</cell><cell>46.5%</cell><cell>55.7%</cell></row><row><cell>CANet [39]</cell><cell>55.5%</cell><cell>67.8%</cell><cell>51.9%</cell><cell>53.2%</cell><cell>57.1%</cell></row><row><cell>Ours*</cell><cell>50.6%</cell><cell>70.3%</cell><cell>58.4%</cell><cell>55.1%</cell><cell>58.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows that</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of models trained and tested on different datasets. Each model (row) shows the training stages, e.g., model I uses the pre-trained weights from ImageNet then fine-tuned on fsCOCO's training classes, and finally tested on the novel test classes in both FSS and fsCOCO. All learning rates are initially set to 10 −4 except the model trained without using ImageNet pretrained weights, which is set to 10 −3 .</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning feed-forward oneshot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Ulrike Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Girshick, and Jitendra Malik. Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Pablo Andrés Arbeláez, Ross B</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Jifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">grabcut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM GIS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
