<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<email>pascale@ece.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called codeswitching. Previous works addressing this challenge mainly focused on word-level aspects such as word embeddings.</p><p>However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of Named Entity Recognition for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing codeswitching entities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Code-switching is a phenomenon that often happens between multilingual speakers, in which they switch between their two languages in conversations, hence, it is practically useful to recognize this well in spoken language systems <ref type="bibr" target="#b20">(Winata et al., 2018a)</ref>. This occurs more often for entities such as organizations or products, which motivates us to focus on the specific problem of Named Entity Recognition (NER) in code-switching scenarios. We show one of the examples as the following:</p><p>• walking dead le quita el apetito a cualquiera • (translation) walking dead (a movie title) takes away the appetite of anyone For this task, previous works have mostly focused on applying pre-trained word embeddings from each language in order to represent noisy mixed-language texts, and combine them with character-level representations <ref type="bibr" target="#b15">(Trivedi et al., 2018;</ref><ref type="bibr" target="#b21">Winata et al., 2018b)</ref>. However, despite the effectiveness of such wordlevel approaches, they neglect the importance of subword-level characteristics shared across different languages. Such information is often hard to capture with word embeddings or randomly initialized character-level embeddings. Naturally, we can turn towards subword-level embeddings such as FastText <ref type="bibr" target="#b4">(Grave et al., 2018)</ref> to help this task, which will evidently allow us to leverage the morphological structure shared across different languages.</p><p>Despite such expected usefulness, there has not been much attention focused around using subword-level features in this task. This is partly because of the non-trivial difficulty of combining different language embeddings in the subword space, which arises from the distinct segmentation into subwords for different languages. This leads us to explore the literature of Meta-Embeddings <ref type="bibr" target="#b23">(Yin and Schütze, 2016;</ref><ref type="bibr" target="#b11">Muromägi et al., 2017;</ref><ref type="bibr" target="#b2">Coates and Bollegala, 2018;</ref>, which is a method to learn how to combine different embeddings.</p><p>In this paper, we propose Hierarchical Meta-Embeddings (HME) 1 which learns how to combine different pre-trained monolingual embeddings in word, subword, and character-level into a single language-agnostic lexical representation without using specific language identifiers. To address the issue of different segmentations, we add a Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>   <ref type="figure">Figure 1</ref>: Hierarchical Meta-Embeddings (HME) architecture for Named Entity Recognition (NER) task. Left: Transformer-CRF architecture for Named Entity Recognition. Right: HME accept words, BPEs, and characters inputs.</p><p>which learns the important subwords in a given sentence. We evaluate our model on the task of Named Entity Recognition for English-Spanish code-switching data, and we use Transformer-CRF, a transformer-based encoder for sequence labeling based on the implementation of . Our experimental results confirm that HME significantly outperforms the state-ofthe-art system in absolute F1 score. The analysis shows that in the task of English-Spanish mixed texts not only similar languages like Portuguese or Catalan help, but also seemingly distant languages from Celtic origin also significantly increase the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Embeddings Previous works have extensively explored different representations such as word <ref type="bibr" target="#b10">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b12">Pennington et al., 2014;</ref><ref type="bibr" target="#b4">Grave et al., 2018;</ref><ref type="bibr" target="#b22">Xu et al., 2018)</ref>, subword <ref type="bibr" target="#b14">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b5">Heinzerling and Strube, 2018)</ref>, and character (dos Santos and Zadrozny, 2014; <ref type="bibr" target="#b18">Wieting et al., 2016)</ref>. <ref type="bibr" target="#b8">Lample et al. (2016)</ref> has successfully concatenated character and word embeddings to their model, showing the potential of combining multiple representations. <ref type="bibr" target="#b9">Liu et al. (2019)</ref> proposed to leverage word and subword embeddings into the application of unsupervised machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-embeddings</head><p>Recently, there are studies on combining multiple word embeddings in pre-processing steps <ref type="bibr" target="#b23">(Yin and Schütze, 2016;</ref><ref type="bibr" target="#b11">Muromägi et al., 2017;</ref><ref type="bibr" target="#b2">Coates and Bollegala, 2018)</ref>. Later,  introduced a method to dynamically learn word-level meta-embeddings, which can be effectively used in a supervised setting.  proposed an idea to leverage multiple embeddings from different languages to generate language-agnostic meta-representations for mixed-language data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Meta-Embeddings</head><p>We propose a method to combine word, subword, and character representations to create a mixture of embeddings. We generate a multilingual metaembeddings of word and subword, and then, we concatenate them with character-level embeddings to generate final word representations, as shown in <ref type="figure">Figure 1</ref>. Let w be a sequence of words with n elements, where w = [w 1 , . . . , w n ]. Each word can be tokenized into a list of subwords s = [s 1 , . . . , s m ] and a list of characters c = [c 1 , . . . , c p ]. The list of subwords s is generated using a function f ; s = f (w). Function f maps a word into a sequence of subwords. Further, let E (w) , E (s) , and E (c) be a set of word, subword, and character embedding lookup tables. Each set consists of different monolingual embeddings. Each element is transformed into a embedding vector in R d . We denote subscripts {i,j} as element and embedding language index, and superscripts <ref type="bibr">(w,s,c)</ref> as word, subword, and character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multilingual Meta-Embeddings (MME)</head><p>We generate a meta-representations by taking the vector representation from multiple monolingual pre-trained embeddings in different subunits such as word and subword. We apply a projection matrix W j to transform the dimensions from the original space x i,j ∈ R d to a new shared space x i,j ∈ R d . Then, we calculate attention weights α i,j ∈ R d with a non-linear scoring function φ (e.g., tanh) to take important information from each individual embedding x i,j . Then, MME is calculated by taking the weighted sum of the projected embeddings x i,j :</p><formula xml:id="formula_0">x i,j = W j · x i,j ,<label>(1)</label></formula><formula xml:id="formula_1">α i,j = exp(φ(x i,j )) n k=1 exp(φ(x i,k )) ,<label>(2)</label></formula><formula xml:id="formula_2">u i = n j=1 α i,j x i,j .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mapping Subwords and Characters to Word-Level Representations</head><p>We propose to map subword into word representations and choose byte-pair encodings (BPEs) <ref type="bibr" target="#b14">(Sennrich et al., 2016)</ref> since it has a compact vocabulary. First, we apply f to segment words into sets of subwords, and then we extract the pre-trained subword embedding vectors x</p><formula xml:id="formula_3">(s) i,j ∈ R d for language j.</formula><p>Since, each language has a different f , we replace the projection matrix with Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> to learn and combine important subwords into a single vector representation. Then, we create u (s) i ∈ R d which represents the subword-level MME by taking the weighted sum of x</p><formula xml:id="formula_4">(s) i,j ∈ R d . x (s) i,j = Encoder(x (s) i,j ),<label>(4)</label></formula><formula xml:id="formula_5">u (s) i = n j=1 α i,j x (s) i,j .<label>(5)</label></formula><p>To combine character-level representations, we apply an encoder to each character.</p><formula xml:id="formula_6">u (c) i = Encoder(x i ) ∈ R d .<label>(6)</label></formula><p>We combine the word-level, subword-level, and character-level representations by concatenation</p><formula xml:id="formula_7">u HM E i = (u (w) i , u (s) i , u (c) i ), where u (w) i ∈ R d and u (s) i ∈ R d</formula><p>are word-level MME and BPElevel MME, and u (c) i is a character embedding. We randomly initialize the character embedding and keep it trainable. We fix all subword and word pre-trained embeddings during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence Labeling</head><p>To predict the entities, we use Transformer-CRF, a transformer-based encoder followed by a Conditional Random Field (CRF) layer <ref type="bibr" target="#b7">(Lafferty et al., 2001)</ref>. The CRF layer is useful to constraint the dependencies between labels. h = Transformer(u), h = CRF(h).</p><p>The best output sequence is selected by a forward propagation using the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We train our model for solving Named Entity Recognition on English-Spanish code-switching tweets data from <ref type="bibr" target="#b0">Aguilar et al. (2018)</ref>. There are nine entity labels with IOB format. The training, development, and testing sets contain 50,757, 832, and 15,634 tweets, respectively. We use FastText word embeddings trained from Common Crawl and Wikipedia <ref type="bibr" target="#b4">(Grave et al., 2018)</ref> for English (es), Spanish (es), including four Romance languages: Catalan (ca), Portuguese (pt), French (fr), Italian (it), and a Germanic language: German (de), and five Celtic languages as the distant language group: Breton (br), Welsh (cy), Irish (ga), Scottish Gaelic (gd), Manx (gv). We also add the English Twitter GloVe word embeddings <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> and BPE-based subword embeddings from Heinzerling and Strube <ref type="formula" target="#formula_0">(2018)</ref>. We train our model in two different settings: (1) multilingual setting, we combine main languages (en-es) with Romance languages and a Germanic language, and (2) cross-lingual setting, we use Romance and Germanic languages without main languages. Our model contains four layers of transformer encoders with a hidden size of 200, four heads, and a dropout of 0.1. We use Adam optimizer and start the training with a learning rate of 0.1 and an early stop of 15 iterations. We replace user hashtags and mentions with &lt;USR&gt;, emoji with &lt;EMOJI&gt;, and URL with &lt;URL&gt;. We evaluate our model using absolute F1 score metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>CONCAT We concatenate word embeddings by merging the dimensions of word representations. This method combines embeddings into a highdimensional input that may cause inefficient com-  putation.</p><formula xml:id="formula_9">x CON CAT i = [x i,1 , ..., x i,n ].<label>(8)</label></formula><p>LINEAR We sum all word embeddings into a single word vector with equal weight. This method combines embeddings without considering the importance of each of them.</p><formula xml:id="formula_10">x LIN EAR i = n j=0 x i,j .<label>(9)</label></formula><p>Random Embeddings We use randomly initialized word embeddings and keep it trainable to calculate the lower-bound performance.</p><p>Aligned Embeddings We align English and Spanish FastText embeddings using CSLS with two scenarios. We set English (en) as the source language and Spanish (es) (en → es) as the target language, and vice versa (es → en). We run MUSE by using the code prepared by the authors of <ref type="bibr" target="#b3">Conneau et al. (2017)</ref>. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Discussion</head><p>In general, from <ref type="table">Table 1</ref>, we can see that wordlevel meta-embeddings even without subword or character-level information, consistently perform better than flat baselines (e.g., CONCAT and LIN-EAR) in all settings. This is mainly because of the attention layer which does not require additional parameters. Furthermore, comparing our approach to previous state-of-the-art models, we can clearly see that our proposed approaches all significantly outperform them. From <ref type="table">Table 1</ref>, in Multilingual setting, which trains with the main languages, it is evident that adding both closely-related and distant language embeddings improves the performance. This shows us that our model is able to leverage the lexical similarity between the languages. This is more distinctly shown in Cross-lingual setting as using distant languages significantly perform less than using closely-related ones (e.g., ca-pt). Interestingly, for distant languages, when adding subwords, we can still see a drastic performance increase. We hypothesize that even though the characters are mostly different, the lexical structure is similar to our main languages.</p><p>On the other hand, adding subword inputs to the model is consistently better than characters. This is due to the transfer of the information from the pre-trained subword embeddings. As shown in Ta-  ble 1, subword embeddings is more effective for distant languages (Celtic languages) than closelyrelated languages such as Catalan or Portuguese.</p><p>Moreover, we visualize the attention weights of the model in word and subword-level to interpret the model dynamics. From the left image of <ref type="figure" target="#fig_0">Figure 2</ref>, in word-level, the model mostly chooses the correct language embedding for each word, but also combines with different languages. Without any language identifiers, it is impressive to see that our model learns to attend to the right languages. The right side of <ref type="figure" target="#fig_0">Figure 2</ref>, which shows attention weight distributions for subword-level, demonstrates interesting behaviors, in which for most English subwords, the model leverages ca, fr, and de embeddings. We hypothesize this is because the dataset is mainly constructed with Spanish words, which can also be verified from <ref type="figure" target="#fig_1">Figure 3</ref> in which most NER tags are classified as es.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Hierarchical Meta-Embeddings (HME) that learns how to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic representations without specific language information. We achieve the state-of-the-art results on the task of Named Entity Recognition for English-Spanish code-switching data. We also show that our model can leverage subword information very effectively from languages from different roots to generate better word representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Heatmap of attention over languages from a validation sample. Left: word-level MME, Right: BPE-level MME. We extract the attention weights from a multilingual model (en-es-ca-pt-de-fr-it).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The average of attention weights for word embeddings versus NER tags from the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-de-fr-it br-cy-ga-gd-gv Flat word-level embeddings CONCAT 65.3 ± 0.38 64.99 ± 1.06 65.91 ± 1.16 65.79 ± 1.36 58.28 ± 2.66 64.02 ± 0.26 50.77 ± 1.55 LINEAR 64.61 ± 0.77 65.33 ± 0.87 65.63 ± 0.92 64.95 ± 0.77 60.72 ± 0.84 62.37 ± 1.01 53.28 ± 0.41</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Multilingual embeddings</cell><cell></cell><cell cols="2">Cross-lingual embeddings</cell></row><row><cell>Model</cell><cell>main languages</cell><cell cols="2">+ closely-related languages</cell><cell>+ distant languages</cell><cell cols="2">closely-related languages</cell><cell>distant languages</cell></row><row><cell cols="7">en-es ca-ptMultilingual Meta-Embeddings (MME) (Winata et al., 2019) ca-pt ca-pt-de-fr-it br-cy-ga-gd-gv ca-pt</cell></row><row><cell>Word</cell><cell cols="2">65.43 ± 0.67 66.63 ± 0.94</cell><cell>66.8 ± 0.43</cell><cell>66.56 ± 0.4</cell><cell cols="2">61.75 ± 0.56 63.23 ± 0.29</cell><cell>53.43 ± 0.37</cell></row><row><cell cols="3">Hierarchical Meta-Embeddings (HME)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ BPE</cell><cell cols="3">65.9 ± 0.72 67.31 ± 0.34 67.26 ± 0.54</cell><cell>66.88 ± 0.37</cell><cell cols="2">63.44 ± 0.33 63.78 ± 0.62</cell><cell>60.19 ± 0.63</cell></row><row><cell>+ Char</cell><cell cols="3">65.88 ± 1.02 67.38 ± 0.84 65.94 ± 0.47</cell><cell>66.1 ± 0.33</cell><cell cols="2">61.97 ± 0.6 63.06 ± 0.69</cell><cell>57.5 ± 0.56</cell></row><row><cell cols="3">+ BPE + Char 66.55 ± 0.72 67.8 ± 0.31</cell><cell>67.07 ± 0.49</cell><cell>67.02 ± 0.16</cell><cell>63.9 ± 0.22</cell><cell>64.52 ± 0.35</cell><cell>60.88 ± 0.84</cell></row><row><cell cols="7">Table 1: Results (percentage F1 mean and standard deviation from five experiments). Multilingual: with main</cell></row><row><cell cols="4">languages, Cross-lingual: without main languages.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell>F1</cell><cell></cell><cell></cell></row><row><cell cols="2">Trivedi et al. (2018)</cell><cell></cell><cell>61.89</cell><cell></cell><cell></cell></row><row><cell cols="2">Wang et al. (2018)</cell><cell></cell><cell>62.39</cell><cell></cell><cell></cell></row><row><cell cols="3">Wang et al. (2018) (Ensemble)</cell><cell>62.67</cell><cell></cell><cell></cell></row><row><cell cols="2">Winata et al. (2018b)</cell><cell></cell><cell>62.76</cell><cell></cell><cell></cell></row><row><cell cols="3">Trivedi et al. (2018) (Ensemble)</cell><cell>63.76</cell><cell></cell><cell></cell></row><row><cell cols="2">Winata et al. (2019) MME</cell><cell cols="2">66.63 ± 0.94</cell><cell></cell><cell></cell></row><row><cell cols="2">Random embeddings</cell><cell cols="2">46.68 ± 0.79</cell><cell></cell><cell></cell></row><row><cell cols="2">Aligned embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MUSE (es → en)</cell><cell cols="2">60.89 ± 0.37</cell><cell></cell><cell></cell></row><row><cell cols="2">MUSE (en → es)</cell><cell cols="2">61.49 ± 0.62</cell><cell></cell><cell></cell></row><row><cell cols="2">Multilingual embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Our approach</cell><cell cols="2">67.8 ± 0.31</cell><cell></cell><cell></cell></row><row><cell cols="2">Our approach (Ensemble)  †</cell><cell></cell><cell>69.17</cell><cell></cell><cell></cell></row><row><cell cols="2">Cross-lingual embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Our approach</cell><cell cols="2">64.52 ± 0.35</cell><cell></cell><cell></cell></row><row><cell cols="2">Our approach (Ensemble)  †</cell><cell></cell><cell>65.99</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison to existing works. Ensemble:</figDesc><table><row><cell>We run a majority voting scheme from five different</cell></row><row><cell>models.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://github. com/gentaiscool/meta-emb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/MUSE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been partially funded by ITF/319/16FP and MRP/055/18 of the Innovation Technology Commission, the Hong Kong SAR Government, and School of Engineering Ph.D. Fellowship Award, the Hong Kong University of Science and Technology, and RDC 1718050-0 of EMOS.AI. We sincerely thank the three anonymous reviewers for their insightful comments on our paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Named entity recognition on code-switched data: Overview of the calcs 2018 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Think globally, embed locally: locally linear meta-embedding of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3970" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly easy meta-embedding-computing metaembeddings by averaging source word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="194" to="198" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bpemb: Tokenization-free pre-trained subword embeddings in 275 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Heinzerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic meta-embeddings for improved sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating word and subword units in unsupervised machine translation using language model rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear ensembles of word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avo</forename><surname>Muromägi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Laur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Nordic Conference on Computational Linguistics</title>
		<meeting>the 21st Nordic Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iit (bhu) submission for the acl shared task on named entity recognition on codeswitched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashwat</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Rangwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Kumar</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Code-switched named entity recognition with embedding attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multilingual meta-embeddings for code-switching named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
		<meeting>the 4th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Code-switching language modeling using syntax-aware multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="62" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilingual character representation for efficiently addressing outof-vocabulary words in code-switching named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emo2vec: Learning generalized emotion representation by multitask training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Ho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning word meta-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1351" to="1360" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
