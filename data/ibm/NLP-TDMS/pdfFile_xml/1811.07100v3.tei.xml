<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RelationNet2: Deep Comparison Columns for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Zhang</surname></persName>
							<email>xueting.zhang@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Qiang</surname></persName>
							<email>qiangyuting.new@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">National Key Laboratory for Software Technology Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
							<email>floodsung@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Independent Researcher</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
							<email>yongxin.yang@ed.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@samsung.com</email>
							<affiliation key="aff4">
								<orgName type="department">Samsung AI Research Centre School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Cambridge</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RelationNet2: Deep Comparison Columns for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot deep learning is a topical challenge area for scaling visual recognition to open ended growth of unseen new classes with limited labeled examples. A promising approach is based on metric learning, which trains a deep embedding to support image similarity matching. Our insight is that effective general purpose matching requires non-linear comparison of features at multiple abstraction levels. We thus propose a new deep comparison network comprised of embedding and relation modules that learn multiple non-linear distance metrics based on different levels of features simultaneously. Furthermore, to reduce over-fitting and enable the use of deeper embeddings, we represent images as distributions rather than vectors via learning parameterized Gaussian noise regularization. The resulting network achieves excellent performance on both miniImageNet and tieredImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The ability to learn from one or few examples is an important property of human learning to function effectively in the real world. In contrast, our most successful deep learningbased approaches to recognition <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> treat each learning problem as tabula-rasa, limiting their application to openended learning with rare data and expensive annotation (e.g., endangered species and medical images).</p><p>These observations have motivated a resurgence of interest in FSL (few-shot learning) for visual recognition <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and beyond. Contemporary deep networks overfit in the few-shot regime -even when exploiting fine-tuning <ref type="bibr" target="#b7">[8]</ref>, data augmentation <ref type="bibr" target="#b0">[1]</ref>, or regularization <ref type="bibr" target="#b8">[9]</ref> techniques. In contrast, 'Metalearning' techniques extract transferable task agnostic knowledge from historical tasks and benefit sparse data learning of specific new target tasks. These take several forms: Fast adaptation methods enable sparse-data adaptation without overfitting -via good initial conditions <ref type="bibr" target="#b4">[5]</ref> or learned optimizers <ref type="bibr" target="#b9">[10]</ref>. Weight synthesis approaches learn a meta-network that synthesizes recognition weights given a training set <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Deep metric learning approaches support representation <ref type="bibr" target="#b12">[13]</ref> and comparison <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> of instances, allowing new categories to be recognized with nearest-neighbour comparison. However, existing approaches have several drawbacks including inference complexity <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, architectural complexity <ref type="bibr" target="#b15">[16]</ref>, the need to fine-tune on the target problem <ref type="bibr" target="#b4">[5]</ref>, or reliance on a simple linear comparison <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>We build on deep metric learning methods due to their architectural simplicity and instantaneous training of new categories. These methods use auxiliary training tasks to learn a deep image-embedding such that the embedded data becomes linearly separable <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. Thus the decision is non-linear in image-space, but linear in the embedding space. For learning the target task, images are simply memorized during few-shot training. But for testing the target task, query images are matched to training examples by deep embedding and similarity comparison function. Within this paradigm, the recent Relation Network <ref type="bibr" target="#b16">[17]</ref> achieved excellent performance by learning a non-linear comparison function. Learning the embedding and non-linear comparison module jointly alleviates the reliance on the embedding's ability to generate linearly separable features.</p><p>We extend this idea of jointly learning an embedding and a non-linear distance metric with the following further insights. First, we introduce the notion of multiple meta-learners operating at multiple abstraction levels. Concretely we train non-linear distance metrics corresponding to each embedding module in a feature hierarchy -thus covering features from simple textures to complex parts <ref type="bibr" target="#b17">[18]</ref>. Secondly, prior studies only use a single linear <ref type="bibr" target="#b5">[6]</ref> or non-linear comparison <ref type="bibr" target="#b16">[17]</ref>. To provide the inductive bias that each layer of representation should be potentially discriminative for matching, and enable better gradient propagation <ref type="bibr" target="#b18">[19]</ref> to each relation module, we deeply supervise <ref type="bibr" target="#b19">[20]</ref> all the relation modules. Finally, to enable deeper embedding architectures to be used without overfitting, we design each embedding module to output a feature distribution, thus representing each image as a distribution rather than a vector. This can be seen as an end-to-end learnable noise regularizer that performs data augmentation in semantic feature space rather than image space.</p><p>Overall RelationNet2 implements a Deep Comparison Network (DCN) that can be seen as jointly learning embedding and comparison as task agnostic meta knowledge <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>. It makes full use of deep networks by making comparisons with the full feature hierarchy extracted by the embedding network, and learning Gaussian noise to improve generalization. The resulting framework maintains the architecture simplicity and efficiency of other methods in this line, while providing excellent performance on both miniImageNet and the more challenging tieredImageNet few shot learning benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Contemporary approaches to deep-network few-shot learning have exploited the learning-to-learn paradigm <ref type="bibr" target="#b20">[21]</ref>. Auxiliary tasks are used to meta-learn some task agnostic knowledge, before exploiting this to learn the target few-sample more effectively problem. The learning-to-learn idea has a long history <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, but contemporary approaches typically cluster into three categories: Fast adaptation, weight synthesis, and metric-learning approaches. Fast Adaptation These approaches aim to meta-learn an optimisation process that enables base models to be finetuned quickly and robustly. So that a base model can be updated for sparse data target problems without extensive overfitting. Effective ideas include the simply meta-learning an effective initial condition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, and learning a recurrent neural network optimizer to replace the standard SGD learning approach <ref type="bibr" target="#b9">[10]</ref>. Recent extensions also include learning perparameter learning rates <ref type="bibr" target="#b24">[25]</ref>, and accelerating fine-tuning through solving some layers in closed form <ref type="bibr" target="#b25">[26]</ref>. Nevertheless, these methods suffer from needing to be fine-tuned for the target problem, often generating costly higher-order gradients during meta-learning process <ref type="bibr" target="#b4">[5]</ref>, and failing to scale to deeper network architectures as shown in <ref type="bibr" target="#b11">[12]</ref>. They also suffer from a fixed parametric architecture. For example, once you train MAML <ref type="bibr" target="#b4">[5]</ref> for 5-way auxiliary classification problems, it is restricted to the same for target problems without being straightforwardly generalizable to a different cardinality of classification. Classifier Synthesis Another line of work focuses on synthesising a classifier based on the provided few-shot training data <ref type="bibr" target="#b26">[27]</ref>. An early method in this line learned a transferrable 'LearnNet' that generated convolutional weights for the base recognition network given a one-shot training example <ref type="bibr" target="#b10">[11]</ref>. However, this was limited to binary classification. Conditional Neural Processes <ref type="bibr" target="#b27">[28]</ref> exploited a similar idea, but in a Bayesian framework. SNAIL obtained excellent results by embedding the training set with temporal convolutions and attention <ref type="bibr" target="#b11">[12]</ref>. The PPA model predicts classification parameters given neuron activations <ref type="bibr" target="#b6">[7]</ref>. In this case the global parameter prediction network is the task agnostic knowledge that is transferred from auxiliary categories. Compared to the fast adaptation approaches, these methods generally synthesize their classifier in a single pass, making them faster to train on the target problem. However learning to synthesize a full classifier does entail some complexity. This process can overfit and generalize poorly to novel target problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Metric Learning</head><p>These approaches aim to learn a deep embedding that extracts robust features, allowing them to be classified directly with nearest neighbour type strategies in the embedding space. The deep embedding forms the task agnostic knowledge transferred from auxiliary to target tasks. Early work simply used Siamese networks <ref type="bibr" target="#b12">[13]</ref> to embed images, such that images of the same class are placed near each other. Matching networks <ref type="bibr" target="#b3">[4]</ref> defined a differentiable nearestneighbour loss based on cosine similarity between the support set and query embedding. Prototypical Networks <ref type="bibr" target="#b5">[6]</ref> provide a simpler but more effective variant of this idea where the support set instances for one class are embedded as a single prototype. Their analysis showed that this leads to a linear classifier in the embedding space. RelationNet <ref type="bibr" target="#b16">[17]</ref> extended this line of work to use a separate non-linear comparison module instead of relying entirely on the embedding networks to make the data linearly separable <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. This division of labour between a deep embedding and a deep relation module improved performance in practice <ref type="bibr" target="#b16">[17]</ref>. Our approach builds on this line of work in general and RelationNet in particular. RelationNet relied on the embedding networks to produce a single embedding for the relation module to compare. We argue that a general purpose comparison function should use any or all of the full feature hierarchy <ref type="bibr" target="#b17">[18]</ref> to make matching decisions. For example matching based on colors, textures, or parts -which may be represented at different layers in a embedding network. To this end we modularise the embedding networks, and pair every embedding module with its own relation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use of Feature Hierarchies</head><p>The general strategy of simultaneously exploiting multiple layers of a feature hierarchy has been exploited in conventional many-shot classification network <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, instance recognition <ref type="bibr" target="#b29">[30]</ref>, and semantic segmentation networks <ref type="bibr" target="#b30">[31]</ref>. However, in the context of deepmetric learning, the conventional pipeline is to extract a complete feature <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Importantly, in contrast to prior approaches single 'short-cut' connection of deeper features to a classifier <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, we uniquely learn a hierarchy of relation modules: One non-linear comparison function for each block of the embedding modules. Our approach is also reminiscent of classic techniques such as spatial pyramids <ref type="bibr" target="#b33">[34]</ref> (since each module in the hierarchy operates at different spatial resolutions) and multi-kernel learning <ref type="bibr" target="#b34">[35]</ref> (since we learn multiple relation modules for each feature in the hierarchy). This can also be seen as the first multiple meta-learner approach for few shot learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leaned Noise and Regularisation</head><p>Many previous FSL models struggle with deeper backbones <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. For best performance, we would like to exploit a state-of-the-art embedding module architecture (we use SENet <ref type="bibr" target="#b2">[3]</ref>), and also benefit from the array of comparison modules mentioned above. To enable DCN to benefit from deep backbones without overfitting, we modify the embedding modules to output a feature distribution at each layer. Rather than generating deterministic features at a module output, we generate means and variances which are sampled in the forward pass, with back propagation relying on the reparamaterization trick. Unlike density networks <ref type="bibr" target="#b35">[36]</ref> where such distributions are only generated at the output layer, or VAEs <ref type="bibr" target="#b36">[37]</ref> here they are generated only once by the generator, we generate such stochastic features at each embedding module's output. This can be seen as an end-to-end learnable data augmentation strategy in semantic feature rather than image space. It is also complementary to standard L2/weight decay and image space augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>We consider a -way -shot classification problem for few shot learning. There are some labelled source tasks with sufficient data, denoted meta-train  m-train , and we ultimately want to solve a new set of target tasks denoted meta-test  m-test , for which the label space is disjoint. Within meta-train and meta-test, we denote each task as being composed of a support set of training examples, and a query set of testing examples. The meta-test tasks are assumed to be few-shot, so  m-test contains a support set with categories and examples each. We want to learn a model on meta-train that can generalize out of the box, without fine-tuning, to learning the new categories in meta-test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Episodic Training</head><p>We adopt an episodic training paradigm for few-shot meta-learning. During meta-training, an episode is formed as follows: (i) Randomly select classes from  m-train , (ii) Sample images each class, which serve as support set  S m-train = ( , ) =1 , where = * , (iii) For the same classes, sample ′ images each class serving as the query set</p><formula xml:id="formula_0"> Q m-train = (̃ ,̃ ) =1 , where = ′ * ,  S m-train ∩  Q m-train = ∅.</formula><p>The support/query distinction mimics the  m-test / real-time testing. Our few-shot DCN will be trained for instance comparison using episodes constructed in this manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>RelationNet2's Deep Comparison Network (DCN) is composed of two module types: embedding and relation modules and , as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The detailed architecture will be given in Section III-C. A pair of images and in the support and query set are fed to embedding modules respectively. Then the multi-level embedding modules output stochastic features to the corresponding multi-level relation modules, and learn the relation score and weights for different relation modules. Finally, the DCN learns weighted non-linear metric of few shot learning tasks. Distribution Embedding Modules Conventionally, an embedding module (e.g., a ResNet or SENet block) outputs deterministic features. As a regularisation strategy, we treat each feature output as a random variable drawn from a parameterized Gaussian distribution, for which the embedding module outputs the mean and variance. This design is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Each th-level embedding module predicts a feature mean , and a feature variance , . To generate a module's output , we use the reparameterization trick to draw one (or more) Gaussian random samples</p><formula xml:id="formula_1">= , + ⊙ , ,<label>(1)</label></formula><p>where is a standard Gaussian  (0, 1) random samples, and ⊙ denotes element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Hierarchy</head><p>The th-level of embedding modules produce query and support image feature maps, which are concatenated as [ ( ), ( )], and then fed into the corresponding th-level relation module for comparison.</p><p>For a pair and at level − 1, the relation module outputs a similarity feature map −1 . The th-level relation module takes both the th-level embedding output for query and support, and also the ( − 1)th-level relation module similarity feature map as input:</p><formula xml:id="formula_2">= ([ ( ), ( ), −1 ]).<label>(2)</label></formula><p>The first relation module is special as it does not have a predecessor to input, and we cannot use zero-padding because 0 has a specific meaning in our context. Thus we set 1 = ([ 1 ( ), 1 ( )]). Simultaneously, after an average pooling and fully connected layer denoted (⋅), each relation module also outputs a real-valued scalar representing similarity/relation score , of two images estimated at the feature level ,</p><formula xml:id="formula_3">= ( ).<label>(3)</label></formula><p>K-Shot For -shot with &gt; 1, the embedding module outputs the average pooling of features, along the sample axis, of all samples from the same class to produce one feature map. Thus, the number of outputs for the -level relation module is , regardless of the value of . Objective Function There are 2 steps to train the DCN network. We first train the embedding network, then fix the embedding network parameters and train the relation network (run the whole DCN consisting of embedding and relation modules, but only update the relation modules). We first train the embedding network as a conventional multi-class classifier for the data in  − using cross entropy loss . To leverage our distribution-embedding, we add a feature variance regularizer:</p><formula xml:id="formula_4">! " # $ % EM1 ! ",' # $ % ! ",( # $ % $ % ∈ * + Classification ! " , $ % EM2 ! ",' , $ % ! ",( , $ % ! " -$ % EM3 ! ",' -$ % ! ",( -$ % ! " . $ % EM4 ! ",' . $ % ! ",( . $ % conv 7*7 RM1 RM2 RM3 RM4 ! " # $ / EM1 ! ",' # $ / ! ",( # $ / $ / ∈ * 0 Classification ! " , $ / E2 ! ",' , $ / ! ",( , $ / ! " -$ / EM3 ! ",' -$ / ! ",( -$ / ! " . $ / EM4 ! ",' . $ / ! ",( . $ /</formula><formula xml:id="formula_5">← argmin ( ) − 1 ∑ =1 ,<label>(4)</label></formula><p>where is the predicted standard deviation of each instance and is their total number, and is the hyperparameter to finetune the influence of the regularizer (here is 0.01). This ensures that feature distributions are learned, and we do not collapse to standard (zero-variance) vector embedding (our mean is about 0.5). This pipeline can be seen as a learnable data augmentation strategy at each level of the feature hierarchy for relation modules. Learning with these augmented features improves generalization. After embedding training, the parameters of embedding modules are fixed.</p><p>We next train the column of relation modules on  − with an episodic strategy <ref type="bibr" target="#b3">[4]</ref> using cross entropy loss at each module <ref type="figure" target="#fig_1">(Fig. 2)</ref>. To weight the relation modules, we assign a learnable attention weight , to the calculated relation similarity score , of each module.</p><formula xml:id="formula_6">← argmin ∑ =1 ∑ =1 ∑ =1 ( , , , ( = ); ),<label>(5)</label></formula><p>where = 1 … refers to query samples and refers to a batch of support examples of class in a -way--shot problem. , are the relation scores between query image and the class support images. Additionally, , = ( , ) is a sigmoid-activated fully connected layer that computes a scalar attention weight given relation feature map , , and the weights of are included in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Strategy</head><p>To evaluate our learned model on -way--shot learning, we calculate the final relation score , of one query image to the images of each support class :</p><formula xml:id="formula_7">, = ∑ =1 ,<label>(6)</label></formula><p>where , is the relation score between image and the support images of class at module .</p><p>Finally, the class with the highest relation score is the final predicted classification. We evaluate the approach by the resulting classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>RelationNet2's DCN architecture <ref type="figure" target="#fig_1">(Fig. 2</ref>) uses 4 embedding modules, each paired with a relation module. We explain our method with SENet for concreteness, but it can be instantiated with any backbone. Embedding Subnetwork As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, first we use a 7 × 7 convolution followed by a 3 × 3 max-pooling, which is a common size reduction as <ref type="bibr" target="#b2">[3]</ref>. Then, we have 4 embedding modules each composed of a number of SENet blocks. Finally, an avg-pooling and a fully-connected layer are used to produce logit values, corresponding to classes in  m-train . More specifically, 4 embedding modules followed the 4 SENet basic blocks composition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>, respectively. In original SENet paper <ref type="bibr" target="#b2">[3]</ref>, they use SE-ResNet-50, but here we use smaller backbones as SE-ResNet-34, where (3+4+6+3) * 2+2 = 34. Otherwise, we follow the other setting in <ref type="bibr" target="#b2">[3]</ref>, e.g., reduction ratio = 16 as suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution Embedding</head><p>Conventually, an embedding module outputs deterministic features. As explained in Section III-B, each DCN embedding module's output is split into two parts: the mean feature , sized [ , , ℎ, ] ([batch_size, channel, height, width]), and standard deviation (std) , sized [ , 1, ℎ, ].</p><p>We assume that every channel shares the same standard deviation (std). This means, in addition to the penultimate-tooutput layer (now it is penultimate-to-mean layer), we have a new penultimate-to-std layer (with its own parameters). The motivation behind sharing stds across channels is to reduce the number of parameters in the newly introduced layer. We also control the amount of noise added by applying Sigmoid activation to constrain the std to the range [0, 1]. We sample one feature vector per image in a single forward pass, but multiple samples are drawn considering the whole batch. Relation Subnetwork As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the relation column consists of 4 serial modules, each of which has 2 SENet blocks, with a pooling and a fully-connected layer to produce the relation score. Thus the relation modules is designed as [2,2,2,2], where the SENet block architecture is the same as the one used in embedding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate RelationNet2's DCN architecture on few-shot classification with miniImageNet and tieredImageNet datasets. PyTorch code to reproduce results is available at https://github. com/zhangxueting/DCN. Baselines We compare several state-of-the-art baselines for few-shot learning including Matching Nets <ref type="bibr" target="#b3">[4]</ref>, Meta Nets <ref type="bibr" target="#b15">[16]</ref>, Meta LSTM <ref type="bibr" target="#b9">[10]</ref>, MAML <ref type="bibr" target="#b4">[5]</ref>, Baseline++ <ref type="bibr" target="#b37">[38]</ref>, Prototypical Nets <ref type="bibr" target="#b5">[6]</ref>, Graph Neural Nets <ref type="bibr" target="#b38">[39]</ref>, Meta-SSL <ref type="bibr" target="#b39">[40]</ref>, Relation Net <ref type="bibr" target="#b16">[17]</ref>, Meta-SGD <ref type="bibr" target="#b24">[25]</ref>, TPN <ref type="bibr" target="#b40">[41]</ref>, CAVIA <ref type="bibr" target="#b41">[42]</ref>, DynamicFSL <ref type="bibr" target="#b26">[27]</ref>, SNAIL <ref type="bibr" target="#b11">[12]</ref>, AdaResNet <ref type="bibr" target="#b42">[43]</ref>, TADAM <ref type="bibr" target="#b43">[44]</ref>, MTL <ref type="bibr" target="#b44">[45]</ref>, TapNet <ref type="bibr" target="#b45">[46]</ref>, MetaOpt Net <ref type="bibr" target="#b14">[15]</ref>, PPA <ref type="bibr" target="#b6">[7]</ref>, LEO <ref type="bibr" target="#b46">[47]</ref>. Data Augmentation We follow the standard data augmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref> with random-size cropping and random horizontal flipping Input images are normalized through mean channel subtraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. miniImagenet</head><p>Dataset miniImageNet has 60,000 images in consist of 100 ImageNet classes, each with 600 images <ref type="bibr" target="#b3">[4]</ref>. Following the split in <ref type="bibr" target="#b9">[10]</ref>, the dataset is divided into a 64-class training set, 16-class validation set and a 20-class testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>We evaluate both 5-way-1-shot and 5-way-5shot, where each episode contains 5 query images for each sampled class. There are 5*5+1*5=30 images per training episode/mini-batch for 5-way-1-shot experiments, and 5*5+5*5=50 images for 5-way-5-shot experiments. When it comes to 5-shot, we calculate the class-wise average feature across the support set. Thus we get 5*5*5*1=125 feature pairs as input for the relation module. For embedding and relation module training, optimization uses SGD with momentum 0.9. The initial learning rate is 0.1, decreased by a factor of 5 every 60 epochs, and the training epoch is 200. All models are trained from scratch, using the robust RELU weight initialization <ref type="bibr" target="#b48">[49]</ref>. We follow <ref type="bibr" target="#b37">[38]</ref> in using 224×224 pixels crops for evaluation on ResNet and SENet, and <ref type="bibr" target="#b9">[10]</ref> in using 84×84 images for the smaller Conv-4 backbone. Results Following the setting of <ref type="bibr" target="#b5">[6]</ref>, when evaluating testing performance, we batch 15 query images per class in a testing episode and the accuracy is calculated by averaging over 600 randomly generated testing tasks (for both 1-shot and 5-shot scenarios). In Tab. I, DCN achieves excellent performance with different embedding backbones. Specifically, the accuracy of 5-way miniImageNet with SENet is 63.19% and 76.58% for 1-shot and 5-shot respectively. We note that MetaOptNet <ref type="bibr" target="#b14">[15]</ref> uses significantly more advanced regularizers than standard among the competitors (which corresponds to about 2% performance according to <ref type="bibr" target="#b14">[15]</ref>), also requires an order of magnitude higher dimensionality of embeddings <ref type="bibr">[64,</ref><ref type="bibr">160,</ref><ref type="bibr">320,</ref><ref type="bibr">640]</ref> than the other competitors <ref type="bibr">[64,</ref><ref type="bibr">96,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>. Overall DCN's 1-shot recognition performance is state-of-theart among methods that do not require optimisation at metatest time (unlike, e.g., MAML <ref type="bibr" target="#b4">[5]</ref> and MetaOptNet <ref type="bibr" target="#b14">[15]</ref>). It is noteworthy that achieving good performance with deeper backbones is not trivially automatic as Dynamic FSL, for example fails to improve from Conv-4 to ResNet embedding. DCN's learned noise regularizer helps it to exploit a powerful SENet backbone without overfitting. Direct comparison among models is complicated by the diversity of embedding networks used in different studies, so we show the results of DCN with each commonly used backbone in Tab.I, e.g. Conv-4 and ResNet-12. We can see that DCN performs favorably across a range of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-way Testing Results</head><p>Standard procedure in fewshot evaluation is to train models for the desired number of categories to discriminate at testing time. However, unlike alternatives such as MAML <ref type="bibr" target="#b4">[5]</ref>, our method is not required to match label cardinality between training and testing. We therefore evaluate 5-way trained model on 20-way testing in Tab. II. It shows that our model outperforms the alternatives clearly despite DCN being trained for 5-way, and the others specifically for 20-way, indicating another important aspect of DCN's flexibility and general applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. tieredImagenet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>tieredImageNet is a larger few-shot recognition benchmark containing 608 classes (779,165 images), in which training/validation/testing categories are organized so as to ensure a larger semantic gap than those in miniImageNet, thus providing a more rigorous test of generalization. This is achieved by dividing according to 34-higher-level nodes in the ImageNet hierarchy <ref type="bibr" target="#b39">[40]</ref>, grouped into 20 for training (351 classes), 6 for validation (97 classes) and 8 for testing (160 classes), respectively. Settings Similar to the setting of miniImageNet, we use 5 query images per training episode. Due to the larger data size, we train embedding modules with a larger batch size 512, initial learning rate 0.3 and 100 training epochs. Other settings remain the same as miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Following the former experiments, we batch 15 query images per class in each testing episode and the accuracy is calculated by averaging over 600 randomly generated testing tasks. From Tab. III, DCN achieves the state-of-the-art performance on the 5-way-1-shot and 5-shot tasks with comfortable margins. Again, this is state-of-the-art performance Model Embedding miniImagenet 5-way Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-shot 5-shot</head><p>MATCHING NETS <ref type="bibr" target="#b3">[4]</ref> Conv-4 43.56 ± 0.84% 55.31 ± 0.73% META LSTM <ref type="bibr" target="#b9">[10]</ref> Conv-4 43.44 ± 0.77% 60.60 ± 0.71% MAML <ref type="bibr" target="#b4">[5]</ref> Conv-4 48.70 ± 1.84% 63.11 ± 0.92% BASELINE++ <ref type="bibr" target="#b37">[38]</ref> Conv-4 48.24 ± 0.75% 66.43 ± 0.63% META NETS <ref type="bibr" target="#b15">[16]</ref> Conv-5 49.21 ± 0.96% -PROTONET <ref type="bibr" target="#b5">[6]</ref> Conv-4 49.42 ± 0.78% 68.20 ± 0.66% GNN <ref type="bibr" target="#b38">[39]</ref> Conv-4 50.33 ± 0.36% 66.41 ± 0.63% META SSL <ref type="bibr" target="#b39">[40]</ref> Conv-4 50.41 ± 0.31% 64.39 ± 0.24% RELATION NET <ref type="bibr" target="#b16">[17]</ref> Conv-4 50.44 ± 0.82% 65.32 ± 0.70% META SGD <ref type="bibr" target="#b24">[25]</ref> Conv-4 50.47 ± 1.87% 64.03 ± 0.94% TPN <ref type="bibr" target="#b40">[41]</ref> Conv-4 52.78 ± 0.27% 66.59 ± 0.28% CAVIA <ref type="bibr" target="#b41">[42]</ref> Conv-4 51.82 ± 0.65% 65.85 ± 0.55% DYNAMIC FSL † <ref type="bibr" target="#b26">[27]</ref> Conv-4 56.20 ± 0.86% 72.81 ± 0.62% RELATIONNET2 (DCN) Conv-4</p><p>53.48 ± 0.78% 67.63 ± 0.59% BASELINE++ <ref type="bibr" target="#b37">[38]</ref> ResNet-18 51.87 ± 0.77% 75.68 ± 0.63% RELATIONNET <ref type="bibr" target="#b37">[38]</ref> ResNet-18 52.48 ± 0.86% 69.83 ± 0.68% PROTONET <ref type="bibr" target="#b37">[38]</ref> ResNet-18 54.16 ± 0.82% 73.68 ± 0.65% SNAIL <ref type="bibr" target="#b49">[50]</ref> ResNet-12 55.71 ± 0.99% 68.88 ± 0.92% DYNAMIC FSL <ref type="bibr" target="#b26">[27]</ref> ResNet-12 55.45 ± 0.89% 70.13 ± 0.68% ADARESNET <ref type="bibr" target="#b42">[43]</ref> ResNet-12 57.10 ± 0.70% 70.04 ± 0.63% TADAM <ref type="bibr" target="#b43">[44]</ref> ResNet-12 58.50 ± 0.30% 76.70 ± 0.30% MTL <ref type="bibr" target="#b44">[45]</ref> ResNet-12 * 61.20 ± 1.80% 75.50 ± 0.80% TAP NET <ref type="bibr" target="#b45">[46]</ref> ResNet-12 61.65 ± 0.15% 76.36 ± 0.10% METAOPTNET <ref type="bibr" target="#b14">[15]</ref> ResNet-12 * 64.09 ± 0.62% 80.00 ± 0.45% RELATIONNET2 (DCN) ResNet-12</p><p>63.92 ± 0.98% 77.15 ± 0.59% PPA <ref type="bibr" target="#b6">[7]</ref> WRN-28-10 59.60 ± 0.41% 73.74 ± 0.19% LEO <ref type="bibr" target="#b46">[47]</ref> WRN-28-10 61.78 ± 0.05% 77.59 ± 0.12%  for methods that do not require optimisation at meta-testing. We note also that Meta-SSL <ref type="bibr" target="#b39">[40]</ref> and TPN <ref type="bibr" target="#b40">[41]</ref> are semisupervised methods that use more information than ours, and have additional requirements such as access to the test set for transduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAML</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Analysis 1) Application to Other Metric Learners:</head><p>Our main insight is the value of feature comparison at multiple abstraction levels in metric learning, as well as that of learned noise regularizers for deep networks in the few-shot regime. We now confirm that these ideas can be applied to other base metric learners. Tab VI shows the 5-way-1-shot miniImageNet results for both RelationNet <ref type="bibr" target="#b16">[17]</ref> and ProtoNet <ref type="bibr" target="#b5">[6]</ref> base learners controlling for these features. We can see that both architectures benefit from deep comparisons and regularizers. However the benefit is greater for RelationNet, which we attribute to the learnable non-linear relation modules. These can learn a different com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Embedding miniImagenet 20-way Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-shot 5-shot</head><p>MATCHING NETS <ref type="bibr" target="#b24">[25]</ref> Conv-4 17.31 ± 0.22% 22.69 ± 0.86% META LSTM <ref type="bibr" target="#b24">[25]</ref> Conv-4 16.70 ± 0.23% 26.06 ± 0.25% MAML <ref type="bibr" target="#b24">[25]</ref> Conv-4 16.49 ± 0.58% 19.29 ± 0.29% META SGD <ref type="bibr" target="#b24">[25]</ref> Conv   <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Embedding tieredImagenet 5-way Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-shot 5-shot</head><p>REPTILE <ref type="bibr" target="#b40">[41]</ref> Conv-4 48.97% 66.47% MAML <ref type="bibr" target="#b40">[41]</ref> Conv-4 51.67% 70.30% META SSL † <ref type="bibr" target="#b39">[40]</ref> Conv-4 52.39 ± 0.44% 70.25 ± 0.31% PROTO NET <ref type="bibr" target="#b40">[41]</ref> Conv-4 53.31% 72.69% RELATION NET <ref type="bibr" target="#b40">[41]</ref> Conv-4 54.48% 71.31% TPN † <ref type="bibr" target="#b40">[41]</ref> Conv-4 59.91% 73.30% TAP NET <ref type="bibr" target="#b45">[46]</ref> ResNet-12 63.08 ± 0.15% 80.26 ± 0.12% METAOPTNET <ref type="bibr" target="#b14">[15]</ref> ResNet  parison function at each abstraction level, but are also more complex so benefit more from the additional regularisation.</p><p>2) Ablation Study: We further investigate the detailed design parameters of our method with a series of ablation studies reported in Tab. IV. The conclusions are as follows:</p><p>Deep Supervision: The DCN-No Deep Sup. result shows that deep supervision is important to gain full benefit from a column of relation modules. Module Weighting: Learning attention weights per module helps somewhat compared to manually tuned module weights. More importantly it eliminates the need for hand-tuning model weights. Multiple Non-linear Metrics: Tab IV also shows the testing accuracy with each DCN relation module output score in isolation (DCN-). Each module performs competitively, but their combination clearly leads to the best overall performance, supporting our argument that multiple levels of the feature hierarchy should be used to make general purpose matching decisions. Multiple meta learner design is a creative contribution of our work.</p><p>3) Architecture: Our DCN benefits from deeper embedding architectures (Tab. I). It improves when going from simple convolutional blocks (used by early studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>), to ResNet <ref type="bibr" target="#b1">[2]</ref> and SENet <ref type="bibr" target="#b2">[3]</ref>. For fair comparison, when fixing a common ResNet-12, DCN outperforms the others that do Relation Module 1 <ref type="bibr">[20,</ref>  58.07 ± 0.80% DCN- <ref type="bibr" target="#b2">3</ref> 60.69 ± 0.81% DCN- <ref type="bibr" target="#b3">4</ref> 58.31 ± 0.79%   simultaneously via a series of paired relation and embedding modules. Relation modules are analyzed to provide insight into the complementarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score-Distance Correlation</head><p>We first check how the relation module (RM) scores relate to distances in the Im-ageNet hierarchy. Using miniImageNet data, we search for ( 1, 2, ) category tuples where the distance ( , 1) and ( , 2) match a certain number of links, and then plot instances from these tuples query categories against the relative relation module scores ( , 1), ( , 2). <ref type="figure">Fig. 3</ref> presents scatter plots for the four relation modules where points are images and colors indicate category tuples with specified distance from the two support classes. We can see that: (1) The scores generally match ImageNet distances: The most/least similar categories (red/magenta) are usually closer to the top right/bottom left of the plot; while query categories closer to one support class are in the opposite corners (blue/yellow-green). (2) Generally higher numbered relation modules are more discriminative, separating classes with larger differences in relation score. Score Correlation We next investigated if relation module predictions are diverse or redundant. We analyzed the correlation in their predictions by randomly picking 10,000 image pairs from miniImageNet and computing the Spearman rankorder correlation coefficient <ref type="bibr" target="#b50">[51]</ref> between each pair of relation module's scores. The results in Tab. V, show that: (1) Many correlations are relatively low (down to 0.34), indicating that they are making diverse, non-redundant predictions; and (2) Adjacent RMs have higher correlation than non-adjacent RMs, indicating that prediction diversity is related to RM position in the feature hierarchy. Prediction Success by Module We know that RM predictions do not necessarily agree. But to find out if they are complementary, we made a scatter plot of the per-class accuracy of RM-1 vs RM-4 in <ref type="figure">Fig. 4</ref>. We can see that many categories lie on the diagonal, indicating that RM-1 and-4 get them right equally often. However there are some categories below the diagonal, indicating that RM-1 gets them right more often than RM-4. Examples include both stereotyped and finegrained categories such as 'hourglass' and 'African hunting dog'. These below diagonal elements confirm the value of using deeper features in metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed RelationNet2, a general purpose matching framework for few-shot learning. It implements a Deep Comparison Network architecture that performs effective few-shot learning via learning multiple non-linear comparisons corresponding to multiple levels of feature extraction, while resisting overfitting through stochastic regularisation. The resulting method achieves state-of-the-art results on miniImageNet and the more ambitious tieredImageNet, while retaining architectural simplicity, and fast training and testing processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SupportFig. 1 :</head><label>1</label><figDesc>Set !"#$%&amp;'( ) Query Set ! "#$%&amp;'( * Meta Train ! +#$%&amp;'( Meta Test ! +#$,-$ Query Set ! "#$,-$ * Support Set !"#$,-$ ) Few-shot learning: Problem Setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>RelationNet2's DCN architecture. There are 4 embedding modules for each embedding branch, and a set of 4 corresponding relation modules . Support set and query set share the same embedding network. Each embedding module outputs a feature distribution  ( , ( ), , ( )), we then randomly sample a feature ( ) as the input of corresponding relation module and next embedding module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Few-shot classification results on miniImageNet.</figDesc><table><row><cell>All ac-</cell></row><row><cell>curacies are averaged over 600 test episodes and are reported with</cell></row><row><cell>95% confidence intervals. Best-performing method is bold, along with</cell></row><row><cell>any others whose confidence intervals overlap. From top to bottom:</cell></row><row><cell>Simple conv block embeddings to other deep embeddings (ResNet,</cell></row><row><cell>WRN, SENet). '-': not reported.</cell></row></table><note>† : use two-step optimization with added attention. : requires gradient-based optimisation at meta-test time.* : Use a wider ResNet than standard and higher dimensional embedding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>20-way classification accuracy on miniImageNet. DCN is trained on 5-way with different embeddings and transferred to 20way. Meta LSTM, MAML, and Meta SGD results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Few-shot classification results on tieredImageNet. All accuracies are averaged over 600 test episodes and reported with 95% confidence intervals. For each task, the best-performing method is bold.</figDesc><table /><note>† : Make use of additional unlabeled data for semi-supervised learning or transductive inference. : requires gradient-based optimi- sation at meta-test time.* : Uses a wider ResNet than standard size and higher dimensional embedding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>class triple matching the specified ImageNet distance relationship [ ( , 1), ( , 2)].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relation Module 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relation Module 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relation Module 4</cell></row><row><cell></cell><cell>0] [20, 10] [20, 20]</cell><cell>[10, 0] [10, 10] [10, 20]</cell><cell>[0, 0] [0, 20] [0, 10]</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.6 Score 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.6 Score 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.6 Score 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6 Score 1</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6 Score 1</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6 Score 1</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell cols="22">Fig. 3: Illustration of query-support score distribution and the link to ImageNet hierarchy. Colors indicate query images of a</cell></row><row><cell>(</cell><cell cols="2">, 2) Model 1,</cell><cell></cell><cell cols="6">miniImageNet 5-way-1-shot Acc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DCN Full model</cell><cell></cell><cell></cell><cell></cell><cell cols="4">63.19 ± 0.87%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DCN-No module weight</cell><cell></cell><cell></cell><cell cols="4">62.88 ± 0.83%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DCN-No deep sup.</cell><cell></cell><cell></cell><cell cols="4">58.02 ± 0.80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DCN-1</cell><cell></cell><cell></cell><cell></cell><cell cols="4">52.25 ± 0.80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DCN-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study using 5-way-1-shot classification on miniImageNet evaluating the impact of regularization techniques and multiple relation modules.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell cols="3">golden retriever</cell><cell>bookshop</cell></row><row><cell>Accuracy(%) of RM4</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell>lion cub</cell><cell></cell><cell cols="2">African hunting dog</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell cols="4">malamute liver-spotted dalmatian hourglass</cell><cell></cell></row><row><cell></cell><cell>20 20</cell><cell>30</cell><cell>40</cell><cell cols="3">50 Accuracy(%) of RM1 60 70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell cols="8">Fig. 4: Category-wise accuracy of RM1 vs RM4. Different relation</cell></row><row><cell cols="8">modules are better at detecting different categories.</cell></row><row><cell cols="8">not require meta-test optimization. Moreover, when fixing</cell></row><row><cell cols="8">a common SENet, competitors RelationNet/ProtoNet/MAML</cell></row><row><cell cols="7">are improved, but still surpassed by DCN.</cell></row><row><cell cols="8">4) Relation Module Analysis: A key contribution in DCN</cell></row><row><cell cols="8">is to perform metric learning at multiple abstraction levels</cell></row><row><cell></cell><cell></cell><cell cols="6">Module RM1 RM2 RM3 RM4</cell></row><row><cell></cell><cell></cell><cell>RM1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RM2</cell><cell></cell><cell>0.75</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RM3</cell><cell></cell><cell>0.55</cell><cell>0.73</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RM4</cell><cell></cell><cell>0.34</cell><cell>0.45</cell><cell>0.61</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Spearman rank-order correlation coefficient between different relation modules: Modules make diverse predictions.</figDesc><table><row><cell>Model</cell><cell cols="2">Noise Reg.? Deep Comparisons?</cell><cell>Acc.</cell></row><row><cell>PROTONET [6]</cell><cell>X</cell><cell>X -1 module</cell><cell>51.04 ± 0.77%</cell></row><row><cell>PROTONET</cell><cell>✓</cell><cell>X -1 module</cell><cell>51.60 ± 0.85%</cell></row><row><cell>PROTONET</cell><cell>X</cell><cell>✓-4 modules</cell><cell>53.62 ± 0.82%</cell></row><row><cell>PROTONET</cell><cell>✓</cell><cell>✓-4 modules</cell><cell>54.78 ± 0.88%</cell></row><row><cell>RELATIONNET [17]</cell><cell>X</cell><cell>X -1 module</cell><cell>52.48 ± 0.86%</cell></row><row><cell>RELATIONNET</cell><cell>✓</cell><cell>X -1 module</cell><cell>57.39 ± 0.86%</cell></row><row><cell>RELATIONNET2 (DCN)</cell><cell>X</cell><cell>✓-4 modules</cell><cell>60.57 ± 0.86%</cell></row><row><cell>RELATIONNET2 (DCN)</cell><cell>✓</cell><cell>✓-4 modules</cell><cell>63.19 ± 0.87%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Multiple deep comparisons and distribution embedding of features benefit both RelationNet (learnable relation modules) and ProtoNet (fixed linear modules) few-shot architectures. Accuracy is calculated on 5-way-1-shot classification of miniImagenet.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by EPSRC grant EP/R026173/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prototypical networks for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks?&quot; in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aston University</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="101" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
