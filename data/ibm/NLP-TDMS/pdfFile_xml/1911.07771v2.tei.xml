<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaskedFusion: Mask-based 6D Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-18">18 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior</orgName>
								<address>
									<settlement>Covilhã</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MaskedFusion: Mask-based 6D Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-18">18 Mar 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nuno Pereira [0000−0001−7177−751X] and Luís A. Alexandre [0000−0002−5133−5025]    Abstract. MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods. The code is available on GitHub (https://github.com/kroglice/MaskedFusion).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection and pose estimation are important tasks for robotic manipulation, scene understanding and augmented reality, and it has recently been the target of much research effort. With the increasing automation and the need for robots that can work in non-restricted environments, the capacity to understand the scene in 3 dimensions is becoming a must. One of the main tasks involving robots is grasping objects. Performing gasping in a non-restricted or/and cluttered environment, e.g., bin picking, is a hard problem to tackle. 6D pose estimation is a task in computer vision that detects the 6D pose (3 degrees of freedom for the position and the other 3 for orientation) of an object. 6D pose estimation is an open important problem because it can be used in several important tasks like grasping, robotic manipulation, augmented reality and others. 6D pose is as important in robotic tasks as in augmented reality, where the pose of real objects can affect the interpretation of the scene and the pose of virtual objects can also improve the augmented reality experience. It can also be useful in human-robot interaction tasks such as learning from demonstration and human-robot collaboration. Estimating the object's 6D pose is a challenging problem due to the ig. 1: Pipeline diagram of the MaskedFusion architecture. MaskedFusion has three sub-tasks: image segmentation, 6D pose estimation, and a pose refinement different objects that exist and how they appear in the real world. Captured scenes from the real world might have occlusions and truncations on some objects. Obtaining the data to retrieve the 6D pose is also a problem, as RGB-D data can be hard to obtain for certain types of object, e.g., fully metallic objects and meshed office garbage bins. 6D pose estimation can be split into three different categories, defined by the type of input data that the methods use. Methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> that use RGB images as input usually rely on the detection and matching of keypoints from the objects in a scene with the 3D render and use the PnP <ref type="bibr" target="#b5">[6]</ref> algorithm to solve the pose of that object. Point cloud methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[33]</ref> rely on descriptors to extract features from the objects in scene to later be matched with features captured in known poses. Finally, RGB-D methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> have the 6D poses directly regressed from the data, and then furthered refined. Most of the methods in this category only use the depth data in the refinement phase. Our method, MaskedFusion, fits in the RGB-D category because we directly regress the 6D pose from the RGB-D data.</p><p>We propose a new method based on a pipeline with 3 sub-tasks that, when combined, can estimate the object's 6D pose. For the first sub-task, we detect and classify each object in the scene using semantic segmentation and retrieve their binary masks. Then, in the second sub-task, for each object detected, we extract features from the different types of data and fuse them in a pixel-wise manner. After the fusion, we have the 6D pose neural network (NN) that will regress the 6D pose of the object. The third sub-task is optional but advisable and refines the 6D pose of the object. For the refinement sub-task, we used a NN that was proposed in <ref type="bibr" target="#b28">[28]</ref>.</p><p>MaskedFusion improves upon the state-of-the-art method by achieving greater scores in two (LineMOD <ref type="bibr" target="#b6">[7]</ref>, YCB-Video <ref type="bibr" target="#b30">[30]</ref>) of the most used datasets in the 6D pose estimation area. Our method achieved an average of 97.3% and our best experience achieved 97.8% in the LineMOD dataset using the ADD (2) metric. For the YCB-Video dataset, we achieved on average 93.3% using the ADD-S (3) AUC metric and 97.1% of ADD-S smaller than 2cm.</p><p>In summary, our work has the following main contributions: -End-to-end modular pipeline to estimate the object's 6D pose; -A new neural network to estimate 6D pose that can be used outside of our pipeline, it is also very fast to execute during inference; -Improvements to the state-of-the-art on the two most used 6D pose evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we present the most relevant literature. In subsection 2.1 we present methods of semantic segmentation that use CNNs and FCNs. On the following subsection 2.2 we present the relevant literature for 6D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>The semantic segmentation goal is to classify each pixel in an image. In contrast, the instance segmentation goal is to detect, delineate it with a bounding box and segment or create a mask for each object instance present in the image. The objective of the panoptic segmentation task <ref type="bibr" target="#b9">[10]</ref> is the unification of the two typically distinct tasks of semantic segmentation and instance segmentation for a given image. Some of the most notable techniques used in semantic segmentation are presented in this section starting with the U-Net <ref type="bibr" target="#b19">[20]</ref>. It is most known for its Ushaped architecture that does semantic segmentation with the encoder-decoder method. The authors of U-net created its architecture with two consecutive convolutions followed by a max-pooling. With this method, the spatial information is decreased but feature information is increased.</p><p>U-Net and SegNet <ref type="bibr" target="#b0">[1]</ref> have similar architectures where the second half of those architectures consists of the same structure in the first half but hierarchically opposite. SegNet is a FCN based on the 13 VGG-16 <ref type="bibr" target="#b10">[11]</ref> convolutional layers. Another technique commonly found in semantic segmentation is addressing it using contextual information.</p><p>Pyramid Scene Parsing Network (PSPNet) <ref type="bibr" target="#b32">[32]</ref>, also uses global contextual information for semantic segmentation. The authors introduced a Pyramid Pooling Module after the last layers of an FCN (based on ResNet-18), the feature maps obtained from the FCN are pooled using 4 different scales corresponding to 4 different pyramid levels each with different sizes, 1×1, 2×2, 3×3 and 6×6. The polled feature maps are then convoluted using a 1×1 convolution layer to reduce the feature maps dimension. So the outputs of each convolution are up-sampled and then concatenated with the initial feature maps that were obtained in the FCN. This concatenation provides the local and global contextual information of the image. After the concatenation, the authors use another convolution layer to generate the final pixel-wise predictions. The main idea of PSPNet is to observe the whole feature map in sub-regions with different locations using the pyramid pooling module, such that, the network achieves a better understanding of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">6D Pose Estimation</head><p>Methods that estimate object 6D pose can be split into three different categories, RGB, Point Cloud, RGB-D. These categories are defined by the type of input data that methods use.</p><p>Methods that use RGB images as input <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> usually rely on the detection and matching of keypoints from the objects in a scene with the 3D render and use the PnP <ref type="bibr" target="#b5">[6]</ref> algorithm to solve the pose of that object. Although they are very fast methods, their recall drops quickly in the presence of occlusion. The methods that do not use this technique are methods that use deep learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>. These methods rely on convolutional neural networks (CNNs) to extract and store features from the multiple viewpoint. This process is done during the training phase of the CNN. On the inference phase, features are extracted from the new scenes and then matched with previously known features and with this matching process it is possible to obtain the object 6D pose.</p><p>Wu et al . <ref type="bibr" target="#b29">[29]</ref> method is made to be fast, running at 20Hz on RGB images with accurate 6D object poses achieved from segmenting the RGB image to obtain the masks and the classes of the objects and then for each mask, with the use of a pose interpreter network, it regresses the 6D pose of the object. The pose interpreter network learns with a new proposed loss similar to L 1 but applied on the point clouds generated from the objects. One of the most accurate method in 6D pose using RGB images is PVNet <ref type="bibr" target="#b16">[17]</ref>. PVNet is a hybrid method that uses the two methods, segmentation and then PnP. With this combination, it can handle mild occlusion and truncations. PVNet has a NN to predict pixel-wise object labels and unit vectors that represent the direction from each pixel to every keypoint of the object. These keypoints are voted and matched with the 3D model of the object to estimate its 6D pose.</p><p>Point cloud methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[33]</ref> rely on descriptors to extract features from the objects in the scene to later be matched with features captured in known poses. Methods like PVFH <ref type="bibr" target="#b12">[13]</ref> and its predecessors <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> achieve remarkable speed acquiring the 6D pose of the object, but these methods need good data retrieval to be accurate and most of the data captured from the real world usually has different types of interference and/or noise. So deep learning is also contributing in this category of methods to achieve better accuracy and get better extracted features from the objects, mitigating some of the noise and interference that can appear. Many deep learning architectures were propose, like PointNets <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and VoxelNet <ref type="bibr" target="#b33">[33]</ref>. These methods achieved good results in multiple datasets.</p><p>Finally, RGB-D methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> usually have the object 6D pose directly regressed from the data, and usually further refined by other methods, e.g., Iterative Closest Point (ICP). Tejani et al . <ref type="bibr" target="#b23">[24]</ref> follow a local approach where small RGB-D patches vote for object pose hypotheses in a 6D space. Kehl et al . <ref type="bibr" target="#b8">[9]</ref> also follow a local approach but they use a convolutional auto-encoder (CAE) to encode each patch of the object to later match these features obtained in the bottleneck of the CAE with a code-book of features learned during the train, and use the code-book matches to predict the 6D pose of the object. Although such methods are not taking global context into account, they proved to be robust to occlusion and the presence of noise artifacts since they infer the object pose using only small patches of the image. SSD-6D <ref type="bibr" target="#b7">[8]</ref> uses an RGB image that is processed by the NN to output localized 2D detection with bounding boxes and then it classifies the bounding boxes into discrete bins of Euler angles and subsequently estimates the object 6D pose. This method is in the RGB-D category because after the first estimation, and with the availability of the depth information, the 6D poses can be further refined. PoseCNN <ref type="bibr" target="#b30">[30]</ref> uses a new loss function that is robust to object symmetry to directly regress the object rotation. It uses a Hough voting approach to obtain the 3D location center of the object to achieve its translation. Using ICP on the refinement phase of SSD-6D and PoseCNN makes their 6D pose estimation accurate. Li et al . <ref type="bibr" target="#b11">[12]</ref> formulates a discriminative representation of 6-D pose that enables predictions of both rotation and translation by a single forward pass of a CNN, and it can be used with many object categories. DenseFusion <ref type="bibr" target="#b28">[28]</ref> extract features from RGB images and depth data with different FCN. After the extraction, it fuses the depth feature with the RGB features while retaining the input's space geometric structure, and then it estimates the object 6D pose. DenseFusion is similar to PointFusion <ref type="bibr" target="#b31">[31]</ref>, as it also estimates the 6D pose while keeping the geometric structure and appearance information of the object, to later fuse this information in a heterogeneous architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our method, MaskedFusion, is a pipeline ( <ref type="figure">Fig. 1</ref>) dived into 3 sub-tasks that combined can solve the task of object 6D pose estimation. MaskedFusion is a modular pipeline, for each sub-task we use a NN to solve the task that it is responsible for. However, since our pipeline is modular every sub-task can have different types of methods that will solve the task in hand and they can be replaced easily.</p><p>In the first sub-task, is where we need to detect and segment each object in the scene. To do that we use a NN based on the encoder-decoder architecture to classify each pixel of the RGB image captured, and predict the mask and the location for each object in the scene.</p><p>For the second sub-task, with the masks obtained in sub-task 1 for each object and the RGB-D data, we can now estimate the object 6D pose. After this subtask, we obtain the rotation matrix and the translation vector according to the 6D pose estimated from the 6D pose NN. After obtaining the translation vector and rotation matrix we can use another method to refine further the estimated 6D pose. This last sub-task is optional but advisable for better results.</p><p>In the last sub-task, we use the same refinement network as the DenseFusion <ref type="bibr" target="#b28">[28]</ref>. This last step can be slow during the training because it needs many training epochs to show its advantages, but during the inference, it is very fast. Other methods usually use ICP that is also a good method for refinement but it has a higher computational cost and will take longer to apply the refinements during the inference time.</p><p>During the training phase of our pipeline, both of the main NNs, semantic segmentation and 6D pose, are trained independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Segmentation</head><p>Semantic labeling provides richer information about the objects and handles occlusions better than object detection and localization methods. For the first sub-task of our pipeline, a semantic segmentation method is used to detect and extract the mask of each object presented in the scene. For the completion of this sub-task, we only use the RGB image.</p><p>For the semantic segmentation, we use an FCN with encoder-decoder architecture. Our implementation is similar to Segnet <ref type="bibr" target="#b0">[1]</ref>, however, since our pipeline is modular it is possible to use other methods to detect and obtain the object masks. We used and tested with a semantic segmentation method but it is also possible to use an instance segmentation, panoptic segmentation, or any other methods that can detect and output the mask for each object in the scene.</p><p>In our implementation of the semantic segmentation, after the output of the mask, we apply 2 filters on the mask before saving or feed-forward it to the next sub-task. First, we use the median filter from OpenCV to smooth the image with a kernel size of 3 × 3. Second, we dilate the mask with a 5 × 5 kernel such that, if the mask has some minor boundary segmentation error, this operation helps to correct it.</p><p>The binary mask obtained from the semantic segmentation method with the 2 filters applied is used to crop the RGB and depth images. This is done for each object present in the scene. To crop the RGB and depth images, we apply a bit-wise and between the RGB image and mask, and also between the depth image and the mask. The result of the bit-wise and of the RGB image and the mask will be inside of a rectangular crop that encloses all the object and this smaller image will serve as input data to the 6D pose NN. In the case of the depth image, a point cloud is further generated from the resultant bit-wise and cropped depth image, and the point cloud will also serve as input to the next phase of our pipeline. Cropping the data with the mask is a pre-processing of the data that helps the 6D pose NN on the second sub-task because it discards the background or other non-relevant data that are around the object leaving only the data that is most relevant to the 6D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">6D Pose Neural Network</head><p>Our NN that estimates the 6D pose of the object is inspired in DenseFusion <ref type="bibr" target="#b28">[28]</ref> and PointFusion <ref type="bibr" target="#b31">[31]</ref>. It has a similar architecture that is used to extract features from different types of data and fuse the information in a pixel-wise manner. Our method improves upon DenseFusion and others, and we show our performance in the results section <ref type="bibr" target="#b4">(5)</ref>. <ref type="figure">Fig. 1</ref> shows the architecture of our 6D pose NN (Sub-task 2). Our 6D pose NN can be divided into two stages: feature extraction and 6D pose estimation.</p><p>On the first stage, feature extraction, all the data that were cropped after the semantic segmentation are separated into different NNs that extract features for each type of data. For the point cloud data that were generated from the cropped depth image, we used the PointNet <ref type="bibr" target="#b18">[19]</ref> to extract 500 features that represent the depth information of the object. For the cropped RGB image we use a fully convolutional neural network (FCN) based on ResNet-18 to also extract 500 features from the color image. Our ResNet-18 is similar to the original but without the fully connected layers at the end making it an FCN. Finally, for the binary mask image, we also used an FCN similar to the FCN used for the color images, but in this case, at the start, we only have one channel as input instead of three. As in the color images for the masks, we extract 500 features. Having features from the mask enables us to have features that represent the shape of the object which is more relevant information for our next stage and improved the accuracy of our method.</p><p>On the second stage, all the extracted features of each data source are then concatenated into a single vector and pass through a convolutional layer to fuse all the features. We then use another NN that receives the features extracted previously and regresses the 6D pose of the object, that is, the rotation matrix and the translation vector of the object. With all the features concatenated from each input, and then feed-forward to a convolutional layer, we will have 2 different paths (each with 4 convolutional layers), one of the paths is for the regression of the translation vector and the other is to regress the rotation matrix.</p><p>So that our NN can regress the 6D pose we use (1) as loss function during the train. This loss function is the same that the DenseFusion used:</p><formula xml:id="formula_0">L p i = 1 M j (Rx j + t) − (R i x j +t i )<label>(1)</label></formula><p>where, x j denotes the j th point of the M randomly selected 3D points from the objects 3D model, p = [R|t] is the ground truth pose, where R is the rotation matrix of the object and t is the translation. The predicted pose generated from the fused embedding of the i th dense-pixel is represented byp i = [R i |t i ] wherê R denotes the predicted rotation andt the predicted translation. After training the 6D pose NN, the output of it (p i = [R i |t i ]) can be retrieved after the second stage or it can be sent to the next sub-task of the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pose Refinement</head><p>In the last sub-task, we used the same pose refinement that DenseFusion developed. The authors of DenseFusion created a pose refinement NN that improves upon the 6D pose previously estimated. The output 6D poses estimated before serves as input to the DenseFusion pose refinement NN.</p><p>We tested the DenseFusion refinement NN and concluded that it is very slow during the training because it needs many training epochs to start showing its advantages, but during the inference, it is very fast. Other refinement methods, can be used but they usually require more computation during the inference and it slows down the process of estimating the 6D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We tested our method on two widely-used datasets, LineMOD <ref type="bibr" target="#b6">[7]</ref> and YCB-Video <ref type="bibr" target="#b30">[30]</ref>. We use these datasets because it is easier to compare our method with previous methods that were also tested in the same datasets. For each experiment we trained the methods from scratch and then evaluate them. We repeat this procedure 5 times and present the average results, whereas other references usually only present one result. All experiments were executed on a desktop with SSD NVME, 64GB of RAM, an NVIDIA GeForce GTX 1080 Ti and Intel Core i7-7700K CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LineMOD</head><p>LineMOD <ref type="bibr" target="#b6">[7]</ref> is one of the most used datasets to tackle the 6D pose estimation problem. Many types of methods that tackle the 6D pose estimation problem use this dataset ranging from the classical methods like <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[27]</ref> to the most recent deep learning approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>. This dataset was captured with a Kinect, and it has a procedure that automatically aligns the RGB and depth images. LineMOD has 15 low-textured objects (although we only use 13 as in previous methods) in over 18000 real images and has the ground truth pose annotated.</p><p>Each object is associated with a test image showing one annotated object instance with significant clutter but only mild occlusion, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Each object also contains the 3D model saved as a point cloud and a distance file with the maximum diameter (cm) of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">YCB-Video</head><p>After the release of the YCB-Video dataset in <ref type="bibr" target="#b30">[30]</ref> it started being used by methods that can estimate the 6D pose of objects even in scenes with heavy occlusions. This dataset has 21 objects that were presented in <ref type="bibr" target="#b3">[4]</ref> by Calli et al . These objects have different shapes and textures, and mild occlusions. YCB-Video is composed of 92 RGB-D videos, each with a subset of the objects placed in the scene. These videos have the segmentation masks and the 6D poses of the objects annotated, and for each frame there are at least 3 objects present in the scene. We have used the dataset in the same splits has previous works, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b28">[28]</ref>, where 80 videos were used for training and 12 for testing. The 80,000 synthetic images released in <ref type="bibr" target="#b30">[30]</ref> were also used during the train of our 6D pose sub-task, but not used for the semantic segmentation sub-task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics</head><p>As in previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref> we used the Average Distance of Model Points (ADD) <ref type="bibr" target="#b6">[7]</ref> as metric of evaluation for non-symmetric objects and for the egg-box and glue we used the Average Closest Point Distance (ADD-S) <ref type="bibr" target="#b30">[30]</ref>. We needed to use another metric for these two objects because they are symmetric objects.</p><formula xml:id="formula_1">ADD = 1 m x∈M (Rx + t) − (Rx +t)<label>(2)</label></formula><p>In the ADD metric (equation 2), assuming the ground truth rotation R and translation t and the estimated rotationR and translationt, the average distance calculates the mean of the pairwise distances between the 3D model points of the ground truth pose and the estimated pose. In equation <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> M represents the set of 3D model points and m is the number of points.</p><p>For the symmetric objects (egg-box and glue), the matching between points is ambiguous for some poses. In these cases we used the ADD-S metric:</p><formula xml:id="formula_2">ADD-S = 1 m x1∈M min x2∈M (Rx 1 + t) − (Rx 2 +t)<label>(3)</label></formula><p>For the YCB-Video we also used the same metrics as in previous works like PoseCNN <ref type="bibr" target="#b30">[30]</ref> and DenseFusion <ref type="bibr" target="#b28">[28]</ref>. We use the area under the ADD-S (3) curve (AUC) as in DenseFusion <ref type="bibr" target="#b28">[28]</ref> where their threshold was 10cm the same as PoseCNN <ref type="bibr" target="#b30">[30]</ref>, and we also calculate the percentage of ADD-S smaller than 2cm, which DenseFusion <ref type="bibr" target="#b28">[28]</ref> authors consider as the minimum tolerance for most of the robot grippers. Using these same metrics in both datasets as previous works enable us to make direct comparisons with their methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we present the results of the experiments made and compare them with other methods that have been evaluated in the same datasets. The inference results in subsection 5.4 present the results for the full pipeline, meaning that we do not use the masks provided in the YCB-Video dataset, but instead rely on the masks produced by the segmentation sub-task of MaskedFusion. This makes the task harder since the masks provided with YCB-Video are all correct whereas the ones we receive from the segmentation can have errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results in LineMOD</head><p>We present here the results of the NN that estimates the object 6D pose of the MaskedFusion pipeline using the LineMOD dataset.  In <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, we show the test results for several different epochs using the LineMOD dataset. We trained both MaskedFusion and the DenseFusion for 100 epochs, and every 10 epochs we tested them and plotted their mean errors. It can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref> (a) that even our worst values are better than the best values of DenseFusion. All MaskedFusion average error values were always below the DenseFusion in all epochs tested, and most important is that our method entered first in the 10mm error mark. Our method achieves the 10mm error, on average, in epoch 30 and DenseFusion only achieved this error in epoch 40. Comparing the mean error of our best run to the DenseFusion best result we have an error of 5.9mm and DenseFusion has 7.6mm. We achieved more accuracy leading to possible better placement of objects in a virtual scene or better grasping accuracy. To train our method for 100 epochs took 40 hours, compared with 33 for DenseFusion. Since we have one more network to train and we have additional computation over the data and more data flowing in our method, it is normal to take longer in the overall training. Note that, since <ref type="table">Table 1</ref>: Quantitative evaluation of 6D pose using the ADD metric on the LineMOD dataset. Symmetric objects are presented in italic and were evaluated using ADD-S. Bold shows best results in a given row we can achieve a smaller error before DenseFusion in terms of training epochs, this increase in training time can be disregarded, since, even if the training was stopped after a fixed number of hours instead of after a fixed number of epochs, MaskFusion would still produce a smaller estimation error, as can be seen in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>. For instance, MaskedFusion entered in the 10mm error mark after 12 hours of training while DenseFusion needed 13.2 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epochs</head><p>In <ref type="table">Table 1</ref> we present a comparison of our test results in a per object comparison with other three methods, SSD-6D <ref type="bibr" target="#b7">[8]</ref>, PointFusion <ref type="bibr" target="#b31">[31]</ref> and DenseFusion <ref type="bibr" target="#b28">[28]</ref>. The values presented in <ref type="table">Table 1</ref> result from the ADD metric (equation 2) and ADD-S metric (equation 3). From <ref type="table">Table 1</ref>, we conclude that our method has overall better accuracy than previous methods. The average results from the 5 repetitions of our method are better than DenseFusion in 11 out of 13 objects. In our worst-performing experience, the second column of MaskedFusion Individual Experiments in <ref type="table">Table 1</ref>, we achieved an average of 97%, which is better than the DenseFusion. Finally, the best of our 5 repetitions improves the overall ADD from the 94.3% of DenseFusion to 97.8%. For the LineMOD dataset, we have achieved overall better results than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results in YCB-Video</head><p>We also use the YCB-Video dataset to evaluate our method and compare it with other methods. The results presented in <ref type="table">Table 2</ref> show the area under the accuracy-threshold curve (AUC) using the ADD-S metric with the maximum threshold of 10cm.</p><p>As shown in <ref type="table">Table 2</ref>, MaskedFusion obtained the best average score using 200 epochs for training. Since DenseFusion, that was the closest one to us, <ref type="table">Table 2</ref>: Quantitative evaluation of 6D pose (area under the ADD-S (3) curve (AUC)) on the YCB-Video Dataset. Bold numbers are the best in a row and underline numbers are the best when comparing MaskedFusion with DenseFusion both with 100 training epochs. The last column of the table is the evaluation of MaskedFusion using the masks that were generated by our first sub-task during the train and test. (*) The values presented were obtained from <ref type="bibr" target="#b28">[28]</ref>  didn't state how many epochs they used, we also present a comparison between MaskedFusion and DenseFusion using 100 epochs for training. All of our experiments have 5 repetitions so we can compute the average of all executions, this also includes the experiment that we have done on DenseFusion with 100 training epochs. MaskedFusion 6D pose NN achieved an average of more 0.2% in the AUC than what DenseFusion presented. But when we compare the average of our method using 100 training epochs and DenseFusion we achieved an an improvement of 1.8% AUC score.</p><p>On <ref type="table">Table 3</ref> we present the percentage of ADD-S smaller than 2cm. This metric was introduced in <ref type="bibr" target="#b28">[28]</ref> since predictions under 2cm are the minimum tolerance for robot manipulation.</p><p>As in the AUC, DenseFusion did not state how many epochs were used for training, so for this metric, we also did the same as in AUC. We trained MaskedFusion and DenseFusion 100 epochs to compare their &lt; 2cm metric and we had an average improvement of 0.3%. <ref type="table">Table 3</ref>: Quantitative evaluation of 6D pose (percentage of ADD-S smaller than 2cm) on the YCB-Video Dataset. Bold numbers are the best in a row and underline numbers are the best when comparing MaskedFusion with DenseFusion both with 100 training epochs. The last column of the table is the evaluation of MaskedFusion using the masks that were generated by our first sub-task during the train and test. (*) The values presented were obtained from <ref type="bibr" target="#b28">[28]</ref>   As in LineMOD we have one more network to train and we have additional computation over the data and more data flowing in our method, so we took more 95 of hours to train 100 epochs. MaskedFusion took 240 hours compared with 145 hours of training time for DenseFusion. <ref type="figure">Fig. 5</ref>.3 presents these results both in terms of epochs and in terms of time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Pipeline results in inference time</head><p>The inference time of MaskedFusion in the first sub-task (image segmentation) is 0.2 seconds per image. For the second sub-task (6D pose estimation) 0.01 seconds were needed to estimate the pose per given object, and on the third sub-task, the pose refinement took 0.002 seconds. So the total inference time of MaskedFusion is 0.212 seconds from the moment of the RGB-D image capture until it has the estimated pose. Using the masks that were generated by our first sub-task during the train we achieved 92.7% in the AUC metrics (shown on the last column of <ref type="table">Table 2</ref>) and 96.1% in the &lt; 2cm metric (shown on the last column of <ref type="table">Table 3</ref>).</p><p>Using our masks we had worst overall scores as expected, since we were not using corrected masks as those provided with YCB. With this test we have learned that our semantic segmentation needs to be improved to obtain better masks so that our second sub-task can achieve better 6D pose estimation. We note that other papers in the literature do not present the results on the full pipeline and always consider that the method receives perfectly segmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Achieving a robust estimation of the 6D pose of objects captured in the real world is still an open challenge. MaskedFusion improved the state-of-the-art in this area using objects' masks retrieved during the semantic segmentation, to identify and localize the object in the RGB image. We achieved an error bellow 6mm in LineMOD with just 100 training epochs and in YCB-Video, a challenging dataset, we have obtained AUC score of 93.3% and 97.1% in the &lt;2cm metric.</p><p>The masks are used to remove non-relevant data from the input of the NN and serve as an additional feature to the 6D pose estimation NN. MaskedFusion has low inference time but at the cost of an increase in training time. We saw that this increased training time can sometimes (such as with LineMOD data and during most of YCB-Video training) be disregarded since we still beat the stateof-the-art if the training time stops after a fixed number of hours instead of a fixed number of epochs. As future work, we intend to study the influence of the instance segmentation method on the MaskedFusion results (since the higher the accuracy of the first sub-task, the better results are expected from the second sub-task) and work towards speeding up the training stage of MaskedFusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Example of images from the datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The methods were evaluated on the test set, after every 10 training epochs and the figure contains the average error in millimeters. Figure (a) shows the error as a function of the training epoch whereas figure (b) presents it as a function of training time. All MaskedFusion runs got smaller average error than DenseFusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 . 3</head><label>53</label><figDesc>Results in YCB-Video</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rotational subgroup voting and pose clustering for robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kiforenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4137" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 international conference on advanced robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgbbased 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning of local rgbd patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9404" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d object recognition and pose estimation for random bin-picking using partition viewpoint feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="148" to="154" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science department</title>
		<imprint>
			<date type="published" when="2009-10" />
		</imprint>
	</monogr>
	<note type="report_type">Technische Universitaet Muenchen</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast 3d recognition and pose using the viewpoint feature histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Taipei</title>
		<meeting>the 23rd IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Taipei<address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2059" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latentclass hough forests for 6 dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2017.2665623</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2665623" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">6d pose estimation using an improved method based on point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Control, Automation and Robotics (ICCAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time object pose estimation with pose interpreter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6798" to="6805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
