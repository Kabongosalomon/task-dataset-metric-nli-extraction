<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PhraseCut: Language-based Image Segmentation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyun</forename><surname>Wu</surname></persName>
							<email>chenyun@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
							<email>scohen@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
							<email>bui@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smaji@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PhraseCut: Language-based Image Segmentation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of segmenting image regions given a natural language phrase, and study it on a novel dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated. Phrases in our dataset correspond to multiple regions and describe a large number of object and stuff categories as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments show that the scale and diversity of concepts in our dataset poses significant challenges to the existing state-of-the-art. We systematically handle the long-tail nature of these concepts and present a modular approach to combine category, attribute, and relationship cues that outperforms existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modeling the interplay of language and vision is important for tasks such as visual question answering, automatic image editing, human-robot interaction, and more broadly towards the goal of general Artificial Intelligence. Existing efforts on grounding language descriptions to images have achieved promising results on datasets such as Flickr30Entities <ref type="bibr" target="#b29">[30]</ref> and Google Referring Expressions <ref type="bibr" target="#b25">[26]</ref>. These datasets, however, lack the scale and diversity of concepts that appear in real-world applications.</p><p>To bridge this gap we present the VGPHRASECUT dataset and an associated task of grounding natural language phrases to image regions called PhraseCut <ref type="figure" target="#fig_0">(Figure 1</ref> and 2). Our dataset leverages the annotations in the Visual Genome (VG) dataset <ref type="bibr" target="#b17">[18]</ref> to generate a large set of referring phrases for each image. For each phrase, we annotate the regions and instance-level bounding boxes that correspond to the phrase. Our dataset contains 77,262 images and 345,486 phrase-region pairs, with some examples shown in <ref type="figure" target="#fig_1">Figure 2</ref>. VGPHRASECUT contains a significantly longer tail of concepts and has a unified treatment of stuff and object cat- Our task and approach. PhraseCut is the task of segmenting image regions given a natural language phrase. Each phrase is templated into words corresponding to categories, attributes, and relationships. Our approach combines these cues in a modular manner to estimate the final output. egories, unlike prior datasets. The phrases are structured into words that describe categories, attributes, and relationships, providing a systematic way of understanding the performance on individual cues as well as their combinations. The PhraseCut task is to segment regions of an image given a templated phrase. As seen in <ref type="figure" target="#fig_0">Figure 1</ref>, this requires connecting natural language concepts to image regions. Our experiments shows that the task is challenging for stateof-the-art referring approaches such as MattNet <ref type="bibr" target="#b39">[40]</ref> and RMI <ref type="bibr" target="#b20">[21]</ref>. We find that the overall performance is limited by the performance on rare categories and attributes.</p><p>To address these challenges we present (i) a modular approach for combining visual cues related to categories, attributes, and relationships, and (ii) a systematic approach to improving the performance on rare categories and attributes by leveraging predictions on more frequent ones. Our category and attribute modules are based on detection models, whose instance-level scores are projected back to the image and further processed using an attention-based model driven by the query phrase. Finally, these are combined with  relationship scores to estimate the segmentation mask (see <ref type="figure" target="#fig_0">Figure 1</ref>). Objects and stuff categories are processed in a unified manner and the modular design, after the treatment of rare categories, outperforms existing end-to-end models trained on the same dataset.</p><p>Using the dataset we present a systematic analysis of the performance of the models on different subsets of the data. The main conclusions are: (i) object and attribute detection remains poor on rare and small-sized categories, (ii) for the task of image grounding, rare concepts benefit from related but frequent ones (e.g., the concept "policeman" could be replaced by "man" if there were other distinguishing attributes such as the color of the shirt), and (iii) attributes and relationship models provide the most improvements on rare and small-sized categories. The performance on this dataset is far from perfect and should encourage better models of object detection and semantic segmentation in the computer vision community. The dataset and code is available at: https://people.cs.umass.edu/Ëœchenyun/ phrasecut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The language and vision community has put significant effort into relating words and images. Our dataset is closely related to datasets for the visual grounding of referring expressions. We also describe recent approaches for grounding natural language to image regions. <ref type="table">Table 1</ref> shows a comparison of various datasets related to grounding referring expressions to images. The ReferIt dataset <ref type="bibr" target="#b16">[17]</ref> was collected on images from ImageCLEF using a ReferItGame between two players. Mao et al. <ref type="bibr" target="#b25">[26]</ref> used the same strategy to collect a significantly larger dataset called Google RefExp, on images from the MS COCO dataset <ref type="bibr" target="#b19">[20]</ref>. The referring phrases describe objects and refer to boxes inside the image across 80 categories, but the descriptions are long and perhaps re-dundant. Yu et al. <ref type="bibr" target="#b40">[41]</ref> instead collect referring expressions using a pragmatic setting where there is limited interaction time between the players to generate and infer the referring object. They collected two versions of the data: RefCOCO that allows location descriptions such as "man on the left", and RefCOCO+ which forbids location cues forcing a focus on other visual clues. One drawback is that Google RefExp, RefCOCO and RefCOCO+ are all collected on MS-COCO objects, limiting their referring targets to 80 object categories. Moreover, the target is always one single instance, and there is no treatment of stuff categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual grounding datasets</head><p>Another related dataset is the Flickr30K Entities <ref type="bibr" target="#b29">[30]</ref>. Firstly entities are mined and grouped (co-reference resolution) from captions by linking phrases that describe the same entity and then the corresponding bounding-boxes are collected. Sentence context is often needed to ground the entity phrases to image regions. While there are a large number of categories (44,518), most of them have very few examples (average 6.2 examples per category) with a significant bias towards human-related categories (their top 7 categories are "man","woman", "people", "shirt", "girl", "boy", "men"). The dataset also does not contain segmentation masks. nor phrases that describe multiple instances.</p><p>Our dataset is based on the Visual Genome (VG) dataset <ref type="bibr" target="#b17">[18]</ref>. VG annotates each image as a "scene graph" linking descriptions of individual objects, attributes, and their relationships to other objects in the image. The dataset is diverse, capturing various object and stuff categories, as well as attribute and relationship types. However, most descriptions do not distinguish one object from other objects in the scene, i.e., they are not referring expressions. Also, VG object boxes are very noisy. We propose a procedure to mine descriptions within the scene graph that uniquely identifies the objects, thereby generating phrases that are more suitable for the referring task. Finally, we collect segmentation annotations of corresponding regions for these phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>ReferIt <ref type="bibr" target="#b16">[17]</ref> Google RefExp <ref type="bibr" target="#b25">[26]</ref> RefCOCO <ref type="bibr" target="#b40">[41]</ref> Flickr30K Entities <ref type="bibr" target="#b29">[30]</ref> Visual Genome <ref type="bibr" target="#b17">[18]</ref>  <ref type="table" target="#tab_3">VGPHRASECUT  # images  19,894  26,711  19,994  31,783  108,077  77,262  # instances  96,654  54,822  50,000  275,775  1,366,673  345,486  # categories  -80  80  44,518  80,138  3103  multi-instance  No  No  No  No  No  Yes  segmentation  Yes  Yes  Yes  No  No  Yes  referring phrase short phrases  long descriptions  short phrases  entities in captions  region descriptions  templated phrases   Table 1</ref>. Comparison of visual grounding datasets. The proposed VGPHRASECUT dataset has a significantly higher number of categories than RefCOCO and Google RefExp, while also containing multiple instances.</p><p>Approaches for grounding language to images Techniques for localizing regions in an image given a natural language phrase can be broadly categorized into two groups: single-stage segmentation-based techniques and two-stage detection-and-ranking based techniques.</p><p>Single-stage methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> predict a segmentation mask given a natural language phrase by leveraging techniques used in semantic segmentation. These methods condition a feed-forward segmentation network, such as a fully-convolutional network or U-Net, on the encoding of the natural language (e.g., LSTM over words). The advantage is that these methods can be directly optimized for the segmentation performance and can easily handle stuff categories as well as different numbers of target regions. However, they are not as competitive on smallsized objects. We compare a strong baseline of RMI <ref type="bibr" target="#b20">[21]</ref> on our dataset.</p><p>More state-of-the-art methods are based on a two-stage framework of region proposal and ranking. Significant innovations in techniques have been due to the improved techniques for object detection (e.g., Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>) as well as language comprehension. Some earlier works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> adopt a joint image-language embedding model to rank object proposals according to their matching scores to the input expressions. More recent works improve the proposal generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref>, introduce attention mechanisms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref> for accurate grounding, or leverage week supervision from captions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The two-stage framework has also been further extended to modular comprehension inspired by neural module networks <ref type="bibr" target="#b1">[2]</ref>. For example, Hu et al. <ref type="bibr" target="#b13">[14]</ref> introduce a compositional modular network for better handling of attributes and relationships. Yu et al. <ref type="bibr" target="#b39">[40]</ref> propose a modular attention network (MattNet) to factorize the referring task into separate ones for the noun phrase, location, and relationships. Liu et al. <ref type="bibr" target="#b23">[24]</ref> improves MattNet by removing easy and dominant words and regions to learn more challenging alignments. Several recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref> also apply reasoning on graphs or trees for more complicated phrases. These approaches have several appealing properties such as more detailed modeling of different aspects of language descriptions. However, these techniques have been primarily evaluated on datasets with a closed set of categories, and often with ground-truth instances provided.</p><p>Sadhu et al. <ref type="bibr" target="#b31">[32]</ref> proposes zero-shot grounding to handle phrases with unseen nouns. Our work emphasizes further on the large number of categories, attributes and relationships, providing supervision over these long-tailed concepts and more detailed and straightforward evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The VGPHRASECUT Dataset</head><p>In this section, we describe how the VGPHRASECUT dataset was collected, the statistics of the final annotations, and the evaluation metrics. Our annotations are based on images and scene-graph annotations from the Visual Genome (VG) dataset. We briefly describe each step in the data-collection pipeline illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, deferring to the supplemental material Section 1.1 for more details.</p><p>Step 1: Box sampling Each image in VG dataset contains 35 boxes on average, but they are highly redundant. We sample an average of 5 boxes from each image in a stratified manner by avoiding boxes that are highly overlapping or are from a category that already has a high number of selected boxes. We also remove boxes that are less than 2% or greater than 90% of the image size.</p><p>Step 2: Phrase generation Each sampled box has several annotations of category names (e.g., "man" and "person"), attributes (e.g., "tall" and "standing") and relationships with other entities in the image (e.g., "next to a tree" and "wearing a red shirt"). We generate one phrase for one box at a time, by adding categories, attributes and relationships that allow discrimination with respect to other VG boxes by the following set of heuristics:</p><p>1. We first examine if one of the provided categories of the selected box is unique. If so we add this to the phrase and tack on to it a randomly sampled attribute or relationship description of the box. The category name uniquely identifies the box in this image. 2. If the box is not unique in terms of any of its category names, we look for a unique attribute of the box that distinguishes it from boxes of the same category. If such an attribute exists we combine it with the category name as the generated phrase. 3. If no such an attribute exists, we look for a distinguishing relationship description (a relationship predicate plus a category name for the supporting object). If such a relationship exists we combine it with the category name as the generated phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>blue | colored car on road Ã  "blue car"</head><p>"license plate on car " "red vehicle" "white building" â€¦ (There are many "car" boxes, but only one of them is "blue")</p><p>Step 1: Box Sampling</p><p>Step 2: Phrase Generation</p><p>Step 3: Region Annotation</p><p>Step 4: Worker Verification</p><p>Step 4. If all of the above fail, we combine all attributes and relationships on the target box and randomly choose a category from the provided list of categories for the box to formulate the phrase. In this case, the generated phrase is more likely to correspond to more than one instance within the image. The attribute and relationship information may be missing if the original box does not have any, but there is always a category name for each box. Phrases generated in this manner tend to be concise but do not always refer to a unique instance in the image.</p><p>Step 3: Region annotation We present the images and generated phrases from the previous steps to human annotators on Amazon Mechanical Turk, and ask them to draw polygons around the regions that correspond to provided phrases. Around 10% of phrases are skipped by workers when the phrases are ambiguous.</p><p>Step 4: Automatic annotator verification Based on manual inspection over a subset of annotators, we design an automatic mechanism to identify trusted annotators based on the overall agreement of their annotations with the VG boxes. Only annotations from trusted annotators are included in our dataset. 9.27% phrase-region pairs are removed in this step.</p><p>Step 5: Automatic instance labeling As a final step we generate instance-level boxes and masks. In most cases, each polygon drawn by the annotators is considered an instance. It is further improved by a set of heuristics to merge multiple polygons into one instance and to split one polygon into several instances leveraging the phrase and VG boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset statistics</head><p>Our final dataset consists of 345,486 phrases across 77,262 images. This roughly covers 70% of the images in Visual Genome. We split the dataset into 310,816 phrases (71,746 images) for training, 20,316 (2,971 images) for validation, and 14,354 (2,545 images) for testing. There is no overlap of COCO trainval images with our test split so that models pre-trained on COCO can be fairly used and evaluated. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates several statistics of the dataset. Our dataset contains 1,272 unique category phrases, 593 unique attribute phrases, and 126 relationship phrases with frequency over 20, as seen by the word clouds. Among the distribution of phrases (bottom left bar plot), one can see that 68.2% of the instances can be distinguished by category alone (category+), while 11.8% of phrases require some treatment of attributes to distinguish instances (at-tributes+). Object sizes and their frequency vary widely. While most annotations refer to a single instance, 17.6% of phrases refer to two or more instances. These aspects of the dataset make the PhraseCut task challenging. In Supplementary Section 1.2, we further demonstrate the long-tailed distribution of concepts and how attributes and relationships vary in different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation metrics</head><p>The PhraseCut task is to generate a binary segmentation of the input image given a referring phrase. We assume that the input phrase is parsed into attribute, category, and relationship descriptions. For evaluation we use the following intersection-over-union (IoU) metrics:</p><formula xml:id="formula_0">â€¢ cumulative IoU: cum-IoU = ( t I t ) / ( t U t ), and â€¢ mean IoU: mean-IoU = 1 N t I t /U t .</formula><p>Here t indexes over the phrase-region pairs in the evaluation set, I t and U t are the intersection and union area between predicted and ground-truth regions, and N is the size of the evaluation set. Notice that, unlike cum-IoU, mean-IoU averages the performance across all image-region pairs and thus balances the performance on small and large objects.</p><p>We also report the precision when each phrase-region task is considered correct if the IoU is above a threshold. We report results with IoU thresholds at 0.5, 0.7, 0.9 as Pr@0.5, Pr@0.7, Pr@0.9 respectively.</p><p>All these metrics can be computed on different subsets of the data to obtain a better understanding of the strengths and failure modes of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Modular Approach to PhraseCut</head><p>We propose Hierarchical Modular Attention Network (HULANet) for the PhraseCut task, as illustrated in <ref type="figure">Fig</ref>  "bare tree outside of window" window <ref type="figure">Figure 5</ref>. Architecture of HULANet. The architecture consists of modules to obtain attribute, category, and relation predictions given a phrase and an image. The attribute and category scores are obtained from Mask-RCNN detections and projected back to the image. The scores across categories and attributes are combined using a module-specific attention model. The relationship module is a convolutional network that takes as input the prediction mask of the related category and outputs a spatial mask given the relationship predicate. The modules are activated based on their presence in the query phrase and combined using an attention mechanism guided by the phrase.</p><p>we design individual modules for category, attribute and relationship sub-phrases. Each module handles the longtail distribution of concepts by learning to aggregate information across concepts using a module-specific attention mechanism. Second, instance-specific predictions are projected onto the image space and combined using an attention mechanism driven by the input phrase. This allows the model to handle stuff and object categories, as well as mul-tiple instances in a unified manner. Details of each module are described next. Backbone encoders We use the Mask-RCNN <ref type="bibr" target="#b10">[11]</ref> detector and bi-directional LSTMs <ref type="bibr" target="#b12">[13]</ref> as our backbone encoders for images and phrases respectively. The Mask-RCNN (with ResNet101 <ref type="bibr" target="#b11">[12]</ref> backbone) is trained to detect instances and predict category scores for the 1,272 categories that have a frequency over 20 on our dataset. Different from instance detection tasks on standard benchmarks, we allow relatively noisy instance detections by setting a low threshold on objectness scores and by allowing at most 100 detections per image to obtain a high recall. For phrase encoding, we train three separate bi-directional LSTMs to generate embeddings for categories, attributes and relationship phrases. They share the same word embeddings initialized from FastText <ref type="bibr" target="#b4">[5]</ref> as the input to the LSTM, and have mean pooling applied on the LSTM output of the corresponding words as the encoded output. Category module The category module takes as input the phrase embedding of the category and detected instance boxes (with masks) from Mask-RCNN, and outputs a scoremap of corresponding regions in the image. We first construct the category channels C âˆˆ R N Ã—HÃ—W by projecting the Mask-RCNN predictions back to the image. Here N = 1272 is the number of categories and H Ã— W is set to 1/4Ã— the input image size. Concretely, for each instance i detected by Mask R-CNN as category c i with score s i , we project its predicted segmentation mask to image as a binary mask m i,HÃ—W , and update the category channel score at the corresponding location as</p><formula xml:id="formula_1">C[c i , m i ] := max(s i , C[c i , m i ]).</formula><p>Finally, each category channel is passed though a "layernorm" which scales the mean and variance of each channel.</p><p>To compute the attention over the category channels, the phrase embedding e cat is passed through a few linear layers f with sigmoid activation at the end to predict the attention weights over the category channels A = Ïƒ(f (e cat )). We calculate the weighted sum of the category channels guided by the attention weights S HÃ—W = c A c Â· C c , and apply a learned affine transformation plus sigmoid to obtain the category module prediction heat-map P HÃ—W = Ïƒ(a Â· S HÃ—W + b). This attention scheme enables the category module to leverage predictions from good category detectors to improve performance on more difficult categories. We present other baselines for combining category scores in the ablation studies in Section 5. Attribute module The attribute module is similar to the category module except for an extra attribute classifier. On top of the pooled ResNet instance features from Mask-RCNN, we train a two-layer multi-label attribute classifier. To account for significant label imbalance we weigh the positive instances more when training attribute classifiers with the binary cross-entropy loss. To obtain attribute score channels we take the top 100 detections and project their top 20 predicted attributes back to the image. Identical with the category module, we use the instance masks from the Mask-RCNN, update the corresponding channels with the predicted attribute scores, and finally apply the attention scheme guided by the attribute embedding from the phrase to obtain the final attribute prediction score heat-map. Relationship module Our simple relationship module uses the category module to predict the locations of the supporting object. The down-scaled (32 Ã— 32) score of the supporting object is concatenated with the embedding of the relationship predicate. This is followed by two dilated convolutional layers with kernel size 7 applied on top, achieving a large receptive field without requiring many parameters. Finally, we apply an affine transformation followed by sigmoid to obtain the relationship prediction scores. The convolutional network can model coarse spatial relationships by learning filters corresponding to each spatial relation. For example, by dilating the mask one can model the relationship "near", and by moving the mask above one can model the relationship "on". Combining the modules The category, attribute, and relation scores P c , P a , P r obtained from individual modules are each represented as a H Ã—W image, 1/4 the image size. To this we append channels of quadratic interactions P i â€¢P j for every pair of channels (including i = j), obtained using elementwise product and normalization, and a bias channel of all ones, to obtain a 10-channel scoremap F (3+6+1 channels). Phrase embeddings of category, attribute and relationship are concatenated together and then encoded into 10-dimensional "attention" weights w through linear layers with LeakyReLU and DropOut followed by normalization. When there is no attribute or relationship in the input phrase, the corresponding attention weights are set to zero and the attention weights are re-normalized to sum up to one. The overall prediction is the attention-weighted sum of the linear and quadratic feature interactions:</p><formula xml:id="formula_2">O = t F t w t .</formula><p>Our experiments show a slight improvement of 0.05% on validation mean-IoU with the quadratic features.</p><p>Training details The Mask-RCNN is initialized with weights pre-trained on the MS-COCO dataset <ref type="bibr" target="#b19">[20]</ref> and finetuned on our dataset. It is then fixed for all the experiments. The attribute classifier is trained on ground-truth instances and their box features pooled from Mask-RCNN with a binary cross-entropy loss specially weighted according to attribute frequency. These are also fixed during the training of the referring modules. On top of the fixed Mask-RCNN and the attribute classifier, we separately train the individual category and attribute modules. When combining the modules we initialize the weights from individual ones and fine-tune the whole model end-to-end. We apply a pixel-wise binary cross-entropy loss on the prediction score heat-map from each module and also on the final prediction heat-map. To account for the evaluation metric (mean-IoU), we increase the weights on the positive pixels and average the loss over referring phrase-image pairs instead of over pixels. All our models are trained on the training set. For evaluation, we require a binary segmentation mask which is obtained by thresholding on prediction scores. These thresholds are set based on mean-IoU scores on the validation set. In the next section, we report results on the test set. <ref type="table">Table 2</ref> shows the overall performance of our model and its ablated versions with two baselines: RMI <ref type="bibr" target="#b20">[21]</ref> and Mat-tNet <ref type="bibr" target="#b39">[40]</ref>. They yield near state-of-the-art performance on datasets such as RefCOCO <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison to baselines</head><p>RMI is a single-stage visual grounding method. It extracts spatial image features through a convolutional encoder, introduces convolutional multi-modal LSTM for jointly modeling of visual and language clues in the bottleneck, and predicts the segmentation through an upsampling  <ref type="table">Table 4</ref>. The mean-IoU on VGPHRASECUT test set for additional subsets. att/rel: the subset with attributes/relationship annotations; att+/rel+: the subset which requires attributes or relationships to distinguish the target from other instances of the same category; single/multi/many: subsets that contain different number of instances referred by a phrase; small/mid/large: subsets with different sizes of the target region.</p><p>decoder. We use the RMI model with ResNet101 <ref type="bibr" target="#b11">[12]</ref> as the image encoder. We initialized the ResNet with weights pretrained on COCO <ref type="bibr" target="#b19">[20]</ref>, trained the whole RMI model on our training data of image region and referring phrase pairs following the default setting as in their public repository, and finally evaluated it on our test set. RMI obtains high cum-IoU but low mean-IoU scores because it handles large targets well but fails on small ones (see <ref type="table">Table 4</ref> "small/mid/large" subsets). cum-IoU is dominated by large targets while our dataset many small targets: 20.2% of our data has the target region smaller than 2% of the image area, while the smallest target in RefCOCO is 2.4% of the image. <ref type="figure" target="#fig_4">Figure 6</ref> also shows that RMI predicts empty masks on challenging phrases and small targets.</p><p>MattNet focuses on ranking the referred box among candidate boxes. Given a box and a phrase, it calculates the subject, location, and relationship matching scores with three separate modules, and predicts attention weights over the three modules based on the input phrase. Finally, the three scores are combined with weights to produce an overall matching score, and the box with the highest score is picked as the referred box.</p><p>We follow the training and evaluation setup described in their paper. We train the Mask-RCNN detector on our dataset, and also train MattNet to pick the target instance box among ground-truth instance boxes in the image. Note that MattNet training relies on complete annotations of object instances in an image, which are used not only as the candidate boxes but also as the context for further reasoning. The objects in our dataset are only sparsely annotated, hence we leverage the Visual Genome boxes instead as context boxes. At test time the top 50 Mask-RCNN detections from all categories are used as input to the MattNet model.</p><p>While this setup works well on RefCOCO, it is problematic on VGPHRASECUT because detection is more challenging in the presence of thousands of object categories. MattNet is able to achieve mean-IoU = 42.4% when the ground-truth instance boxes are provided in evaluation, but its performance drops to mean-IoU = 20.2% when Mask-RCNN detections are provided instead. If we only input the detections of the referred category to MattNet, mean-IoU improves to 34.7%, approaching the performance of Mask-RCNN self, but it still performs poorly on rare categories.</p><p>Our modular approach for computing robust category scores from noisy detections alone (HULANet cat) outperforms both baselines by a significant margin. Example results using various approaches are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Heatmaps from submodules and analysis of failure cases are included in Supplemental Section 3. <ref type="table" target="#tab_3">Table 3</ref> shows that the performance is lower for rare categories. Detection of thousands of categories is challenging, but required to support open-vocabulary natural language descriptions. However, natural language is also redundant. In this section we explore if a category can leverage scores from related categories to improve performance, especially when it is rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation studies and analysis</head><p>First we evaluate Mask-RCNN as a detector, by using the mask of the top-1 detected instance from the referred category as the predicted region. The result is shown as the row "Mask-RCNN self" in <ref type="table" target="#tab_3">Table 3</ref>. The row below "Mask-RCNN top" shows the performance of the model where each category is matched to a single other category based on the best mean-IoU on the training set. For example, a category "pedestrian" may be matched to "person" if the person detector is more reliable. Supplemental Section 2 shows the full matching between source and target categories. As one can see in <ref type="table" target="#tab_3">Table 3</ref>, the performance on the tail categories jumps significantly (10.1% â†’ 23.2% on the 500+ subset.) In general the tail category detectors are poor and rarely used. This also points to a curious phenomenon in referring expression tasks where even though the named category is specific, one can get away with a coarse category detector. For example, if different animal species never appear together in an image, one can get away with a generic animal detector to resolve any animal species. This also explains the performance of the category module with the category-level attention mechanism. Compared to the single category picked by the Mask-RCNN top model, the ability of aggregating multiple category scores using the attention model provides further improvements for the tail categories. Although not included here, we find a similar phenomenon with attributes, where a small number of base attributes can support a larger, heavy-tailed distribution over the attribute phrases. It is reassuring that the number of visual concepts to be learned grows sub-linearly with the number of language concepts. However, the problem is far from solved as the performance on tail categories is still significantly lower. <ref type="table">Table 4</ref> shows the results on additional subsets of the test data. Some high-level observations are that: (i) Object categories are more difficult than stuff categories. (ii) Small objects are extremely difficult. (iii) Attributes and relationships provide consistent improvements across different subsets. Remarkably, the improvements from attributes and re-lationships are more significant on rare categories and small target regions where the category module is less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a new dataset, VGPHRASECUT, to study the problem of grounding natural language phrases to image regions. By scaling the number of categories, attributes, and relations we found that existing approaches that rely on high-quality object detections show a dramatic reduction in performance. Our proposed HULANet performs significantly better, suggesting that dealing with long-tail object categories via modeling their relationship to other categories, attributes, and spatial relations is a promising direction of research. Another take away is that decoupling representation learning and modeling long-tails might allow us to scale object detectors to rare categories, without requiring significant amount of labelled visual data. Nevertheless, the performance of the proposed approach is still significantly below human performance which should encourage better modeling of language and vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">VGPHRASECUT Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Further details on data collection</head><p>As described in the main paper, we start our data collection by mining Visual Genome (VG) boxes and phrases that are discriminative. We then collect region annotations for each phrase from various human annotators. Human annotators are verified by comparing their annotations against VG boxes. Finally, we merge and split collected regions to produce instance-level segmentation masks for each phrase. The steps are described in more details below.</p><p>Step 1: Box sampling Each image in VG dataset contains an average of 35 boxes, many of which are redundant. We sample a set of non-overlapping boxes across categories, also removing ones that are too small or large.</p><p>Define r as the box size proportional to the image size. We ignore boxes with r &lt; 0.02 or r &gt; 0.9. For each image, we add the VG boxes to a sample pool one by one. The current box is ignored if it overlaps a box already in the sample pool by IoU&gt; 0.2. When sampling from the pool, the weight of each box being sampled is w = min(0.1, r) so that we are less likely to get boxes with r &lt; 0.1. Every time a new box is sampled, we divide the weights by 5 for all boxes from the same category as the newly sampled box, so that category diversity is encouraged. Since the VG boxes are noisy, we only use annotations on these boxes to generate phrases, but not use the boxes as the ground-truth of the corresponding regions.</p><p>Step 2: Phrase generation <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of how we generate query phrases when collecting the VG-PHRASECUT dataset. The goal is to construct phrases that are concise, yet sufficiently discriminative of the target.</p><p>For VG boxes that have no other box from the same category in the image, the "basket ball on floor" box for example, we randomly add an additional attribute / relationship annotation (if there is one) to the generated phrase. This avoids ambiguity caused by the missing VG box annotations, and makes it easier to find the corresponding regions, without making the phrase very long. Phrases generated this way are recognized as the "cat+" subset in evaluation.</p><p>For VG boxes with unique attribute / relationship annotations within the same category, we generate the phrase by combining its attribute / relationship annotation with the category name. In the "wizard bear" and "bear holding paper" examples, we obtain the phrase to refer to a single VG box, and avoided adding less helpful information ("on wall" or "on floor") to the generated phrase. They are recognized as the "att+" and "rel+" subsets in evaluation.</p><p>For the rest, we include all annotations we have on the sampled box into the generated phrase, like what we did in the "small white bear on wall" example. In these cases, the sampled VG boxes are usually more difficult to distinguish from other boxes, so we add all annotations to make the descriptions as precise as possible. This is one of the sources of multi-region descriptions in our dataset.</p><p>Step 3: Referred region annotation We present the images and phrases from the previous step to human annotators on Amazon Mechanical Turk, and ask them to draw polygons around the regions that correspond to those phrases. In total we collected 383, 798 phrase-region pairs from 869 different workers, which will be filtered in the next step. In addition, we have 42, 587 phrases skipped by workers, 50.0% with reason "difficult to select", 24.8% for "wrong or ambiguous description", 23.7% for "described target not in image", and 1.5% for "other reasons".</p><p>Step 4: Automatic worker verification We designed an automatic worker verification metric in the spirit that statistically, annotations from better workers overlap more with corresponding VG boxes.</p><p>We rate each worker based on their overall agreement with VG boxes and the number of annotations they have done to identify a set of trusted workers.</p><p>We label a small set of annotations as "good", "so-so", or "bad" ones, and notice that the quality of annotations are strongly correlated with IoP = s intersection /s polygon and IoU = s intersection /s union , where s polygon is the area of worker labeled polygons, s intersection and s union are the intersection and union between worker labeled polygons and VG boxes. On the labeled set of annotations, we learn a linear combination of IoP and IoU that best separates "good" and "bad / so-so" annotations, defined as agreement = IoP + 0.8 Ã— IoU.</p><p>We calculate the average agreement score of all annotations from each worker, and set a score threshold based on the total number of annotations from this worker: thresh = max(0.7, 0.95 âˆ’ 0.05 Ã— #annotations). A worker is trusted if the average agreement score is above the thresh. Workers with fewer than 10 annotations are ignored. Only annotations from trusted workers are included in our dataset.</p><p>In this step, 371 out of 869 workers are verified as trusted. 9.27% (35,565 out of 383,798) phrase-region pairs are removed from our dataset. In rare cases we have multiple annotations on the same input. We randomly select one of them, resulting in 345,486 phrase-region pairs.</p><p>Step 5: Automatic instance labeling from polygons Non-overlapping polygons are generally considered as sep- Step 1: Unique category?</p><p>Step 2: Unique attribute?</p><p>Step 3: Unique relationship?</p><p>Step </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"wizard bear"</head><p>No.</p><p>(There are other "bears" with "white")</p><p>Yes: holding paper (No other "bear" that's "holding paper") "bear holding paper"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final phrase</head><p>No.</p><p>(There are other "bears" with "white" or "small")</p><p>No.</p><p>(There are other "bears" with "on wall") Use all attributes &amp; relationships on this box "small white bear on wall" <ref type="figure" target="#fig_0">Figure 1</ref>. An illustrative example of phrase generation. We show how we generate phrases from sampled Visual Genome (VG) boxes in four steps. The raw image and additional VG boxes are displayed on top. We do not generate phrases on these additional VG boxes, but they are used to decide the uniqueness of sampled boxes. VG annotations of categories, attributes and relationships on each box are highlighted in blue, yellow and green respectively. arate instances with a few exceptions. As a final step, we heuristically refine these instance annotations.</p><p>First, one instance can be split into multiple polygons when there are occlusions. If the category for a box is not plural and the bounding box of two polygons has a high overlap with it, then they are merged into a single instance. Second, workers sometimes accidentally end a polygon and then create a new one to cover the remaining part of the same instance. We merge two polygons into one instance if they overlap with each other, especially when one is much larger than the other. Third, people tend to cover multiple instances next to each other with a single polygon. If a single polygon matches well with a set of VG boxes, these VG boxes are of similar sizes, and the referring phrase indicate plural instances, we split the polygon into multiple instances according to the VG boxes. <ref type="figure" target="#fig_2">Figure 3</ref> shows the frequency histograms for categories, attributes and relationship descriptions. Compared with tag clouds, the histograms better reveal the long-tail distribution of our dataset. <ref type="figure" target="#fig_3">Figure 4</ref> provides detailed visualizations for three typical categories: "man", "car" and "tree". The attribute and relationship description distributions vary a lot for different categories. Attributes and relationships mainly describe clothing, states and actions for "man". In the "car" category, attributes are focused on colors, while relationships are about locations and whether the car is parked or driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Additional dataset visualizations</head><p>We can see several sets of opposite concepts in "tree" attributes, such as "large -small", "green -brown", "bare/leafless -leafy", "growing -dead", etc. Relationships for "tree" mainly describe the locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Category Matching in Mask-RCNN top</head><p>In Mask-RCNN top we map each input category to its substitute category. Given an input category, we consider every referring phrase in the training set containing this category, and pick the best category with which the detected mask yields highest mean-IoU on each referring phrase. The final substitute category is the one that most frequently picked as the best. The mappings are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Using detections of a related and frequent category is often better. Detections from categories with frequency ranked beyond 600 are rarely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Additional Results from HULANet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modular heatmaps</head><p>In <ref type="figure">Figure 5</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>, we show HULANet predictions and modular heatmaps. <ref type="figure">Figure 5</ref> demonstrates that our attribute module is able to capture color ("black", "brown"), state ("closed"), material ("metal") and long and rare attributes ("pink and white"). In the first ("black jacket") example, the category module detects two jackets, while the attribute module is able to select out the "black" one against the white one. <ref type="figure" target="#fig_4">Figure 6</ref> shows how our relationship module modifies the heatmaps of supporting objects depending on different relationship predicates. With the predicate "wearing", the relationship module predicts expanded regions of the detected "jacket" especially vertically. The relational prediction of "parked on" includes regions of the "street" itself as well as regions directly above the "street", while the predicate "on" leads to the identical region prediction as the supporting object. In the last example of "sitting at", a broader region around the detected "table" is predicted, covering almost the whole image area. <ref type="figure">Figure 7</ref> displays typical failure cases from our proposed HULANet. Heatmaps from internal modules provide more insights where and why the model fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Failure case analysis</head><p>In the first example, our backbone Mask-RCNN fails to detect the ground-truth "traffic cones", which are extremely small and from rare categories. Similarly, in the second "dark grey pants" example, the "pants" is not detected as a separate instance in the backbone Mask-RCNN, therefore the category module can only predict the whole mask of the skateboarder.</p><p>The third "window" example shows when the category module (and the backbone Mask-RCNN) fails to distinguish mirrors from windows. In the fourth example, our attribute module fails to recognize which cat is "darker" than the other.</p><p>We then display two failure cases for the relationship module. It fails on the first one because the supporting object ("suitcase") is not detected by the category module, and fails on the second one for unable to accurately model the relation predicate "on side of".</p><p>In the last example, although our attribute module figures out which sofa is "plaid", the final prediction is dominated by the category module and fails to exclude non-plaid sofas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">More comparisons against baseline methods</head><p>As an extension to <ref type="figure">Figure 7</ref> in the main paper, <ref type="figure">Figure 8</ref> here shows more examples of prediction results compared against baseline methods. The "white building", "chair" and "large window" examples demonstrate that our HU-LANet is better at handling occlusions.  <ref type="figure" target="#fig_3">Figure 4</ref>. Visualizations of "man", "car" and "tree" categories. From left two right, we display two examples, attribute cloud and relationship cloud within the given category. The size of each phrase is proportional to the square root of its frequency in the given category.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our task and approach. PhraseCut is the task of segmenting image regions given a natural language phrase. Each phrase is templated into words corresponding to categories, attributes, and relationships. Our approach combines these cues in a modular manner to estimate the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example annotations from the VGPHRASECUT dataset. Colors (blue, red, green) of the input phrases correspond to words that indicate attributes, categories, and relationships respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustrations of our VGPHRASECUT dataset collection pipeline. Step 1: blue boxes are the sampling result; red boxes are ignored. Step 2: Phrase generation example in the previous image. Step 3: User interface for collecting region masks. Step 4: Example annotations from trusted and excluded annotators. Step 5: Instance label refinement examples. Blue boxes are final instance boxes, and red boxes are corresponding ones from Visual Genome annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Statistics of the VGPHRASECUT dataset. Top row: Word clouds of categories (left), attributes (center), and relationship descriptions (right) in the dataset. The size of each phrase is proportional to the square root of its frequency in the dataset. Bottom row: breakdowns of the dataset into different subsets including contents in phrases (first), category frequency (second), size of target region relative to the image size (third), number of target instances per query phrase (fourth), and types of category (last). The leftmost bar chart shows the breakdown of phrases into those that have category annotation (cat) and those that can be distinguished by category information alone (cat+), and similarly for attributes and relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Prediction results on VGPHRASECUT dataset. Rows from top to down are: (1) input image; (2) ground-truth segmentation and instance boxes; (3) MattNet baseline; (4) RMI baseline; (5) HULANet (cat + att + rel). See more results in the supplemental material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Category matching in Mask-RCNN top. Each input category (left) are matched to its best substitute (right) measured by performance on training set. Categories are ordered from top to bottom by frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Frequency histograms of categories (left), attributes (middle) and relationship descriptions (right). Y-axis shows each entry and its frequency ranking; X-axis shows their frequency in the whole dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2008.01187v1 [cs.CV] 3 Aug 2020</figDesc><table><row><cell>short deer</cell><cell>walking people</cell><cell>wipers on trains</cell><cell>zebra lying on savanna</cell><cell>black shirt</cell></row><row><cell>hatchback car</cell><cell>mark on chicken</cell><cell>glass bottles</cell><cell>blonde hair</cell><cell>pedestrian crosswalk</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ure 5. The approach is based on two design principles. First,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Content in Phrases</cell><cell cols="2">Categories ranked by frequency</cell><cell>Target region size</cell><cell># referred instances</cell><cell cols="2">Types of categories</cell></row><row><cell>rel att cat</cell><cell>+ att+ 2.7%</cell><cell>11.8%</cell><cell>12.5%</cell><cell>cat+</cell><cell>44.9%</cell><cell>68.2%</cell><cell>101~500, 500+, 12.9% 32.1%</cell><cell>top100, 55.0%</cell><cell>large (20%+), 24.64% medium (2% ~ 20%), small (&lt;2%), 20.22% 55.14%</cell><cell>2 ~ 5 (multi):16.4% &gt;5 (many):1.2% 1 (single):82.4%</cell><cell>stuff, 35.1%</cell><cell>object, 64.9%</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The mean-IoU on VGPHRASECUT test set for various category subsets. The column coco refers to the subset of data corresponding to the 80 coco categories, while the remaining columns show the performance on the top 100, 101-500 and 500+ categories in the dataset sorted by frequency.</figDesc><table><row><cell>Model</cell><cell></cell><cell>all</cell><cell cols="4">coco 1-100 101-500 500+</cell></row><row><cell>HULANet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cat</cell><cell></cell><cell cols="2">39.9 46.5</cell><cell>46.8</cell><cell>31.8</cell><cell>25.2</cell></row><row><cell>cat+att</cell><cell></cell><cell cols="2">41.3 48.3</cell><cell>48.2</cell><cell>33.6</cell><cell>26.6</cell></row><row><cell>cat+rel</cell><cell></cell><cell cols="2">41.1 47.9</cell><cell>47.8</cell><cell>33.6</cell><cell>26.6</cell></row><row><cell>cat+att+rel</cell><cell></cell><cell cols="2">41.3 47.8</cell><cell>47.8</cell><cell>33.8</cell><cell>27.1</cell></row><row><cell cols="4">Mask-RCNN self 36.2 44.9</cell><cell>45.5</cell><cell>27.9</cell><cell>10.1</cell></row><row><cell cols="4">Mask-RCNN top 39.4 46.1</cell><cell>46.4</cell><cell>31.6</cell><cell>23.2</cell></row><row><cell>RMI</cell><cell></cell><cell cols="2">21.1 23.7</cell><cell>28.4</cell><cell>12.7</cell><cell>5.5</cell></row><row><cell>MattNet</cell><cell></cell><cell cols="2">20.2 19.3</cell><cell>24.9</cell><cell>14.8</cell><cell>10.6</cell></row><row><cell>Model</cell><cell>all</cell><cell>att</cell><cell>att+</cell><cell>rel</cell><cell cols="2">rel+ stuff</cell><cell>obj</cell></row><row><cell>HULANet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cat</cell><cell>39.9</cell><cell>37.6</cell><cell>37.4</cell><cell>32.3</cell><cell cols="3">33.0 47.2 33.9</cell></row><row><cell>cat+att</cell><cell>41.3</cell><cell>39.1</cell><cell>38.8</cell><cell>33.7</cell><cell cols="3">33.8 48.4 35.5</cell></row><row><cell>cat+rel</cell><cell>41.1</cell><cell>38.8</cell><cell>38.4</cell><cell>33.8</cell><cell cols="3">34.0 48.1 35.4</cell></row><row><cell>cat+att+rel</cell><cell>41.3</cell><cell>39.0</cell><cell>38.5</cell><cell>34.1</cell><cell cols="3">33.9 48.3 35.6</cell></row><row><cell cols="2">Mask-RCNN self 36.2</cell><cell>34.5</cell><cell>34.7</cell><cell>29.0</cell><cell cols="3">30.8 44.4 29.5</cell></row><row><cell cols="2">Mask-RCNN top 39.4</cell><cell>37.3</cell><cell>36.6</cell><cell>31.9</cell><cell cols="3">32.6 46.4 33.6</cell></row><row><cell>RMI</cell><cell>21.1</cell><cell>19.0</cell><cell>21.0</cell><cell>11.6</cell><cell cols="3">12.2 31.1 13.0</cell></row><row><cell>MattNet</cell><cell>20.2</cell><cell>19.0</cell><cell>18.9</cell><cell>15.6</cell><cell cols="3">15.1 25.5 16.0</cell></row><row><cell>Model</cell><cell>all</cell><cell cols="6">single multi many small mid large</cell></row><row><cell>HULANet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cat</cell><cell>39.9</cell><cell>41.2</cell><cell>37.0</cell><cell>34.3</cell><cell cols="3">15.1 40.3 67.6</cell></row><row><cell>cat+att</cell><cell>41.3</cell><cell>42.6</cell><cell>38.6</cell><cell>35.9</cell><cell cols="3">17.1 42.0 68.0</cell></row><row><cell>cat+rel</cell><cell>41.1</cell><cell>42.5</cell><cell>38.2</cell><cell>35.5</cell><cell cols="3">17.1 41.5 68.2</cell></row><row><cell>cat+att+rel</cell><cell>41.3</cell><cell>42.6</cell><cell>38.4</cell><cell>35.7</cell><cell cols="3">17.3 41.7 68.2</cell></row><row><cell cols="2">Mask-RCNN self 36.2</cell><cell>37.2</cell><cell>34.1</cell><cell>29.9</cell><cell cols="3">17.0 35.7 59.4</cell></row><row><cell cols="2">Mask-RCNN top 39.4</cell><cell>40.6</cell><cell>36.8</cell><cell>33.4</cell><cell cols="3">18.5 39.3 63.6</cell></row><row><cell>RMI</cell><cell>21.1</cell><cell>23.1</cell><cell>16.9</cell><cell>12.7</cell><cell>1.2</cell><cell cols="2">18.6 49.5</cell></row><row><cell>MattNet</cell><cell>20.2</cell><cell>22.2</cell><cell>15.9</cell><cell>12.6</cell><cell>6.1</cell><cell cols="2">18.9 39.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The project is supported in part by NSF Grants 1749833 and 1617917, and Faculty awards from Adobe. Our experiments were performed on the UMass GPU cluster obtained under the Collaborative Fund managed by the Massachusetts Technology Collaborative.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>The supplementary material provides details on the data collection pipeline and the long-tail distribution of concepts in the dataset. We also visualize more results from our proposed HULANet, including predictions from individual modules, failure cases, and comparison against baselines.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-level multimodal common semantic space for image-phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GraphGround: Graph-Based Language Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GraphGround: Graph-Based Language Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Align2ground: Weakly supervised phrase grounding guided by image-caption alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karuna</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural sequential phrase grounding (seqGROUND)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pelin</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical methods in natural language processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Can</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to assemble neural module tree networks for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Comprehensionguided referring expressions. Computer Vision and Pattern Recognition (CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>ArbelÃ¡ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional image-text embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paige</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot grounding of objects from natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weaklysupervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic graph attention for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A fast and accurate onestage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MattNet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1805.03508</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grounding referring expressions in images by variational context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
