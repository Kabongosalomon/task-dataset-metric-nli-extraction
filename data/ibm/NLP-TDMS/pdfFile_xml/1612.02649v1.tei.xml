<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<email>jhoffman@cs.stanford</email>
							<affiliation key="aff0">
								<orgName type="department">CS Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
							<email>dqwang@cs.berkeley</email>
							<affiliation key="aff1">
								<orgName type="department">EECS Department UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">CS Department Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@cs.berkeley</email>
							<affiliation key="aff3">
								<orgName type="department">EECS Department UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a critical visual recognition task for a variety of applications ranging from autonomous agent tasks, such as robotic navigation and self-driving cars, to mapping and categorizing the natural world. As such, a significant amount of recent work has been introduced to tackle the supervised semantic segmentation problem using pixel-wise annotated images to train convolutional networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>While performance is improving for segmentation models trained and evaluated on the same data source, there has yet been limited research exploring the applicability of these models to new related domains. Many of the chal- lenges faced when considering adapting between visual domains for classification, such as changes in appearance, lighting, and pose, are also present when considering adapting for semantic segmentation. In addition, some new factors take on more prominence when considering recognition with localization tasks. In both classification and segmentation, the prevalence of classes may vary between different domains, but this variance can be more exaggerated with semantic segmentation applications as an individual object class may now appear many times within a single scene. For instance, semantic segmentation for self-driving applications will focus on outdoor street scenes with objects of varying sizes, whose distribution may vary between cities or driving routes; in addition appearance statistics can vary considerably when, e.g., adapting a person recognition model trained only using indoor scene images. Moreover, pixel-wise annotations are expensive and tedious to collect, making it particularly appealing to learn to share and transfer information between related settings.</p><p>In this work, we propose the first unsupervised domain adaptation method for transferring semantic segmentation FCNs across image domains. A second contribution of our approach is the combination of global and local alignment methods, using global and category specific adaptation techniques that are themselves individually innovative contributions. We align the global statistics of our source and target data using a convolutional domain adversarial training technique, using a novel extension of previous image-level classification approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Given a domain aligned representation space, we introduce a generalizable constrained multiple instance loss function, which expands on weak label learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref>, but can be applied to the target domain without any extra annotations and explicitly transfers category layout information from a labeled source dataset.</p><p>We evaluate our approach using multiple large scale datasets. We first make use of recently released synthetic drive-cam data from both the GTA5 <ref type="bibr" target="#b27">[28]</ref> and SYN-THIA <ref type="bibr" target="#b28">[29]</ref> datasets, in order to examine a large adaptation shift from simulated to the real images available in CityScapes <ref type="bibr" target="#b2">[3]</ref>. Next, we explore the domain shift of cross season adaptation within the SYNTHIA dataset. We then focus on adaptation across cities in the real world. We perform a detailed quantitative analysis of cross-city adaptation within the CityScapes dataset.</p><p>A final contribution of our paper is the introduction of a new unconstrained drive-cam dataset for semantic segmentation, Berkeley Deep Driving Segmentation (BDDS). Below we demonstrate initial qualitative adaptation results from Cityscapes cities to the cities in BDDS. Across all of these studies, we show that our adaptation algorithm improves the target semantic segmentation performance without any target annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation Semantic segmentation is a key computer vision task and has been studied in a plethora of publications. Following the success of large-scale image classification, most current semantic segmentation models use some convolutional network architecture <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> with many recent approaches using fully convolutional networks (FCNs) <ref type="bibr" target="#b19">[20]</ref> to map the input RGB space to a semantic pixel space. These models are compelling because they al-low a direct end-to-end function that can be trained using back propagation. The original FCN formulation has since been improved using dilated convolution <ref type="bibr" target="#b32">[33]</ref> and postprocessing techniques, such as Markov/conditional random fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Motivated by the high cost of collecting pixel level supervision, a related body of work has explored using weak labels (typically image-level tags defining presence / absence of each class), to improve semantic segmentation performance. Pathak et al. <ref type="bibr" target="#b25">[26]</ref> and Pinheiro et al. <ref type="bibr" target="#b26">[27]</ref> modeled this problem as multiple instance learning (MIL) and reinforce confident predictions during the learning process. An improved method was suggested by <ref type="bibr" target="#b23">[24]</ref> who use an EM algorithm to better model global properties of the image segments. This work was in turn generalized by Pathak et al. who proposed a Constrained CNN which is able to model any linear constraints on the label space (i.e. presence / absence, percent cover) <ref type="bibr" target="#b24">[25]</ref>. In another recent paper <ref type="bibr" target="#b14">[15]</ref>, Hong et al. used auxiliary segmentation to generalize semantic segmentations to categories where only weak label information was available.</p><p>From a domain adaptation perspective, these methods all assume that weak labels are present during training time for both source domain and target domain. In this work, we consider a related, but different learning scenario: strong supervision is available in the source domain, but that no supervision is available in the target domain.</p><p>Domain Adaptation Domain adaptation in computer vision has focused largely on image classification, with much work dedicated to generalizing across the domain shift between stock photographs of objects and the same objects photographed in the world <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref>. Recent work includes <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> which all learn a feature representation which encourages maximal confusion between the two domains. Other work aims to align the features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> by minimizing the distance between their distributions in the two domains. Based on Generative Adversarial Network <ref type="bibr" target="#b8">[9]</ref>, Liu et al. proposed coupled generative adversarial network to learn a joint distribution of images from both source and target datasets <ref type="bibr" target="#b17">[18]</ref>.</p><p>Much less attention has been given to other important computer vision tasks such as detection and segmentation. In detection, Hoffman et al. proposed a domain adaptation system by explicitly modeling the representation shift between classification and detection models <ref type="bibr" target="#b10">[11]</ref> along with a follow-up work which incorporated per-category adaptation using multiple instance learning <ref type="bibr" target="#b11">[12]</ref>. The detection models were later converted into FCNs for evaluating semantic segmentation performance <ref type="bibr" target="#b12">[13]</ref>, but this work did not propose any segmentation specific adaptation approach. So far as we know, our method is the first to introduce domain adaptation techniques for semantic segmentation models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fully Convolutional Adaptation Models</head><p>In this section, we describe our adaptation algorithm for semantic segmentation using fully convolutional networks (FCNs) across domains which share a common label space. Without loss of generality, our method can be applied to other segmentation models, though we focus here on FCNs due to their broad impact. We consider having access to a source domain, S, with both images, I S , and labels, L S . We train a source only model for semantic segmentation which produces a pixel-wise per-category score map φ S (I S ).</p><p>Our goal is to learn a semantic segmentation model which is adapted for use on the unlabeled target domain, T , with images, I T , but no annotations. We denote the parameters of such as network as φ T (·). If there is no domain shift between the source and target domains then one could simply apply the source model directly to the target with no need for an adaptive approach. However, there is commonly a difference between the distribution of the source labeled domain and the target test domain.</p><p>Therefore, we present an unsupervised adaptation approach. We begin by noting that there are two main opportunities for domain shift. First, global changes may occur between the two domains resulting in a marginal distribution shift of corresponding feature space. This may occur between any two different domains, but will be most distinct in large shifts between very distinct domains, such as adapting between simulated and real domains. The second main shift occurs due to category specific parameter changes. This may result from individual categories having specific biases in the two domains. For example, when adapting between two different cities the distribution of cars and the appearance of signs may change.</p><p>We propose an unsupervised domain adaptation framework for adapting semantic segmentation models which di-rectly tackles both the need for minimizing the global and the category specific shifts. For our model, we first make the necessary assumption that the source and target domains share the same label space and that the source model achieves performance greater than chance on the target domain. Then, we introduce two new semantic segmentation loss objectives, one to minimize the global distribution distance, which operates over both source and target images, L da (I S , I T ). Another to adapt the category specific parameters using target images and transferring label statistics from the source domain P L S , L mi (I T , P L S ). Finally, to ensure that we do not diverge too far from the source solution, which is known to be effective for the final semantic segmentation task, we continue to optimize the standard supervised segmentation objective on the source domain, L seg (I S , L S ). Together, our adaptive learning approach is to optimize the following joint objective:</p><formula xml:id="formula_0">L(I S , L S , I T ) = L seg (I S , L S ) (1) +L da (I S , I T ) + L mi (I T , P L S )</formula><p>We illustrate overall adaptation framework in <ref type="figure" target="#fig_1">Figure 2</ref>. Source domain data is used to update the standard supervised loss objective, trained using the source pixel-wise annotations. Both source and target data are used without any category annotations within fully-convolutional domain adversarial training to minimize the global distance of feature space between the two domains. Finally, category specific updates using a constrained pixel-wise multiple instance learning objective is performed on the target images, with source category statistics used to determine the constraints.</p><p>Note, our approach may be generally applied to any FCN-based semantic segmentation framework. For our experiments, we use the recently proposed front-end dilated fully convolutional network <ref type="bibr" target="#b32">[33]</ref>, based on 16 layers VG-GNet <ref type="bibr" target="#b30">[31]</ref>, as our base model. There are 16 convolutional layers, where the last three convolutional layer converted from fully connected layers, called f c 6 , f c 7 , f c 8 , followed by 8 times bilinear up-sample layer to produce segmentation in the same resolution as input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Domain Alignment</head><p>Here, we propose a new domain adversarial learning objective which may be applied for pixel-wise approaches to aid in learning domain invariant representations for semantic segmentation models. The first question to answer is what should comprise an instance within the dense prediction framework. Since recognition is sought at the pixel level alignment of full image representations will marginalize out too much distribution information limiting the alignment capability of the adversarial learning approach.</p><p>Instead, we consider the region corresponding to the natural receptive field of each spatial unit in the final representation layer (e.g. f c 7 ), as individual instances. In doing so, we directly supply our adversarial training procedure with the same information which is used to do final pixel prediction. Therefore, this provides a more meaningful view of the overall source and target pixel-space representation distribution distance which needs to be minimize.</p><p>Let φ −1 (θ, I) denote the output of the last layer before pixel prediction according to network parameters, θ. Then, our domain adversarial loss, L da (I S , I T ) consists of alternating minimization objectives. One concerning the parameters of the representation space, θ, under which we would like to minimize the observed source and target distance,</p><formula xml:id="formula_1">min d(φ −1 (θ, I S ), φ −1 (θ, I T )</formula><p>, for a given distance function, d(·). The second concerning estimating the distance function through training a domain classifier to distinguish instances of the source and target domains. Let us denote the domain classifier parameters as θ D . We then seek to learn a domain classifier to recognize the difference between source and target regions and use that classifier to guide the distance minimization of the source and target representations.</p><p>Let σ(·) denote the softmax function and let the domain classifier predictions be indicated as p θ D (x) = σ(φ(θ D , x))). Assuming the output of layer −1 has H×W spatial units, then we can define the domain classifier loss, L D , as follows:</p><formula xml:id="formula_2">L D = − I S ∈S h∈H w∈W log(p θ D (R S hw )) (2) − I T ∈T h∈H w∈W log(1 − p θ D (R T hw )) (3) where R S hw = φ −1 (θ, I S ) hw and R T hw = φ −1 (θ, I T )</formula><p>hw denote the source and target representation of each units, respectively.</p><p>For convenience let us also define the inverse domain loss, L Dinv as follows:</p><formula xml:id="formula_3">L Dinv = − I S ∈S h∈H w∈W log(1 − p θ D (R S hw )) (4) − I T ∈T h∈H w∈W log(p θ D (R T hw ))<label>(5)</label></formula><p>Finally, with these definitions, we may now describe the alternating minimization procedure.</p><formula xml:id="formula_4">min θ D L D (6) min θ 1 2 [L D + L Dinv ]<label>(7)</label></formula><p>Optimizing these two objectives iteratively amounts to learning the best possible domain classifier for relevant image regions (Eq <ref type="formula">(6)</ref>) and then using the loss of that domain classifier to inform the training of the image representations so as to minimize the distance between the source and target domains (Eq <ref type="formula" target="#formula_4">(7)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Category Specific Adaptation</head><p>Given our representation which has minimized the global domain distribution distance through our fully convolutional adversarial training objective, the next step is to further adapt our source model through modifying the category specific network parameters. In order to do this, we draw upon recent weak learning literature <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, which introduced a fully convolutional constrained multiple instance learning objective. This work used size and existence constraints to produce a predicted target labeling to use for further training. We present the novel application of such approaches for domain adaptation and generalize the technique for use in our unlabeled setting.</p><p>First, we consider new constraints which are useful for our pixel-wise unsupervised adaptation problem. In particular, we begin by computing per image labeling statistics in the source domain, P L S . Specifically, for each source image which contains class c, we compute the percentage of image pixels which have a ground truth label corresponding to this class. We can then compute a histogram over these percentages and denote the lower 10% boundary as, α c , the average value as δ c , and the upper 10% as γ c . We may then use this distribution to inform our target domain size constraints, thereby explicitly transferring scene layout information from the source to the target domain. For example, in a driving scenario, often the road occupies a large portion of the image while street signs occupy relatively little image real estate. This information is critical to the constrained multiple instance learning procedure. In contrast, prior work used a single size threshold across classes known to be in the image.</p><p>We begin by presenting our constrained multiple instance loss for the case where image-level labels are known. Thus, for a given target image for which a certain class c is present, we impose the following constraints on the output prediction map, p = arg max φ(θ, I T ).</p><formula xml:id="formula_5">δ c ≤ h,w p hw (c) ≤ γ c<label>(8)</label></formula><p>Thus, our constraint encourages pixels to be assigned to class c such that the percentage of the image labeled with class c is within the expected range observed in the source domain. Practically, we optimize this objective with lower bound slack to allow for outlier cases where c simply occupies less of the image than is average in the source domain. However, we do not allow slack on the upper bound constraint as it is important that no single class occupies too much of any given image. Notice that our updated constraint is general and can be equivalently applied to all classes regardless if they correspond with traditional object notion (e.g. bikes or people) or stuff notion (e.g. sky or vegetation).</p><p>Given this constraint we may now optimize for a new class prediction space to use for future learning. For the specific optimization details we refer the reader to Pathak et al. <ref type="bibr" target="#b24">[25]</ref>. We provide one important modification. As we seek to optimize over both object and stuff categories, we note that the relative number of pixels devoted to each may vary significantly which could cause the model to diverge, over-fitting to those classes which are highly represented in the images. Instead, we use a simple size constraint that if the lower 10% of the source class distribution, α c , is greater than 0.1, then we down-weight the gradients due to these classes by a factor of 0.1. This is re-weighting approach can be viewed as a re-sampling of the classes so as to come closer to a balanced set, allowing the relatively small classes potential to inform the learning objective.</p><p>While the approach described above describes a generalized constrained multiple instance objective, it relies on known image-level labels. Since we lack such information in our unsupervised adaptation setting, we now describe our procedure for predicting image level labels. Thus, our complete approach can be described as first predicting imagelevel labels and then optimizing for pixel predictions which satisfy the source transferred class size constraints.</p><p>In contrast to weakly-supervised settings, we do not learn a segmentation model from scratch with known image-level annotations. Instead, we have access to a fully supervised source dataset and use domain transferred constraints to facilitate transfer to an unsupervised target domain. Thus we both have a stronger initial model with a fully supervised using pixel-level annotations from source domain and are additionally able to regularize the learning procedure by training with weak label loss on target domain. Again, given a target image, I T , we compute the current output class prediction map, p = arg max φ(θ, I T ). For each class we compute the percentage of pixels assigned to that class in our current prediction, d c = 1 H·W h∈H w∈W (p hw = c). Finally, we assign an image-level label to class c if d c &gt; 0.1 * α c , meaning if we currently label at least as many pixels as 10% of the expected number for a true class appearing in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we report our experimental results on three different domain adaptation tasks: cities → cities, season → season, and synthetic → real, studied across four different datasets. We analyze both our overall adaptation approach as well as the sub-components to verify that both our global and category specific alignment offer meaningful contributions.</p><p>For all experiments we use the front-end dilated fully convolutional network <ref type="bibr" target="#b32">[33]</ref> as both the initialization for our method and as the baseline model for comparison. All code and models are trained and evaluated in the Caffe <ref type="bibr" target="#b15">[16]</ref> framework and will be made available before camera-ready.</p><p>For fair comparison, we use the Intersection over Union (IoU) evaluation metric for all experiments. For cities → cities and synthetic → real tasks, we followed the evaluation protocol of <ref type="bibr" target="#b2">[3]</ref> and train our models with 19 semantic labels of Cityscapes. For season → season task, we use 13 semantic labels of SYNTHIA instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Cityscapes contains 34 categories in high resolution, 2048 × 1024. The whole dataset is divided into three parts: 2, 975 training samples, 500 validation samples and 1, 525 test samples. The split of this dataset is city-level, which covers individual European cities in different geographic and population distribution.</p><p>SYNTHIA contains 13 classes with different scenarios and sub-conditions. As for season → season task, we regard SYNTHIA-VIDEO-SEQUENCES as play ground. There are 7 sequences, covering different scenarios (highway, roundabout, mountain path, New York City, Old European Town) with several sub-sequences, such as seasons(Spring, Summer, Fall, Winter), weathers(Rain, Soft-Rain, Fog), and illuminations(Sunset, Dawn, Night). These frames are captured by 8 RGB cameras forming a binocular 360 • visual field. In order to minimize the impact of viewpoint, we only pick up the dashcam-like frames for all the time. As for synthetic → real task, we take SYNTHIA-RAND-CITYSCAPES, providing 9, 000 random images from all the sequences with Cityscape-compatible annotations, as source domain data.</p><p>GTA5 contains 24, 966 high quality labeled frames from realistic open-world computer games, Grand Theft Auto V (GTA5). Each frame, with high resolution 1914 × 1052, is generated from fictional city of Los Santos, based on Los Angeles in Southern California. We take the whole dataset with labels compatible to Cityscapes categories for synthetic → real adaptation.</p><p>BDDS contains thousands of dense annotated dashcam video frames and hundreds of thousands of unlabeled frames. Each sample, with high resolution 1280 × 720, provides 34 categories compatible to Cityscapes label space. <ref type="table">Method   road  sidewalk  building   wall  fence  pole  t light  t sign  veg  terrain  sky  person  rider  car  truck  bus  train  mbike</ref>   <ref type="table">Table 1</ref>: Adaptation from synthetic to real. We study the performance using GTA5 and SYNTHIA as source labeled training data adapted and Cityscapes train as an unlabeled target domain, while evaluating our adaptation algorithm on Cityscapes val. Meanwhile, we show an ablation of the components of our method and how each contributes to the overall performance of our approach. Here GA represents global domain alignment and CA indicates category specific adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5 → Cityscapes</head><p>The majority of our data comes from New York and San Francisco, which are the representative of eastern and western coasts. Different from the other existing driving datasets, this dataset covers diverse driving scenarios under different conditions, such as urban street view at night, highway scene in rain and so on, providing challenging domain adaptation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative and Qualitative Results</head><p>We broadly study three types of shifts. First we study a large distribution shift, as seen when adapting from simulated to real imagery. Next, we study a medium sized shift, through adaptation across season patterns observed within the SYNTHIA dataset. Finally, we explore situations of relatively smaller domain shift, though exploring adaptation between different cities within the CityScapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Large Shift: Synthetic to Real Adaptation</head><p>We begin the evaluation of our method by studying the large domain shift of adapting between simulated driving data and real world drive-cam data. <ref type="table">Table 1</ref> shows semantic segmentation performance for the shift between GTA5 to CityScapes and between SYNTHIA to CityScapes. This illustrates that even with this large domain difference our unsupervised adaptation solution is capable of improving the performance of the source dilation model. Notice that for this larger shift setting, such as GTA5→Cityscapes, the domain adversarial training contributes 4.4% raw and ∼20% relative percentage mIoU improvement and multiple instance loss contributes yet another 1.6% raw and ∼6% relative percentage mIoU improvement. As for SYNTHIA→Cityscapes, our method also offers a measurable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Medium Shift: Cross Seasons Adaptation</head><p>As our next experiment, we seek to analyze adaptation across season patterns. To this end, we use the SYNTHIA dataset which has synthetic images available along with season annotations. We first produce one domain per each of the season labels available: Summer, Fall and Winter. We then perform adaptation across each of the 6 shifts and report the performance of our method in comparison to the source dilation model in <ref type="table">Table 2</ref>. On average we get ∼3 percentage mIoU improvement for season → season adaptation and find that for 12/13 object categories our adaptation method provides higher mIoU. The one class we saw no improvement after adaptation is for car. We presume this results from the fact that cars have little or no appearance difference across seasons in this synthetic dataset. For example, consider the qualitative results shown in <ref type="figure" target="#fig_3">Figure 3</ref> for the shift of fall to winter. While the roads and side-walks have been rendered in a white to simulate snow in winter, the cars are rendered in the same appearance as in fall. In fact some of the largest performance improvements we saw from our method we in categories like road in the shift of fall to winter, and our method is able to overcome this large appearance shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Small Shift: Cross City Adaptation</head><p>For our third quantitative experiment we move towards studying cross city adaptation within the CityScapes dataset. In <ref type="table">Table 3</ref> we report performance on the task of adapting between the labeled cities in the CityScapes train to the unlabeled cities in either the Cityscapes val. The top row shows the performance of the dilation frontend model <ref type="bibr" target="#b32">[33]</ref>. We report performance after only global alignment through domain adversarial training (indicated   <ref type="table">Table 2</ref>: Adaptation across seasons. We study the cross season performance using sub-sequences of SYNTHIA dataset. We report quantitative comparisons of performance before and after adaptation for training on one season and evaluating on another unannotated novel season. (Avg: the average performance of adaptation from one to another.)</p><p>as Our method (GA only)) and after the category specific alignment with the constrained multiple instance loss (indicated as Our method (GA+CA)). We note that for this adaptation experiment the majority of the improvement from our method is as a result of the domain adversarial training (3.6 percentage mIoU) whereas after category specific alignment only offers a noticeable improvement on the categories of traffic light, rider and train. One reason could be that the domain shift between train and val mainly results from a change in the global appearance, due to the difference in city, whereas the specific category appearance may not change that significantly. Since per-formance on this within dataset adaptation is already quite high, the primary improvements arise from producing more consistent within object segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">BDDS Adaptation</head><p>Finally, we analyze another real world cities → cities adaptation using our new large scale driving image dataset BDDS. To understand this difficulty and evaluate our methods more extensively, we create a new image dataset based on dash cam videos. Although CityScapes covers various cities in Germany and neighboring countries, we observe that the cities in the other places have different visual ap-  <ref type="table">Table 3</ref>: Adaptation across cities. We study the performance using Cityscapes train cities as source labeled training data adapted and evaluate our adaptation algorithm on Cityscapes val as unlabeled target domains. Meanwhile, we show an ablation of the components of our method and how each contributes to the overall performance of our approach. Here GA indicates global domain alignment in section 3.1 and CA represents category specific adaptation in section 3.2. pearance and street layout. They may post serious challenges to the models learned from CityScapes. Up to now, we have collected more than 100, 000 images covering outdoor scenes at different time and places. Based on the current annotation progress, there would 5, 000∼10, 000 images with fine segmentation annotation before CVPR 2017. We aim for 10, 000∼20, 000 finely segmented street scene images eventually. We take ∼60, 000 images in area of San Francisco from BDDS and study how well we can adapt the model learned on Cityscapes to San Francisco. Because our methods don't require labels in the target domain, we can use all the new images in our training the adaptation. Some results are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. From these qualitative results, we observe that there is a significant segmentation quality drop when a model trained on Cityscapes is used in BDDS directly. It usually appears as noisy segmentation or wrong context. After adaptation, the segmentation results usually become much cleaner. We expect to conduct extensive quantitative evaluation when annotations are ready.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an unsupervised domain adaptation framework with fully convolutional networks for semantic segmentation. We propose fully convolutional networks with domain adversarial training for global domain alignment, while leveraging class-aware constrained multiple instance loss for transferring spatial layout. We demonstrate the effectiveness of our method on domain shifts between different cities, seasons and from synthetic to real, and we offer a new large-scale real-city driving image dataset. While the task of image classification has seen the bulk of the effort in developing domain adaptation methods, our experiments demonstrate the importance of adaptation in pixel-level dense prediction as well. Our approach is the first step in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Unsupervised domain adaptation for pixel-level semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our pixel-level adversarial and constraint-based adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>bike mIoU Dialation Frontend [33] 31.9 18.9 47.7 7.4 3.1 16.0 10.4 1.0 76.5 13.0 58.9 36.0 1.0 67.1 9.5 3.7 0.0 0.0 0.0 21.1 Our Method (GA only) 67.4 29.2 64.9 15.6 8.4 12.4 9.8 2.7 74.1 12.8 66.8 38.1 2.3 63.0 9.4 5.1 0.0 3.5 0.0 25.5 Our Method (GA + CA) 70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0 3.5 0.0 27.1 SYNTHIA → Cityscapes Dialation Frontend [33] 6.4 17.7 29.7 1.2 0.0 15.1 0.0 7.2 30.3 0.0 66.8 51.1 1.5 47.3 0.0 3.9 0.0 0.1 0.0 14.7 Our Method (GA only) 11.5 18.3 33.3 6.1 0.0 23.1 0.0 11.2 43.6 0.0 70.5 45.5 1.3 45.1 0.0 4.6 0.0 0.1 0.5 16.6 Our Method (GA + CA) 11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 0.0 68.7 51.2 3.8 54.0 0.0 3.2 0.0 0.2 0.6 17.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on adaptation from cities in SYNTHIA fall to cities in SYNTHIA winter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on adaptation from cities in Cityscapes to cities in BDDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Cityscapes train → Cityscapes val Dialation Frontend [33] 96.2 76.0 88.4 32.5 46.4 53.5 52.0 68.7 88.6 46.6 91.0 74.8 46.0 90.5 46.9 58.0 44.7 45.2 70.3 64.0 Our Method (GA Only) 97.0 79.6 89.6 42.8 49.9 55.0 55.2 70.2 91.2 59.8 92.5 75.4 46.5 91.6 51.4 66.0 49.3 48.9 71.6 67.6 Our Method (GA + CA) 97.0 79.6 89.8 42.2 49.0 55.4 56.3 70.1 91.2 59.8 92.6 75.5 48.1 91.7 50.4 65.8 53.2 48.0 71.6 67.8</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t light</cell><cell>t sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We begin by describing in more detail our global domain alignment objective, L da (I S , I T ). Recall, that we seek to minimize the domain shift between representations of the source and target data. A recent line of research has shown that the domain discrepancy distance may be minimized through an adversarial learning procedure, whereby simultaneously a domain classifier is trained to best distinguish the source and target distributions and the representation space is updated according to the inverse objective<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. The approaches heretofore have been introduced for classification models where each individual instance in the domain corresponds exactly to an image.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinsk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale visual recognition through adaptation using joint representation and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
