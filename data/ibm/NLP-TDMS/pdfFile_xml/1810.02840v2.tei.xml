<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Complex Models with Multi-Task Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-10">December 10, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
							<email>ajratner@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Braden</forename><surname>Hancock</surname></persName>
							<email>bradenjh@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
							<email>jdunnmon@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
							<email>fredsala@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyash</forename><surname>Pandey</surname></persName>
							<email>shreyash@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training Complex Models with Multi-Task Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-10">December 10, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the greatest roadblocks to using modern machine learning models is collecting hand-labeled training data at the massive scale they require. In real-world settings where domain expertise is needed and modeling goals change frequently, hand-labeling training sets is prohibitively slow, expensive, and static. For these reasons, practitioners are increasingly turning to weak supervision techniques wherein noisier, often programmatically-generated labels are used instead. Common weak supervision sources include external knowledge bases <ref type="bibr">[24; 37; 8; 31</ref>], heuristic patterns <ref type="bibr">[14; 27]</ref>, feature annotations <ref type="bibr">[23; 36]</ref>, and noisy crowd labels <ref type="bibr">[17; 11]</ref>. The use of these sources has led to state-of-the-art results in a range of domains <ref type="bibr">[37; 35]</ref>. A theme of weak supervision is that using the full diversity of available sources is critical to training high-quality models <ref type="bibr">[27; 37]</ref>.</p><p>The key technical difficulty of weak supervision is determining how to combine the labels of multiple sources that have different, unknown accuracies, may be correlated, and may label at different levels of granularity. In our experience with users in academia and industry, the complexity of real world weak supervision sources makes this integration phase the key time sink and stumbling block. For example, if we are training a model to classify entities in text, we may have one available source of high-quality but coarse-grained labels (e.g. "Person" vs. "Organization") and one source that provides lower-quality but finer-grained labels (e.g. "Doctor" vs. "Lawyer"); moreover, these sources might be correlated due to some shared component or data source <ref type="bibr">[2; 33]</ref>. Handling such diversity requires addressing a core technical challenge: estimating the unknown accuracies of multi-granular and potentially correlated supervision sources without any labeled data.</p><p>To overcome this challenge, we propose MeTaL, a framework for modeling and integrating weak supervision sources with different unknown accuracies, correlations, and granularities. In MeTaL, we view each source as labeling one of several related sub-tasks of a problem-we refer to this as the multi-task weak supervision setting. We then show that given the dependency structure of the sources, we can use their observed agreement and disagreement rates to recover their unknown accuracies. Moreover, we exploit the relationship structure between tasks to observe additional cross-task agreements and disagreements, effectively providing extra signal from which to learn. In contrast to previous approaches based on sampling from the posterior of a graphical model directly <ref type="bibr">[28; 2]</ref>, we develop a simple and scalable matrix completion-style algorithm, which we are able to analyze by applying strong matrix concentration bounds <ref type="bibr" target="#b31">[32]</ref>. We use this algorithm to learn and model the accuracies <ref type="figure">Figure 1</ref>: A schematic of the MeTaL pipeline. To generate training data for an end model, such as a multi-task model as in our experiments, the user inputs a task graph G task defining the relationships between task labels Y 1 , ..., Y t ; a set of unlabeled data points X; a set of multi-task weak supervision sources s i which each output a vector λ i of task labels for X; and the dependency structure between these sources, G source . We train a label model to learn the accuracies of the sources, outputting a vector of probabilistic training labelsỸ for training the end model. of diverse weak supervision sources, and then combine their labels to produce training data that can be used to supervise arbitrary models, including increasingly popular multi-task learning models <ref type="bibr">[5; 29]</ref>.</p><p>Compared to previous methods which only handled the single-task setting <ref type="bibr">[28; 27]</ref>, and generally considered conditionally-independent sources <ref type="bibr">[1; 11]</ref>, we demonstrate that our multi-task aware approach leads to average gains of 4.1 points in accuracy in our experiments, and has at least three additional benefits. First, many dependency structures between weak supervision sources may lead to non-identifiable models of their accuracies, where a unique solution cannot be recovered. We provide a compiler-like check to establish identifiability-i.e. the existence of a unique set of source accuracies-for arbitrary dependency structures, without resorting to the standard assumption of non-adversarial sources <ref type="bibr" target="#b10">[11]</ref>, alerting users to this potential stumbling block that we have observed in practice. Next, we provide sample complexity bounds that characterize the benefit of adding additional unlabeled data and the scaling with respect to the user-specified task and dependency structure. While previous approaches required thousands of sources to give non-vacuous bounds, we capture regimes with small numbers of sources, better reflecting the real-world uses of weak supervision we have observed. Finally, we are able to solve our proposed problem directly with SGD, leading to over 100× faster runtimes compared to prior Gibbs-sampling based approaches <ref type="bibr">[28; 26]</ref>, and enabling simple implementation using libraries like PyTorch.</p><p>We validate our framework on three fine-grained classification tasks in named entity recognition, relation extraction, and medical document classification, for which we have diverse weak supervision sources at multiple levels of granularity. We show that by modeling them as labeling hierarchically-related sub-tasks and utilizing unlabeled data, we can get an average improvement of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a basic majority voting weak supervision baseline, and 4.1 points over data programming <ref type="bibr" target="#b27">[28]</ref>, an existing weak supervision approach in the literature that is not multi-task-aware. We also extend our framework to handle unipolar sources that only label one class, a critical aspect of weak supervision in practice that leads to an average 2.8 point contribution to our gains over majority vote. From a practical standpoint, we argue that our framework represents an efficient way for practitioners to supervise modern machine learning models, including new multi-task variants, for complex tasks by opportunistically using the diverse weak supervision sources available to them. To further validate this, we have released an open-source implementation of our framework. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds on and extends various settings studied in machine learning.</p><p>Weak Supervision: We draw motivation from recent work which models and integrates weak supervision using generative models <ref type="bibr">[28; 27; 2]</ref> and other methods <ref type="bibr">[13; 18]</ref>. These approaches, however, do not handle multigranularity or multi-task weak supervision, require expensive sampling-based techniques that may lead to nonidentifiable solutions, and leave room for sharper theoretical characterization of weak supervision scaling properties. More generally, our work is motivated by a wide range of specific weak supervision techniques, which include traditional distant supervision approaches <ref type="bibr">[24; 8; 37; 15; 31]</ref>, co-training methods <ref type="bibr" target="#b3">[4]</ref>, pattern-based supervision <ref type="bibr">[14; 37]</ref>, and feature-annotation techniques <ref type="bibr">[23; 36; 21]</ref>. <ref type="figure">Figure 2</ref>: An example fine-grained entity classification problem, where weak supervision sources label three sub-tasks of different granularities: (i) Person vs. Organization, (ii) Doctor vs. Lawyer (or N/A), (iii) Hospital vs. Office (or N/A). The example weak supervision sources use a pattern heuristic and dictionary lookup respectively. Crowdsourcing: Our approach also has connections to the crowdsourcing literature <ref type="bibr">[17; 11]</ref>, and in particular to spectral and method of moments-based approaches <ref type="bibr">[38; 9; 12; 1]</ref>. In contrast, the goal of our work is to support and explore settings not covered by crowdsourcing work, such as sources with correlated outputs, the proposed multi-task supervision setting, and regimes wherein a small number of labelers (weak supervision sources) each label a large number of items (data points). Moreover, we theoretically characterize the generalization performance of an end model trained with the weakly labeled data.</p><p>Multi-Task Learning: Our proposed approach is motivated by recent progress on multi-task learning models <ref type="bibr">[5; 29; 30]</ref>, in particular their need for multiple large hand-labeled training datasets. We note that the focus of our paper is on generating supervision for these models, not on the particular multi-task learning model being trained, which we seek to control for by fixing a simple architecture in our experiments.</p><p>Our work is also related to recent techniques for estimating classifier accuracies without labeled data in the presence of structural constraints <ref type="bibr" target="#b25">[26]</ref>. We use matrix structure estimation <ref type="bibr" target="#b21">[22]</ref> and concentration bounds <ref type="bibr" target="#b31">[32]</ref> for our core results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Programming Machine Learning with Weak Supervision</head><p>As modern machine learning models become both more complex and more performant on a range of tasks, developers increasingly interact with them by programmatically generating noisier or weak supervision. These approaches of effectively programming machine learning models have recently been formalized by the following pipeline <ref type="bibr">[28; 27]</ref>: First, users provide one or more weak supervision sources, which are applied to unlabeled data to generate a set of noisy labels. These labels may overlap and conflict; we model and combine them via a label model in order to produce a final set of training labels. These labels are then used to train some discriminative model, which we refer to as the end model. This programmatic weak supervision approach can utilize sources ranging from heuristic rules to other models, and in this way can also be viewed as a pragmatic and flexible form of multi-source transfer learning.</p><p>In our experiences with users from science and industry, we have found it critical to utilize all available sources of weak supervision for complex modeling problems, including ones which label at multiple levels of granularity. However, this diverse, multi-granular weak supervision does not easily fit into existing paradigms. We propose a formulation where each weak supervision source labels some sub-task of a problem, which we refer to as the multi-task weak supervision setting. We consider an example:</p><p>Example 1 A developer wants to train a fine-grained Named Entity Recognition (NER) model to classify mentions of entities in the news <ref type="figure">(Figure 2</ref>). She has a multitude of available weak supervision sources which she believes have relevant signal for her problem-for example, pattern matchers, dictionaries, and pre-trained generic NER taggers. However, it is unclear how to properly use and combine them: some of them label phrases coarsely as PERSON versus ORGANIZATION, while others classify specific fine-grained types of people or organizations, with a range of unknown accuracies. In our framework, she can represent them as labeling tasks of different granularities-e.g. Y 1 = {Person, Org}, Y 2 = {Doctor, Lawyer, N/A}, Y 3 = {Hospital, Office, N/A}, where the label N/A applies, for example, when the type-of-person task is applied to an organization.</p><p>In our proposed multi-task supervision setting, the user specifies a set of structurally-related tasks, and then provides a set of weak supervision sources which are user-defined functions that either label each data point or abstain for each task, and may have some user-specified dependency structure. These sources can be arbitrary black-box functions, and can thus subsume a range of weak supervision approaches relevant to both text and other data modalities, including use of pattern-based heuristics, distant supervision <ref type="bibr" target="#b23">[24]</ref>, crowd labels, other weak or <ref type="figure">Figure 3</ref>: An example of a weak supervision source dependency graph G source (left) and its junction tree representation (right), where Y is a vector-valued random variable with a feasible set of values, Y ∈ Y. Here, the output of sources 1 and 2 are modeled as dependent conditioned on Y. This results in a junction tree with singleton separator sets, Y. Here, the observable cliques are</p><formula xml:id="formula_0">Y λ 1 λ 2 λ 3 λ 4 Y, λ 1 , λ 2 Y, λ 3 Y, λ 4 Y Y</formula><formula xml:id="formula_1">O = {λ 1 , λ 2 , λ 3 , λ 4 , {λ 1 , λ 2 }} ⊂ C.</formula><p>biased classifiers, declarative rules over unsupervised feature extractors <ref type="bibr" target="#b32">[33]</ref>, and more. Our goal is to estimate the unknown accuracies of these sources, combine their outputs, and use the resulting labels to train an end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Multi-Task Weak Supervision</head><p>The core technical challenge of the multi-task weak supervision setting is recovering the unknown accuracies of weak supervision sources given their dependency structure and a schema of the tasks they label, but without any ground-truth labeled data. We define a new algorithm for recovering the accuracies in this setting using a matrix completion-style optimization objective. We establish conditions under which the resulting estimator returns a unique solution. We then analyze the sample complexity of our estimator, characterizing its scaling with respect to the amount of unlabeled data, as well as the task schema and dependency structure, and show how the estimation error affects the generalization performance of the end model we aim to train. Finally, we highlight how our approach handles abstentions and unipolar sources, two critical scenarios in the weak supervision setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Multi-Task Weak Supervision Estimator</head><p>Problem Setup Let X ∈ X be a data point and Y = [Y 1 , Y 2 , . . . , Y t ] T be a vector of categorical task labels, Y i ∈ {1, . . . , k i }, corresponding to t tasks, where (X, Y) is drawn i.i.d. from a distribution D (for a glossary of all variables used, see Appendix A.1).</p><p>The user provides a specification of how these tasks relate to each other; we denote this schema as the task structure G task . The task structure expresses logical relationships between tasks, defining a feasible set of label vectors Y, such that Y ∈ Y. For example, <ref type="figure">Figure 2</ref> illustrates a hierarchical task structure over three tasks of different granularities pertaining to a fine-grained entity classification problem. Here, the tasks are related by logical subsumption relationships: for example, if Y 2 = DOCTOR, this implies that Y 1 = PERSON, and that Y 3 = N/A, since the task label Y 3 concerns types of organizations, which is inapplicable to persons. Thus, in this task structure,</p><formula xml:id="formula_2">Y = [PERSON, DOCTOR, N/A] T is in Y while Y = [PERSON, N/A,</formula><p>HOSPITAL] T is not. While task structures are often simple to define, as in the previous example, or are explicitly defined by existing resources-such as ontologies or graphs-we note that if no task structure is provided, our approach becomes equivalent to modeling the t tasks separately, a baseline we consider in the experiments.</p><p>In our setting, rather than observing the true label Y, we have access to m multi-task weak supervision sources s i ∈ S which emit label vectors λ i that contain labels for some subset of the t tasks. Let 0 denote a null or abstaining label, and let the coverage set τ i ⊆ {1, . . . , t} be the fixed set of tasks for which the ith source emits non-zero labels, such that λ i ∈ Y τi . For convenience, we let τ 0 = {1, . . . , t} so that Y τ0 = Y. For example, a source from our previous example might have a coverage set τ i = {1, 3}, emitting coarse-grained labels such as λ i = [PERSON, 0, N/A] T . Note that sources often label multiple tasks implicitly due to the constraints of the task structure; for example, a source that labels types of people (Y 2 ) also implicitly labels people vs. organizations (Y 1 = PERSON), and types of organizations (as Y 3 = N/A). Thus sources tailored to different tasks still have agreements and disagreements; we use this additional cross-task signal in our approach.</p><p>The user also provides the conditional dependency structure of the sources as a graph G source = (V, E), where V = {Y, λ 1 , λ 2 , . . . , λ m } <ref type="figure">(Figure 3</ref>). Specifically, if (λ i , λ j ) is not an edge in G source , this means that λ i is independent of λ j conditioned on Y and the other source labels. Note that if G source is unknown, it can be estimated using statistical techniques such as <ref type="bibr" target="#b1">[2]</ref>. Importantly, we do not know anything about the strengths of the correlations in G source , or the sources' accuracies.</p><p>Our overall goal is to apply the set of weak supervision sources S = {s 1 , . . . , s m } to an unlabeled dataset X U consisting of n data points, then use the resulting weakly-labeled training set to supervise an end model f w : X → Y ( <ref type="figure">Figure 1</ref>). This weakly-labeled training set will contain overlapping and conflicting labels, from sources with unknown accuracies and correlations. To handle this, we will learn a label model P µ (Y|λ), parameterized by a vector of source correlations and accuracies µ, which for each data point X takes as input the noisy labels λ = {λ 1 , . . . , λ m } and outputs a single probabilistic label vectorỸ. Succinctly, given a user-provided tuple (X U , S, G source , G task ), our key technical challenge is recovering the parameters µ without access to ground truth labels Y.</p><p>Modeling Multi-Task Sources To learn a label model over multi-task sources, we introduce sufficient statistics over the random variables in G source . Let C be the set of cliques in G source , and define an indicator random variable for the event of a clique C ∈ C taking on a set of values y C :</p><formula xml:id="formula_3">ψ(C, y C ) = 1 {∩ i∈C V i = (y C ) i } , where (y C ) i ∈ Y τi . We define ψ(C) ∈ {0, 1} i∈C (|Yτ i |−1)</formula><p>as the vector of indicator random variables for all combinations of all but one of the labels emitted by each variable in clique C-thereby defining a minimal set of statistics-and define ψ(C) accordingly for any set of cliques C ⊆ C. Then µ = E [ψ(C)] is the vector of sufficient statistics for the label model we want to learn.</p><p>We work with two simplifying conditions in this section. First, we consider the setting where G source is triangulated and has a junction tree representation with singleton separator sets. If this is not the case, edges can always be added to G source to make this setting hold; otherwise, we describe how our approach can directly handle non-singleton separator sets in Appendix A.3.3.</p><p>Second, we use a simplified class-conditional model of the noisy labeling process, where we learn one accuracy parameter for each label value λ i that each source s i emits. This is equivalent to assuming that a source may have a different accuracy on each different class, but that if it emits a certain label incorrectly, it does so uniformly over the different true labels Y. This is a more expressive model than the commonly considered one, where each source is modeled by a single accuracy parameter, e.g. in <ref type="bibr">[11; 28]</ref>, and in particular allows us to capture the unipolar setting considered later on. For further details, see Appendix A.3.4.</p><p>Our Approach The chief technical difficulty in our problem is that we do not observe Y. We overcome this by analyzing the covariance matrix of an observable subset of the cliques in G source , leading to a matrix completionstyle approach for recovering µ. We leverage two pieces of information: (i) the observability of part of Cov [ψ(C)], and (ii) a result from <ref type="bibr" target="#b21">[22]</ref> which states that the inverse covariance matrix Cov [ψ(C)] −1 is structured according to G source , i.e., if there is no edge between λ i and λ j in G source , then the corresponding entries are 0.</p><p>We start by considering two disjoint subsets of C: the set of observable cliques, O ⊆ C-i.e., those cliques not containing Y-and the separator set cliques of the junction tree, S ⊆ C. In the setting we consider in this section, S = {Y} (see <ref type="figure">Figure 3</ref>). We can then write the covariance matrix of the indicator variables for O ∪ S, Cov [ψ(O ∪ S)], in block form, similar to <ref type="bibr" target="#b5">[6]</ref>, as:</p><formula xml:id="formula_4">Cov [ψ(O ∪ S)] ≡ Σ = Σ O Σ OS Σ T OS Σ S<label>(1)</label></formula><p>and similarly define its inverse:</p><formula xml:id="formula_5">K = Σ −1 = K O K OS K T OS K S<label>(2)</label></formula><p>Here,</p><formula xml:id="formula_6">Σ O = Cov [ψ(O)] ∈ R d O ×d O is the observable block of Σ, where d O = C∈O i∈C (|Y τi | − 1). Next, Σ OS = Cov [ψ(O), ψ(S)]</formula><p>is the unobserved block which is a function of µ, the label model parameters that we wish to recover. Finally,</p><formula xml:id="formula_7">Σ S = Cov [ψ(S)] = Cov [ψ(Y)</formula><p>] is a function of the class balance P (Y).</p><p>We make two observations about Σ S . First, while the full form of Σ S is the covariance of the |Y| − 1 indicator variables for each individual value of Y but one, given our simplified class-conditional label model, we in fact only need a single indicator variable for Y (see Appendix A.3.4); thus, Σ S is a scalar. Second, Σ S is a function of the class balance P (Y), which we assume is either known, or has been estimated according to the unsupervised approach we detail in Appendix A.3.5. Thus, given Σ O and Σ S , our goal is to recover the vector Σ OS from which we can recover µ. </p><formula xml:id="formula_8">Σ −1 O + zz T Ω c ← Σ −1 S (1 +ẑ TΣ Oẑ ),Σ OS ←Σ Oẑ / √ĉ µ ←Σ OS +Ê [ψ(Y)]Ê [ψ(O)] return ExpandTied(μ )</formula><p>Applying the block matrix inversion lemma, we have:</p><formula xml:id="formula_9">K O = Σ −1 O + cΣ −1 O Σ OS Σ T OS Σ −1 O ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_10">c = Σ S − Σ T OS Σ −1 O Σ OS −1 ∈ R + . Let z = √ cΣ −1 O Σ OS ;</formula><p>we can then express (3) as:</p><formula xml:id="formula_11">K O = Σ −1 O + zz T<label>(4)</label></formula><p>The right hand side of (4) consists of an empirically observable term, Σ −1 O , and a rank-one term, zz T , which we can solve for to directly recover µ. For the left hand side, we apply an extension of Corollary 1 from <ref type="bibr" target="#b21">[22]</ref> (see Appendix A.3.2) to conclude that K O has graph-structured sparsity, i.e., it has zeros determined by the structure of dependencies between the sources in G source . This suggests an algorithmic approach of estimating z as a matrix completion problem in order to recover an estimate of µ (Algorithm 1). In more detail: let Ω be the set of indices (i, j) where (K O ) i,j = 0, determined by G source , yielding a system of equations,</p><formula xml:id="formula_12">0 = (Σ −1 O ) i,j + zz T i,j for (i, j) ∈ Ω,<label>(5)</label></formula><p>which is now a matrix completion problem. Define ||A|| Ω as the Frobenius norm of A with entries not in Ω set to zero; then we can rewrite (5) as Σ −1 O + zz T Ω = 0. We solve this equation to estimate z, and thereby recover Σ OS , from which we can directly recover the label model parameters µ algebraically.</p><p>Checking for Identifiability A first question is: which dependency structures G source lead to unique solutions for µ? This question presents a stumbling block for users, who might attempt to use non-identifiable sets of correlated weak supervision sources.</p><p>We provide a simple, testable condition for identifiability. Let G inv be the inverse graph of G source ; note that Ω is the edge set of G inv expanded to include all indicator random variables ψ(C). Then, let M Ω be a matrix with dimensions |Ω| × d O such that each row in M Ω corresponds to a pair (i, j) ∈ Ω with 1's in positions i and j and 0's elsewhere.</p><p>Taking the log of the squared entries of (5), we get a system of linear equations M Ω l = q Ω , where l i = log(z 2 i ) and</p><formula xml:id="formula_13">q (i,j) = log(((Σ −1 O ) i,j ) 2 )</formula><p>. Assuming we can solve this system (which we can always ensure by adding sources; see Appendix), we can uniquely recover the z 2 i , meaning our model is identifiable up to sign. Given estimates of the z 2 i , we can see from (5) that the sign of a single z i determines the sign of all other z j reachable from z i in G inv . Thus to ensure a unique solution, we only need to pick a sign for each connected component in G inv . In the case where the sources are assumed to be independent, e.g., <ref type="bibr">[10; 38; 11]</ref>, it suffices to make the assumption that the sources are on average non-adversarial; i.e., select the sign of the z i that leads to higher average accuracies of the sources. Even a single source that is conditionally independent from all the other sources will cause G inv to be fully connected, meaning we can use this symmetry breaking assumption in the majority of cases even with correlated sources. Otherwise, a sufficient condition is the standard one of assuming non-adversarial sources, i.e. that all sources have greater than random accuracy. For further details, see Appendix B.1.</p><p>Source Accuracy Estimation Algorithm Now that we know when a set of sources with correlation structure G source is identifiable, yielding a unique z, we can estimate the accuracies µ using Algorithm 1. We also use the function ExpandTied, which is a simple algebraic expansion of tied parameters according to the simplified class-conditional model used in this section; see Appendix A.3.4 for details. In <ref type="figure" target="#fig_0">Figure 4</ref>, we plot the performance of our algorithm on synthetic data, showing its scaling with the number of unlabeled data points n, the density of pairwise dependencies in G source , and the runtime performance as compared to a prior Gibbs sampling-based approach. Next, we theoretically analyze the scaling of the error ||μ − µ * ||. The runtime of MeTaL is independent of n after an initial matrix multiply, and can thus be multiple orders of magnitude faster than Gibbs sampling-based approaches <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theoretical Analysis: Scaling with Diverse Multi-Task Supervision</head><p>Our ultimate goal is to train an end model using the source labels, denoised and combined by the label modelμ we have estimated. We connect the generalization error of this end model to the estimation error of Algorithm 1, ultimately showing that the generalization error scales as n − 1 2 , where n is the number of unlabeled data points. This key result establishes the same asymptotic scaling as traditionally supervised learning methods, but with respect to unlabeled data points.</p><p>Let Pμ(Ỹ | λ) be the probabilistic label (i.e. distribution) predicted by our label model, given the source labels λ as input, which we compute using the estimatedμ. We then train an end multi-task discriminative model f w : X → Y parameterized by w, by minimizing the expected loss with respect to the label model over n unlabeled data points. Let l(w, X, Y) = 1 t t s=1 l t (w, X, Y s ) be a bounded multi-task loss function such that without loss of generality l(w, X, Y) ≤ 1; then we minimize the empirical noise aware loss:</p><formula xml:id="formula_14">w = argmin w 1 n n i=1 EỸ ∼Pμ(·|λ) l(w, X i ,Ỹ) ,<label>(6)</label></formula><p>and letw be the w that minimizes the true noise-aware loss. This minimization can be performed by standard methods and is not the focus of our paper; let the solutionŵ satisfy E ŵ −w 2 ≤ γ. We make several assumptions, following <ref type="bibr" target="#b27">[28]</ref>: (1) that for some label model parameters µ * , sampling (λ, Y) ∼ P µ * (·) is the same as sampling from the true distribution, (λ, Y) ∼ D; and (2) that the task labels Y s are independent of the features of the end model given λ sampled from P µ * (·), that is, the output of the optimal label model provides sufficient information to discern the true label. Then we have the following result:</p><p>Theorem 1 Letw minimize the expected noise aware loss, using weak supervision source parametersμ estimated with Algorithm 1. Letŵ minimize the empirical noise aware loss with E ŵ −w 2 ≤ γ, w * = min w l(w, X, Y), and let the assumptions above hold. Then the generalization error is bounded by:</p><formula xml:id="formula_15">E [l(ŵ, X, Y) − l(w * , X, Y)] ≤ γ + 4|Y| ||μ − µ * || .</formula><p>Thus, to control the generalization error, we must control ||μ − µ * ||, which we do in Theorem 2:</p><p>Theorem 2 Letμ be an estimate of µ * produced by Algorithm 1 run over n unlabeled data points. Let a :=</p><formula xml:id="formula_16">d O Σ S + d O Σ S 2 λ max (K O ) 1 2 and b := Σ −1 O 2 (Σ −1 O )min .</formula><p>Then, we have:</p><formula xml:id="formula_17">E [||μ − µ * ||] ≤ 16(r − 1)d 2 O 32π n abσ max (M + Ω ) 3 d O aλ −1 min (Σ O ) + 1 κ(Σ O ) + λ −1 min (Σ O ) .</formula><p>Interpreting the Bound We briefly explain the key terms controlling the bound in Theorem 2; more detail is found in Appendix B. Our primary result is that the estimation error scales as n − 1 2 . Next, σ max (M + Ω ), the largest singular value of the pseudoinverse M + Ω , has a deep connection to the density of the graph G inv . The smaller this quantity, the more information we have about G inv , and the easier it is to estimate the accuracies. Next, λ min (Σ O ),  <ref type="table">Table 1</ref>: Performance Comparison of Different Supervision Approaches. We compare the micro accuracy (avg. over 10 trials) with 95% confidence intervals of an end multi-task model trained using the training labels from the hand-labeled development set (Gold Dev), hierarchical majority vote (MV), data programming (DP), and our approach (MeTaL).</p><p>the smallest eigenvalue of the observed covariance matrix, reflects the conditioning of Σ O ; better conditioning yields easier estimation, and is roughly determined by how far away from random guessing the worst weak supervision source is, as well as how conditionally independent the sources are. λ max (K O ), the largest eigenvalue of the upper-left block of the inverse covariance matrix, similarly reflects the overall conditioning of Σ. Finally, (Σ −1 O ) min , the smallest entry of the inverse observed matrix, reflects the smallest non-zero correlation between source accuracies; distinguishing between small correlations and independent sources requires more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extensions: Abstentions &amp; Unipolar Sources</head><p>We briefly highlight two extensions handled by our approach which we have found empirically critical: handling abstentions, and modeling unipolar sources.</p><p>Handling Abstentions. One fundamental aspect of the weak supervision setting is that sources may abstain from labeling a data point entirely-that is, they may have incomplete and differing coverage <ref type="bibr">[27; 10]</ref>. We can easily deal with this case by extending the coverage ranges Y τi of the sources to include the vector of all zeros, 0, and we do so in the experiments.</p><p>Handling Unipolar Sources. Finally, we highlight the fact that our approach models class conditional source accuracies, in particular motivated by the case we have frequently observed in practice of unipolar weak supervision sources, i.e., sources that each only label a single class or abstain. In practice, we find that users most commonly use such unipolar sources; for example, a common template for a heuristic-based weak supervision source over text is one that looks for a specific pattern, and if the pattern is present emits a specific label, else abstains. As compared to prior approaches that did not model class-conditional accuracies, e.g. <ref type="bibr" target="#b27">[28]</ref>, we show in our experiments that we can use our class-conditional modeling approach to yield an improvement of 2.8 points in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our approach on three fine-grained classification problems-entity classification, relation classification, and document classification-where weak supervision sources are available at both coarser and finer-grained levels (e.g. as in <ref type="figure">Figure 2</ref>). We evaluate the predictive accuracy of end models supervised with training data produced by several approaches, finding that our approach outperforms traditional hand-labeled supervision by 20.2 points, a baseline majority vote weak supervision approach by 6.8 points, and a prior weak supervision denoising approach <ref type="bibr" target="#b27">[28]</ref> that is not multi-task-aware by 4.1 points.</p><p>Datasets Each dataset consists of a large (3k-63k) amount of unlabeled training data and a small (200-350) amount of labeled data which we refer to as the development set, which we use for (a) a traditional supervision baseline, and (b) for hyperparameter tuning of the end model (see Appendix C). The average number of weak supervision sources per task was 13, with sources expressed as Python functions, averaging 4 lines of code and comprising a mix of pattern matching heuristics, external knowledge base or dictionary lookups, and pre-trained models. In all three cases, we choose the decomposition into sub-tasks so as to align with weak supervision sources that are either available or natural to express.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition (NER):</head><p>We represent a fine-grained named entity recognition problem-tagging entity mentions in text documents-as a hierarchy of three sub-tasks over the OntoNotes dataset <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_18">Y 1 ∈ {Person, Organization}, Y 2 ∈ {Businessperson, Other Person, N/A}, Y 3 ∈ {Company,</formula><p>Other Org, N/A}, where again we use N/A to represent "not applicable".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction (RE):</head><p>We represent a relation extraction problem-classifying entity-entity relation mentions in text documents-as a hierarchy of six sub-tasks which either concern labeling the subject, object, or subject-object pair of a possible or candidate relation in the TACRED dataset <ref type="bibr" target="#b38">[39]</ref>. For example, we might label a relation as having a Person subject, Location object, and Place-of-Residence relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Document Classification (Doc):</head><p>We represent a radiology report triaging (i.e. document classification) problem from the OpenI dataset <ref type="bibr" target="#b24">[25]</ref> as a hierarchy of three sub-tasks:</p><formula xml:id="formula_19">Y 1 ∈ {Acute, Non-Acute}, Y 2 ∈ {Urgent, Emergent, N/A}, Y 3 ∈ {Normal, Non-Urgent, N/A}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End Model Protocol</head><p>Our goal was to test the performance of a basic multi-task end model using training labels produced by various different approaches. We use an architecture consisting of a shared bidirectional LSTM input layer with pre-trained embeddings, shared linear intermediate layers, and a separate final linear layer ("task head") for each task. Hyperparameters were selected with an initial search for each application (see Appendix), then fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core Validation</head><p>We compare the accuracy of the end multi-task model trained with labels from our approach versus those from three baseline approaches <ref type="table">(Table 1)</ref>:</p><formula xml:id="formula_20">• Traditional Supervision [Gold (Dev)]:</formula><p>We train the end model using the small hand-labeled development set.</p><p>• Hierarchical Majority Vote [MV]: We use a hierarchical majority vote of the weak supervision source labels: i.e.</p><p>for each data point, for each task we take the majority vote and proceed down the task tree accordingly. This procedure can be thought of as a hard decision tree, or a cascade of if-then statements as in a rule-based approach.</p><p>• Data Programming [DP]: We model each task separately using the data programming approach for denoising weak supervision <ref type="bibr" target="#b26">[27]</ref>.</p><p>In all settings, we used the same end model architecture as described above. Note that while we choose to model these problems as consisting of multiple sub-tasks, we evaluate with respect to the broad primary task of finegrained classification (for subtask-specific scores, see Appendix). We observe in <ref type="table">Table 1</ref> that our approach of leveraging multi-granularity weak supervision leads to large gains-20.2 points over traditional supervision with the development set, 6.8 points over hierarchical majority vote, and 4.1 points over data programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations We examine individual factors:</head><p>Unipolar Correction: Modeling unipolar sources (Sec 4.3), which we find to be especially common when finegrained tasks are involved, leads to an average gain of 2.8 points of accuracy in MeTaL performance.</p><p>Joint Task Modeling: Next, we use our algorithm to estimate the accuracies of sources for each task separately, to observe the empirical impact of modeling the multi-task setting jointly as proposed. We see average gains of 1.3 points in accuracy (see Appendix).</p><p>End Model Generalization: Though not possible in many settings, in our experiments we can directly apply the label model to make predictions. In <ref type="table">Table 6</ref>, we show that the end model improves performance by an average 3.4 points in accuracy, validating that the models trained do indeed learn to generalize beyond the provided weak supervision. Moreover, the largest generalization gain of 7 points in accuracy came from the dataset with the most available unlabeled data (n=63k), demonstrating scaling consistent with the predictions of our theory <ref type="figure">(Fig. 5</ref>). This ability to leverage additional unlabeled data and more sophisticated end models are key advantages of the weak supervision approach in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented MeTaL, a framework for training models with weak supervision from diverse, multi-task sources having different granularities, accuracies, and correlations. We tackle the core challenge of recovering the unknown source accuracies via a scalable matrix completion-style algorithm, introduce theoretical bounds characterizing the key scaling with respect to unlabeled data, and demonstrate empirical gains on real-world datasets. In future work, we hope to learn the task relationship structure and cover a broader range of settings where labeled training data is a bottleneck. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Problem Setup &amp; Modeling Approach</head><p>In Section A, we review our problem setup and modeling approach in more detail, and for more general settings than in the body. In Section B, we provide an overview, additional interpretation, and the proofs of our main theoretical results. Finally, in Section C, we go over additional details of our experimental setup.</p><p>We begin in Section A.1 with a glossary of the symbols and notation used throughout this paper. Then, in Section A.2 we present the setup of our multi-task weak supervision problem, and in Section A.3 we present our approach for modeling multi-task weak supervision, and the matrix completion-style algorithm used to estimate the model parameters. Finally, in Section A.4, we present in more detail the subcase of hierarchical tasks considered in the main body of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Glossary of Symbols</head><p>Symbol Used for</p><formula xml:id="formula_21">X Data point, X ∈ X n Number of data points Y s Label for one of the t classification tasks, Y s ∈ {1, . . . , k s } t Number of tasks Y Vector of task labels Y = [Y 1 , Y 2 , . . . , Y t ] T r Cardinality of the output space, r = |Y| G task Task structure graph Y Output space of allowable task labels defined by G task , Y ∈ Y D Distribution from which we assume (X, Y) data points are sampled i.i.d. s i</formula><p>Weak supervision source, a function mapping X to a label vector λ i Label vector λ i ∈ Y output by the ith source for X m</p><p>Number of sources λ m × t matrix of labels output by the m sources for X Y 0</p><p>Source output space, which is Y augmented to include elements set to zero τ i Coverage set of λ i -the tasks s i gives non-zero labels to; for convenience,</p><formula xml:id="formula_22">τ 0 = {1, ..., t} Y τi</formula><p>The output space for λ i given coverage set τ i Y min τi The output space Y τi with all but the first value, for defining a minimal set of statistics G source Source dependency graph,</p><formula xml:id="formula_23">G source = (V, E), V = {Y, λ 1 , ..., λ m } C Cliqueset (maximal and non-maximal) of G sourcẽ C, S</formula><p>The maximal cliques (nodes) and separator sets of the junction tree over G source ψ(C, y C ) The indicator variable for the variables in clique C ∈ C taking on values y C ,</p><formula xml:id="formula_24">(y C ) i ∈ Y τi µ</formula><p>The parameters of our label model we aim to estimate;</p><formula xml:id="formula_25">µ = E [ψ] O</formula><p>The set of observable cliques, i.e. those corresponding to cliques without</p><formula xml:id="formula_26">Y Σ Generalized covariance matrix of O ∪ S, Σ ≡ Cov [ψ(O ∪ S)] K The inverse generalized covariance matrix K = Σ −1 d O , d S</formula><p>The dimensions of O and S respectively G aug</p><p>The augmented source dependencies graph G aug = (ψ, E aug ) Ω</p><p>The edge set of the inverse graph of G aug P Diagonal matrix of class prior probabilities,</p><formula xml:id="formula_27">P (Y) P µ (Y, λ) The label model parameterized by μ Y</formula><p>The probabilistic training label, i.e. P µ (Y|λ) f w (X)</p><p>The end model trained using (X,Ỹ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Problem Setup</head><p>Let X ∈ X be a data point and Y = [Y 1 , Y 2 , . . . , Y t ] T be a vector of task labels corresponding to t tasks. We consider categorical task labels, Y i ∈ {1, . . . , k i } for i ∈ {1, . . . , t}. We assume (X, Y) pairs are sampled i.i.d. from distribution D; to keep the notation manageable, we do not place subscripts on the sample tuples.</p><p>Task Structure The tasks are related by a task graph G task . Here, we consider schemas expressing logical relationships between tasks, which thus define feasible sets of label vectors Y, such that Y ∈ Y. We let r = |Y| be the number of feasible task vectors. In section A.4, we consider the particular subcase of a hierarchical task structure as used in the experiments section of the paper.</p><p>Multi-Task Sources We now consider multi-task weak supervision sources s i ∈ S, which represent noisy and potentially incomplete sources of labels, which have unknown accuracies and correlations. Each source s i outputs label vectors λ i , which contain non-zero labels for some of the tasks, such that λ i is in the feasible set Y but potentially with some elements set to zero, denoting a null vote or abstention for that task. Let Y 0 denote this extended set which includes certain task labels set to zero.</p><p>We also assume that each source has a fixed task coverage set τ i , such that (λ i ) s = 0 for s ∈ τ i , and (λ i ) s = 0 for s / ∈ τ i ; let Y τi ⊆ Y 0 be the range of λ i given coverage set τ i . For convenience, we let τ 0 = {1, . . . , t} so that Y τ0 = Y. The intuitive idea of the task coverage set is that some labelers may choose not to label certain tasks; Example 2 illustrates this notion. Note that sources can also abstain for a data point, meaning they emit no label (which we denote with a symbol 0); we include this in Y τi . Thus we have s i : X → Y τi , where, again, λ i denotes the output of the function s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>Our overall goal is to use the noisy or weak, multi-task supervision from the set of m sources, S = {s 1 , . . . , s m }, applied to an unlabeled dataset X U consisting of n data points, to supervise an end model f w : X → Y. Since the sources have unknown accuracies, and will generally output noisy and incomplete labels that will overlap and conflict, our intermediate goal is to learn a label model P µ : λ → [0, 1] |Y| which takes as input the source labels and outputs a set of probabilistic label vectors,Ỹ, for each X, which can then be used to train the end model. Succinctly, given a user-provided tuple (X U , S, G source , G task ), our goal is to recover the parameters µ.</p><p>The key technical challenge in this approach then consists of learning the parameters of this label modelcorresponding to the conditional accuracies of the sources (and, for technical reasons we shall shortly explain, cliques of correlated sources)-given that we do not have access to the ground truth labels Y. We discuss our approach to overcoming this core technical challenge in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Our Approach: Modeling Multi-Task Sources</head><p>Our goal is to estimate the parameters µ of a label model that produces probabilistic training labels given the observed source outputs,Ỹ = P µ (Y|λ), without access to the ground truth labels Y. We do this in three steps: 1. We start by defining a graphical model over the weak supervision source outputs and the true (latent) variable Y, (λ 1 , . . . , λ m , Y), using the conditional independence structure G source between the sources.</p><p>2. Next, we analyze the generalized covariance matrix Σ (following Loh &amp; Wainwright <ref type="bibr" target="#b21">[22]</ref>), which is defined over binary indicator variables for each value of each clique (or specific subsets of cliques) in G source . We consider two specific subsets of the cliques in G source , the observable cliques O and the separator sets S, such that:</p><formula xml:id="formula_28">Σ = Σ O Σ OS Σ T OS Σ S Σ −1 = K = K O K OS K T OS K S ,</formula><p>where Σ O is the block of Σ that we can observe, and Σ OS is a function of µ, the parameters (corresponding to source and clique accuracies) we wish to recover. We then apply a result by Loh and Wainwright <ref type="bibr" target="#b21">[22]</ref> to establish the sparsity pattern of K = Σ −1 . This allows us to apply the block-matrix inversion lemma to reformulate our problem as solving a matrix completion-style objective.</p><p>3. Finally, we describe how to recover the class balance P (Y); with this and the estimate of µ, we then describe how to compute the probabilistic training labelsỸ = P µ (Y|λ).</p><p>We start by focusing on the setting where G source has a junction tree with singleton separator sets; we note that a version of G source where this holds can always be formed by adding edges to the graph. We then discuss how to handle graphs with non-singleton separator sets, and finally describe different settings where our problem reduces to rank-one matrix completion. In Section B, we introduce theoretical results for the resulting model and provide our model estimation strategy. We consider a model G source = (V, E), where V = {Y, λ 1 , ..., λ m }, and E consists of pairwise interactions (i.e. we consider an Ising model, or equivalently, a graph rather than a hypergraph of correlations). We assume that G source is provided by the user. However, if G source is unknown, there are various techniques for estimating it statistically <ref type="bibr" target="#b1">[2]</ref> or even from static analysis if the sources are heuristic functions <ref type="bibr" target="#b32">[33]</ref>. We provide an example G source with singleton separator sets in <ref type="figure" target="#fig_2">Figure 7</ref>.</p><formula xml:id="formula_29">Y λ 1 λ 2 λ 3 λ 4 Y, λ 1 , λ 2 Y, λ 3 Y, λ 4 Y Y</formula><p>Augmented Sufficient Statistics Finally, we extend the random variables in V by defining a matrix of indicator statistics over all cliques in G source , in order to estimate all the parameters needed for our label model P µ . We assume that the provided G source is chordal, meaning it has no chordless cycles of length greater than three; if not, the graph can easily be triangulated to satisfy this property, in which case we work with this augmented version.</p><p>Let C be the set of maximal and non-maximal cliques in the chordal graph G source . We start by defining a binary indicator random variable for the event of a clique C ∈ C in the graph G source = (V, E) taking on a set of values y C :</p><formula xml:id="formula_30">ψ(C, y C ) = 1 {∩ i∈C V i = (y C ) i } , where (y C ) i ∈ Y min</formula><p>τi and Y min τi contains all but one values of Y τi , thereby leading to a minimal set of statistics. Note that in our notation, V 0 = Y, Y τ0 = Y, and V i&gt;0 = λ i . Accordingly, we define ψ(C) ∈ {0, 1} i∈C (|Yτ i |−1) as the vector of indicator random variables for all combinations of all but one of the labels emitted by each variable in clique C, and define ψ(C) accordingly for any set of cliques C ⊆ C. Then µ = E [ψ(C)] is the vector of sufficient statistics for the label model we want to learn. Our model estimation goal is now stated simply: we wish to estimate µ, without access to the ground truth labels Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Model Estimation without Ground Truth Using Inverse Covariance Structure</head><p>Our goal is to estimate µ = E [ψ(C)]; this, along with the class balance P (Y) (which we assume we know, or else estimate using the approach in Section A.3.5), is sufficient information to compute P µ (Y|λ). If we had access to a large enough set of ground truth labels Y, we could simply take the empirical expectationÊ [ψ]; however in our setting we cannot directly observe this. Instead, we proceed by analyzing a sub-block of the covariance matrix of ψ(C), which corresponds to the generalized covariance matrix of our graphical model as in <ref type="bibr" target="#b21">[22]</ref>, and leverage two key pieces of information:</p><p>• A sub-block of this generalized covariance matrix is observable, and</p><p>• By a simple extension of Corollary 1 in <ref type="bibr" target="#b21">[22]</ref>, we know the sparsity structure of the inverse generalized covariance matrix Σ −1 , i.e. we know that it will have elements equal to zero according to the structure of G source .</p><p>Since G source is triangulated, it admits a junction tree representation <ref type="bibr" target="#b18">[19]</ref>, which has maximal cliques (nodes)C and separator sets S. Note that we follow the convention that S includes the full powerset of separator set cliques, i.e. all subset cliques of separator set cliques are also included in S. We proceed by considering two specific subsets of the cliques of our graphical model G source : those that are observable (i.e. not containing Y), O = {C | Y / ∈ C, C ∈ C}, and the set of separator set cliques (which will always contain Y, and thus be unobservable).</p><p>For simplicity of exposition, we start by considering graphs G source which have singleton separator sets; given our graph structure, this means that S = {{Y}}. Note that in general we will write single-element sets without braces when their type is obvious from context, so we have S = {Y}. Intuitively, this corresponds to models where weak supervision sources are correlated in fully-connected clusters, corresponding to real-world settings in which sources are correlated due to shared data sources, code, or heuristics. However, we can always either (i) add edges to G source such that this is the case, or (ii) extend our approach to many settings where G source does not have singleton separator sets (see Section A.3.3).</p><p>In this singleton separator set setting of S = {Y}, we now have:</p><formula xml:id="formula_31">O = {C | Y / ∈ C, C ∈ C} S = {Y}.</formula><p>where ψ(O) and ψ(Y) are the corresponding vectors of minimal indicator variables. We define corresponding dimensions d O and d S :</p><formula xml:id="formula_32">d O = C∈O i∈C (|Y τi | − 1) d S = r − 1.</formula><p>We now decompose the generalized covariance matrix and its inverse as:</p><formula xml:id="formula_33">Cov [ψ(O ∪ S)] ≡ Σ = Σ O Σ OS Σ T OS Σ S Σ −1 = K = K O K OS K T OS K S ,<label>(7)</label></formula><p>This is similar to the form used in <ref type="bibr" target="#b5">[6]</ref>, but with several important differences: we consider discrete (rather than Gaussian) random variables and have additional knowledge of the graph structure. Here, Σ O is the observable block of the generalized covariance matrix Σ, and Σ OS is the unobserved block which is a function of µ, the parameters (corresponding to source and source clique accuracies) we wish to recover. Note that with the singleton separator sets we are considering, Σ S is a function of the class balance P (Y), which we assume is either known, or has been estimated according to the unsupervised approach we detail in Section A.3.5. Therefore, we assume that Σ S is also known. Concretely then, our goal is to recover Σ OS given Σ O , Σ S .</p><p>We start by applying the block matrix inversion lemma to get the equation:</p><formula xml:id="formula_34">K O = Σ −1 O + Σ −1 O Σ OS Σ S − Σ T OS Σ −1 O Σ OS −1 Σ T OS Σ −1 O .<label>(8)</label></formula><p>Next, let JJ T = Σ S − Σ T OS Σ −1 O Σ OS −1 . We justify this decomposition by showing that this term is positive semidefinite. We start by applying the Woodbury matrix inversion lemma:</p><formula xml:id="formula_35">Σ S − Σ T OS Σ −1 O Σ OS −1 = Σ −1 S + Σ −1 S Σ T OS Σ O + Σ OS Σ −1 S Σ T OS −1 Σ OS Σ −1 S .<label>(9)</label></formula><p>Now, note that Σ O and Σ S are both covariance matrices themselves and are therefore PSD. Furthermore, from <ref type="bibr" target="#b21">[22]</ref> we know that Σ −1 must exist, which implies that Σ O and Σ S are invertible (and thus in fact positive definite). Therefore we also have that</p><formula xml:id="formula_36">Σ OS Σ −1 S Σ T OS 0 =⇒ Σ O + Σ OS Σ −1 S Σ T OS −1</formula><p>0, and therefore (9) is positive definite, and can therefore always be expressed as JJ T for some J. Therefore, we can write (8) as:</p><formula xml:id="formula_37">K O = Σ −1 O + Σ −1 O Σ OS JJ T Σ T OS Σ −1 O .</formula><p>Finally, define Z = Σ −1 O Σ OS J; we then have:</p><formula xml:id="formula_38">K O = Σ −1 O + ZZ T .<label>(10)</label></formula><p>Note that Z ∈ R d O ×d H , where d H = r − 1, and therefore ZZ T is a rank-(r − 1) matrix. Therefore, we now have a form (10) that appears close to being a matrix completion-style problem. We complete the connection by leveraging the known sparsity structure of K O .</p><p>Define G aug = (ψ, E aug ) to be the augmented version of our graph G source . In other words, let i = (C 1 , y C1 ) and j = (C 2 , y C2 ) according to the indexing scheme of our augmented indicator variables; then, (i, j) ∈ E aug if C 1 , C 2 are subsets of the same maximal clique in G source . Then, let G inv-aug = (ψ, Ω) be the inverse graph of G aug , such that (i, j) ∈ E aug =⇒ (i, j) / ∈ Ω and vice-versa.</p><p>We start with a result that extends Corollary 1 in Loh &amp; Wainwright <ref type="bibr" target="#b21">[22]</ref> to our specific setting where we consider a set of the variables that contains all observable cliques, O, and all separator sets S (note that this result holds for all S, not just S = {Y}):</p><formula xml:id="formula_39">Corollary 1 Let U = O ∪ S.</formula><p>Let Σ U be the generalized covariance matrix for U . Then (Σ −1 U ) i,j = 0 whenever i, j correspond to cliques C 1 , C 2 respectively such that C 1 , C 2 are not subsets of the same maximal clique.</p><p>Proof: We partition the cliques C into two sets, U and W = C \ U . Let Σ be the full generalized covariance matrix (i.e. including all maximal and non-maximal cliques) and Γ = Σ −1 . Thus we have:</p><formula xml:id="formula_40">Σ = Σ U Σ U W Σ T U W Σ W Σ −1 = Γ = K U K U W K T U W K W .</formula><p>By the block matrix inversion lemma we have:</p><formula xml:id="formula_41">Σ −1 U = K U − K U W K −1 W K T U W .</formula><p>We now follow the proof structure of Corollary 1 of <ref type="bibr" target="#b21">[22]</ref>. We know K U is graph structured by Theorem 1 of <ref type="bibr" target="#b21">[22]</ref>. Next, using the same argument as in the proof of Corollary 1 of <ref type="bibr" target="#b21">[22]</ref>, we know that K W , and therefore K −1 W , is block-diagonal. Intuitively, because the set U contains all of the separator set cliques, and due to the running intersection property of a junction tree, each clique in W belongs to precisely one maximal clique-leading to block diagonal structure of K W . We thus need only to show that the following quantity is zero for two cliques C i , C j that are not subsets of the same maximal clique, with corresponding indices i, j:</p><formula xml:id="formula_42">K U W K −1 W K T U W i,j = B (K U W ) i,B (K −1 W ) B,B (K T U W ) B,j ,</formula><p>where B are the indices corresponding to the blocks in K −1 W , which correspond to maximal cliques. Our argument follows again as in Corollary 1 of <ref type="bibr" target="#b21">[22]</ref>: since U contains the separator sets, if the two cliques C 1 , C 2 are not subsets of the same maximal clique, then for each B, either (K U W ) i,B or (K T U W ) B,j must be zero, completing the proof. Now, by Corollary 1, we know that K i,j = 0 if (i, j) ∈ Ω. Let A Ω denote a matrix A with all entries (i, j) /</p><p>∈ Ω masked to zero. Then, we have:</p><formula xml:id="formula_43">Σ −1 O Ω + ZZ T Ω = 0.<label>(11)</label></formula><p>Thus, given the dependency graph G source , we can solve for Z as a rank-(r − 1) matrix completion problem, with mask Ω. Defining the semi-norm ||A|| Ω = ||A Ω || F , we can solve:</p><formula xml:id="formula_44">Z = argmin Z Σ −1 O + ZZ T Ω .<label>(12)</label></formula><p>Now, we have an estimate of Z. Note that at this point, we can only recover Z up to orthogonal transformations. We proceed by considering a reduced rank-one model, detailed in Section A.3.4, and in Section B.1 establish concrete conditions under which this model is uniquely identifiable.</p><p>We denote this rank-one setting by switching to writing Z as z ∈ R d O ×1 , in which case we now have:</p><formula xml:id="formula_45">z = argmin z Σ −1 O + zz T Ω .<label>(13)</label></formula><p>Once we have recovered z uniquely (see Section B.1), we next need to recover Σ OS = c − 1 2 Σ O z. We use the fact that c = Σ −1 S (1 + z T Σ O z), which we can confirm explicitly below, starting from the definition of c:</p><formula xml:id="formula_46">c = Σ S − Σ T OS Σ −1 O Σ OS −1 = Σ S − (c − 1 2 Σ O z) T Σ −1 O (c − 1 2 Σ O z) −1 = Σ S − c −1 z T Σ O z −1 =⇒ c −1 = Σ S − c −1 z T Σ O z =⇒ c −1 1 + z T Σ O z = Σ S =⇒ c = Σ −1 S 1 + z T Σ O z</formula><p>Thus, we can directly recover an estimate of Σ OS from the observed Σ O , known Σ S , and estimated z. Finally, we have:</p><formula xml:id="formula_47">Σ OS + E [ψ(O)] E [ψ(S)] T = E ψ(O)ψ(S) T .<label>(14)</label></formula><p>Here, we can clearly observe E [ψ(O)], and given that we know the class balance P (Y), we also have E [ψ(S)]; therefore we can compute E ψ(O)ψ(S) T . Our goal now is to recover the columns E [ψ(O)ψ(Y i )], which together make up µ; we can do this based on the constraints of our rank-one model (Section A.3.4), thus recovering an estimate of µ, which given the uniqueness ofẑ (Section B.1) is also unique. The overall procedure is described in the main body, in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Handling Non-Singleton Separator Sets</head><p>Now, we consider the setting where G source has arbitrary separator sets. Let d S = S∈S i∈S (|Y τi | − 1). We see that we could solve this using our standard approach-this time, involving a rank-d S matrix completion problem-except for the fact that we do not know Σ S , as it now involves terms besides the class balance.</p><p>Note first of all that we can always add edges between sources to G source such that it has singleton separator sets (intuitively, this consists of "completing the clusters"), and as long as our problem is still identifiable (see Section B.1), we can simply solve this instance as above.</p><p>Instead, we can also take a multi-step approach, wherein we first consider one or more subgraphs of G source that contain only singleton separator sets, and contain the cliques in S. We can then solve this problem as before, which then gives us the needed information to identify the elements of Σ S in our full problem, which we can then solve. In particular, we see that this multi-step approach is possible whenever the graph G source has at least three components that are disconnected except for through Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Rank-One Settings</head><p>We now consider settings where we can estimate the parameters of our label model, µ, involving only a rank-one matrix completion problem.</p><p>First, in the simplest setting of a single-task problem with binary class variable, Y ∈ {0, 1} and G source with singleton separator sets, d H = r − 1 = 1 and our problem is directly a rank-one instance.</p><p>Next, we consider the setting of general Y, with |Y| = r and G source with singleton separator sets. By default, our problem now involves a rank-(r − 1) matrix completion problem. However, we can reduce this to involving only a rank-one matrix completion problem by adding one simplifying assuption to our model: namely, that sources emit different incorrect labels with uniform conditional probability. Concretely, we add the assumption that:</p><formula xml:id="formula_48">(λ C ) i = Y ⇐⇒ (λ C ) i = Y ∀i ∈ C =⇒ P (λ C |Y) = P (λ C |Y)<label>(15)</label></formula><p>Note that this is the same assumption as in the main body, but expressed more explicitly with respect to a clique C. For example, under this assumption, P (λ i = y |Y = y) is the same for all y such that y = y. As another example, P (λ i = y, λ j = y |Y = y) is the same for all y such that y = y. Intuitively, under this commonly-used model, we are not modeling the different class-wise errors a source makes, but rather just whether it is correct or not given the correctness of other sources it is correlated with. The idea then is that with assumption (15) even though |H| = r − 1 (and thus Σ OS has r − 1 columns), we only actually need to solve for a single parameter per element of O.</p><p>We can operationalize this by forming a new graph with a binarized version of Y, Y B ∈ {0, 1}, such that the r classes are mapped to either 0 or 1. We see that this new variable still results in the same structure of dependencies G source , and still allows us to recover the parameters α y (and thus µ). We now have:</p><formula xml:id="formula_49">S = {Y B }</formula><p>We now solve in the same rank-one way as in the binary Y case. Now, for singleton cliques, {λ i , Y}, given that we know P (Y), we can directly recover P (λ i = y|Y = y ) for all y , given our simplified model.</p><p>For non-singleton cliques {λ C , Y}, note that we can directly recover P (∩ i∈C λ i = y|Y = y ) in the exact same way. From these, computed for all cliques, we can then recover any probability in our model. For example, for y = y:</p><formula xml:id="formula_50">P (λ i = y, λ j = y |Y = y) = P (λ i = y|Y = y) − y =y P (λ i = y, λ j = y |Y = y) = P (λ i = y|Y = y) − P (λ i = y, λ j = y|Y = y)− × (r − 2)P (λ i = y, λ j = y |Y = y)</formula><p>=⇒ P (λ i = y, λ j = y |Y = y) = 1 r − 1 (P (λ i = y|Y = y) − P (λ i = y, λ j = y|Y = y)) .</p><p>In this way, we can recover all of the parameters µ while only involving a rank-one matrix completion problem. Note that this also suggests a way to solve for the more general model, i.e. without <ref type="bibr" target="#b14">(15)</ref>, using a hierarchical classification approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 Recovering the Class Balance P &amp; Computing P (Y |λ)</head><p>We now turn to the task of recovering the class balance P (Y), for Y ∈ Y. In many practical settings, P (Y) can be estimated from a small labeled sample, or may be known in advance. However here, we consider using a subset of conditionally independent sources, s 1 , . . . , s k to estimate P (Y). We note first of all that simply taking the majority vote of these sources is a biased estimator.</p><p>Instead, we consider a simplified version of the matrix completion-based approach taken so far. Here, we consider a subset of the sources s 1 , . . . , s k such that they are conditionally independent given G source , i.e. λ i ⊥ λ j |Y, and consider only the unary indicator statistics. Denote the vector of these unary indicator statistics over the conditionally independent subset of sources as φ, and let the observed overlaps matrix between sources i and j be</p><formula xml:id="formula_51">A i,j = E φ i φ T j .</formula><p>Note that due to the conditional independence of λ i and λ j , for any k, l we have:</p><formula xml:id="formula_52">(A i,j ) k,l = E [(φ i ) k (φ j ) l ] = P (λ i = y k , λ j = y l ) = y∈Y P (λ i = y k , λ j = y l |Y = y)P (Y = y) = y∈Y P (λ i = y k |Y = y)P (λ j = y l |Y = y)P (Y = y).</formula><p>Letting B i be the |Y τi | × |Y| matrix of conditional probabilities, (B i ) j,k = P (λ i = y j |Y = y k ), and P be the diagonal matrix such that P i,i = P (Y = y i ), we can re-express the above as:</p><formula xml:id="formula_53">A i,j = B i P B T j .</formula><p>Since P is composed of strictly positive elements, and is diagonal (and thus PSD), we re-express this as:</p><formula xml:id="formula_54">A i,j =B iB T j ,<label>(16)</label></formula><formula xml:id="formula_55">whereB i = B i √ P .</formula><p>We could now try to recover P by decomposing the observed A i,j to recover theB i , and from there recover P via the relation:</p><formula xml:id="formula_56">P = diag B T i 1 2 ,<label>(17)</label></formula><p>since summing the column ofB i corresponding to label Y is equal to P (Y) y∈Yi P (λ i = y|Y) = P (Y) by the law of total probability. However, note thatB i U for any orthogonal matrix U also satisfies <ref type="bibr" target="#b15">(16)</ref>, and could thus lead to a potentially infinite number of incorrect estimates of P .</p><p>Class Balance Identifiability with Three-Way View Constraint A different approach involves considering the three-way overlaps observed as A i,j,k . This is equivalent to performing a tensor decomposition. Note that above, the problem is that matrix decomposition is typically invariant to rotations and reflections; tensor decompositions have easier-to-meet uniqueness conditions (and are thus more rigid).</p><p>Specifically, we apply Kruskal's classical identifiability condition for unique 3-tensor decomposition. Consider some tensor</p><formula xml:id="formula_57">T = R r=1 X r ⊗ Y r ⊗ Z r ,</formula><p>where X r , Y r , Z r are column vectors that make up the matrices X, Y, Z. The Kruskal rank k X of X is the largest k such that any k columns of X are linearly independent. Then, the decomposition above is unique if <ref type="bibr">[20; 3]</ref>. In our case, our triple views have R = |Y|, and we have</p><formula xml:id="formula_58">k X + k Y + k Z ≥ 2R + 2</formula><formula xml:id="formula_59">A i,j,k =B i ⊗B j ⊗B k .<label>(18)</label></formula><p>Thus, if kB i + kB j + kB k ≥ 2|Y| + 2, we have identifiability. Thus, it is sufficient to have the columns of each of theB i 's be linearly independent. Note that each of the B i 's have columns with the same sum, so these columns are only linearly dependent if they are equal, which would only be the case if the sources were random voters.</p><p>Thus, we can use <ref type="bibr" target="#b17">(18)</ref> to recover theB i in a stable fashion, and then use <ref type="bibr" target="#b16">(17)</ref> to recover the P (Y). <ref type="figure">Figure 8</ref>: Example task hierarchy G task for a three-task classification problem. Task Y 1 classifies a data point X as a PERSON or BUILDING. If Y 1 classifies X as a PERSON, Y 2 is used to distinguish between DOCTOR and NON-DOCTOR. Similarly, if Y 2 classifies X as a BUILDING, Y 3 is used to distinguish between HOSPITAL and NON-HOSPITAL. Tasks Y 2 , Y 3 are more specific, or finer-grained tasks, constrained by their parent task Y 1 .</p><formula xml:id="formula_60">Y 1 Y 2 Y 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.6 Predicting Labels with the Label Model</head><p>Once we have an estimate of µ, we can make predictions with the label model-i.e. generate our probabilistic training labels P µ (Y|λ)-using the junction tree we have already defined over G source . Specifically, letC be the set of maximal cliques (nodes) in the junction tree, and let S be the set of separator sets. Then we have:</p><formula xml:id="formula_61">P µ (Y, λ) = C∈C P (V C ) S∈S P (V S ) = C∈C µ (C,(Y,λ C )) S∈S µ (S,(Y,λ S )) , where again, V C = {V i } i∈C , where V 0 = Y and V i&gt;0 = λ i .</formula><p>Thus, we can directly compute the predicted labels P µ (Y|λ) based on the estimated parameters µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Example: Hierarchical Multi-Task Supervision</head><p>We now consider the specific case of hierarchical multi-task supervision, which can be thought of as consisting of coarser-and finer-grained labels, or alternatively higher-and lower-level labels, and provides a way to supervise e.g. fine-grained classification tasks at multiple levels of granularity. Specifically, consider a task label vector Y = [Y 1 , . . . , Y t ] T as before, this time with Y s ∈ {N/A, 1, . . . , k s }, where we will explain the meaning of the special value N/A shortly. We then assume that the tasks Y s are related by a task hierarchy which is a hierarchy G task = (V, E) with vertex set V = {Y 1 , Y 2 , . . . , Y t } and directed edge set E. The task structure reflects constraints imposed by higher level (more general) tasks on lower level (more specific) tasks. The following example illustrates a simple tree task structure:</p><p>Example 2 Let Y 1 classify a data point X as either a PERSON (Y 1 = 1) or BUILDING (Y 1 = 2). If Y 1 = 1, indicating that X represents a PERSON, then Y 2 can further label X as a DOCTOR or NON-DOCTOR. Y 3 is used to distinguish between HOSPITAL and NON-HOSPITAL in the case that Y 1 = 2. The corresponding graph G task is shown in <ref type="figure">Figure 8</ref>. If Y 1 = 2, then task Y 2 is not applicable, since Y 2 is only suitable for persons; in this case, Y 2 takes the value N/A. In this way the task hierarchy defines a feasible set of task vector values: Y = [1, 1, N/A] T , [1, 2, N/A] T , [2, N/A, 1] T , [2, N/A, 2] T are valid, while e.g. Y = [1, 1, 2] T is not.</p><p>As in the example, for certain configurations of Y's, the parent tasks logically constrain the one or more of the children tasks to be irrelevant, or rather, to have inapplicable label values. In this case, the task takes on the value N/A. In Example 2, we have that if Y 1 = 1, representing a building, then Y 2 is inactive (since X corresponds to a building). We define the symbol N/A (for incompatible) for this scenario. More concretely, let</p><formula xml:id="formula_62">N (Y i ) = {Y j : (Y j , Y i ) ∈ E} be the in-neighborhood of Y i . Then, the values of the members of N (Y i ) determine whether Y i = N/A, i.e., 1{Y j = N/A} is deterministic conditioned on N (Y i ).</formula><p>Hierarchical Multi-Task Sources Observe that in the mutually-exclusive task hierarchy just described, the value of a descendant task label Y d determines the values of all other task labels in the hierarchy besides its descendants. For example, in Example 2, a label Y 2 = 1 =⇒ (Y 1 = 1, Y 3 = N/A); in other words, knowing that X is a DOCTOR also implies that X is a PERSON and not a BUILDING.</p><p>For a source λ i with coverage set τ i , the label it gives to the lowest task in the task hierarchy which is non-zero and non-N/A determines the entire label vector output by λ i . E.g. if the lowest task that λ i labels in the hierarchy is Y 1 = 1, then this implies that it outputs vector [1, 0, N/A] T . Thus, in this sense, we can think of each sources λ i as labeling one specific task in the hierarchy, and thus can talk about coarser-and finer-grained sources.</p><p>Reduced-Rank Form: Modeling Local Accuracies In some cases, we can make slightly different modeling assumptions that reflect the nature of the task structure, and additionally can result in reduced-rank forms of our model. In particular, for the hierarchical setting introduced here, we can divide the statistics µ into local and global subsets, and for example focus on modeling only the local ones to once again reduce to rank-one form.</p><p>To motivate with our running example: a finer-grained source that labels DOCTOR versus NON-DOCTOR probably is not accurate on the building type subtask; we can model this source using one accuracy parameter for the former label set (the local accuracy) and a different (or no parameter) for the global accuracy on irrelevant tasks. More specifically, for cliques involving λ i , we can model P (λ i , Y) for all Y with only non-N/A values in the coverage set of λ i using a single parameter, and call this the local accuracy; and we can either model µ for the other Y using one or more other parameters, or simply set it to a fixed value and not model it, to reduce to rank one form, as we do in the experiments. In particular, this allows us to capture our observation in practice that if a developer is writing a source to distinguish between labels at one sub-tree, they are probably not designing or testing it to be accurate on any of the other subtrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Results</head><p>In this section, we focus on theoretical results for the basic rank-one model considered in the main body of the paper. In Section B.1, we start by going through the conditions for identifiability in more detail for the rank-one case. In Section B.2, we provide additional interpretation for the expression of our primary theoretical result bounding the estimation error of the label model. In Section B.3, we then provide the proof of Theorem 1, connecting this estimation error to the generalization error of the end model; and in Section B.4, we provide the full proof of the main bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Conditions for Identifiability</head><p>We consider the rank-one setting as in the main body, where we have</p><formula xml:id="formula_63">−(Σ −1 O ) Ω = zz T Ω ,<label>(19)</label></formula><p>where Ω is the inverse augmented edge set, i.e. a pair of indices (i, j), corresponding to elements of ψ(C), and therefore to cliques A, B ∈ C, is in Ω if A, B are not part of the same maximal clique in G source (and therefore (K O ) i,j = 0). This defines a set of |Ω| equations, which we can encode using a matrix M Ω , where if (i, j) is the (r − 1)th entry in Ω, then</p><formula xml:id="formula_64">(M Ω ) r,s = 1 s ∈ {i, j}, 0 else.<label>(20)</label></formula><p>Let l i = log(z 2 i ) and q (i,j) = log(((Σ −1 O ) i,j )); then by squaring and taking the log of both sides of 19, we get a system of linear equations:</p><formula xml:id="formula_65">M Ω l = q Ω .<label>(21)</label></formula><p>Thus, we can identify z (and therefore µ) up to sign if the system of linear equations (21) has a solution.</p><p>Notes on Invertibility of M Ω Note that if the inverse augmented edge graph consists of a connected triangle (or any odd-numbered cycle), e.g. Ω = {(i, j), (j, k), (i, k)}, then we can solve for the z i up to sign, and therefore M Ω must be invertible:</p><formula xml:id="formula_66">z 2 i = (Σ −1 O ) i,j (Σ −1 O ) i,k (Σ −1 O ) j,k</formula><p>, and so on for z j , z k . Note additionally that if other z i are connected to this triangle, then we can also solve for them up to sign as well. Therefore, if Ω contains at least one triangle (or odd-numbered cycle) per connected component, then M Ω is invertible.</p><p>Also note that this is all in reference to the inverse source dependency graph, which will generally be dense (assuming the correlation structure between sources is generally sparse). For example, note that if we have one source λ i that is conditionally independent of all the other sources, then Ω is fully connected, and therefore if there is a triangle in Ω, then M Ω is invertible.</p><p>Identifying the Signs of the z i Finally, note that if we know the sign of one z i , then this determines the signs of every other z j in the same connected component. Therefore, for z to be uniquely identifiable, we need only know the sign of one of the z i in each connected component. As noted already, if even one source λ i is conditionally independent of all the other sources, then Ω is fully connected; in this case, we can simply assume that the average source is better than random, and therefore identify the signs of z without any additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Interpreting the Main Bound</head><p>We re-state Theorem 2, which bounds the average error on the estimate of the label model parameters, providing more detail on and interpreting the terms of the bound.</p><p>Theorem 2 Letμ be an estimate of µ * produced by Algorithm 1 run over n unlabeled data points. Let a :=</p><formula xml:id="formula_67">d O Σ S + d O Σ S 2 λ max (K O ) 1 2 and b := Σ −1 O 2 (Σ −1 O )min .</formula><p>Then, we have: It is not hard to see that</p><formula xml:id="formula_68">E [||μ − µ * ||] ≤ 16(r − 1)d 2 O 32π n abσ max (M + Ω ) 3 d O aλ −1 min (Σ O ) + 1 κ(Σ O ) + λ −1 min (Σ O ) .</formula><formula xml:id="formula_69">M T Ω M Ω = diag(deg(G inv )) + Adj(G inv ).</formula><p>Here, deg(G inv ) are the degrees of the nodes in G inv and Adj(G inv ) is its adjacency matrix. This form closely resembles the graph Laplacian, which differs in the sign of the adjacency matrix term: L(G) = diag(deg(G)) − Adj(G). We bound</p><formula xml:id="formula_70">σ max (M + Ω ) ≤ (d min + λ min (Adj(G inv )))) −1 ,</formula><p>where d min is the lowest-degree node in G inv (that is, the source s with fewest appearances in Ω). In general, computing λ min (Adj(G inv ))) can be challenging. A closely related task can be done via Cheeger inequalities, which state that</p><formula xml:id="formula_71">2h G ≥ λ min (L(G)) ≥ 1 2 h 2 G ,</formula><p>where λ min (L(G)) is the smallest non-zero eigenvalue of L(G) and</p><formula xml:id="formula_72">h G = min X |E(X,X)| min x∈X d x , y∈X d y</formula><p>is the Cheeger constant of the graph <ref type="bibr" target="#b6">[7]</ref>. The utility of the Cheeger constant is that it measures the presence of a bottleneck in the graph; the presence of such a bottleneck limits the graph density and is thus beneficial when estimating the structure in our case. Our Cheeger-constant like term σ max (M + Ω ) acts the same way. Now, in the easiest and most common case is that of conditionally independent sources <ref type="bibr">[9; 38; 9; 17]</ref>., Adj(G inv ) has 1's everywhere but the diagonal, and we can compute explicitly that</p><formula xml:id="formula_73">σ max (M + Ω ) = 1 √ m − 2 .</formula><p>In the general setting, we must compute the minimal eigenvalue of the adjacency matrix, which is tractable, for example, for tree structures.</p><p>Influence of λ min (Σ O ) the smallest eigenvalue of the observed matrix. This quantity reflects the conditioning of the observed (correlation) matrix; the better conditioned the matrix, the easier it is to estimate Σ O .</p><p>Influence of (Σ −1 O ) min the smallest entry of the inverse observed matrix. This quantity contributes to Σ −1 , the geenralized precision matrix that we centrally use; it is a measure of the smallest non-zero correlation between source accuracies (that is, the smallest correlation between non-independent source accuracies). Note that the tail bound of Theorem 2 scales as exp(−((Σ −1 O ) min ) 2 ). This is natural, as distinguishing between small correlations and independencies requires more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 1</head><p>Let D be the true data generating distribution, such that (X, Y) ∼ D. Let P µ (Y|λ) be the label model parameterized by µ and conditioned on the observed source labels λ. Furthermore, assume that:</p><p>1. For some optimal label model parameters µ * , P µ * (λ, Y) = P (λ, Y); 2. The label Y is independent of the features of our end model given the source labels λ</p><p>That is, we assume that (i) the optimal label model, parameterized by µ * , correctly matches the true distribution of source labels λ drawn from the true distribution, (s(X), Y) ∼ D; and (ii) that these labels λ provide sufficient information to discern the label Y. We note that these assumptions are the ones used in prior work <ref type="bibr" target="#b27">[28]</ref>, and are intended primarily to illustrate the connection between the estimation accuracy ofμ, which we bound in Theorem 2, and the end model performance. Now, suppose that we have an end model parameterized by w, and that to learn these parameters we minimize a normalized bounded loss function l(w, X, Y), such that without loss of generality, l(w, X, Y) ≤ 1. Normally our goal would be to find parameters that minimize the expected loss, which we denote w * :</p><formula xml:id="formula_74">L(w) = E (X,Y)∼D [l(w, X, Y)]<label>(22)</label></formula><p>However, since we do not have access to the true labels Y, we instead minimize the expected noise-aware loss, producing an estimatew:</p><formula xml:id="formula_75">L µ (w) = E (X,Y)∼D EỸ ∼Pµ(·|λ(X)) l(w, X,Ỹ) .<label>(23)</label></formula><p>In practice, we actually minimize the empirical version of the noise aware loss over an unlabeled dataset U = {X <ref type="bibr" target="#b0">(1)</ref> , . . . , X (n) }, producing an estimateŵ:</p><formula xml:id="formula_76">L µ (w) = 1 n n i=1</formula><p>EỸ ∼Pµ(·|λ(X (i) )) l(w, X (i) ,Ỹ) .</p><p>Let w * be the minimizer of the expected loss L, letw be the minimizer of the noise-aware loss for estimated label model parameters µ, L µ , and letŵ be the minimizer of the empirical noise aware lossL µ . Our goal is to bound the generalization riskthe difference between the expected loss of our empirically estimated parameters and of the optimal parameters,</p><formula xml:id="formula_78">L(ŵ) − L(w * ).<label>(25)</label></formula><p>Additionally, since analyzing the empirical risk minimization error is standard and not specific to our setting, we simply assume that the error |L µ (w) − L µ (ŵ)| ≤ γ(n), where γ(n) is a decreasing function of the number of unlabeled data points n.</p><p>To start, using the law of total expectation first, followed by our assumption (2) about condtional independence, and finally using our assumption (1) about our optimal label model µ * , we have that:</p><formula xml:id="formula_79">L(w) = E (X ,Y )∼D [L(w)] = E (X ,Y )∼D E (X,Y)∼D [l(w, X , Y)|X = X ] = E (X ,Y )∼D E (X,Y)∼D [l(w, X , Y)|s(X) = s(X )] = E (X ,Y )∼D E (λ,Ỹ)∼µ * l(w, X ,Ỹ)|λ = s(X ) = L µ * (w).</formula><p>Now, we have:</p><formula xml:id="formula_80">L(ŵ) − L(w * ) = L µ * (ŵ) + L µ (ŵ) − L µ (ŵ) + L µ (w) − L µ (w) − L µ * (w * ) ≤ L µ * (ŵ) + L µ (ŵ) − L µ (ŵ) + L µ (w * ) − L µ (w) − L µ * (w * ) ≤ |L µ (ŵ) − L µ (w)| + |L µ * (ŵ) − L µ (ŵ)| + |L µ (w * ) − L µ * (w * )| ≤ γ(n) + 2 max w |L µ * (w ) − L µ (w )|,</formula><p>where in the first step we use our result that L = L µ * as well as add and subtract terms; and in the second step we use the fact that L µ (w) ≤ L µ (w * ). We now have our generalization risk controlled primarily by |L µ * (w ) − L µ (w )|, which is the difference between the expected noise aware losses given the estimated label model parameters µ and the true label model parameters µ * . Next, we see that, for any w :</p><formula xml:id="formula_81">|L µ * (w ) − L µ (w )| = E (X,Y)∼D EỸ ∼P µ * (·|λ) l(w, X,Ỹ) − EỸ ∼Pµ(·|λ) l(w, X,Ỹ) = E (X,Y)∼D Y ∈Y l(w, X, Y ) P µ * (Y |λ) − P µ (Y |λ) ≤ Y ∈Y E (X,Y)∼D P µ * (Y |λ) − P µ (Y |λ) ≤ |Y| max Y E (X,Y)∼D P µ * (Y |λ) − P µ (Y |λ) ,</formula><p>where we have now bounded |L µ * (w ) − L µ (w )| by the size of the structured output space |Y|, and a term having to do with the difference between the probability distributions of µ and µ * . Now, we use the result from <ref type="bibr" target="#b15">[16]</ref> (Lemma 19) which establishes that the log probabilities of discrete factor graphs with indicator features (such as our model P µ (λ, Y)) are (l ∞ , 2)-Lipschitz with respect to their parameters, and the fact that for x, y s.t. |x|, |y| ≤ 1, |x − y| ≤ | log(x) − log(y)|, to get:</p><formula xml:id="formula_82">P µ * (Y |λ) − P µ (Y |λ) ≤ log P µ * (Y |λ) − log P µ (Y |λ) ≤ log P µ * (λ, Y ) − log P µ (λ, Y ) + |log P µ * (λ) − log P µ (λ)| ≤ 2 ||µ * − µ|| ∞ + 2 ||µ * − µ|| ∞ ≤ 4 ||µ * − µ|| ,</formula><p>where we use the fact that the statement of Lemma 19 also holds for every marginal distribution as well. Therefore, we finally have:</p><formula xml:id="formula_83">L(ŵ) − L(w * ) ≤ γ(n) + 4|Y| ||µ * − µ|| .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Theorem 2</head><p>Proof: First we briefly provide a roadmap of the proof of Theorem 2. We consider estimatingμ with our procedure in the rank-one setting, and we seek a tail bound on μ − µ . The challenge here is that the observed matrix Σ O we see is itself constructed from a series of observed i.i.d. samples ψ(O) <ref type="bibr" target="#b0">(1)</ref> , . . . , ψ(O) (n) . We bound (through a matrix concentration inequality) the error ∆ O =Σ O − Σ O , and view ∆ O as a perturbation of Σ O . Afterwards, we use a series of perturbation analyses to ultimately bound Σ OS − Σ OS , and then use this directly to bound μ − µ ; each of the perturbation results is in terms of ∆ O . We begin with some notation. We write the following perturbations (note that all the terms written with ∆ are additive, while the δ term is relative)Σ</p><formula xml:id="formula_84">OS = Σ OS + ∆ OS , Σ O = Σ O + ∆ O , = + ∆ , z = (I + diag(δ z ))z.</formula><p>Now we start our perturbation analysis:</p><formula xml:id="formula_85">Σ OS = 1 √cΣ Oz = 1 √c (Σ O + ∆ O )(I + diag(δ z ))z = 1 √c (Σ O z + Σ O diag(δ z )z + ∆ O (I + diag(δ z ))z)</formula><p>.</p><formula xml:id="formula_86">Subtracting Σ OS = 1 √ c Σ O z, we get ∆ OS = 1 √c − 1 √ c Σ O z + 1 √c (Σ O diag(δ z )z + ∆ O (I + diag(δ z ))z) .<label>(26)</label></formula><p>The rest of the analysis requires us to bound the norms for each of these terms.</p><p>Left-most term. We have that</p><formula xml:id="formula_87">1 √c − 1 √ c Σ O z = √ c √c − 1 1 √ c Σ O z = √ c √c − 1 Σ OS ≤ d O √ c √c − 1 ≤ d O |c − c|.</formula><p>Here, we bounded Σ OS by</p><formula xml:id="formula_88">√ d O , since Σ OS ∈ [−1, 1] d O . Then, note that c = Σ −1 S (1 + z T Σ O z) ≥ 0, since Σ S &lt; 1 and Σ O 0 =⇒ z T Σ O z ≥ 0, so therefore c,c ≥ 1.</formula><p>In the last inequality, we use this to imply that</p><formula xml:id="formula_89">| √ c/ √c − 1| ≤ | √ c − √c | ≤ |c − c|.</formula><p>Next we work on bounding |c − c|. We have</p><formula xml:id="formula_90">|c − c| = |Σ −1 S ||z TΣ Oz − z T Σ O z| = |Σ −1 S ||z T (I + diag(δ z )) T (Σ O + ∆ O )(I + diag(δ z ))z − z T Σ O z| = |Σ −1 S ||z T Σ O diag(δ z )z + z T ∆ O (I + diag(δ z ))z + z T diag(δ z ) T (Σ O + ∆ O )(I + diag(δ z ))z| ≤ |Σ −1 S | z 2 Σ O 2 δ z + δ z 2 + ∆ O 2 δ z + δ z 2 + 1 ≤ z 2 Σ O 2 δ z + δ z 2 + ∆ O 2 δ z + δ z 2 + 1 .</formula><p>Thus,</p><formula xml:id="formula_91">1 √c − 1 √ c Σ O z ≤ d O z 2 Σ O 2 δ z + δ z 2 + ∆ O 2 δ z + δ z 2 + 1 .<label>(27)</label></formula><p>Bounding c. We will need a bound on c to bound z. We have that</p><formula xml:id="formula_92">c = (Σ S − Σ T OS Σ −1 O Σ OS ) −1 .</formula><p>Applying the Woodbury matrix inversion lemma, we have:</p><formula xml:id="formula_93">c = Σ −1 S + Σ −1 S Σ T OS Σ O − Σ OS Σ −1 S Σ T OS −1 Σ OS Σ −1 S</formula><p>Now, by the blockwise inversion lemma, we know that</p><formula xml:id="formula_94">K O = Σ O − Σ OS Σ −1 S Σ T OS −1</formula><p>So we then have:</p><formula xml:id="formula_95">c = Σ −1 S + Σ −1 S Σ T OS K O Σ OS Σ −1 S ≤ Σ −1 S + (Σ −1 S ) 2 Σ OS 2 K O Bounding z. We'll use our bound on c, since z = √ cΣ −1 O Σ OS . z = √ cΣ −1 O Σ OS ≤ Σ −1 S + (Σ −1 S ) 2 Σ OS 2 K O 1 2 Σ −1 O Σ OS ≤ Σ −1 S + (Σ −1 S ) 2 d O K O 1 2 Σ −1 O d O = d O Σ S Σ S d O + λ max (K O ) 1 2 λ −1 min (Σ O )</formula><p>In the last inequality, we used the fact that Σ OS 2 ≤ d O . Now we want to control ∆ .</p><p>Perturbation bound. We have the perturbation bound</p><formula xml:id="formula_96">∆ ≤ M + Ω q S − q S .<label>(28)</label></formula><p>We need to work on the term q S − q S . To avoid overly heavy notation, we write P = Σ −1 O ,P =Σ −1 O , and ∆ P = P −P . Then we have:</p><formula xml:id="formula_97">q S − q S 2 = (i,j)∈S log(P 2 i,j ) − log(P 2 i,j ) 2 = 4 (i,j)∈S log(|P i,j |) − log(|P i,j |) 2 = 4 (i,j)∈S (log(|P i,j + (∆ P ) i,j |) − log(|P i,j |)) 2 ≤ 4 (i,j)∈S log 1 + (∆ P ) i,j P i,j 2 ≤ 8 (i,j)∈S |(∆ P ) i,j | |P i,j | 2 ≤ 8 P 2 min (i,j)∈S (∆ P ) 2 i,j ≤ 8 Σ −1 O − Σ −1 O 2 ((Σ −1 O ) min ) 2 .</formula><p>Here, the second inequality uses (log(1 + x)) 2 ≤ x 2 , and the fourth inequality sums over squared values. Next, we use the perturbation bound</p><formula xml:id="formula_98">Σ −1 O − Σ −1 O ≤ Σ −1 O 2 ∆ O , so that we have q S − q S ≤ 2 √ 2 Σ −1 O 2 ∆ O (Σ −1 O ) min .</formula><p>Then, plugging this into (28), we get that</p><formula xml:id="formula_99">∆ ≤ σ max (M + Ω ) 2 √ 2 Σ −1 O 2 ∆ O (Σ −1 O ) min .<label>(29)</label></formula><p>Bounding δ z . Note also that ∆ 2 = m i=1 (log(z 2 i ) − log(z 2 i )). We have that</p><formula xml:id="formula_100">∆ 2 = m i=1 log z 2 i z 2 i = 2 m i=1 log |z i | |z i | = 2 m i=1 log(1 + |(δ z ) i |), ≥ 2 m i=1 (δ z ) 2 i = 2 δ z 2 ,</formula><p>where in the fourth step, we used the bound log(1 + a) ≥ a 2 for small a. Then, we have</p><formula xml:id="formula_101">δ z ≤ √ 2 Σ −1 O 2 ∆ O (Σ −1 O ) min σ max (M + Ω ).<label>(30)</label></formula><p>Putting it together. Using (26), we have that</p><formula xml:id="formula_102">∆ OS = 1 √c − 1 √ c Σ O z + 1 √c (Σ O diag(δ z )z + ∆ O (I + diag(δ z ))z) ≤ 1 √c − 1 √ c Σ O z + ( Σ O diag(δ z ) + ∆ O (I + diag(δ z )) ) z ≤ d O z 2 Σ O 2 δ z + δ z 2 + ∆ O 2 δ z + δ z 2 + 1 + Σ O δ z z + ∆ O z (1 + δ z ) ≤ d O z 2 (3 Σ O δ z + 3 ∆ O δ z + ∆ O ) + Σ O δ z z + ∆ O z (1 + δ z ) ≤ z 3 d O z + 1 (( Σ O + ∆ O ) δ z + ∆ O )</formula><p>Where in the first inequality, we use the triangle inequality and the fact thatc &gt; 1, and in the third inequality, we relied on the fact that we can control δ z (through ∆ O ) so that we can make it small enough and thus take δ z 2 ≤ δ z . Now we can plug in our bounds on z and δ z from before:</p><formula xml:id="formula_103">∆ OS ≤ d O Σ S Σ S d O + λ max (K O ) 1 2 λ −1 min (Σ O ) 3 d O d O Σ S Σ S d O + λ max (K O ) 1 2 λ −1 min (Σ O ) + 1 × ( Σ O + ∆ O ) √ 2 Σ −1 O 2 ∆ O (Σ −1 O ) min σ max (M + Ω ) + ∆ O For convenience, we set ∆ O = t. Recall that a = d O Σ S + d O Σ S 2 λ max (K O ) 1 2 and b = Σ −1 O 2 (Σ −1 O ) min .</formula><p>Then, we have</p><formula xml:id="formula_104">∆ OS ≤ (3 d O aλ −1 min (Σ O ) + 1) √ 2abκ(Σ O )σ max (M + Ω )t + √ 2ab σ max (M + Ω ) λ min (Σ O ) t 2 + aλ −1 min (Σ O )t .</formula><p>Again we can take t small so that t 2 ≤ t. Simplifying further, we have</p><formula xml:id="formula_105">∆ OS ≤ (3 d O aλ −1 min (Σ O ) + 1) √ 2abσ max (M + Ω ) κ(Σ O ) + λ −1 min (Σ O ) + aλ −1 min (Σ O ) t.</formula><p>Finally, since the aλ −1 min (Σ O ) is smaller than the left-hand term inside the parentheses, we can write </p><formula xml:id="formula_106">∆ OS ≤ (3 d O aλ −1 min (Σ O ) + 1) 2 √ 2abσ max (M + Ω ) κ(Σ O ) + λ −1 min (Σ O ) t.<label>(31)</label></formula><formula xml:id="formula_107">||∆ O || = Σ O −Σ O = (R − rr T ) − 1 n n i=1 r i r T i − (r + ∆ r ) (r + ∆ r ) T ≤ R − 1 n n i=1 r i r T i ∆ R + rr T − (r + ∆ r ) (r + ∆ r ) T ∆r .</formula><p>We start by applying the matrix Hoeffding inequality <ref type="bibr" target="#b31">[32]</ref> to bound the first term, ∆ R . Let S k = 1 n (R − R k ), and thus clearly E [S k ] = 0. We seek a sequence of symmetric matrices A k s.t. S 2 k A 2 k . First, note that, for some vectors x, v,</p><formula xml:id="formula_108">x T ||v|| 2 I − vv T x = ||v|| 2 ||x|| 2 − x, v 2 ≥ 0</formula><p>using Cauchy-Schwarz; therefore ||v|| 2 I vv T , so that</p><formula xml:id="formula_109">d 2 O I ||r k || 4 I ||r k || 2 r k r T k = (r k r T k ) 2 .</formula><p>Next, note that (r k r T k + R) 2 0. Now, we use this to see that:</p><p>(nS k ) 2 = (r k r T k − R) 2 (r k r T k − R) 2 + (r k r T k + R) 2 = 2((r k r T k ) 2 + R 2 ) 2(d 2 O I + R 2 ).</p><p>Therefore, let A 2 k = 2 n 2 (d 2 O I + R 2 ), and note that R 2 ≤ ||R|| 2 ≤ (d O ||R|| max ) 2 = d 2 O . We then have</p><formula xml:id="formula_110">σ 2 = n k=1 A 2 k ≤ 2 n d 2 O + R 2 ≤ 4d 2 O n .</formula><p>And thus,</p><formula xml:id="formula_111">P (||∆ R || ≥ γ) ≤ 2d O exp − nγ 2 32d 2 O .<label>(32)</label></formula><p>Next, we bound ∆ r . We see that:</p><p>||∆ r || = rr T − (r + ∆ r ) (r + ∆ r )</p><formula xml:id="formula_112">T = r∆ T r + ∆ r r T + ∆ r ∆ T r ≤ r∆ T r + ∆ r r T + ∆ r ∆ T r ≤ 2 ||r|| ||∆ r || + ||∆ r || 2 ≤ 3 ||r|| ||∆ r || ≤ 3 ||r|| 1 ||∆ r || 1 ≤ 3d 2 O |∆ r |,</formula><p>where ∆ r is the perturbation for a single element of ψ(O). We can then apply the standard Hoeffding's bound to get:</p><formula xml:id="formula_113">P (||∆ r || ≥ γ) ≤ 2 exp − 2nγ 2 3d 2 O ,</formula><p>Combining the bounds for ||∆ R || and ||∆ r ||, we get:</p><formula xml:id="formula_114">P ( ∆ O ≥ γ) = P (t ≥ γ) ≤ 3d O exp − nγ 2 32d 2 O .<label>(33)</label></formula><p>Final steps Now, we use the bound on t in (31) and the concentration bound above to write</p><formula xml:id="formula_115">P ( ∆ OS ≥ t ) ≤ P (V t ≥ t ) = P t ≥ t V ≤ 2d O exp − nt 2 32V 2 d 2 O , where V = (3 √ d O aλ −1 min (Σ O ) + 1) 2 √ 2abσ max (M + Ω ) κ(Σ O ) + 1 λmin(Σ O )</formula><p>. GivenΣ OS , we recoverμ 1 =Σ OS + E [ψ(H)]Ê [ψ(O)]. We assume E [ψ(H)] is known, and we can bound the error introduced by E [ψ(H)]Ê [ψ(O)] as above, which we see can be folded into the looser bound for the error iñ Σ OS .</p><p>Finally, we expand the rank-one formμ 1 intoμ algebraically, according to our weight tying in the rank one model we use. Suppose in the rank one reduction (see Section A.3.4), we let Y B = 1 {Y = y 1 }. Then each element of µ 1 that we track corresponds to either the probability of being correct, α C,y = P (∩ i∈C {λ i = y}, Y = y) or the probability of being incorrect, 1 r−1 (1 − α C,y ), for each source clique C and label output combination y C , and this value is simply copied r − 1 times (for the other, weight-tied incorrect values), except for potentially one entry where it is multiplied by (r − 1) and then subtracted from 1 (to transform from incorrect to correct). Therefore, ||∆ µ || = ||µ −μ|| ≤ 2(r − 1) ||µ 1 −μ 1 ||. Thus, we have:</p><formula xml:id="formula_116">P ( ∆ µ ≥ t ) ≤ 4(r − 1)d O exp − nt 2 32V 2 d 2 O ,</formula><p>where V is defined as above. We only have one more step:</p><formula xml:id="formula_117">E [||μ − µ||] = ∞ 0 P ( μ − µ ≥ γ)dγ ≤ ∞ 0 4(r − 1)d O exp − n 32V 2 d 2 O γ 2 dγ = 4(r − 1)d O √ π 2 n 32V 2 d 2 O = 4(r − 1)d 2 O 32π n V.</formula><p>Here, we used the fact that ∞ 0 exp(−aγ 2 )dγ = √ π 2 √ a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Data Balancing and Label Model Training Procedure</head><p>For each application, rebalancing was applied via direct subsampling to the training set in the manner that was found to most improve development set micro-averaged accuracy. Specifically, we rebalance with respect to the median class for OpenI (i.e., removing examples from majority class such that none had more than the original median class), the minimum class for TACRED, and perform no rebalancing for OntoNotes. For generative model training, we use stochastic gradient descent with a step size, step number, and 2 penalty listed in <ref type="table" target="#tab_5">Table 3</ref> below. These parameters were found via 10-trial coarse random search, with all values determined via maximum micro-averaged accuracy evaluated on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 End Model Training Procedure</head><p>Before training over multiple iterations to attain averaged results for reporting, a 10-trial random search over learning rate and 2 regularization with the Adam optimizer was performed for each application based on microaveraged development set accuracy. Learning rate was decayed by an order of magnitude if no increases in training loss improvement or development set accuracy were observed for 10 epochs, and the learning rate was frozen during the first 5 epochs. Models are reported using early stopping, wherein the best performing model on the development set is eventually used for evaluation on the held-out test set, and maximum epoch number is set for each application at a point beyond which minimal additional decrease in training loss was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Dataset Statistics</head><p>We give additional detail in here (see <ref type="table">Table 4</ref>) on the different datasets used for the experimental portion of this work. All data in the development and test sets is labeled with ground truth, while data in the training set is treated as unlabeled. Each dataset has a particular advantage in our study. The OntoNotes set, for instance, contains a particularly large number of relevant data points (over 63k), which enables us to investigate empirical performance scaling with the number of unlabeled data points. Further, the richness of the TACRED dataset allowed for the creation of an 8-class, 7-sub-task hierarchical classification problem, which demonstrates the utility of being OntoNotes TACRED OpenI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Model Training</head><p>Step Size 5e-3 1e-2 5e-4  able to supervise at each of the three levels of task granularity. Finally, the OpenI dataset represents a real-world, non-benchmark problem drawn from the domain of medical triage, and domain expert input was directly leveraged to create the relevant supervision sources. The fact that these domain expert weak supervision sources naturally occurred at multiple levels of granularity, and that the could be easily integrated to train an effective end model, demonstrates the utility of the MeTaL framework in practical settings.  <ref type="table">Table 4</ref>: Dataset split sizes and sub-task structure for the three fine-grained classification tasks on which we evaluate MeTaL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Task Accuracies</head><p>For clarity, we present in <ref type="table" target="#tab_8">Table 5</ref> the individual task accuracies of both the learned MeTaL model and MV for each experiment. These accuracies are computed from the output of evaluating each model on the test set with ties broken randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Ablation Study: Unipolar Correction and Joint Modeling</head><p>We perform an additional ablation to demonstrate the relative gains of modeling unipolar supervision sources and jointly modeling accuracies across multiple tasks with respect to the data programming (DP) baseline <ref type="bibr" target="#b26">[27]</ref>. Results of this investigation are presented in <ref type="table">Table 6</ref>. We observe an average improvement of 2.8 points using the unipolar correction (DP-UI), and an additional 1.3 points from joint modeling within MeTaL, resulting in an aggregate gain of 4.1 accuracy points over the data programming baseline.</p><p>OntoNotes   <ref type="table">Table 6</ref>: Effect of Unipolar Correction. We compare the micro accuracy (avg. over 10 trials) with 95% confidence intervals of a model trained using data programming (DP), data program with a unipolar correction (DP-UI), and our approach (MeTaL).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>(Left) Estimation error ||μ − µ * || decreases with increasing n. (Middle) Given G source , our model successfully recovers the source accuracies even with many pairwise dependencies among sources, where a naive conditionally-independent model fails.(Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 : 0 Figure 6 :</head><label>506</label><figDesc>Micro-Avg. Accuracy Accuracy vs. n (Log-Scale) In the OntoNotes dataset, end model accuracy scales with the amount of available unlabeled data. # Train LM EM Gain NER 62,547 75.2 82.2 7.0 RE 9,090 55.3 57.4 2.1 Doc 2,630 75.6 76.6 1.Using the label model (LM) predictions directly versus using an end model trained on them (EM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>A simple example of a weak supervision source dependency graph G source (left) and its junction tree representation (right). Here Y is as a vector-valued variable with a feasible set of values, Y ∈ |Y|, and the output of sources 1 and 2 are modeled as dependent conditioned on Y. This results in a junction tree with singleton separator sets Y. Here, the observable cliques are O = {λ 1 , λ 2 , λ 3 , λ 4 , {λ 1 , λ 2 }} ⊂ C. A.3.1 Defining a Multi-Task Source Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Influence of σ max (M + Ω ) the largest singular value of the pseudoinverse M + Ω . Note that M + Ω 2 = (λ min (M T Ω M Ω )) −1 . As we shall see below, λ min (M T Ω M Ω ) measures a quantity related to the structure of the graph G inv . The smaller this quantity, the more information we have about G inv , and the easier it is to estimate the accuracies. The smallest value of M + Ω 2 (corresponding to the largest value of the eigenvalue) is ∼ 1 √ m ; the square of this quantity in the bound reduces the m 2 cost of estimating the covariance matrix to m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Source Accuracy Estimation for Multi-Task Weak SupervisionInput: Observed labeling ratesÊ [ψ(O)] and covarianceΣ O ; class balanceÊ [ψ(Y)] and variance Σ S ; correlation sparsity structure Ω z ← argmin z</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Dev) 63.7 ± 2.1 28.4 ± 2.3 62.7 ± 4.5 51.6 MV 76.9 ± 2.6 43.9 ± 2.6 74.2 ± 1.2 65.0 DP [28] 78.4 ± 1.2 49.0 ± 2.7 75.8 ± 0.9 67.7 MeTaL 82.2 ± 0.8 56.7 ± 2.1 76.6 ± 0.4 71.8</figDesc><table><row><cell>NER</cell><cell>RE</cell><cell>Doc Average</cell></row><row><cell>Gold (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Glossary of variables and symbols used in this paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Concentration bound. We need to bound t = ∆ O , the error when estimating Σ O from observations ψ(O)<ref type="bibr" target="#b0">(1)</ref> , . . . , ψ(O) (n) over n unlabeled data points.To start, recall that O is the set of observable cliques, ψ(O) ∈ {0, 1} d O is the corresponding vector of minimal statistics, and Σ O = Cov [ψ(O)]. For notational convenience, let R = E ψ(O)ψ(O) T , r = E [ψ(O)], and r k = ψ(O) (k) , and ∆ r = 1</figDesc><table><row><cell>n</cell><cell>n i=1 r k − r. Then we have:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Model architecture and training parameter details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Label model task accuracies for each task for for both our approach and majority vote (MeTaL/MV)</figDesc><table><row><cell></cell><cell cols="4">OntoNotes (NER) TACRED (RE) OpenI (Doc) Average</cell></row><row><cell>DP [28] DP-UI MeTaL</cell><cell>78.4 ± 1.2 81.0 ± 1.2 82.2 ± 0.8</cell><cell>49.0 ± 2.7 54.2 ± 2.6 56.7 ± 2.1</cell><cell>75.8 ± 0.9 76.4 ± 0.5 76.6 ± 0.4</cell><cell>67.7 70.5 71.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/HazyResearch/metal</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning the structure of generative models without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Uniqueness of tensor decompositions with applications to polynomial identifiability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vijayaraghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1610" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Laplacians of graphs and cheeger inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumlien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<title level="m">Aggregating crowdsourced binary ratings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<title level="m">Aggregating crowdsourced binary ratings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Who moderates the moderators?: Crowdsourcing abuse detection in user-generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcafee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08774</idno>
		<title level="m">Who said what: Modeling individual labelers improves classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved pattern learning for bootstrapped entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lipschitz parametrization of probabilistic graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3733</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning from noisy singly-labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04577</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="95" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="955" to="984" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">National Institutes of Health. Open-i</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Estimating accuracy from unlabeled data: A probabilistic logic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<title level="m">Data programming: Creating large training sets, quickly</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An introduction to matrix concentration inequalities. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inferring generative model structure with static analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khandwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ontonotes: A large training corpus for enhanced processing. Handbook of Natural Language Processing and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modeling annotators: A generative approach to learning from annotator rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepDive: Declarative knowledge base construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="93" to="102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Spectral methods meet em: A provably optimal algorithm for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
