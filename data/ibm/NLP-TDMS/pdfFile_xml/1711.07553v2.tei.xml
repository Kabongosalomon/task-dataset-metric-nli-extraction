<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RESIDUAL GATED GRAPH CONVNETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xbresson@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
							<email>tlaurent@lmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics Loyola Marymount University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RESIDUAL GATED GRAPH CONVNETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures. Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance. * XB is supported by NRF Fellowship</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional neural networks of <ref type="bibr" target="#b20">LeCun et al. (1998)</ref> and recurrent neural networks of <ref type="bibr" target="#b17">Hochreiter &amp; Schmidhuber (1997)</ref> are deep learning architectures that have been applied with great success to computer vision (CV) and natural language processing (NLP) tasks. Such models require the data domain to be regular, such as 2D or 3D Euclidean grids for CV and 1D line for NLP. Beyond CV and NLP, data does not usually lie on regular domains but on heterogeneous graph domains. Users on social networks, functional time series on brain structures, gene DNA on regulatory networks, IP packets on telecommunication networks are a a few examples to motivate the development of new neural network techniques that can be applied to graphs. One possible classification of these techniques is to consider neural network architectures with fixed length graphs and variable length graphs.</p><p>In the case of graphs with fixed length, a family of convolutional neural networks has been developed on spectral graph theory by <ref type="bibr" target="#b6">Chung (1997)</ref>. The early work of <ref type="bibr" target="#b5">Bruna et al. (2013)</ref> proposed to formulate graph convolutional operations in the spectral domain with the graph Laplacian, as an analogy of the Euclidean Fourier transform as proposed by <ref type="bibr" target="#b14">Hammond et al. (2011)</ref>. This work was extended by <ref type="bibr" target="#b16">Henaff et al. (2015)</ref> to smooth spectral filters for spatial localization. <ref type="bibr" target="#b9">Defferrard et al. (2016)</ref> used Chebyshev polynomials to achieve linear complexity for sparse graphs, <ref type="bibr" target="#b21">Levie et al. (2017)</ref> applied Cayley polynomials to focus on narrow-band frequencies, and <ref type="bibr" target="#b27">Monti et al. (2017b)</ref> dealt with multiple (fixed) graphs. Finally, <ref type="bibr" target="#b19">Kipf &amp; Welling (2017)</ref> simplified the spectral convnets architecture using 1-hop filters to solve the semi-supervised clustering task. For related works, see also the works of ,  and references therein.</p><p>For graphs with variable length, a generic formulation was proposed by <ref type="bibr" target="#b12">Gori et al. (2005)</ref>; <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref> based on recurrent neural networks. The authors defined a multilayer perceptron of a vanilla RNN. This work was extended by <ref type="bibr" target="#b22">Li et al. (2016)</ref> using a GRU architecture and a hidden state that captures the average information in local neighborhoods of the graph. The work of <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref> introduced a vanilla graph ConvNet and used this new architecture to solve learning communication tasks. <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref> introduced an edge gating mechanism in graph ConvNets for semantic role labeling. Finally, <ref type="bibr" target="#b4">Bruna &amp; Li (2017)</ref> designed a network to learn nonlinear approximations of the power of graph Laplacian operators, and applied it to the unsupervised graph clustering problem. Other works for drugs design, computer graphics and vision are presented by <ref type="bibr" target="#b10">Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b1">Boscaini et al. (2016)</ref>; <ref type="bibr" target="#b26">Monti et al. (2017a)</ref>.</p><p>In this work, we study the two fundamental classes of neural networks, RNNs and ConvNets, in the context of graphs with arbitrary length. Section 2 reviews the existing techniques. Section 3 presents the new graph NN models. Section 4 reports the numerical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NEURAL NETWORKS FOR GRAPHS WITH ARBITRARY LENGTH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RECURRENT NEURAL NETWORKS</head><p>Generic formulation. Consider a standard RNN for word prediction in natural language processing. Let h i be the feature vector associated with word i in the sequence. In a regular vanilla RNN, h i is computed with the feature vector h j from the previous step and the current word x i , so we have:</p><formula xml:id="formula_0">h i = f VRNN ( x i , {h j : j = i − 1} )</formula><p>The notion of neighborhood for regular RNNs is the previous step in the sequence. For graphs, the notion of neighborhood is given by the graph structure. If h i stands for the feature vector of vertex i, then the most generic version of a feature vector for a graph RNN is</p><formula xml:id="formula_1">h i = f G-RNN ( x i , {h j : j → i} )<label>(1)</label></formula><p>where x i refers to a data vector and {h j : j → i} denotes the set of feature vectors of the neighboring vertices. Observe that the set {h j } is unordered, meaning that h i is intrinsic, i.e. invariant by vertex re-indexing (no vertex matching between graphs is required). Other properties of f G-RNN are locality as only neighbors of vertex i are considered, weight sharing, and such vector is independent of the graph length. In summary, to define a feature vector in a graph RNN, one needs a mapping f that takes as input an unordered set of vectors {h j }, i.e. the feature vectors of all neighboring vertices, and a data vector x i , <ref type="figure" target="#fig_0">Figure 1</ref>(a).</p><p>We refer to the mapping f G-RNN as the neighborhood transfer function in graph RNNs. In a regular RNN, each neighbor as a distinct position relatively to the current word (1 position left from the center). In a graph, if the edges are not weighted or annotated, neighbors are not distinguishable. The only vertex which is special is the center vertex around which the neighborhood is built. This explains the generic formulation of Eq. (1). This type of formalism for deep learning for graphs with variable length is described by <ref type="bibr" target="#b29">Scarselli et al. (2009);</ref><ref type="bibr" target="#b11">Gilmer et al. (2017)</ref>;  with slightly different terminology and notations.</p><p>Graph Neural Networks of <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref>. The earliest work of graph RNNs for arbitrary graphs was introduced by <ref type="bibr" target="#b12">Gori et al. (2005)</ref>; <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref>. The authors proposed to use a vanilla RNN with a multilayer perceptron to define the feature vector h i :</p><formula xml:id="formula_2">h i = f G-VRNN (x i , {h j : j → i}) = j→i C G-VRNN (x i , h j ) (2) with C G-VRNN (x i , h j ) = Aσ(Bσ(U x i + V h j )),</formula><p>and σ is the sigmoid function, A, B, U, V are the weight parameters to learn.</p><p>Minimization of Eq.</p><p>(2) does not hold a closed-form solution as the dependence computational graph of the model is not a directed acyclic graph (DAG). <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref> proposed a fixed-point iterative scheme: for t = 0, 1, 2, ... The iterative scheme is guaranteed to converge as long as the mapping is contractive, which can be a strong assumption. Besides, a large number of iterations can be computational expensive.</p><formula xml:id="formula_3">h t+1 i = j→i C(x i , h t j ), h t=0 i = 0 ∀i.<label>(3)</label></formula><p>Gated Graph Neural Networks of <ref type="bibr" target="#b22">Li et al. (2016)</ref>. In this work, the authors use the gated recurrent units (GRU) of <ref type="bibr" target="#b7">Chung et al. (2014)</ref>:</p><formula xml:id="formula_4">h i = f G-GRU (x i , {h j : j → i}) = C G-GRU (x i , j→i h j )<label>(4)</label></formula><p>As the minimization of Eq. (4) does not have an analytical solution, <ref type="bibr" target="#b22">Li et al. (2016)</ref> designed the following iterative scheme:</p><formula xml:id="formula_5">h t+1 i = C G-GRU (h t i ,h t i ), h t=0 i = x i ∀i, whereh t i = j→i h t j , and C G-GRU (h t i ,h t i ) is equal to z t+1 i = σ(U z h t i + V zh t i ) r t+1 i = σ(U r h t i + V rh t i ) h t+1 i = tanh U h (h t i r t+1 i ) + V hh t i h t+1 i = (1 − z t+1 i ) h t i + z t+1 i h t+1 i ,</formula><p>where is the Hadamard point-wise multiplication operator. This model was used for NLP tasks by <ref type="bibr" target="#b22">Li et al. (2016)</ref> and also in quantum chemistry by <ref type="bibr" target="#b11">Gilmer et al. (2017)</ref> for fast organic molecule properties estimation, for which standard techniques (DFT) require expensive computational time.</p><p>Tree-Structured LSTM of <ref type="bibr" target="#b31">Tai et al. (2015)</ref>. The authors extended the original LSTM model of <ref type="bibr" target="#b17">Hochreiter &amp; Schmidhuber (1997)</ref> to a tree-graph structure:</p><formula xml:id="formula_6">h i = f T-LSTM (x i , {h j : j ∈ C(i)}) = C T-LSTM (x i , h i , j∈C(i) h j ),<label>(5)</label></formula><p>where C(i) refers the set of children of node i.</p><formula xml:id="formula_7">C T-LSTM (x i , h i , j∈C(i) h j ) is equal tō h i = j∈C(i) h j i i = σ(U i x i + V ihi ) o i = σ(U o x i + V ohi ) c i = tanh U c x i + V chi f ij = σ(U f x i + V f h j ) c i = i i c i + i∈C(i) f ij c j h i = o i tanh(c i )</formula><p>Unlike the works of <ref type="bibr" target="#b29">Scarselli et al. (2009);</ref><ref type="bibr" target="#b22">Li et al. (2016)</ref>, Tree-LSTM does not require an iterative process to update its feature vector h i as the tree structure is also a DAG as original LSTM. Consequently, the feature representation (5) can be updated with a recurrent formula. Nevertheless, a tree is a special case of graphs, and such recurrence formula cannot be directly applied to arbitrary graph structure. A key property of this model is the function f ij which acts as a gate on the edge from neighbor j to vertex i. Given the task, the gate will close to let the information flow from neighbor j to vertex i, or it will open to stop it. It seems to be an essential property for learning systems on graphs as some neighbors can be irrelevant. For example, for the community detection task, the graph neural network should learn which neighbors to communicate (same community) and which neighbors to ignore (different community). In different contexts, <ref type="bibr" target="#b8">Dauphin et al. (2017)</ref> added a gated mechanism inside the regular ConvNets in order to improve language modeling for translation tasks, and van den Oord et al. (2016) considered a gated unit with the convolutional layers after activation, and used it for image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONVOLUTIONAL NEURAL NETWORKS</head><p>Generic formulation. Consider now a classical ConvNet for computer vision. Let h ij denote the feature vector at layer associated with pixel (i, j). In a regular ConvNet, h +1 ij is obtained by applying a non linear transformation to the feature vectors h i j for all pixels (i , j ) in a neighborhood of pixel (i, j). For example, with 3 × 3 filters, we would have:</p><formula xml:id="formula_8">h +1 ij = f CNN {h i j : |i − i | ≤ 1 and |j − j | ≤ 1}</formula><p>In the above, the notation {h i j : |i − i | ≤ 1 and |j − j | ≤ 1} denote the concatenation of all feature vectors h i j belonging to the 3 × 3 neighborhood of vertex (i, j). In ConvNets, the notion of neighborhood is given by the euclidian distance. As previously noticed, for graphs, the notion of neighborhood is given by the graph structure. Thus, the most generic version of a feature vector h i at vertex i for a graph ConvNet is</p><formula xml:id="formula_9">h +1 i = f G-CNN h i , {h j : j → i}<label>(6)</label></formula><p>where {h j : j → i} denotes the set of feature vectors of the neighboring vertices. In other words, to define a graph ConvNet, one needs a mapping f G-CNN taking as input a vector h i (the feature vector of the center vertex) as well as an unordered set of vectors {h j } (the feature vectors of all neighboring vertices), see <ref type="figure" target="#fig_0">Figure 1</ref>(b). We also refer to the mapping f G-CNN as the neighborhood transfer function. In a regular ConvNet, each neighbor as a distinct position relatively to the center pixel (for example 1 pixel up and 1 pixel left from the center). As for graph RNNs, the only vertex which is special for graph ConvNets is the center vertex around which the neighborhood is built.</p><p>CommNets of <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>. The authors introduced one of the simplest instantiations of a graph ConvNet with the following neighborhood transfer function:</p><formula xml:id="formula_10">h +1 i = f G-VCNN h i , {h j : j → i} = ReLU   U h i + V j→i h j   ,<label>(7)</label></formula><p>where denotes the layer level, and ReLU is the rectified linear unit. We will refer to this architecture as the vanilla graph ConvNet. <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref> used this graph neural network to learn the communication between multiple agents to solve multiple tasks like traffic control.</p><p>Syntactic Graph Convolutional Networks of <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref>. The authors proposed the following transfer function:</p><formula xml:id="formula_11">h +1 i = f S-GCN {h j : j → i} = ReLU   j→i η ij V h j   (8)</formula><p>where η ij act as edge gates, and are computed by:</p><formula xml:id="formula_12">η ij = σ A h i + B h j .<label>(9)</label></formula><p>These gated edges are very similar in spirit to the Tree-LSTM proposed in <ref type="bibr" target="#b31">Tai et al. (2015)</ref>. We believe this mechanism to be important for graphs, as they will be able to learn what edges are important for the graph learning task to be solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODELS</head><p>Proposed Graph LSTM. First, we propose to extend the Tree-LSTM of <ref type="bibr" target="#b31">Tai et al. (2015)</ref> to arbitrary graphs and multiple layers:</p><formula xml:id="formula_13">h +1 i = f G-LSTM x i , {h j : j → i} = C G-LSTM (x i , h i , j→i h j , c i )<label>(10)</label></formula><p>As there is no recurrent formula is the general case of graphs, we proceed as <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref> and use an iterative process to solve Eq. (10): At layer , for t = 0, 1, ..., T  <ref type="figure">Figure 4</ref>).</p><formula xml:id="formula_14">h ,t i = j→i h ,t j , i ,t+1 i = σ(U i x i + V ih ,t i ) o ,t+1 i = σ(U o x i + V oh ,t i ) c ,t+1 i = tanh U c x i + V ch ,t i f ,t+1 ij = σ(U f x i + V f h ,t j ) c ,t+1 i = i ,t+1 i c ,t+1 i + j→i f ,t+1 ij c ,t+1 j h ,t+1 i = o ,t+1 i tanh(c ,t+1 i ) and initial conditions: h ,t=0 i = c ,t=0 i = 0, ∀i, x i = h −1,T i , x =0 i = x i , ∀i,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In other words, the vector h</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Gated Graph ConvNets.</head><p>We leverage the vanilla graph ConvNet architecture of <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>, Eq. <ref type="formula" target="#formula_10">(7)</ref>, and the edge gating mechanism of <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref>, Eq.(8), by considering the following model:</p><formula xml:id="formula_15">h +1 i = f G-GCNN h i , {h j : j → i} = ReLU   U h i + j→i η ij V h j  <label>(11)</label></formula><p>where h =0 i = x i , ∀i, and the edge gates η ij are defined in Eq. (9). This model is the most generic formulation of a graph ConvNet (because it uses both the feature vector h i of the center vertex and the feature vectors h j of neighboring vertices) with the edge gating property.</p><p>Residual Gated Graph ConvNets. In addition, we formulate a multi-layer gated graph ConvNet using residual networks (ResNets) introduced by <ref type="bibr" target="#b15">He et al. (2016)</ref>. This boils down to add the identity operator between successive convolutional layers:</p><formula xml:id="formula_16">h +1 i = f h i , {h j : j → i} + h i .<label>(12)</label></formula><p>As we will see, such multi-layer strategy work very well for graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>(a) Subgraph matching (b) Semi-supervised clustering <ref type="figure">Figure 2</ref>: Graph learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SUBGRAPH MATCHING</head><p>We consider the subgraph matching problem presented by <ref type="bibr" target="#b29">Scarselli et al. (2009)</ref>, see <ref type="figure">Figure 2</ref>(a). The goal is to find the vertices of a given subgraph P in larger graphs G k with variable sizes. Identifying similar localized patterns in different graphs is one of the most basic tasks for graph neural networks. The subgraph P and larger graph G k are generated with the stochastic block model (SBM), see for example <ref type="bibr" target="#b0">Abbe (2017)</ref>. A SBM is a random graph which assigns communities to each node as follows: any two vertices are connected with the probability p if they belong to the same community, or they are connected with the probability q if they belong to different communities.</p><p>For all experiments, we generate a subgraph P of 20 nodes with a SBM q = 0.5, and the signal on P is generated with a uniform random distribution with a vocabulary of size 3, i.e. {0, 1, 2}. Larger graphs G k are composed of 10 communities with sizes randomly generated between 15 and 25. The SBM of each community is p = 0.5. The value of q, which acts as the noise level, is 0.1, unless otherwise specified. Besides, the signal on G k is also randomly generated between {0, 1, 2}. Inputs of all neural networks are the graphs with variable size, and outputs are vertex classification vectors of input graphs. Finally, the output of neural networks are simple fully connected layers from the hidden states.</p><p>All reported results are averaged over 5 trails. We run 5 algorithms; Gated Graph Neural Networks of <ref type="bibr" target="#b22">Li et al. (2016)</ref>  <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref>. The learning schedule is as follows: the maximum number of iterations, or equivalently the number of randomly generated graphs with the attached subgraph is 5,000 and the learning rate is decreased by a factor 1.25 if the loss averaged over 100 iterations does not decrease. The loss is the cross-entropy with 2 classes (the subgraph P class and the class of the larger graph G k ) respectively weighted by their sizes. The accuracy is the average of the diagonal of the normalized confusion matrix w.r.t. the cluster sizes (the confusion matrix measures the number of nodes correctly and badly classified for each class). We also report the time for a batch of 100 generated graphs. The choice of the architectures will be given for each experiment. All algorithms are optimized as follow. We fix a budget of parameters of B = 100K and a number of layers L = 6. The number of hidden neurons H for each layer is automatically computed. Then we manually select the optimizer and learning rate for each architecture that best minimize the loss. For this task, <ref type="bibr" target="#b22">Li et al. (2016)</ref>; <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>; <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref> and our gated ConvNets work well with Adam and learning rate 0.00075. Graph LSTM uses SGD with learning rate 0.075. Besides, the value of inner iterative steps T for graph LSTM and <ref type="bibr" target="#b22">Li et al. (2016)</ref> is 3.</p><p>The first experiment focuses on shallow graph neural networks, i.e. with a single layer L = 1. We also vary the level of noise, that is the probability q in the SBM that connects two vertices in two different communities (the higher q the more mixed are the communities). The hyper-parameters are selected as follows. Besides L = 1, the budget is B = 100K and the number of hidden neurons H is automatically computed for each architecture to satisfy the budget. First row of <ref type="figure">Figure 3</ref>   <ref type="formula" target="#formula_1">2016)</ref>) performs much better, but they also take more time than the graph ConvNets architectures we propose, as well as <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>; <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref>. As expected, all algorithms performances decrease when the noise increases.</p><p>The second experiment demonstrates the importance of having multiple layers compared to shallow networks. We vary the number of layers L = {1, 2, 4, 6, 10} and we fix the number of hidden neurons to H = 50. Notice that the budget is not the same for all architectures. Second row of <ref type="figure">Figure 3</ref> reports the accuracy and time w.r.t. L (middle figure is a zoom in the left <ref type="figure">figure)</ref>. All models clearly benefit with more layers, but RNN-based architectures see their performances decrease for a large number of layers. The ConvNet architectures benefit from large L values, with the proposed graph ConvNet performing slightly better than <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>; <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref>. Besides, all ConvNet models are faster than RNN models.</p><p>In the third experiment, we evaluate the algorithms for different budgets of parameters B = {25K, 50K, 75K, 100K, 150K}. For this experiment, we fix the number of layers L = 6 and the number of neurons H is automatically computed given the budget B. The results are reported in the third row of <ref type="figure">Figure 3</ref>. For this task, the proposed graph ConvNet best performs for a large budget, while being faster than RNNs.</p><p>We also show the influence of hyper-parameter T for <ref type="bibr" target="#b22">Li et al. (2016)</ref> and the proposed graph LSTM. We fix H = 50, L = 3 and B = 100K. <ref type="figure">Figure 4</ref> reports the results for T = {1, 2, 3, 4, 6}. The T value has an undesirable impact on the performance of graph LSTM. Multi-layer <ref type="bibr" target="#b22">Li et al. (2016)</ref> is not really influenced by T . Finally, the computational time naturally increases with larger T values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEMI-SUPERVISED CLUSTERING</head><p>In this section, we consider the semi-supervised clustering problem, see <ref type="figure">Figure 2</ref>(b). This is also a standard task in network science. For this work, it consists in finding 10 communities on a graph given 1 single label for each community. This problem is more discriminative w.r.t. to the architectures than the previous single pattern matching problem where there were only 2 clusters to find (i.e. 50% random chance). For clustering, we have 10 clusters (around 10% random chance). As in the previous section, we use SBM to generate graphs of communities with variable length. The size for each community is randomly generated between 5 and 25, and the label is randomly selected in each community. Probability p is 0.5, and q depends on the experiment. For this task, <ref type="bibr" target="#b22">Li et al. (2016)</ref>; <ref type="bibr" target="#b30">Sukhbaatar et al. (2016)</ref>; <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref> and the proposed gated ConvNets work well with Adam and learning rate 0.00075. Graph LSTM uses SGD with learning rate 0.0075. The value of T for graph LSTM and <ref type="bibr" target="#b22">Li et al. (2016)</ref> is 3.</p><p>The same set of experiments as in the previous task are reported in <ref type="figure">Figure 5</ref>. ConvNet architectures get clearly better than RNNs when the number of layers increase (middle row), with the proposed Gated ConvNet outperforming the other architectures. For a fixed number of layers L = 6, our graph ConvNets and <ref type="bibr" target="#b25">Marcheggiani &amp; Titov (2017)</ref> best perform for all budgets, while paying a reasonable computational cost.   Next, we report the learning speed of the models. We fix L = 6, B = 100K with H being automatically computed to satisfy the budget. <ref type="figure">Figure 6</ref> reports the accuracy w.r.t. time. The ConvNet architectures converge faster than RNNs, in particular for the semi-supervised task.</p><p>To close this study, we are interested in comparing learning based approaches to non-learning variational ones. To this aim, we solve the variational Dirichlet problem with labeled and unlabelled data as proposed by <ref type="bibr" target="#b13">Grady (2006)</ref>. We run 100 experiments and report an average accuracy of 45.37% using the same setting as the learning techniques (one label per class). The performance of the best learning model is 82%. Learning techniques produce better performances with a different paradigm as they use training data with ground truth, while variational techniques do not use such information. The downside is the need to see 2000 training graphs to get to 82%. However, when the training is done, the test complexity of these learning techniques is O(E), where E is the number of edges in the graph. This is an advantage over the variational Dirichlet model that solves a sparse linear system of equations with complexity O(E 3/2 ), see <ref type="bibr" target="#b24">Lipton et al. (1979)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work explores the choice of graph neural network architectures for solving learning tasks with graphs of variable length. We developed analytically controlled experiments for two fundamental graph learning problems, that are subgraph matching and graph clustering. Numerical experiments showed that graph ConvNets had a monotonous increase of accuracy when the network gets deeper, unlike graph RNNs for which performance decreases for a large number of layers. This led us to consider the most generic formulation of gated graph ConvNets, Eq. (11). We also explored the benefit of residuality for graphs, Eq. (12). Without residuality, existing graph neural networks are not able to stack more than a few layers. This makes this property essential for graph neural networks, which receive a 10% boost of accuracy when more than 6 layers were stacked. Future work will focus on solving domain-specific problems in chemistry, physics, and neuroscience.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generic feature representation h i of vertex i on a graph RNN (a) and a graph convNet (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>+1 i is computed by running the model from t = 0, .., T at layer . It produces the vector h ,t=T i which becomes h +1 i and also the input x +1 i for the next layer. The proposed Graph LSTM model differs from Liang et al. (2016); Peng et al. (2017) mostly because the cell C G-LSTM in these previous models is not iterated over multiple times T , which reduces the performance of Graph LSTM (see numerical experiments on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>reports the accuracy and time for the five algorithms and for different levels of noise q = {0.1, 0.2, 0.35, 0.5}. RNN architectures are plotted in dashed lines and ConvNet architectures in solid lines. For shallow networks, all RNN architectures (graph LSTM and Li et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Subgraph matching: First row studies shallow networks w.r.t. noise. Second row investigates multilayer graph networks. Third row reports graph architectures w.r.t. budget. Influence of hyper-parameter T on RNN architectures. Left figure is for graph matching, middle figure for semi-supervised clustering, and right figure are the batch time for the clustering task (same trend for matching).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Semi-supervised clustering: First row reports shallow networks w.r.t. noise q. Second row shows multilayer graph networks w.r.t. L. Third row is about graph architectures w.r.t. budget B. Learning speed of RNN and ConvNet architectures. Left figure is for graph matching and right figure semi-supervised clustering.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbe</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10146</idno>
		<title level="m">Community Detection and Stochastic Block Models: Recent Developments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Tutorial on Geometric Deep Learning on Graphs and Manifolds. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<title level="m">Community Detection with Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<title level="m">Language Modeling with Gated Convolutional Networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural Message Passing for Quantum Chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A New Model for Learning in Graph Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random Walks for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Wavelets on Graphs via Spectral Graph Theory. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Graph Convolutional Neural Networks with Complex Rational Spectral Filters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Gated Graph Sequence Neural Networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic Object Parsing with graph LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generalized Nested Dissection. SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="358" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04826</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-Sentence N-ary Relation Extraction with Graph LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Multiagent Communication with Backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved Semantic Representations from Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional Image Generation with PixelCNN Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
