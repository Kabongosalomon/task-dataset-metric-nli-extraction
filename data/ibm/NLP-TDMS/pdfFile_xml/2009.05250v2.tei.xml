<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Devil&apos;s in the Details: Aligning Visual Clues for Conditional Embedding in Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fufu</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Devil&apos;s in the Details: Aligning Visual Clues for Conditional Embedding in Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Person Re-Identification has made impressive progress, difficult cases like occlusion, change of view-point and similar clothing still bring great challenges. Besides overall visual features, matching and comparing detailed information is also essential for tackling these challenges. This paper proposes two key recognition patterns to better utilize the detail information of pedestrian images, that most of the existing methods are unable to satisfy. Firstly, Visual Clue Alignment requires the model to select and align decisive regions pairs from two images for pair-wise comparison, while existing methods only align regions with predefined rules like high feature similarity or same semantic labels. Secondly, the Conditional Feature Embedding requires the overall feature of a query image to be dynamically adjusted based on the gallery image it matches, while most of the existing methods ignore the reference images. By introducing novel techniques including correspondence attention module and discrepancy-based GCN, we propose an end-to-end ReID method that integrates both patterns into a unified framework, called CACE-Net ((C)lue (A)lignment and (C)onditional (E)mbedding). The experiments show that CACE-Net achieves state-of-the-art performance on three public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) increasingly draws attention due to its wide applications in surveillance, tracking, smart retail, etc <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b20">20]</ref>. Although ReID methods progress rapidly and achieve impressive performance on benchmark datasets, in practice, difficult cases like occlusion, change of view-point and similar clothing still bring great challenges. As shown in <ref type="figure">Figure 1</ref> a), in these cases, the overall appearance of a pedestrian may not always be * Fufu Yu and Xinyang Jiang contribute equally † Correspondance Author: winfredsun@tencent.com <ref type="figure">Figure 1</ref>: Visual Clue Alignment: The illustration of matching image pairs with global feature and local correspondence reliable, and comparing the detailed information becomes essential. Thus, this paper focuses on how to effectively utilize the detailed information for matching pedestrian images.</p><p>Looking at how human annotators would compare the similarities between two images, we find that there are two key recognition patterns involving matching detailed features. As shown in <ref type="figure">Figure 1</ref> a) and b) existing methods usually compare the overall visual similarity of the entire body or densely compare the similarity of all local regions, which could be unreliable in many hard cases. On the other hand, a human annotator will select several local regions crucial for recognition, and align the selected visual clues between two images for pairwise comparison. For example, in <ref type="figure">Figure 1</ref> c), visual clue pairs including hat, shoulder, arms, and shoes are selected for comparison, and since all of these pairs have high feature similarity, one will have high confidence to accept the images as the same person. The same logic can be applied to negative pairs, as shown in <ref type="figure">Figure 1 c)</ref>, the general appearance of the image pair is very similar, but one recognize the images pair as irrelevant by comparing visual clues including the head, legs, shoes and coat pockets.</p><p>Secondly, for the same query image, a human annotator's attention to visual features varies drastically when matching with different gallery images. <ref type="figure" target="#fig_0">Figure 2</ref> is an intuitive example to explain this recognition pattern. For the same query image in <ref type="figure" target="#fig_0">Figure 2</ref>, the value of its feature vectors is conditioned on the gallery image it matches. In other word, different feature vectors are needed to match gallery image A and gallery image B. Since the face and glasses cannot be seen in gallery image A, the channels related to these semantics are suppressed in the corresponding feature. Similarly, when matching query image with gallery image B, channels related to the black jacket and plastic bag are suppressed.</p><p>In conclusion, a good ReID matching model should meet two requirements: 1) Locally, decisive visual clues need to be discovered and aligned for pair-wise comparison, namely Visual Clue Alignment. 2) Globally, the overall feature extracted from a query image should be dynamically adjusted based on the gallery image it matches, namely Conditional Feature Embedding.</p><p>Most of the existing methods do not satisfy the aforementioned two requirements. Some recent alignment-based methods propose to densely align the local parts between image pair based on semantic human part labels <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b19">19]</ref> or feature similarity <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b5">6]</ref> for pairwise comparison. However, these methods align local regions for pair-wise comparison based on pre-defined rules (e.g., regions with highest feature similarity or region with the same human part label). These rules in many cases are not able to discover the most decisive and discriminative visual clues. For example, as shown in <ref type="figure">Figure 1</ref> the importance of the aligned pairs is irrelevant to the feature similarity. In <ref type="figure">Figure 1 b)</ref>, several visual clue pairs with high visual similarity are selected, while in <ref type="figure">Figure 1</ref> d) region pairs with large visual difference should be selected to reject the image pair. In-stead of predefined rules, we propose to learn a novel correspondence attention module to automatically select decisive key-point pairs based on the visual content of both images. Furthermore, most of the existing methods extract individual features from each image and unable to learn dynamically adaptive conditional features.</p><p>As a result, we propose a novel ReID model that integrates both recognition patterns into a unified framework, called CACE-Net. As shown in the orange box of <ref type="figure" target="#fig_1">Figure 3</ref>, for clue alignment, instead of pre-defined alignment rules, we propose a novel correspondence attention module to automatically select and align crucial regions between images. Secondly, since the region correspondence obtained by the last stage forms very complex correspondence graph, GCN is an appropriate tool to capture the the high ordered topological relation among multiple regions. As shown in grey box of <ref type="figure" target="#fig_1">Figure 3</ref>, our model takes the obtained region correspondence graph as inputs and extracts conditional feature embedding with a novel graph convolutional network <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b0">1]</ref>. Instead of the standard GCN that smooths the adjacent node features, a novel discrepancy-based graph convolution is proposed to obtain the feature difference between crucial regions.</p><p>The contribution of our proposed method is listed as follows: 1) CACE-Net is able to integrate both visual clue alignment and conditional feature embedding into a unified ReID framework. 2) Instead of using a pre-defined Adjacency Matrix, our CACE-Net uses a novel correspondence attention module where the visual clues is automatically predicted and dynamically adjusted during training. 3) A novel discrepancy-based graph convolution is proposed to analyze the feature difference between adjacent graph nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Part base methods</head><p>Part-based models learn local features of different body parts to enhance the global ReID feature on cross-view matching. One of the most common and effective type of part-based models simply split the output feature-maps of ReID model's inter-mediate layers into several horizontal stripes and learn local features for each stripe, such as PCB <ref type="bibr" target="#b20">[20]</ref>, MGN <ref type="bibr" target="#b23">[23]</ref>, Pyramid <ref type="bibr" target="#b33">[33]</ref>, RelationNet <ref type="bibr" target="#b14">[14]</ref> and VA-ReID <ref type="bibr" target="#b37">[37]</ref>. Another type of part-based models <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34]</ref>segment human body into meaningful body parts and learn local feature for each body parts. SPReID <ref type="bibr" target="#b8">[9]</ref> learns a human parsing branch for body part segmentation and fuses local features for different parts by weighted average pooling. DSA-ReID <ref type="bibr" target="#b29">[29]</ref> proposes projects human parts into a UV space and uses this UV space branch to guide the learning of a stripe model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Alignment-based methods</head><p>On top of the local features, instead of fusing the local features directly, some methods propose to align parts from a pair of images, and match a pair of images based on the similarity of their aligned part pairs. AlignReID <ref type="bibr" target="#b28">[28]</ref> proposes a dynamic programming algorithm to align a stripe in the image to a stripe in another image based on their local feature similarity. DSR <ref type="bibr" target="#b5">[6]</ref> and SFR <ref type="bibr" target="#b6">[7]</ref> propose a sparse coding method to implicitly look for similar key-point pairs by reconstructing one image's feature map with another. VPM <ref type="bibr" target="#b19">[19]</ref> proposes to align the stripes from two images based on the visibility of each stripes. PGFA <ref type="bibr" target="#b13">[13]</ref> exploits pose landmarks to align stripes. HOReID <ref type="bibr" target="#b22">[22]</ref> aligns same semantic parts for Occluded ReID by the aid of extra keypoints information. CDPM <ref type="bibr" target="#b24">[24]</ref> proposes to localize and align local parts by a sliding window method. Some GAN based methods like FD-GAN <ref type="bibr" target="#b3">[4]</ref> proposes to align local features by directly transfer the image to the same pose and viewpoint of the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint Feature Learning</head><p>In this paper, Joint Feature Learning refers to methods that feed both images into a model simultaneously to obtain a conditional feature embedding. Existing methods like DCCs <ref type="bibr" target="#b26">[26]</ref> and Deep Spatially Multiplicative Integration <ref type="bibr" target="#b27">[27]</ref> learns a RNN to iteratively encode couple features step-by-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attention-based methods</head><p>Attention based methods are a type of State-of-the-Art ReID methods that propose to select important regions or channels of a feature-maps to form the ReID feature and discard region irrelevant to recognition such as background. Unlike our CACE-Net that selects crucial region pairs based on a pair of images, most of the existing attention based methods focus on selecting important information from individual images. Method in <ref type="bibr" target="#b32">[32]</ref> proposes to predict multiple attention maps for different human parts. HA-CNN <ref type="bibr" target="#b12">[12]</ref> uses a Harmonious Attention module to conduct feature selection both spatially and in channel-wise for a individual image. ABDNet <ref type="bibr" target="#b1">[2]</ref> proposes a similar spatial and channel attention with an orthogonality constraint. RGA-SC <ref type="bibr" target="#b30">[30]</ref> stacks the pairwise relations between single feature and all features together with the feature itself to infer the current position's attention on spatial and channel dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Graph-based methods</head><p>Recently some methods propose to use graph-based techniques to learn more complex relationship for ReID model. However, instead of exploring the complex correlation between detailed local regions inside image pairs, most of the existing methods are based on global features. SG-GNN <ref type="bibr" target="#b17">[17]</ref> use graph to represent the relation between multiple probe images and gallery images and use a graph neural network update samples' global features with a massage passing method. Group shuffling random walk <ref type="bibr" target="#b18">[18]</ref> further extend the probe-gallery relation to gallery-gallery relationship. <ref type="figure" target="#fig_1">Figure 3</ref> shows the general training workflow of CACE-Net. CACE-Net is an end-to-end learning framework containing three stages, namely individual feature embedding, visual clue alignment, and conditional feature embedding. At the first stage, our method extracts individual feature maps for both images. Secondly, given the feature-maps of two images, a correspondence attention module is used to select crucial region pairs both within an image and between image pair. Finally, a novel discrepancy-based GCN is used to extract conditional features from the region correspondence graph. The following three sub-sections elaborate on the implementation and formulation the three stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Individual Feature Embedding</head><p>The individual feature embedding stage is responsible for extracting a feature-map for each individual pedestrian image. Any type of CNN backbones for person Reidentification can be applied for the individual feature extraction.</p><p>As shown in the blue box in <ref type="figure" target="#fig_1">Figure 3</ref>, to enforce the backbone network extracting good individual features, an additional training loss branch is attached to the module. Given a training set D = {(I (u) , y (u) )} N u=1 , the input image I (u) is first fed into a backbone network. Then, the output feature-map is further fed into a encoder (i.e. a Global Average Pooling followed by a 1 * 1 convolution layer) to obtain the individual feature vector for I <ref type="bibr">(u)</ref> , denoted as f (I (u) ). Then, a cross entropy based ID loss is used to train the individual feature extractor:</p><formula xml:id="formula_0">L CE (y (u) , I (u) ) = 1 C C c 1(y (u) = c)log(p(c|f (I (u) ))) (1) where, p(c|f (I (u) )) = e W T C f (I (u) ) K j=1 e W T j f (I (u) )</formula><p>, and W is a weight matrix of a fully connect layer to classify f (I (u) ) into different identities, and C is the total number of identities in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Visual Clue Alignment</head><p>Visual Clue Alignment select and align crucial regions for pairwise detailed comparison. We denote the feature- map extracted at individual feature embedding stage as X ∈ R H * W * D , where H and W is the height and width of the feature-map and D is the number of feature channels. X is further reshaped into a two-dimensional HW * D matrix denoted asX. Given the feature-maps of two images indexed asX (u) andX (v) , we select and align the visual clues between them by evaluating the importance of each pair of pixels, denoted as</p><formula xml:id="formula_1">S (u,v) ∈ R HW * HW , where each element S (u,v) ij</formula><p>in the matrix indicate the importance of an aligned pixel pair. Then, The selected pairs of pixels in X forms a undirected graph, whose adjacent matrix A (u,v) ∈ R HW * HW will be used to extract conditional features of both images with Graph Convolutional Network.</p><p>One of the intuitive way to obtain the correlation between two different visual clues is to compute feature similarity. In the ablation study, a cosine similarity is used to evaluate the importance and correlation between a pixel pair from two feature-maps X</p><formula xml:id="formula_2">(u) i , X (v)</formula><p>j . Then, following a similar strategy in <ref type="bibr" target="#b28">[28]</ref>, the crucial region pairs are selected by assigning each pixel with the pixel with highest similarity from the other feature-map. Another way to build the pixel correspondence between feature-map pairs is by the human part or other semantic labels of each pixel, where all the corresponding human part are selected with the same importance weight.</p><p>As discussed in Section 1, the aforementioned two types of methods for visual clue alignment and selection are based on pre-defined rules and in many cases not able to find the most decisive and discriminative region pairs. Hence, we further propose a novel correspondence attention module that automatically selects crucial visual clue pairs.</p><p>Our method not only focuses on finding crucial visual clues between two images, but also discovering intrarelationship between feature-map pixels within an individual image. The importance of pixel pairs within a feature-mapX (u) is computed as follows:</p><formula xml:id="formula_3">S (u) =X (u) WX (u)T<label>(2)</label></formula><p>where W is a diagonal parameter matrix that assigns a learn-able weight for each feature-map channel. Similarly, given two images whose feature-map is denoted as X (u) and X (v) , the inter-image importance is computed as follows:</p><formula xml:id="formula_4">S (u,v) = X (u) W X (v)T<label>(3)</label></formula><p>We combine the importance weight of both intra-image and inter-image visual clues, and select the crucial pairs based on the importance matrix as follow:</p><formula xml:id="formula_5">A (u,v) = ReLU S (u) S (u,v) S (v,u) S (v)<label>(4)</label></formula><p>where a ReLU activation is used to select the regions pairs with positive importance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Conditional Feature Embedding</head><p>Given a pair of images (I (u) , I (v) ), the conditional feature embedding of I (u) should be dynamically adjusted based on I (v) , denoted as f cond (I i |I j ).</p><p>In order to fully exploit the detailed information in both images, we propose to extract the conditional feature embedding based on the crucial visual clue pairs between I (u) and I <ref type="bibr">(v)</ref> . Given the alignment graph A (u,v) of visual clues predicted by the correspondence attention module, we propose a novel discrepancy-based GCN to encode the complex graph structured contextual information into conditional feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Discrepancy-based GCN</head><p>In this paper, we choose a spectral based GCN <ref type="bibr" target="#b10">[10]</ref> to extract conditional features. Given the i-th pixel of an image's feature-map, denoted asX (u) i (i.e. the node feature), the graph convolution operation is formulated as follows:</p><formula xml:id="formula_6">g θ * X (u) i = θ 0 X (u) i + θ 1 (L − I)X (u) i = θ 0 X (u) i − θ 1 D − 1 2 A (u,v) D 1 2 X (u) i<label>(5)</label></formula><p>where L is the Laplacian matrix of the adjacent matrix A <ref type="bibr">(u,v)</ref> To further decrease the number of learn-able parameter, common GCN sets θ = θ 0 = −θ 1 , which leads to following expression:</p><formula xml:id="formula_7">g θ * X (u) i = θ(I + D − 1 2 A (u,v) D 1 2 )X (u) i<label>(6)</label></formula><p>By taking a closer look at this equation, we will find that this operation is essentially a weighted average or smooth operation over current node feature itself and its connected neighbours. However, this is not what we want for our graph convolution. Under the setting of the Re-identification, instead of smoothing the value between connected nodes, we require the model to obtain the features difference between the aligned crucial regions. Thus, We propose a novel graph convolution operation, that instead of letting θ 0 = −θ 1 , sets θ = θ 0 = θ 1 , which leads to following graph convolution operation:</p><formula xml:id="formula_8">g θ * X (u) i = θ(I − D − 1 2 A (u,v) D 1 2 )X (u) i = θD − 1 2 LD 1 2 X (u) i<label>(7)</label></formula><p>From Eq. 7, we can see that for our new graph convolution, the coefficient of g θ becomes the normalized graph Laplacian matrix, which is equivalent to computing a secondary gradient of the node feature. Hence this convolution is able to obtain the level of feature change between adjacent nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Mixed up ID Loss</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, given the feature-maps of image pair (I (u) , I (v) ) and the adjacent matrix A (u,v) generated from the feature-map pair, we obtain the conditional feature-map by applying the graph convolution described in Eq. 7. The outputs are a pair of conditional feature-map with the same size of the input feature-map. Similar to the individual exam stage, the conditional feature-maps are then fed into a feature encoder consisting of a Global Average Pooling layer and a 1 * 1 convolution layer for dimension reduction to obtain the encoded conditional feature vectors .</p><p>In the training process, we use both triplet loss and cross entropy loss as the supervised signals. In every training iteration, we sample P identities from the training set and for each identity in the training set, we sample M samples. For triplet loss, we use a common hard triplet loss for person ReID <ref type="bibr" target="#b7">[8]</ref>. As for the cross-entropy loss, since the conditional feature f cond (I (u) |I (v) ) is extracted based on the information from both I u and I (v) , the identity labels from both image should be used to supervised the feature extraction. Instead of the common cross entropy loss, we propose a mix-up cross entropy loss specifically for training conditional feature vector. Given a mini-batch containing P M images, the mix-up cross-entropy loss is formulated as:</p><formula xml:id="formula_9">L mix−up = P M u=1 P M v=1 αL CE (y (u) , f cond (I (u) |I (v) )) + (1 − α)L CE (y (v) , f cond (I (u) |I (v) ))<label>(8)</label></formula><p>where L CE is the softmax and cross entropy loss shown in Eq.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model Inference</head><p>Like most of the ReID method, the model is evaluated as an image retrieval task. Given a query image set containing N q images and a gallery set containing N g images, we need to retrieve the images with the same identity of I q . Our method obtains the similarity between two images with both individual features and conditional features.</p><p>We first extract the individual features for all images in query and gallery set, with computational complexity of O(N q + N g ). Then, for each query, we first sort the gallery images based on the similarity of the individual features. After that, the feature-maps of query image and the top-K images in sorted gallery forms K feature-map pairs, which are fed into the key-point alignment stage and conditional feature embedding stage to obtain conditional features, with computational complexity of O(N q K). Finally, the top-K gallery images are sorted once more by the similarity of coupled features, forming the final ranking result. Compared to the individual feature extraction, much fewer computing operations are needed to obtain conditional feature embedding, so the entire computation cost of CACE-Net is very close to normal ReID method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we propose the performance comparison of CACE-Net with the state-of-the-art methods and ablation study of different components in CACE-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Our experiments are conducted on three widely used ReID benchmark datasets. Market-1501 <ref type="bibr" target="#b35">[35]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Detail</head><p>The input images are resized into 384 × 128. In training stage, we set batch size to be 16 by sampling 4 identities and 4 images per identity. The ResNet-50 <ref type="bibr" target="#b4">[5]</ref> model pretrained on ImageNet is used as the backbone network. Some common data augmentation strategies including horizontal flipping, random cropping, random erasing <ref type="bibr" target="#b36">[36]</ref> (with a probability of 0.5) are used. We adopt Gradient Descent optimizer to train our model and set weight decay 5 × 10 −4 . The total number of epoch is 80. The learning rate is initialized to 6.25 × 10 −3 and is decayed by cosine method until it equals to 0. At the beginning, we warm up the models for 5 epochs and the learning rate grows linearly from 0 to 6.25 × 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this sub-section, we report the evaluation results of the influence of different components and hyper-parameters of our method. <ref type="table" target="#tab_1">Table 1</ref> shows the influence of the three stages of CACE-Net (i.e. individual feature embedding, visual clue alignment and conditional feature embedding). Following methods are compared:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Influence of Model Components</head><p>• Individual Feature Embedding. This is a baseline method using only the individual feature vector extracted by the encoder in the individual exam stage. We observe that this method achieves lowest performance. • Visual Clue Alignment. The feature-maps extracted by the individual stage are further fed into the Visual Clue Alignment stage to get correspondent pixel pairs between two images. Instead of applying GCN, we directly compute the average feature similarity of the cross-image keypoint pairs as the overall similarity of the two images. <ref type="table" target="#tab_1">Table 1</ref> shows an extra key-point alignment improves the baseline model by more than 1 percentage point in terms of MAP.</p><p>• Individual-GCN. All three stages are performed, but all cross image connections in the adjacent matrix are discarded and only intra-frame relations are considered. As show in <ref type="table" target="#tab_1">Table 1</ref>, intra-frame relation based GCN gives around 1 percent improvement in terms of MAP, which indicates that extracting features based on local information and correlation inside individual image can boost ReID performance. • Pairwise-GCN (Normal). Pairwise-GCN considers both intra-frame relation and inter-frame relation in GCN.</p><p>Here the common GCN that uses a smooth operation on adjacent node is applied and we observe that normal GCN does not achieve performance improvement, which indicates that normal GCN is not suitable for extracting conditional features for ReID. • Pairwise-GCN (Discrepancy-based).</p><p>Our novel discrepancy-based GCN is used and achieves obvious improvement compared to normal GCN. Furthermore, adding inter-frame relation between two images outperforms individual-GCN by a clear margin, showing the effectiveness of extracting conditional features based on local correlation between image pairs. We visualize the conditional feature-map obtained by CACE-NET in <ref type="figure">Figure 4</ref>. The feature-map is visualized by computing the L2 norm of each feature in the feature-map. As shown in <ref type="figure">Figure 4</ref>, the feature-map of the query image (the left image in each column) changes drastically based on the gallery image it matches (the right image in each column). For example, the feature-map will have higher activation on the lower body on column 1 row 2, column 2 row 1 and column 3 row 1, because the lower body has more distinctive features to tell the query image and the corresponding gallery image apart. This visualization results prove that CACE-NET is able to dynamically adjust the feature embedding based on the image it matches, i.e. the conditional feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Influence of Alignment Strategy</head><p>In order to prove the advantage of using automatically learned neural network to predict the correspondence be- <ref type="figure">Figure 4</ref>: The visualization of Conditional Feature Embedding. The feature-map difference of the same query when matching with different images is shown. For each column, the images on the left are different feature-maps of the same query image, and the images on the right are the different gallery images the query matches. The query's feature-map is dynamically adjusted based on the gallery image.  tween pixel in the feature-map pairs, <ref type="table" target="#tab_2">Table 2</ref> compares the influence of different alignment strategies to our CACE-Net method. Following alignment strategy is evaluated:</p><p>• No Alignment: The baseline method uses only feature vector from individual exam stage where no connection exists between any pixels. • Part-based Alignment: Similar to the part-based model like PCB <ref type="bibr" target="#b20">[20]</ref>, two pixels at the same location of the images are aligned. As shown in <ref type="table" target="#tab_2">Table 2</ref>, part alignment strategy outperforms the baseline method but achieve lower performance than correspondence attention, because it is not robust to scale changes and unable to rule out non-decisive pairs. • Fully Connect: A fully connected adjacent matrix is applied where all pixels are correspondent. In this way, the contextual information for all other pixels are explored when extracting a conditional feature. This method outperforms the baseline but does not achieve the best performance, because too much redundant contextual information is involved in a fully connected graph. • Similarity-based Alignment: Similar to AlignedReID <ref type="bibr" target="#b28">[28]</ref>, this method selects the most similar pixel as each pixel's correspondent neighbour. It does not perform as well as our method because as discussed in section 1, the predefined similarity-based alignment strategy is able to find the most decisive and discriminative region pairs. • Semantic Alignment: Pixels lie in the same semantic body part are connected as neighbours whose distances are 1, unlike soft distances in our correspondence attention. Despite the help of extra human parsing network, this method is inferior to our correspondence attention. • Correspondence Attention Module: With a correspondence attention module, CACE-Net is able to build a more crucial correspondence compared to pre-defined rules and achieves the best performance.</p><p>In <ref type="figure" target="#fig_2">Figure 5</ref> we compare the visualization results of similarity-based alignment and correspondence attention, where crucial region pairs with highest scores are visualized. We observe that similarity-based alignment that only aligns region pairs with high visual similarity, causing mismatching images with similar local parts (e.g., the lower body in <ref type="figure" target="#fig_2">Figure 5</ref> a,b and upper-body in <ref type="figure" target="#fig_2">Figure 5 c)</ref>. On the other hand, our correspondence attention disregards the visual similarity and is able to focus on correct decisive clues to reject image pairs (e.g. the shoulder area in <ref type="figure" target="#fig_2">Figure 5</ref>). <ref type="figure">Figure 6</ref> compares the retrieval results of baseline method and CACE-Net. We observe that in hard cases like similar clothing or view-point variance show in <ref type="figure">Figure 6</ref>,  <ref type="figure">Figure 6</ref>: The comparison of retrieval results of baseline method and CACE-Net.</p><p>CACE-Net is able to achieve much better results. It proves that even for images with extremely similar general visual appearance, CACE-Net is able to discover and compare crucial visual clues, such as the hair style in Figure6 row 1 and row 2, pattern on the t-shirt in row 3 and shoes in row 4. <ref type="table" target="#tab_3">Table 3</ref> shows the influence of our customized Mix-up Loss for conditional feature embedding. As shown in <ref type="table" target="#tab_3">Table 3</ref>, with the right hyper-parameter α, our proposed Mix-up Loss significantly outperforms common cross entropy loss. We also observe the influence of the hyper-parameter α to the model performance, where CACE-Net achieves the best performance when α is set to 0.9, which shows that conditional feature contains only small amount of information from contextual image compared to the target image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Influence of Mix-up ID Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the State-of-the-Art</head><p>We evaluate our proposed CACE-Net with the stateof-the-art ReID models. These methods include: (1) the part-based models such as Pyramid, RelationNet; (2) the alignment-based methods like AlignReID, VPM, HOReID;</p><p>(3) the attention-based methods like ABD-Net, RGA-SC, SCSN; (4) Joint learning methods including DCCs and SMI that learns conditional features with RNN; (5) ReID methods that utilizes graph structure such as Group-shuffling Random Walk and SGGNN. <ref type="table" target="#tab_4">Table 4</ref> shows the performance comparison of CACE-Net with State-of-the-Art methods. As shown in <ref type="table" target="#tab_4">Table 4</ref>, thanks to our novel ReID framework that integrates visual clue alignment and conditional feature embedding, CACE-Net outperforms most of the state-ofthe-art methods on Market1501, DukeMTMC and MSMT-17.</p><p>Furthermore, our CACE-Net can also be applied in ReID in occluded person. As shown in <ref type="table" target="#tab_5">Table 5</ref>, other than archieves state-of-the-art method on the general ReID datasets, CACE-Net also outperforms most of the existing occluded ReID methods. It further demonstrates CACE-Net has the ability to solve hard cases like body occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Conditional Feature Embedding: The illustration of changing conditional feature when comparing a image with different contextual images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>General Pipeline of CACE-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The cross-image alignment result obtained by similarity-based alignment and correspondence attention. The red lines denote a visual clue correspondence between two images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>dataset contains 32,668 person images of 1,501 identities captured by six cameras. Training set is composed of 12,936 images of 751 identities while testing data is composed of the other images of 750 identities. MSMT-17 [25] dataset contains 124,068 person images of 4,101 identities captured by 15 cameras (12 outdoor, 3 indoor). Training set is composed of 30,248 images of 1,041 identities while testing data is composed of the other images of 3060 identities. DukeMTMC-reID [16] dataset contains 36,411 person images of 1,404 identities captured by eight cameras. They are randomly divided, with 702 identities as the training set and the remaining 702 identities as the testing set. In the testing set, for each ID in each camera, one image is picked for the query set while the rest remain for the gallery set. Occluded-DukeMTMC re-splits the DukeMTMC-reID dataset to generate the new Occluded-DukeMTMC dataset. All query images and 10% gallery images in the new dataset are occluded person images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance (%) comparisons of three stages (Individual Feature Embedding, Visual Clue Alignment and Conditional Embedding) on DukeMTMC.</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>Individual Feature Embedding</cell><cell cols="2">77.03 87.84</cell></row><row><cell>+ Visual Clue Alignment</cell><cell cols="2">78.29 89.68</cell></row><row><cell>+ Individual-GCN</cell><cell cols="2">79.19 89.77</cell></row><row><cell>+ Pairwise-GCN (Normal)</cell><cell cols="2">78.58 89.77</cell></row><row><cell cols="3">+ Pairwise-GCN (Discrepancy-based) 81.29 90.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Performance (%) comparisons of different align-</cell></row><row><cell cols="3">ment strategy for graph generation in CACE-Net on</cell></row><row><cell>DukeMTMC.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>No Alignment (Baseline)</cell><cell cols="2">77.03 87.84</cell></row><row><cell>Part Alignment</cell><cell cols="2">78.39 89.45</cell></row><row><cell>Top-K Similarity</cell><cell cols="2">79.30 89.72</cell></row><row><cell>Fully Connect</cell><cell cols="2">80.19 89.90</cell></row><row><cell>Semantic Alignment</cell><cell cols="2">80.64 90.08</cell></row><row><cell cols="3">Correspondence Attention 81.29 90.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance (%) comparisons of different α in mix-up id loss on Market1501.</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell cols="3">Cross Entropy (Baseline) 88.57 94.74</cell></row><row><cell>Mix-up α = 0.95</cell><cell cols="2">89.44 95.75</cell></row><row><cell>Mix-up α = 0.9</cell><cell cols="2">90.30 95.96</cell></row><row><cell>Mix-up α = 0.8</cell><cell cols="2">89.56 95.84</cell></row><row><cell>Mix-up α = 0.7</cell><cell cols="2">88.72 95.43</cell></row><row><cell>Mix-up α = 0.6</cell><cell cols="2">85.67 94.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance (%) comparisons to the state-of-the-art results on Market-1501, DukeMTMC-reID and MSMT-17. Our proposed CACE-Net outperforms the state-of-the-art methods.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell cols="2">Market-1501 mAP Rank-1</cell><cell cols="2">DukeMTMC-reID mAP Rank-1</cell><cell cols="2">MSMT-17 mAP Rank-1</cell></row><row><cell></cell><cell>PCB (ECCV 2018) [20]</cell><cell>77.4</cell><cell>92.3</cell><cell>66.1</cell><cell>81.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Part-based</cell><cell>MGN (ACM MM 2018) [23] Pyramid (CVPR 2019) [33]</cell><cell>86.9 88.2</cell><cell>95.7 95.7</cell><cell>78.4 79.0</cell><cell>88.7 89.0</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>RelationNet (AAAI 2020) [14]</cell><cell>88.9</cell><cell>95.2</cell><cell>78.6</cell><cell>89.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>AlignedReID (ECCV 2018) [20]</cell><cell>79.3</cell><cell>91.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Alignment</cell><cell>FD-GAN (NIPS 2018) [4] VPM (CVPR 2019) [19]</cell><cell>77.7 80.8</cell><cell>90.5 93.0</cell><cell>64.5 -</cell><cell>80.0 -</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>HOReID (CVPR 2020) [22]</cell><cell>84.9</cell><cell>94.2</cell><cell>75.6</cell><cell>86.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ABD-Net (ICCV 2019) [2]</cell><cell>88.28</cell><cell>95.6</cell><cell>78.59</cell><cell>89.0</cell><cell>60.8</cell><cell>82.3</cell></row><row><cell>Attention</cell><cell>RGA-SC (CVPR 2020) [30]</cell><cell>88.4</cell><cell>96.1</cell><cell>-</cell><cell>-</cell><cell>57.5</cell><cell>80.3</cell></row><row><cell></cell><cell>SCSN (CVPR 2020) [3]</cell><cell>88.5</cell><cell>95.7</cell><cell>79.0</cell><cell>91.0</cell><cell>58.5</cell><cell>83.8</cell></row><row><cell>Joint Learning</cell><cell>SMI (PR 2018) [27] DCCs (TNNLS 2020) [26]</cell><cell>65.25 71.1</cell><cell>86.15 88.4</cell><cell>-59.2</cell><cell>-80.3</cell><cell>--</cell><cell>--</cell></row><row><cell>Graph-based</cell><cell cols="2">Group-shuffling (CVPR 2018) [17] 82.5 SGGNN (ECCV 2018) [18] 82.8</cell><cell>92.7 92.3</cell><cell>66.4 68.2</cell><cell>80.7 81.1</cell><cell>--</cell><cell>--</cell></row><row><cell>CACE-Net</cell><cell>CACE-Net</cell><cell>90.3</cell><cell>95.96</cell><cell>81.29</cell><cell>90.89</cell><cell>62.0</cell><cell>83.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance (%) comparisons of State-of-the-art occluded ReID method and CACE-Net on Occluded Duke.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP Rank-1</cell></row><row><cell>PCB (ECCV2018)[21]</cell><cell>33.7</cell><cell>42.6</cell></row><row><cell>DSR (CVPR2018)[6]</cell><cell>30.4</cell><cell>40.8</cell></row><row><cell>SFR [7]</cell><cell>32.0</cell><cell>42.3</cell></row><row><cell>PGFA (ICCV 2019)[13]</cell><cell>37.3</cell><cell>51.4</cell></row><row><cell cols="2">HOReID (CVPR 2020) [22] 43.8</cell><cell>55.1</cell></row><row><cell>SGSFA (ACML 2020) [15]</cell><cell>47.4</cell><cell>62.3</cell></row><row><cell>CACE-Net</cell><cell>50.8</cell><cell>58.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel Person ReID framework that integrates both visual clue alignment and conditional feature embedding. Our proposed CACE-Net is able to automatically select crucial region pairs by correspondence attention module, and extract conditional feature embedding from the key-point pairs with a novel discrepancy-based GCN. The experiments show the effectiveness of our model.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Abd-net: Attentive but diverse person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01114</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salienceguided cascaded suppression network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fd-gan: Poseguided feature distilling gan for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1222" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7073" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07399</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Recognizing partial biometric patterns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">E</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11839" to="11847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic-guided shared feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Xuena Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bao</surname></persName>
		</author>
		<idno>PMLR, 2020. 8</idno>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-order information matters: Learning relation and topology for occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Guan&amp;apos;an Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cdpm: convolutional deformable part models for semantically aligned person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep coattentionbased comparator for relative representation learning in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Whatand-where to match: Deep spatially multiplicative integration networks for person re-identification. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="727" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multiloss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose invariant embedding for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Viewpoint-aware loss with angular regularization for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01300</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
