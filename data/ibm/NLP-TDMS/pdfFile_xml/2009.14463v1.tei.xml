<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural RST-based Evaluation of Discourse Coherence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
							<email>g.guz@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British</orgName>
								<address>
									<addrLine>Columbia 1</addrLine>
									<country>Inverted AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
							<email>pbateni@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British</orgName>
								<address>
									<addrLine>Columbia 1</addrLine>
									<country>Inverted AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Muglich</surname></persName>
							<email>darius.muglich@alumni</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British</orgName>
								<address>
									<addrLine>Columbia 1</addrLine>
									<country>Inverted AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
							<email>carenini@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British</orgName>
								<address>
									<addrLine>Columbia 1</addrLine>
									<country>Inverted AI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural RST-based Evaluation of Discourse Coherence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper evaluates the utility of Rhetorical Structure Theory (RST) trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence (GCDC) and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62% fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>. Most current methods can be described as either stemming from explicit representations based on the Centering Theory <ref type="bibr" target="#b5">(Grosz et al., 1994)</ref>, or deep learning approaches that learn without the use of hand-crafted linguistic features.</p><p>Our work explores a third research avenue based on the Rhetorical Structure Theory (RST) <ref type="bibr" target="#b10">(Mann and Thompson, 1988)</ref>. We hypothesize that texts of low/high coherence tend to adhere to different discourse structures. Thus, we pose that using even silver-standard RST features should help in separating coherent texts from incoherent ones. This stems from the definition of the coherence itselfas the writer of a document needs to follow specific rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect * to its local and global context, and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents. However, if a parser has difficulty interpreting a given document, it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents. This idea was first explored by , who followed an approach similar to <ref type="bibr" target="#b0">Barzilay and Lapata (2008)</ref> by estimating entity transition likelihoods, but instead using discourse relations (predicted by a state of the art discourse parser ) that entities participate in as opposed to their grammatical roles. Their method achieved significant improvements in performance even when using silver-standard discourse trees, showing potential in the use of parsed RST features for classifying textual coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors contributed equally</head><p>Our work, however, is the first to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation. <ref type="bibr">Furthermore, Feng et al. (2014)</ref>   <ref type="figure">Figure 2</ref>: Recursive LSTM architecture used in RST-Recursive adapted from <ref type="bibr" target="#b16">(Tai et al., 2015)</ref>. sentence permutation task, which involves ranking a sentence-permuted text against the original. As noted by <ref type="bibr" target="#b8">Lai and Tetreault (2018)</ref>, this is not an accurate proxy for realistic coherence evaluation. We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence (GCDC), where the model needs to classify a naturally produced text into one of three levels of coherence. Our contributions involve: (1) RST-Recursive, an RST-based neural tree-recursive method for coherence evaluation that achieves 2% below the state of the art performance on the GCDC while having 62% fewer parameters.</p><p>(2) When ensembled with the current state of the art, namely Parseq <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>, we achieve a notable improvement over the plain ParSeq model. (3) We demonstrate the usefulness of silver-standard RST features in coherence classification, and establish our results as a lower-bound for performance improvements to be gained using RST features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coherence Evaluation of Text</head><p>Centering Theory <ref type="bibr" target="#b5">(Grosz et al., 1994)</ref> states that subsequent sentences in coherent texts are likely to continue to focus on the same entities (i.e., subjects, objects, etc.) as within the previous sentences. Building on top of this, <ref type="bibr" target="#b0">Barzilay and Lapata (2008)</ref> were the first to propose the Entity-Grid model that constructs a two-dimensional array G n,m for a text of n sentences and m entities, which are used to estimate transition probabilities for entity occurrence patterns. More recently, <ref type="bibr" target="#b2">Elsner and Charniak (2011)</ref>    <ref type="bibr" target="#b8">Lai and Tetreault (2018)</ref> developed a hierarchical neural architecture named ParSeq with three stacked LSTM Networks, designed to encode the coherence at sentence, paragraph and document levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rhetorical Structure Theory (RST)</head><p>RST describes the structure of a text in the following way: first, the text is segmented into elementary discourse units (EDUs), which describe spans of text constituting clauses or clause-like units <ref type="bibr" target="#b10">(Mann and Thompson, 1988)</ref>. Second, the EDUs are recursively structured into a tree hierarchy where each node defines an RST relation between the constituting sub-trees. The sub-tree with the central purpose is called the nucleus, and the one bearing secondary intent is called the satellite while a connective discourse relation is assigned to both. An example of a nucleus-satellite" relation pairing is presented in <ref type="figure" target="#fig_0">Figure 1</ref> where a claim is followed by the evidence for the claim; RST posits an Evidence relation between these two spans with the left sub-tree being the nucleus" and the right sub-tree as satellite".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RST-Recursive</head><p>We parse silver-standard RST trees for documents using the CODRA <ref type="bibr" target="#b6">(Joty et al., 2015)</ref> RST parser, which we then employ as input to our recursive neural model, RST-Recursive. The overall procedure for RST-Recursive is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Given a document of n EDUs E 1:n with each EDU E i represented as a list of GloVe embeddings <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>, we use an LSTM to process each E i , using the final hidden state as the EDU embedding e i = LSTM(E i ) for each leaf i of the document's RST tree. Afterwards, we apply a recursive LSTM architecture ( <ref type="figure">Figure 2</ref>) that traverses the RST tree bottom-up. At each node s, we use the children's sub-tree embeddings [h l , c l , r l ] and [h r , c r , r r ] to form the node's sub-tree embedding:</p><formula xml:id="formula_0">[h s , c s ] = TreeLSTM([h l , c l , r l ], [h r , c r , r r ])<label>(1)</label></formula><p>where h l /c l and h r /c r are the LSTM hidden and cell states from the left and right sub-trees respectively. The relation embeddings of the children sub-trees, r l and r r , are learned vector embeddings for each of the 31 pre-defined relation labels in the form of [relation] [nucleus/satellite]" (e.g., Evidence Satellite" for the last EDU in <ref type="figure" target="#fig_0">Figure 1</ref>). At the root of the tree, the output hidden states from both children are concatenated into a single document embedding d = [h l , h r ]. As shown in <ref type="figure" target="#fig_1">Figure  3</ref>, a fully connected layer is applied to this representation before using a Softmax function to obtain the coherence class probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ensemble: ParSeq + RST-Recursive</head><p>To evaluate if the addition of silver-standard RST features to existing methods can improve coherence evaluation, we ensemble RST-Recursive with the current state of the art coherence classifier: ParSeq. A deep learned non-linguistic classifier, ParSeq employs three layers of LSTMs that intend to capture coherence at different granularities. An overview of the ParSeq architecture is presented in <ref type="figure">Figure 4</ref>. First, LSTM 1 (not shown) produces a single sentence embedding for each sentence in the text. Next, LSTM 2 generates paragraph embeddings using the corresponding sentence embeddings from LSTM 1 . Finally, LSTM 3 reads the paragraph embeddings, generating the final document embedding, which is passed to a fully connected layer to produce Softmax label probabilities.</p><p>In this augmented variation of our model, we operate ParSeq on the document independently until a document level embedding d p is obtained at the highest-level LSTM. This document embedding is then concatenated to the RST-Recursive coherence embedding d = [h l , h r , d parseq ] in <ref type="figure">Figure   Figure 4</ref>: The architectural overview of ParSeq; an illustration of ParSeq's structure, taken directly from the original paper <ref type="bibr" target="#b8">(Lai and Tetreault, 2018).</ref> 3 to produce class probabilities. Note that in this ensemble variation, we initialize tree leaves e 1:n with zero-vectors as opposed to EDU embeddings since ParSeq is sufficiently capable of capturing semantic information on its own, and early experiments using 5-fold cross-validation on the training set revealed model overfitting when training with EDU embeddings simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate RST-Recursive and Ensemble on the GCDC dataset <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>. This dataset consists of 4 separate sub-datasets: Clinton emails, Enron emails, Yahoo answers, and Yelp reviews, each containing 1000 documents for training and 200 documents for testing. Each document is assigned a discrete coherence label of incoherent (1), neutral (2), and coherent <ref type="formula">(3)</ref>.</p><p>We parse RST trees for each example within the GCDC dataset using CODRA <ref type="bibr" target="#b6">(Joty et al., 2015)</ref>. Due to CODRA's imperfect parsing of documents, RST trees could not be obtained for approximately 1.5%-2% of the documents, which were then excluded from the study. In addition, we re-evaluated ParSeq on only the RST-parsed portion of documents to assure consistent comparability of results. For more details, see Appendix A/B. Our code and dataset can be accessed below 1 , and the access to the original GCDC corpus can be obtained here 2 . We can share RST-parsings of GCDC examples with interested readers upon request once access to the GCDC dataset has also been obtained.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We train all models with hyperparameter settings consistent with that of ParSeq reported by <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>. Specifically, we use a learning rate of 0.0001, hidden size of 100, relation embedding size of 50, and 300-dimensional pre-trained GloVe embeddings <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>. We train with the Adam optimizer (Kingma and Ba, 2014) for 2 epochs. For every model/variation, the reported results represent the corresponding accuracies and F1 scores averaged over 1000 independent runs, each initialized with a different random seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RST-Recursive's Performance</head><p>Our full model incorporates the RST Tree (T) structure, nucleus/satellite properties (nuclearity) of subtrees (NS), RST specific connective relations (R), and EDU embeddings at leaves of the RST tree (E), as previously described in 3.1. Here, (T) defines the tree traversal operation and (NS) and (R) are learned vector embeddings for nuclearity and relations. We examine three ablations, each removing one of (NS), (R) and (E) from the model.</p><p>The results are provided in <ref type="table" target="#tab_3">Tables 1 and 2</ref>. As shown, the complete model is able to achieve a competitive overall accuracy and F1 at 53.04% and 44.30% respectively, which is close to the state of the art. Although this lags behind ParSeq by a noticeable 2% margin, RST-Recursive is able to achieve this performance with 62% fewer parameters <ref type="bibr">(1,230k vs. 3,241k)</ref>, demonstrating the usefulness of linguistically-motivated features. Removing EDU embeddings reduces accuracy and F1 scores to 50.46% and 39.13%. This is still significantly better than the majority class baseline, signifying that even without any semantic infor-  mation about the text and its contents, it is still possible to evaluate coherence using just the silverstandard RST features of the text. Removing RST relations and nuclearity, however, decreases performance substantially, dropping to the majority class level. This indicates that an RST tree structure alone (of the quality delivered by silver-standard parsers) is not sufficient to classify coherence. It must also be noted that since we employ silverstandard RST parsing as performed by CODRA <ref type="bibr" target="#b6">(Joty et al., 2015)</ref>, the reported results act as a lower bound which we would expect to improve as parsing quality increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble's Performance</head><p>We examine three variations of the Ensemble. The full model augments ParSeq with the text's RST tree, relations and nuclearity. This model is able to achieve the new state of the art performance, at 55.39% accuracy and 46.98% F1. Using final layer concatenation for ensembling is widely applicable to many other neural methods, and serves as a lower bound for the accuracy/F1 boost to be appreciated by incorporating RST features into the model. Removing the RST relations and/or nuclearity information completely eliminates the performance gain, which shows that the RST tree on its own is not sufficient as an RST source of information for distinguishing coherence, even when ensembled with ParSeq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Classification Trends</head><p>As demonstrated in <ref type="figure" target="#fig_2">Figure 5</ref>, coherence classifiers have difficulty predicting the neutral class (2), experiencing modal collapse towards the extreme ends in the best performing models. Early experiments using alternative objective functions such as the Ordinal Loss or Mean Squared Error resulted in a similar modal collapse or poor overall performance. We leave further exploration of this problem to future research. Furthermore, RST-Recursive shows a notably stronger recall on the coherent class (3) as compared to ParSeq. On the other hand, ParSeq has a higher recall/precision on class (1) and slightly higher precision on class (3). The Ensemble method, however, is able to take the best of both, achieving better recall, precision and F1 on both the incoherent and coherent classes as compared to ParSeq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we explore the usefulness of silverstandard parsed RST features in neural coherence classification. We propose two new methods, RST-Recursive and Ensemble. The former achieves reasonably good performance, only 2% short of state of the art, while more robust with 62% fewer parameters. The latter demonstrates the added advantage of RST features in improving classification accuracy of the existing state of the art methods by setting new state of the art performance with a modest but promising margin. This signifies that the document's rhetorical structure is an important aspect of its perceived clarity. Naturally, this improvement in performance is bounded by the quality of parsed RST features and could increase as better discourse parsers are developed.</p><p>In the future, exploring other RST-based architectures for coherence classification, as well as better RST ensemble schemes and improving RST parsing can be avenues of potentially fruitful research. Additional research on multipronged approaches that draw from Centering Theory, RST and deep learning all together can also be of value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Dataset Description</head><p>For model evaluation, we use the recently released Grammarly Corpus for Discourse Coherence <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>. GCDC consists of 4 sections -Clinton and Enron emails, as well as Yelp review and Yahoo answers, with 1000 training and 200 testing examples in each section. Each text is given a score from 1 (least coherent) to 3 (most coherent) by expert raters. GCDC's key advantage, compared to the ranking corpora used in the past <ref type="bibr" target="#b15">(Prasad et al., 2008)</ref>, is that all the datapoints are human-labelled and not artificially permuted. Examples from the Coherence / Example Incoherent (1) For good Froyo, you just got to love some MoJo, yea baby yea! Creamy goodness with half the guilt of ice cream, a spread of tasty toppings, this in the TMP in definitely the place to be! They have little cups for sampling to find your favorite flavor. Great prices and with a yelping good 25% off discount just for "checking in" and half off Tuesdays with the FB word of the day, you just can't beat it! Perfect summer treat located in front of the TMP splash pad, you can soak up some sun and enjoy some fromazing yogurt in their outdoor sitting area! Go get you some Mojo froyo! Neutral (2) So Spintastic gets 5 stars because it's about as good as it gets for a laundromat, me thinks. Came here bc the dryer at my place was busted and waiting on the repairman. I found the people working the place extremely helpful. It was my first time there and she walked me through the steps of how to get a card, which machines to use, where I could buy the soap... only thing she didn't do was fold my dried laundry! Heh. Will remember this place for the future in the event that I need to get my clothes washed and ready. Free wi-fi and a soda machine is convenient.  <ref type="table">Table 3</ref>: Text examples of incoherent (class 1), neutral (class 2), and coherent (class 3) snippets from the Yelp subset of the GCDC dataset <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>.  dataset are provided in <ref type="table">Table 3</ref>. When assigning the ranking to each text, the experts received the following instructions <ref type="bibr" target="#b8">(Lai and Tetreault, 2018)</ref>:</p><p>A text that is highly coherent (score 3) is easy to understand and easy to read. This usually means the text is well-organized, logically structured, and presents only information that supports the main idea. On the other hand, a text with low coherence (score 1) is difficult to understand. This may be because the text is not well organized, contains unrelated information that distracts from the main idea, or lacks transitions to connect the ideas in the text. Try to ignore the effects of grammar or spelling errors when assigning a coherence rating.</p><p>We generated a discourse tree for each text in the GCDC dataset, utilizing the available CODRA discourse parser <ref type="bibr" target="#b6">(Joty et al., 2015)</ref>. Early iterations resulted in up to 30% unsuccessful parsing rate on some sub-datasets. As a result, a punctuation fixing script was developed to fix minor punctuation problems without changing the text's structure or coherence. Post-fixing results lowered this RST parsing failure rate to reasonable margins in the 1% to 3% region (see <ref type="table">Table 5</ref>). Note that all examples for which RST parsing was not successfully performed were excluded in our experiments. All baselines were re-evaluated using the RST-parsed set of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CODRA Quality</head><p>While partial parsing of the dataset (see Appendix A) allows us to evaluate the accuracy of our models, it must be emphasized that as with the goal of this paper, we've used silver-standard RST parsing which lags well behind the human gold-standard. As shown in <ref type="table" target="#tab_7">Table 4</ref>, CODRA is far from reaching human-level accuracy in RST parsing. Additionally, since it was trained on RST-DT <ref type="bibr" target="#b1">(Carlson et al., 2002)</ref>, it lacks out-of-domain adaptability, which becomes a bottle-neck in achieving substantial performance boost on badly structured domains of text such Yelp review. We again re-iterate the importance of RST parsing for RST-based coherence evaluation, and motivate future work in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of RST-Recursive; EDU embeddings are generated for the leaf nodes using the EDU network. Subsequently, the RST tree is recursively traversed bottom-up using the RST network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the classification layer in RST-Recursive; At the root of the RST tree, children's hidden states are concatenated to form the document representation d = [h l , h r ] which is then transformed into a 3-dimensional vector of Softmax probabilities. next sentence given the current sentence and viceversa. Mesgar and Strube (2018) constructed a local coherence model that encodes patterns of changes on how adjacent sentences within the text are semantically related. Recently, Moon et al. (2019) used a multi-component model to capture both local and global coherence perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of Recall, Precision and F1 on overall classification of each coherence level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>33±0.00 44.39±0.00 38.02±0.00 54.82±0.00 48.14±0.00 RST-REC 53.74±0.14 44.67±0.07 44.61±0.09 53.76±0.11 49.20±0.07 RST-REC 54.07±0.10 43.99±0.07 49.39±0.10 54.39±0.12 50.46±0.05 RST-REC 55.70±0.08 53.86±0.11 50.92±0.13 51.70±0.16 53.04±0.09 PARSEQ 61.05±0.13 54.23±0.10 53.29±0.14 51.76±0.21 55.09±0.09 ENSEMBLE * 61.12±0.13 54.20±0.12 52.87±0.16 51.52±0.22 54.93±0.10 ENSEMBLE * 60.82±0.13 54.01±0.10 52.92±0.15 51.63±0.24 54.85±0.10 ENSEMBLE * 61.17±0.12 53.99±0.10 53.99±0.14 52.40±0.21 55.39±0.09 08±0.07 31.21±0.13 41.97±0.14 42.27±0.09 39.13±0.08 RST-REC 45.90±0.12 44.33±0.16 43.85±0.18 43.13±0.10 44.30±0.08 PARSEQ 52.12±0.21 44.90±0.15 46.22±0.18 43.36±0.09 46.65±0.10 ENSEMBLE * 52.35±0.22 44.92±0.16 45.48±0.22 43.70±0.11 46.61±0.11 ENSEMBLE * 51.90±0.22 44.76±0.14 45.48±0.22 43.83±0.13 46.49±0.10 ENSEMBLE * 52.42±0.19 44.69±0.15 46.88±0.17 43.94±0.09 46.98±0.09</figDesc><table><row><cell>MODEL</cell><cell>T NS R E CLINTON</cell><cell>ENRON</cell><cell>YAHOO</cell><cell>YELP</cell><cell>AVERAGE</cell></row><row><cell>MAJORITY</cell><cell>55.33</cell><cell>44.39</cell><cell>38.02</cell><cell>54.82</cell><cell>48.14</cell></row><row><cell cols="6">RST-REC 55.Table 1: Overall and sub-dataset specific coherence classification accuracy on the GCDC dataset. Error boundaries</cell></row><row><cell cols="6">describe 95% confidence intervals. Values in bold describe statistically significant state of the art performance. *</cell></row><row><cell cols="5">indicates availability of EDU-level semantic information through the ensembling with ParSeq.</cell><cell></cell></row><row><cell>MODEL</cell><cell>T NS R E CLINTON</cell><cell>ENRON</cell><cell>YAHOO</cell><cell>YELP</cell><cell>AVERAGE</cell></row><row><cell>MAJORITY</cell><cell>39.42</cell><cell>27.29</cell><cell>20.95</cell><cell>38.82</cell><cell>31.62</cell></row><row><cell>RST-REC</cell><cell cols="5">39.42±0.00 27.29±0.00 20.95±0.00 38.82±0.00 31.62±0.00</cell></row><row><cell>RST-REC</cell><cell cols="5">39.20±0.03 30.81±0.16 35.67±0.18 39.93±0.08 36.40±0.09</cell></row><row><cell>RST-REC</cell><cell>41.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall and sub-dataset specific coherence classification F1 scores on the GCDC dataset. Error boundaries describe 95% confidence intervals. Values in bold describe statistically significant state of the art performance. F1 scores are calculated by macro-averaging the corresponding class-wise F1 scores.</figDesc><table /><note>* indicates availability of EDU- level semantic information through the ensembling with ParSeq.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Oh and if you have a balance left on your card, you can redeem the card and any remaining balance if you like. dmo out Coherent (3) vet for almost 6 years. He is kind, compassionate and very loving and gentle with my dogs. All my dogs are shelter dogs and I am very picky about who cares for my animals. I walked in once with a dog I found running around the neighborhood and the staff could not find a chip so Dr. Besemer came out to help. He was busy but made time for me. He looked over the dog and could not find a chip, he also did a quick check on the dog and said that he appeared healthy. He didn't charge me for his time. This dog became my third adoped dog. Dr. Besemer is the best and I highly recommend him if you are looking for a vet. His staff is kind and compassionate.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Micro-averaged F1 scores on the RST parsing of text by CODRA vs. Human Standard<ref type="bibr" target="#b13">(Morey et al., 2017)</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/grig-guz/coherence-rst 2 https://github.com/aylai/GCDC-corpus</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">TRAIN   TEST  CLINTON ENRON YAHOO YELP CLINTON ENRON YAHOO YELP   EXAMPLES  1000  1000  1000  1000  200  200  200  200  PRE-FIX RST-TREES  667  710  940  950  136  142  188  190  POST-FIX RST-TREES  985  976  986  999  199  195  192  197   POST-FIX VERY COHERENT  503  499  368  511  109  87  73  109  POST-FIX MEDIUM COHERENT  204  192  170  218  38  50  41  42  POST-FIX INCOHERENT  277  289  442  270  50  59</ref> <p>78 47 We believe that improvements in RST parsing will result in better accuracy for both future and existing RST-based coherence evaluation methods.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2008.34.1.1</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rst discourse treebank. Linguistic-Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The impact of deep hierarchical discourse structures in the evaluation of text coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="940" to="949" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centering: A framework for modelling the coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Reports</title>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Codra: A novel discriminative framework for rhetorical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00226</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980Comment</idno>
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published as a conference paper at</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Discourse coherence in the wild: A dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<idno>abs/1805.04993</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.1515/text.1.1988.8.3.243</idno>
		<title level="m">Rethorical structure theory: Toward a functional theory of text organization. Text</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural local coherence model for text quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4328" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified neural coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnim</forename><surname>Han Cheol Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1231</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2272" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How much progress have we made on rst discourse parsing? a replication study of recent results on the rst-dt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>The penn discourse treebank 2.0</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A neural local coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Dat Tien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1121</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1320" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
