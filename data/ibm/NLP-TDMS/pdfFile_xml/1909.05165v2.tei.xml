<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Köpüklü</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatiotemporal modeling</term>
					<term>CNNs</term>
					<term>RNNs</term>
					<term>activity under- standing</term>
					<term>action/gesture recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding actions and gestures in video streams requires temporal reasoning of the spatial content from different time instants, i.e., spatiotemporal (ST) modeling. In this survey paper, we have made a comparative analysis of different ST modeling techniques for action and gecture recognition tasks. Since Convolutional Neural Networks (CNNs) are proved to be an effective tool as a feature extractor for static images, we apply ST modeling techniques on the features of static images from different time instants extracted by CNNs. All techniques are trained end-to-end together with a CNN feature extraction part and evaluated on two publicly available benchmarks: The Jester and the Something-Something datasets. The Jester dataset contains various dynamic and static hand gestures, whereas the Something-Something dataset contains actions of human-object interactions. The common characteristic of these two benchmarks is that the designed architectures need to capture the full temporal content of videos in order to correctly classify actions/gestures. Contrary to expectations, experimental results show that Recurrent Neural Network (RNN) based ST modeling techniques yield inferior results compared to other techniques such as fully convolutional architectures. Codes and pretrained models of this work are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been successfully applied in the area of image processing, providing state of the art solutions for many of its problems such as superresolution <ref type="bibr" target="#b19">[20]</ref>, image denoising <ref type="bibr" target="#b21">[22]</ref>, and classification <ref type="bibr" target="#b3">[4]</ref>. Due to the outstanding performance of two-dimensional (2D) Convolutional Neural Networks (CNNs) on processing static images, many attempts have been made to generalize 2D CNN architectures to capture the spatiotemporal (ST) structure of videos <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Until recently, 2D CNNs were the only options for video analysis tasks since lack of large scale video datasets made it impossible to train 3D CNNs properly. <ref type="figure">Fig. 1</ref>. Spatio-Temporal Modeling Architecture: One input video containing an action/gesture is divided into N segments. Afterwards, equidistant frames (m1,m2, .. mN ) are selected from the segments and fed to a 2D CNN for feature extraction. Extracted features are fed to a ST modeling block, which produces the final class score of the input video. In this example, action of "taking something from somewhere" is depicted (a sequence from the Something-Something dataset).</p><p>With the availability of large scale video datasets such as Kinetics <ref type="bibr" target="#b1">[2]</ref>, deeper and wider 3D CNN architectures can be successfully trained to achieve better performance compared to 2D CNNs <ref type="bibr" target="#b10">[11]</ref>. More importantly, 3D CNNs can capture the ST patterns in videos inherently without requiring additional mechanisms. However, their drawback is that the input size should always remain the same for 3D CNNs such as 16 or 32 frames, which makes them not suitable for capturing temporally varying actions. This is not a problem for activity recognition tasks for Kinetics <ref type="bibr" target="#b1">[2]</ref> or UCF-101 <ref type="bibr" target="#b30">[31]</ref> datasets, as videos can be successfully classified using even very small snippets of the complete video. However, there are tasks where the designed architectures need to observe the complete video at once in order to make successful decisions. For these tasks, 2D CNN based architectures are still useful as the complete videos can be sparsely sampled with the desired number of segments and features of the selected frames can be extracted. Still, these architectures need an extra mechanism to provide ST modeling of the extracted features.</p><p>This work aims to analyze and compare various techniques for ST modeling of the features extracted by a 2D CNN from sparsely sampled frames of action/gesture videos. <ref type="figure">Fig. 1</ref> depicts the used ST modeling architecture. A complete action/gesture video is divided into a predefined number of segments. From each segment, a frame is selected (randomly in training and equidistant in testing) and fed into the 2D CNN to extract its features. In order to understand which type of action/gesture is performed, an ST modeling technique is used. In this work, we have analyzed multi-layer perceptron (MLP) based techniques such as simple MLP, Temporal Relational Network (TRN) and Temporal Segment Network (TSN), Recurrent Neural Network (RNN) based techniques such as vanilla RNN, gated recurrent unit (GRU), long short-term memory (LSTM), bidirectional LSTM (B-LSTM) and convolutional LSTM (ConvLSTM) techniques, and finally fully convolutional network (FCN) techniques.</p><p>Although analyzed techniques have been used at several works in the literature, there has not been any comparative analysis to highlight the advantages of each ST modeling technique. With this survey paper, we try to fill this gap by comparing each technique in terms of efficiency (i.e. number of parameters and floating-point operations) and classification accuracy.</p><p>The proposed ST modeling techniques are evaluated on two publicly available benchmarks: (i) The Jester dataset that contains dynamic and static hand gesture videos, (ii) the Something-Something dataset that contains videos of various human-object interactions. The common aspect of both these videos is that the proposed recognition architectures need to analyze the full content of the video in order to make a successful recognition, which makes them perfect benchmarks for analyzing ST modeling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning architectures for ST modeling have been extensively studied in recent years, particularly in the context of action and gesture recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Karpathy et al. <ref type="bibr" target="#b14">[15]</ref> suggest several CNN architectures that fuse information across the temporal domain and applied the resulting models to the Spots-1M classification and UCF Action Recognition data sets. To speed up the training, they proposed a CNN-based multi-resolution architecture that could slightly improve the final results. Two stream CNNs <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b6">[7]</ref> fuse a spatial network processing the video frames with a temporal network using optical flow to obtain a common class score. These methods rely on separately processing the spatial and temporal components of the video, which can be a disadvantage. 3D convolutional neural networks, on the other hand, can be used to inherently learn the spatiotemporal structure of videos <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Tran et al. <ref type="bibr" target="#b33">[34]</ref> apply a 3D CNN architecture to obtain spatiotemporal feature volumes of input videos. To reduce training complexity, Sun et al. <ref type="bibr" target="#b31">[32]</ref> propose a factorization of 3D spatiotemporal kernels into sequential 2D spatial kernels and separately handle sequence alignment. Although a sparse sampling strategy can be applied to the input value to span a larger time duration <ref type="bibr" target="#b17">[18]</ref>, all 3D architectures have the disadvantage that the input size needs to be fixed, which limits their capability of handling data sets with varying video lengths.</p><p>Recurrent neural networks are a natural choice for processing dynamic length video sequences, and several modern architectures have been proposed for action recognition in videos. LSTM <ref type="bibr" target="#b11">[12]</ref> has been used in various video understanding tasks. Donahue et al. <ref type="bibr" target="#b4">[5]</ref> employ an LSTM after CNN-based feature extraction on the individual frames to learn spatiotemporal components and apply the architecture on the UCF Action Recognition data set. Similarly, Baccouche et al. <ref type="bibr" target="#b0">[1]</ref> use 3D convolutional neural networks together with an LSTM network. Liu et al. <ref type="bibr" target="#b20">[21]</ref> suggest to modify the Vanilla LSTM architecture to learn spatiotemporal domains. GRU <ref type="bibr" target="#b2">[3]</ref> is a popular variant of LSTM architecture which is actively used in video recognition tasks such as <ref type="bibr" target="#b5">[6]</ref>. There have been many other variants of LSTM architecture, which are summarized in <ref type="bibr" target="#b9">[10]</ref>. Another recurrent method is the Differentiable RNN <ref type="bibr" target="#b35">[36]</ref> generated by salient motion patterns in consecutive video frames.</p><p>Although LSTM structure is proven to be stable and powerful in modeling long range temporal relations in various studies <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b8">[9]</ref>, it handles spatiotemporal data using only full connections where no spatial information is encoded. ConvLSTM <ref type="bibr" target="#b37">[38]</ref> addresses this problem by using convolutional structures in both the input-to-state and state-to-state transitions. ConvLSTM is first introduced for precipitation nowcasting task <ref type="bibr" target="#b37">[38]</ref>, and later used for many other applications such as video saliency prediction <ref type="bibr" target="#b29">[30]</ref>, medical segmentation <ref type="bibr" target="#b38">[39]</ref>.</p><p>Fully Convolutional Networks (FCN) is first proposed for image segmentation task <ref type="bibr" target="#b22">[23]</ref> and currently majority of segmentation architectures are based on FCNs. Later FCN architectures have been used at many other tasks such as object detection <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The idea of using convolution layers can also be applied to the task of ST modeling.</p><p>Methods like Temporal Segment Networks <ref type="bibr" target="#b36">[37]</ref> enable processing longer videos by segmenting the input video into a certain number of segments, selecting shortlength snippets randomly from each segment and finally fusing individual prediction scores. These prediction scores are the result of a spatial convolutional network operating on the samples frames and a temporal convolutional network operating on optical flow components. However, it must be noted that averaging is applied for the fusion of extracted features in TSN, which causes to lose the temporal order of the features. Similarly, Temporal Relation Networks <ref type="bibr" target="#b39">[40]</ref> extract a number of ordered frames from the input video, which are then passed through a convolutional neural network for feature extraction. Different from TSN, TRN keeps the order of the extracted features and tries to discover possible temporal relations at multiple time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first describe the complete ST modeling architecture, which is based on a 2D CNN feature extraction part and one ST modeling block. Afterward, we investigate different ST modeling techniques in detail that can be used within this architecture. Finally, we will give the training details used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ST modeling Architecture</head><p>As illustrated in <ref type="figure">Fig. 1</ref>, a video clip V that contains a complete action/gesture is divided into N segments. Each segment is represented as S n ∈ R w×h×c×m of m ≥ 1 sequential frames with 224 × 224 spatial resolution and c = 3 channels. RGB modality is used in all of the trainings. Afterward, within segments, equidistant frames are selected and passed to a 2D CNN model for feature extraction. Extracted features are first pooled and transformed to a fixed size of 256 (except for TSN where features are transformed to number-of-classes) via a one-layer Multi-layer Perceptron (MLP) except for ConvLSTM and 3D-FCN techniques. For these two techniques, no pooling is applied at the feature extraction and number of channels is transformed to 256 by using a 1 × 1 2D convolution layer.</p><p>For feature extraction, two different CNN models are used: (i) SqueezeNet <ref type="bibr" target="#b12">[13]</ref> with simple bypass and (ii) Inception with Batch Normalization (BNInception) <ref type="bibr" target="#b13">[14]</ref>. The reason to choose these models is that the performance of the investigated ST modeling techniques can be evaluated with a lightweight CNN feature extractor (SqueezeNet) and relatively more complex and heavyweight CNN feature extractor (BNInception). In this way, CNN-model-agnostic performance of evaluated techniques can be observed.</p><p>Extracted features are finally fed to an ST modeling block, which produces the final class scores of the input video clip. Next, we are going to investigate different ST modeling techniques in detail that are used in this block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-layer Perceptron (MLP) based Techniques</head><p>MLP-based ST modeling techniques are simple but effective to incorporate temporal information. These techniques make use of MLPs once or multiple times. Extracted features are then fed to these MLP-based ST modeling blocks keeping their order intact. The intuition is that MLPs can capture the temporal information of the sequence inherently without knowing that it is a sequence at all.</p><p>Simple MLP As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, extracted features are concatenated preserving their order. Then, the concatenated single N ×256 dimensional vector is fed to a 2-layer MLP with 512 and Number-of-classes neurons. Finally, the output is fed to a softmax layer to get class conditional scores. This is a simple but effective approach. Combined with other modalities such as optical flow, infrared and depth, competitive results can be achieved <ref type="bibr" target="#b16">[17]</ref>.</p><p>Temporal Segment Network (TSN) TSN aims to achieve long-range temporal structure modeling using sparse sampling strategy <ref type="bibr" target="#b36">[37]</ref>. When the original paper was written, TSN achieved state-of-the-art performance on two activity recognition datasets, namely the UCF-101 <ref type="bibr" target="#b30">[31]</ref> dataset and the HMDB <ref type="bibr" target="#b18">[19]</ref> dataset.  The original TSN architecture uses optical flow and RGB modalities, as well as different consensus methods such as evenly averaging, maximum, and weighted averaging. Among them, evenly averaging achieved the best results in the original experiments. Therefore, we have also experimented with evenly averaging for RGB modality only.</p><p>The corresponding TSN approach is depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>. Unlike other ST modeling techniques, the extracted frame features are transformed into a fixed size of number-of-classes instead of 256. Afterward, all extracted features are averaged and fed to a softmax layer to get class conditional scores.</p><p>Although TSN achieved state-of-the-art performance on UCF-101 and HMDB benchmarks at the time, it achieves inferior performance in the Jester and Something-Something benchmarks. The reason is that averaging causes loss of temporal information. This does not create a huge problem for the UCF-101 and HMDB benchmarks as temporal order is not critical for these. Correct classification can even be achieved using only one frame of the complete video. However, the Jester and Something-Something datasets require the incorporation of the complete video in order to infer correct class scores.</p><p>Temporal Relation Network (TRN) TRNs <ref type="bibr" target="#b39">[40]</ref> aim to discover possible temporal relations between observations at multiple time scales. The main inspiration for this work comes from the relational reasoning module for visual question answering <ref type="bibr" target="#b26">[27]</ref>. The pairwise temporal relations (2-frame relations) on the observations of the video V are defined as</p><formula xml:id="formula_0">T 2 (V ) = h φ   i&lt;j g θ (f i , f j )   ,</formula><p>where the input is the features of the n selected frames of the video V = {f 1 , f 1 , ..., f n }, in which f i represents the feature of the i th frame segment extracted by a 2D CNN. Here, h φ and g θ represent the feature fusing functions, which are MLPs with parameters φ and θ, respectively. For these functions, the exact same MLP block as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref> is used. These two-frame temporal relations functions are further extended to higher frame relations, but the order of the segments should always be kept same in order to learn temporal relations inherently. Finally, all frame relations can be incorporated in order to get a sin-</p><formula xml:id="formula_1">gle final output M T N (V ) = T 2 (V ) + T 3 (V ) + ... + T N (V )</formula><p>, which is referred as multiscale TRN, where each T d captures temporal relationships between features of d ordered frames. <ref type="figure" target="#fig_2">Fig. 4</ref> depicts the overall TRN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recurrent Neural Networks (RNN) based Techniques</head><p>Recurrent neural networks (RNNs) are a special type of artificial neural networks and consist of recurrently connected hidden layers which are capable of capturing temporal information. Furthermore, they allow the input and output sequences to vary in size. It is important to note that the hidden layer parameters do not depend on the time step but are shared across all RNN slices. The ability to keep information from previous time steps makes the hidden layer work like a memory. General M-layered RNN architecture is depicted in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>In our experiments, we use two different vanilla RNNs, based on the hyperbolic tangent activation function, and the rectified linear unit (ReLU) activation function, respectively. Vanilla RNN with hyperbolic tangent activation function can be described by following equations  Generally, we feed the output of the last node to a fully connected layer to obtain a vector size of the number of classes in the dataset. We also proceed in the same manner for all other RNN types except for the Bidirectional LSTM.</p><formula xml:id="formula_2">h t = tanh (W hh h t−1 + W xh x t ) y t = W hy h t</formula><p>Long Short-Term Memory (LSTM) LSTMs <ref type="bibr" target="#b11">[12]</ref> are recurrent neural networks consisting of a cell, an input gate, a forget gate, and an output gate. The input gate i t decides how much the current x t contributes to the overall output. The cell c t is responsible for remembering the previous state information, and also uses the results of the forget gate f t , which decides how much of the previous cell c t−1 flows into the current cell. As the name suggests, the forget gate can completely erase the previous state if necessary. Finally, the output gate determines the contribution of the current cell c t . All in all, the standard LSTM can be described by the following equations</p><formula xml:id="formula_3">i t = σ (W xi x t + W hi h t−1 + W ci • c t−1 + b i ) f t = σ (W xf x t + W hf h t−1 + W cf • c t−1 + b f ) c t = f t • c t−1 + i t • tanh (W xc x t + W hc h t−1 + b c ) o t = σ (W xo x t + W ho h t−1 + W co • c t + b o ) h t = o t • tanh (c t )</formula><p>where '•' denotes the Hadamard product.</p><p>Gated Recurrent Units (GRU) GRUs <ref type="bibr" target="#b2">[3]</ref> are very similar to LSTMs and consist of two gates -an update gate z t and a reset gate r t . However, unlike LSTMs, GRUs do not have their own memory control mechanism. Instead, the entire hidden layer information is directed to the next time step. The advantage of GRUs compared to LSTMs is their simplicity in structure, which significantly reduces the number of parameters to be learned. GRU can be described by the following qeuations</p><formula xml:id="formula_4">z t = σ (W xz x t + W hz h t−1 + b z ) r t = σ (W xr x t + W hr h t−1 + b r ) h t = tanh (W xh x t + W hh (r t • h t−1 ) + b h ) h t = z t • h t−1 + (1 − z t ) •h t</formula><p>where '•' denotes the Hadamard product;h t and h t represent the intermediate memory and output, respectively.</p><p>Bidirectional LSTM (BLSTM) BLSTMs <ref type="bibr" target="#b27">[28]</ref> are a special form of LSTMs, but are trained in both directions. The fully connected layer is obtained by concatenating two halved outputs h 1,1 and h m,2 , namely the first output of the positive time direction and the last output of the negative time direction. The data flow for BLSTM architecture is depicted in <ref type="figure" target="#fig_4">Fig. 6</ref>. We also investigate the effect of the hidden size by reducing it to half of the hidden size value we used for the other RNN-structures. This allows us to make meaningful comparisons with the latter. The reduction of the hidden layer size means that the vector size remains unchanged before the last fully connected layer. Consequently, the same number of output neurons is used for the classification.</p><p>Convolutional LSTM (ConvLSTM) The main drawback of conventional LSTM (also GRU and vanilla RNN) in handling spatiotemporal data is that input-to-state and state-to-state transitions are made by full connections, where no spatial information is encoded. To overcome this drawback, convolutional LSTM proposes to use convolutional structures for the mentioned transitions. The main equations of ConvLSTM are given as</p><formula xml:id="formula_5">i t = σ (W xi * x t + W hi * h t−1 + W ci • c t−1 + b i ) f t = σ (W xf * x t + W hf * h t−1 + W cf • c t−1 + b f ) c t = f t • c t−1 + i t • tanh (W xc * x t + W hc * h t−1 + b c ) o t = σ (W xo * x t + W ho * h t−1 + W co • c t + b o ) h t = o t • tanh (c t )</formula><p>where ' * ' and '•' denote the convolution operator and Hadamard product, respectively. In order to make use of ConvLSTM technique for ST modeling, we have made some modifications. First, to keep spatial resolution, we have removed the final pooling layer of our feature extractor 2D CNN. Then, the output features are concatenated in time dimension forming a D × W × H tensor. The last output of ConvLSTM is average pooled and fed to a final fully connected layer and a softmax layer to get class conditional scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fully Convolutional Network (FCN) based Techniques</head><p>As the name implies, all of the layers of a fully convolutional network are convolutional layers. FCNs do not contain any linear (fully connected) layers at the end, which is the typical use for classification task. In order to utilize FCNs as ST modeling technique, output features coming from the 2D CNNs can be concatenated over a dimension such that convolution operation can be performed over the concatenated tensor. If the features are pooled, concatenated features form a 2D tensor with over which 2D convolutional layers can operate. If pooling is not applied at feature extraction stage, concatenated features form a 2D tensor over which 3D convolutional layers can operate.</p><p>2D-FCN The inputs to 2D-FCN are the concatenated feature vectors of each segment resulting N ×256 such that each row represents features from a segment. The input volume enters a series of 2D convolutions with stride (1, 2), which keeps the temporal dimension (i.e. the number of segments) intact throughout convolution operations. The kernel size is set to 3 × 3 with the same padding for all convolutions. After applying five convolutions, 2D convolution with 1 × 1 kernel is applied where the number of channels equals the number of classes. Finally, average pooling with N × 8 is applied to get class conditional scores. After each convolution, batch normalization and ReLU is applied. The details of the used 2D-FCN are given in <ref type="table">Table 1</ref>.</p><p>3D-FCN In order to make use of spatial information, similar to ConvLSTM, we have not used a pooling layer at the end of feature extractor 2D CNN, and output features are concatenated in depth dimension to create D×W ×H tensor.</p><p>This tensor enters a series of 3D convolutions with stride (2, 1, 1) in order to keep the spatial resolution same. The kernel size is set to 3 × 3 × 3 with the same padding for all convolutions. After applying three convolutions, 3D convolution with 1 × 1 × 1 kernel is applied to reduce the number of channels to number of classes, which is pooled later to get class scores. After each convolution, batch normalization and ReLU is applied. The details of the used 3D-FCN are given in <ref type="table">Table 2</ref>.</p><p>The proposed approach in fact very similar to rMCx models in <ref type="bibr" target="#b34">[35]</ref> except for that 3D convolutions are applied at the very end. It is again similar to ECO architecture <ref type="bibr" target="#b40">[41]</ref>, but 2D features in ECO architecture are extracted again at an early stage of the 2D CNN feature extractor. Although, 3D CNN architectures can be used for varying video lengths, we can use 3D convolution layers as ST modeling technique since we are using a fixed segment size for all the input videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Details</head><p>Given the ST modeling architecture in <ref type="figure">Fig. 1</ref>, the CNN architecture used to extract frame features plays a critical role in the performance of the overall architecture. In order to get CNN-model-agnostic performance of the applied ST modeling techniques, the SqueezeNet and BNInception models are used. For both models, features are transformed to 256-dimensional vectors (Number-ofclasses-dimensional vectors for only TSN) via an MLP after global pooling layer except for ConvLSTM and FCN3D. For these two ST modeling approach, spatial resolution (13 × 13 and 7 × 7 for SqueezeNet and BNInception) is preserved by removing the final pooling layer, and 1 × 1 convolution layer is used to transform the number of channels to 256. For all experiments, CNN models pretrained on ImageNet dataset are used.</p><p>Learning: Stochastic gradient descent (SGD) with standard categorical crossentropy loss is applied. For momentum and weight decay, 9 × 10 −1 and 5 × 10 −4 are used, respectively. The learning rate is initialized with 1 × 10 −3 and reduced twice with a factor of 10 −1 after validation loss converges.</p><p>Regularization: Several regularization techniques are applied in order to reduce over-fitting and achieve a better generalization. Weight decay of γ = 5 × 10 −4 is applied to all parameters of the architecture. A dropout layer is applied after the global pooling layer of 2D CNN architectures with a ratio of 0.3. Moreover, data augmentation of multiscale random cropping is applied for both datasets and random horizontal flip is applied for Something-Something dataset. Random horizontal flipping is not performed for the trainings of Jester dataset since this changes the annotations of some classes.</p><p>Implementation: The complete ST modeling architecture is implemented and trained (end-to-end) in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The Jester-V1 dataset is currently the largest hand gesture dataset that is publicly available <ref type="bibr" target="#b23">[24]</ref>. It is an extensive collection of segmented video clips that contain humans performing pre-defined hand gestures in front of a laptop camera or webcam. The dataset consists of 148092 video clips under 27 classes, which is split into training, validation and test sets containing 118562, 14787 and 14743 videos, respectively. For the experiments, the validation set is used as the labels of the test set are not made available by dataset providers.</p><p>The Something-Something-V2 dataset is a collection of segmented video clips that show humans performing pre-defined basic actions with everyday objects <ref type="bibr" target="#b7">[8]</ref>. It allows researchers to develop machine learning models capturing a finegrained understanding of basic actions. The dataset consists of 220847 video clips under 174 classes, which is split into training, validation and test sets containing 168913, 24777 and 27157 videos, respectively. For the experiments, the validation set is used as the labels of the test set are not made available by dataset providers.</p><p>The duration of gesture clips in Jester dataset is concentrated between 30 -40 frames. However, the Something-Something dataset has videos with relatively varying temporal dimension between 20 and 70 frames, which is the reason why 3D CNN architectures accepting fixed-size inputs are not suitable for this benchmark. In order to recognize video clips correctly, the used architectures should incorporate information coming from all parts of the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Resource Efficiency Analysis</head><p>For real-time systems, the resource efficiency of the applied ST modeling techniques is as essential as the achieved classification accuracy. Therefore, we have investigated the number of parameters and floating-point operations (FLOPs) of each technique, which can be found in <ref type="table" target="#tab_2">Table 3</ref>. For all calculations, we have used 8-segment case for Jester dataset.</p><p>Out of all ST modeling techniques, TSN comes for free since it requires no parameters and there is only averaging operation. However, temporal information is lost due to averaging, which results in inferior performance compared to simple-MLP or TRN techniques.</p><p>ConvLSTM and 3D-FCN does not employ pooling at the end of feature extractor, which requires highest number of FLOPs compared to others. The number of FLOPs for SqueezeNet is 13 2 /7 2 = 3.48 times higher than BNInception due to the output resolution of feature maps. In terms of number of parameters, ConvLSTM, 3D-FCN and TRN-multiscale requires the highest number with 4.  On the other hand, the SqueezeNet architecture contains only 1.24 M parameters and requires 338 MFLOPs to extract the features of a same-sized frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results Using Jester Dataset</head><p>For the Jester dataset, the spatial content for all classes are the same: A hand in front of a camera performing a gesture. Therefore, a designed architecture should capture the form, position, and the motion of the hand in order to recognize the correct class.</p><p>Comparative results of different ST-modeling techniques for Jester dataset can be found in <ref type="table" target="#tab_2">Table 3</ref>. Inspired from <ref type="bibr" target="#b16">[17]</ref>, we have used eight segments for this benchmark as it achieves the best performance for MFF architecture. Compared to BNInception, architectures with SqueezeNet have 5-10% inferior classification accuracy for the same ST modeling technique. However, the technique-wise comparison remains similar within the same 2D CNN backbone.</p><p>Out of all ST modeling techniques, TRN-multiscale, 2D-FCN, 3D-FCN and ConvLSTM stand out for classification accuracy. Considering the resource efficiency, the simple-MLP model can also be preferred over TRN-multiscale. Surprisingly, RNN-based methods except ConvLSTM, which first come to mind for modeling sequences, perform worse than these techniques.</p><p>The superiority of ConvLSTM over other RNN based techniques and superiority of 3D-FCN over 2D-FCN validates the importance of the spatial content. 3D-FCN is the best performing technique in terms of accuracy for Jester dataset.</p><p>However, preserving the spatial resolution brings the burden of increased computation and number of parameters. As expected, TSN yields the lowest classification accuracy as the averaging operation causes a loss of temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results Using Something-Something Dataset</head><p>Compared to the Jester dataset, the Something-Something dataset contains much more classes with more complex spatial content. In order to identify the correct class label, the designed architectures need to extract the spatial content and temporally link this content successfully. Therefore, the frame feature extractors (i.e., 2D CNNs) are critical for the overall performance.</p><p>Comparative results of different ST-modeling techniques for the Something-Something dataset can be found in <ref type="table" target="#tab_2">Table 3</ref>. Beside 8-segment architectures, we have also made experiments for 16-segment architectures as the spatial complexity of the dataset is higher compared to Jester. Due to this complexity, architectures with SqueezeNet have 10% to 15% inferior classification accuracy compared to architectures with BNInception. However, similar to Jester dataset, the technique-wise comparison remains similar within the same 2D CNN backbone.</p><p>Compared to 8-segments, 16-segment architectures perform better. However, performance improvement is not as drastic as the effect of feature extractors. This shows that the main complexity of this task comes from the complexity of scenes, not the complexity of finer temporal details. In order to get better performance on Something-Something dataset, more complex architectures with deeper and wider structure can be preferred.</p><p>Out of all ST modeling techniques, 3D-FCN, ConvLSTM and TRN-multiscale again stand out for classification accuracy. Specifically, 3D-FCN with SqueezeNet performs best outperforming the second best model TRN-multiscale by 3.37%. Similar performance difference cannot be observed when BNInception used as feature extractor. We conjecture that this is due to the higher feature resolution of the SqueezeNet architecture.</p><p>Moreover, ConvLSTM also outperforms all other RNN based techniques by 3-6% showing the importance of spatial information again for this task. On the other hand, 2D-FCN cannot achieve the performance it reached in the Jester dataset and performs inferior to GRU and LSTM. Similar to the Jester dataset, Vanilla RNN and TSN yield lowest accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have analyzed various techniques for CNN-based spatiotemporal modeling and compared them based on a consistent 2D CNN feature extraction of sparsely sampled frames. The individual methods were then evaluated on the Jester and Something-Something datasets. It has been shown that the CNN models used for feature extraction and the number of frames sampled affect the results. For the Jester dataset, the 3D-FCN technique achieves the best results using both SqueezeNet and BNInception. On the Something-Something dataset, again 3D-FCN and TRN-multiscale techniques outperform all other models while ConvLSTM performs similar results. It has also been shown that simple vanilla RNNs are unable to understand the complex spatiotemporal relationships of the data. All the more complex RNNs tested perform very similarly.</p><p>Interestingly, the TSN model, which showed state-of-the-art performance on the UCF-101 and HMDB benchmarks, performs rather poorly in our experiments, which shows the importance of maintaining the temporal information. Among all techniques, ConvLSTM and 3D-FCN requires the highest number of FLOPs, since they do not employ pooling at the feature extractor and preserve spatial resolution. For number of parameters, again ConvLSTM, 3D-FCN and TRN-multiscale techniques are the most expensive ones. While some models like TRN, LSTM, GRU, and B-LSTM can benefit from an increase in the number of segments, Vanilla RNNs and the TSN model can suffer from overfitting. One possibility for future research would be to develop resource efficient ST modeling techniques that preserves the spatial resolution of the extracted features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Simple MLP technique. Extracted features are concatenated keeping their order same to form N dimensional vector. This vector is fed to a 2layer MLP to get final class scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Temporal Segment Network (TSN) architecture. Extracted frame features are transformed to Number-ofclasses dimension and averaged to get class conditional scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Temporal Relation Networks. Features extracted from different segments of a video by a 2D CNN are fed into different frame relation modules. Only a subset of the 2-frame, 3-frame, and 4-frame relations are shown in this example (4 segments), as there are higher frame relations included according to the segment size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>M-layered architecture of Recurrent Neural Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The data flow for Bidirectional LSTM architecture. .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Layer/Stride Filter size Output size Details of 2D fully convolutional ST modeling architecture. Details of 3D fully convolutional ST modeling architecture.</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell>1×N×256</cell></row><row><cell>Conv1/s(1,2)</cell><cell></cell><cell>3×3</cell><cell>64×N×128</cell></row><row><cell>Conv2/s(1,2)</cell><cell></cell><cell>3×3</cell><cell>64×N×64</cell></row><row><cell>Conv3/s(1,2)</cell><cell></cell><cell>3×3</cell><cell>128×N×32</cell></row><row><cell>Conv4/s(1,2)</cell><cell></cell><cell>3×3</cell><cell>128×N×16</cell></row><row><cell>Conv5/s(1,2)</cell><cell></cell><cell>3×3</cell><cell>256×N×8</cell></row><row><cell>Conv6/s(1,1)</cell><cell></cell><cell>1×1</cell><cell>NumCls×N×8</cell></row><row><cell cols="2">AvgPool/s(1,1)</cell><cell>N×8</cell><cell>NumCls</cell></row><row><cell>Layer/Stride</cell><cell cols="3">Filter size Output size</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell>256×D×W×H</cell></row><row><cell>Conv1/s(2,1,1)</cell><cell></cell><cell>3×3×3</cell><cell>64×D/2×W×H</cell></row><row><cell>Conv2/s(2,1,1)</cell><cell></cell><cell>3×3×3</cell><cell>128×D/4×W×H</cell></row><row><cell>Conv3/s(2,1,1)</cell><cell></cell><cell>3×3×3</cell><cell>256×D/8×W×H</cell></row><row><cell>Conv4/s(1,1,1)</cell><cell></cell><cell>1×1×1</cell><cell>NumCls×D/8×W×H</cell></row><row><cell cols="4">AvgPool/s(1,1,1) D/8×W×H NumCls</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>73 M, 1.56 M and 2.34 M parameters, respectively. On the other hand, the resource efficiency of the feature extractors (i.e. 2D CNNs) are also important. The BNInception architecture contains 11.30 M parameters and requires 1894 MFLOPs to extract features of a 224 × 224 frame.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>MFLOPs</cell><cell>Params</cell><cell cols="6">Jester (8 seg.) Something (8 seg.) Something (16 seg.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Squeez. BNIncep. Squeez. BNIncep. Squeez. BNIncep.</cell></row><row><cell>Simple-MLP</cell><cell>2.13</cell><cell>1.06M</cell><cell>87.28</cell><cell>92.80</cell><cell>31.89</cell><cell>46.35</cell><cell>33.96</cell><cell>47.01</cell></row><row><cell>TSN</cell><cell>0.001</cell><cell>0.00M</cell><cell>72.84</cell><cell>82.74</cell><cell>20.91</cell><cell>37.28</cell><cell>22.15</cell><cell>36.22</cell></row><row><cell>TRN-multiscale</cell><cell>11.95</cell><cell>2.34M</cell><cell>88.39</cell><cell>93.20</cell><cell>33.73</cell><cell>46.91</cell><cell>34.38</cell><cell>47.73</cell></row><row><cell>RNN tanh</cell><cell>3.16</cell><cell>0.14M</cell><cell>70.51</cell><cell>79.53</cell><cell>16.12</cell><cell>25.17</cell><cell>14.48</cell><cell>21.64</cell></row><row><cell>RNN ReLU</cell><cell>3.16</cell><cell>0.14M</cell><cell>78.33</cell><cell>88.15</cell><cell>21.40</cell><cell>36.01</cell><cell>15.84</cell><cell>24.88</cell></row><row><cell>LSTM</cell><cell>8.42</cell><cell>0.53M</cell><cell>84.28</cell><cell>90.80</cell><cell>25.24</cell><cell>39.04</cell><cell>28.25</cell><cell>42.83</cell></row><row><cell>GRU</cell><cell>6.32</cell><cell>0.40M</cell><cell>83.10</cell><cell>90.86</cell><cell>25.40</cell><cell>40.69</cell><cell>30.24</cell><cell>43.31</cell></row><row><cell>B-LSTM</cell><cell>6.33</cell><cell>0.40M</cell><cell>84.87</cell><cell>91.12</cell><cell>25.04</cell><cell>39.35</cell><cell>27.88</cell><cell>42.41</cell></row><row><cell>ConvLSTM</cell><cell cols="2">1849.70/6379.54 4.73M</cell><cell>89.57</cell><cell>93.38</cell><cell>31.31</cell><cell>46.40</cell><cell>32.86</cell><cell>46.64</cell></row><row><cell>2D-FCN</cell><cell>39.07</cell><cell>0.56M</cell><cell>88.11</cell><cell>93.64</cell><cell>27.72</cell><cell>39.17</cell><cell>29.95</cell><cell>40.56</cell></row><row><cell>3D-FCN</cell><cell cols="3">152.07/524.49 1.56M 90.19</cell><cell>94.07</cell><cell>37.10</cell><cell>46.66</cell><cell>37.59</cell><cell>47.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different ST modeling techniques over classification accuracy, number of parameters and computation complexity (i.e., number of Floating Point Operations -FLOPs). Methods are evaluated using 8 and 16 segments on validation sets of Jester-V1 and Something-Something-V2 datasets. The number of parameters and FLOPs are calculated for only ST modeling blocks excluding CNN feature extractors for Jester dataset using 8 segments. FLOPs values of ConvLSTM and 3D-FCN are reported separately for BNInception (left) and SqueezeNet (right) since their spatial resolution is 7×7 and 13×13, respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on human behavior understanding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal Reasoning in Videos using Convolutional Gated Recurrent Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1111" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02422</idno>
		<title level="m">Resource efficient 3d convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Motion fused frames: Data level fusion strategy for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2103" to="2111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis on temporal dimension of inputs for 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing, Applications and Systems (IPAS)</title>
		<meeting><address><addrLine>In</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The jester dataset: A largescale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatio-temporal convolutional lstms for tumor growth prediction by learning 4d longitudinal patient data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1114" to="1126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
