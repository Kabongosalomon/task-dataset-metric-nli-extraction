<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<email>matthias.fey@udo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Graphics</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
							<email>janeric.lenssen@udo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Graphics</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Graphics</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Graphics</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Both authors contributed equally to this work.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors.</p><p>For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domainindependence. Our source code is available on GitHub 1 .</p><p>Recently, a set of methods brought together under the term geometric deep learning [3] emerged, which aim to achieve this transfer by defining convolution operations for deep neural networks that can handle irregular input data. Existing work in this field can loosely be divided into two different subsets: the spectral and the spatial filtering approaches. The former is based on spectral graph theory <ref type="bibr" target="#b4">[5]</ref>, where eigenvalues of a graph's Laplacian matrix are interpreted as frequencies of node signals <ref type="bibr" target="#b21">[22]</ref>. They are filtered in the spectral domain, analogously to Fourier domain filtering of traditional signals. The latter subset, the spatial approaches, perform convolution in local Euclidean neighborhoods w.r.t. local positional relations between points, represented for example as polar, spherical or Cartesian coordinates, as shown as examples in <ref type="figure">Figure 1</ref>.</p><p>Contribution. We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured data. The main contribution is a trainable, spatial, continuous convolution kernel that leverages properties of B-spline bases to efficiently filter geometric input of arbitrary dimensionality. We show</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most achievements obtained by deep learning methods over the last years heavily rely on properties of the convolution operation in convolutional neural networks <ref type="bibr" target="#b13">[14]</ref>: local connectivity, weight sharing and shift invariance. Since those layers are defined on inputs with a grid-like structure, they are not trivially portable to non-Euclidean domains like discrete manifolds, or (embedded) graphs. However, a large amount of data in practical tasks naturally comes in the form of such irregular structures, e.g. graph data or meshes. Transferring the high performance of traditional convolutional neural networks to this kind of data holds the potential for large improvements in several relevant tasks. that our method • can be applied on different kinds of irregular structured data, e.g., arbitrary (embedded) graphs and meshes,</p><p>• uses spatial geometric relations of the input,</p><p>• allows for end-to-end training without using handcrafted feature descriptors, and</p><p>• improves or pars the state-of-the-art in geometric learning tasks.</p><p>In addition, we provide an efficient GPGPU algorithm and implementation that allows for fast training and inference computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Deep learning on graphs. The history of geometric deep learning began with attempts to generalize convolutional neural networks for graph inputs. A large number of successful approaches are based on spectral graph theory.</p><p>Bruna et al. <ref type="bibr" target="#b3">[4]</ref> introduced convolution-like operators on spectral graphs, interpreting the eigenvectors of the Laplacian as Fourier basis. As an extension, Henaff et al. <ref type="bibr" target="#b8">[9]</ref> suggest to use spline interpolation for smoothing kernels in the spectral domain. Defferrard et al. <ref type="bibr" target="#b5">[6]</ref> approximates spectral filters with Chebyshev polynomials, providing a more efficient filtering algorithm, whose kernel size determines the range of aggregated local K-neighborhoods. This approach was further simplified by Kipf and Welling <ref type="bibr" target="#b11">[12]</ref>, who consider only the one-neighborhood for one filter application. A filter based on the Caley transform was proposed as an alternative for the Chebyshev approximation by Levie et al. <ref type="bibr" target="#b14">[15]</ref>. Together with a trainable zooming parameter, this results in a more stable and flexible spectral filter. It should be noted that all these spectral approaches assume that information is only encoded in the connectivity, edge weights and node features of the input. While this is true for general graphs, it does not hold for embedded graphs or meshes, where additional information is given by relative positions of nodes, which we consider with our method.</p><p>A downside of many spectral approaches is the fact that they use domain-dependent Fourier bases, which restricts generalization to inputs with identical graph connectivity. Yi et al. <ref type="bibr" target="#b24">[25]</ref> tackle this problem by applying a spectral transformer network that synchronizes the spectral domains. Since our approach works directly in the spatial domain, it is not prone to this problem. For the shape correspondence task on meshes, which we also analyze in this work, Litany et al. <ref type="bibr" target="#b15">[16]</ref> present a siamese network using a soft error criterion based on geodesic distances between nodes. We compare our method against this specialized method.</p><p>Local descriptors for discrete manifolds. The issue of not representing local positional relations can be tackled by using methods that extract representations for local Euclidean neighborhoods from discrete manifolds.</p><p>Based on the intrinsic shape descriptors of Kokkinos et al. <ref type="bibr" target="#b12">[13]</ref>, Masci et al. <ref type="bibr" target="#b16">[17]</ref> present such a method for extraction of two-dimensional Euclidean neighborhoods from meshes and propose a convolution operation locally applied on these neighborhoods. Boscaini et al. <ref type="bibr" target="#b1">[2]</ref> improve this approach by introducing a patch rotation method to align extracted patches based on the local principal curvatures of the input mesh.</p><p>Our convolution operator can but does not have to receive those local representations as inputs. Therefore, our approach is orthogonal to improvements in this field.</p><p>Spatial continuous convolution kernels. While the first continuous convolution kernels for graphs work in the spectral domain (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>), spatial continuous convolution kernels for irregular structured data were introduced recently as a special case in the fields of neural message passing and self-attention mechanisms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref>. Furthermore, Monti et al. <ref type="bibr" target="#b17">[18]</ref> presented the MoNet framework for interpreting different kind of inputs as directed graphs, on which we built upon in our work. We show that our kernels achieve the same or better accuracy as the trainable Gaussian mixture model (GMM) kernels of MoNet, while being able to be trained directly on the geometric structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SplineCNN</head><p>We define SplineCNNs as a class of deep neural networks that are built using a novel type of spline-based convolutional layer. This layer receives irregular structured data, which is mapped to a directed graph, as input. In the spatial convolutional layer, node features are aggregated using a trainable, continuous kernel function, which we define in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Input graphs. Similar to the work of Monti et al. <ref type="bibr" target="#b17">[18]</ref>, we expect the input of our convolution operator to be a directed graph G = (V, E, U) with V = {1, . . . , N } being the set of nodes, E ⊆ V × V the set of edges, and U ∈ [0, 1] N ×N ×d containing d-dimensional pseudo-coordinates u(i, j) ∈ [0, 1] d for each directed edge (i, j) ∈ E. Note that U can be interpreted as an adjacency matrix with d-dimensional, normalized entries u(i, j) ∈ [0, 1] d if (i, j) ∈ E and 0 otherwise. Also, U is usually sparse with E = |E| N 2 entries. For a node i ∈ V its neighborhood set is denoted by N (i). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main concept</head><p>Our convolution operator aggregates node features in local neighborhoods weighted by a trainable, continuous kernel function. The node features f (i) represent features on an irregular geometric structure, whose spatial relations are locally defined by the pseudo-coordinates in U. Therefore, when locally aggregating feature values in a node's neighborhood, the content of U is used to determine how the features are aggregated and the content of f (i) defines what is aggregated. We argue that common inputs for geometric deep learning tasks can be mapped to this model while preserving relevant information:</p><p>• For graphs, V and E are given and U can contain edge weights or, for example, features like the node degree of the target nodes.</p><p>• For discrete manifolds, V contains points of the discrete manifold, E represents connectivity in local Euclidean neighborhoods and U can contain local relational information like polar, spherical or Cartesian coordinates of the target point in respect to the origin point for each edge.</p><p>We state no restriction on the values of U, except being element of a fixed interval range. Therefore, meshes, for example, can be either interpreted as embedded threedimensional graphs or as two-dimensional manifolds, using local Euclidean neighborhood representations like obtained by the work of Boscaini et al. <ref type="bibr" target="#b1">[2]</ref>. Also, either po-lar/spherical coordinates or Cartesian coordinates can be used, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Independent from the type of coordinates stored in U, our trainable, continuous kernel function, which we define in the following section, maps each u(i, j) to a scalar that is used as a weight for feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolution operator</head><p>We begin with the definition of a continuous kernel function using B-spline bases, which is parametrized by a constant number of trainable control values. The local support property of B-spline basis functions <ref type="bibr" target="#b18">[19]</ref>, which states that basis functions evaluate to zero for all inputs outside of a known interval, proves to be advantageous for efficient computation and scalability. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the following kernel construction method for differing B-spline basis degree m. We introduce a trainable parameter w p,l ∈ W for each element p from the Cartesian product P = (N m</p><formula xml:id="formula_0">1,i ) i × · · · × (N m d,i ) i of the B-spline bases and each of the M in input feature maps, indexed by l. This results in K = M in · d i=1 k i trainable parameters.</formula><p>We define our continuous convolution kernel as func-</p><formula xml:id="formula_1">tions g l : [a 1 , b 1 ] × · · · × [a d , b d ] → R with g l (u) = p∈P w p,l · B p (u),<label>(1)</label></formula><p>with B p being the product of the basis functions in p:</p><formula xml:id="formula_2">B p (u) = d i=1 N m i,pi (u i ).<label>(2)</label></formula><p>One way to interpret this kernel is to see the trainable parameters w p,l as control values for the height of a d + 1-dimensional B-spline surface, from which a weight is sampled for each neighboring point j, depending on u(i, j). However, in contrast to traditional (d + 1)-dimensional B-spline approximation, we only have one-dimensional control points and approximate functions</p><formula xml:id="formula_3">g l : [a 1 , b 1 ] × · · · × [a d , b d ] → R instead of curves.</formula><p>The definition range of g l is the interval in which the partition of unity property of the B-spline bases holds <ref type="bibr" target="#b18">[19]</ref>. Therefore, a i and b i depend on B-spline degree m and kernel size (k 1 , . . . , k d ). We scale the spatial relation vectors u(i, j) to exactly match this interval, c.f . <ref type="figure" target="#fig_2">Figure 3</ref>. Given our kernel functions g = (g 1 , . . . , g Min ) and input node features f , we define our spatial convolution operator for a node i as</p><formula xml:id="formula_4">(f g)(i) = 1 |N (i)| Min l=1 j∈N (i) f l (j) · g l (u(i, j)). (3)</formula><p>Similar to traditional CNNs, the convolution operator can be used as a module in a deep neural network architecture, which we do in our SplineCNNs. To this end, the operator is applied M out times on the same input data with different trainable parameters, to obtain a convolutional layer that produces M out output feature maps. It should be highlighted that, in contrast to self-attention methods, we train an individual set of weights for each combination of input and output feature map.</p><formula xml:id="formula_5">b 1 a 1 u 1 a 2 b 2 u 2 g l (u) (a) Linear B-spline basis functions b 1 a 1 u 1 a 2 b 2 u 2 g l (u) (b) Quadratic B-spline basis functions</formula><p>Local support. Due to the local support property of Bsplines, B p = 0 only holds true for s := (m + 1) d of the K different vectors p ∈ P. Therefore, g l (u) only depends on M in · s of the M in · K trainable parameters for each neighbor j, where s, d and m are constant and usually small. In addition, for each pair of nodes (i, j) ∈ E, the vectors p ∈ P with B p = 0, which we denote as P(u(i, j)), can be found in constant time, given constant m and d. This allows for an alternative representation of the inner sums of our convolution operation, c.f . Equation <ref type="bibr" target="#b2">3</ref>, as</p><formula xml:id="formula_6">(f l g l )(i) = j∈N (i) p∈P(u(i,j)) f l (j) · w p,l · B p (u(i, j)). (4)</formula><p>and K can be replaced by s in the time complexity of the operation. Also, B p (u(i, j)) does not depend on l and can therefore be computed once for all input features. <ref type="figure" target="#fig_3">Figure 4</ref> shows a scheme of the computation. The gradient flow for the backward pass can also be derived by following the solid arrows backwards.</p><p>Closed B-splines. Depending on the type of coordinate in vectors u, we use closed B-spline approximation in some dimensions. One frequently occurring example of such a W * *</p><formula xml:id="formula_7">+ + B p1 B ps f l (0)g l (u(i, 0)) f l (j)g l (u(i, j)) u(i, j) f l (j) u(i, j) (f l g l )(i) w p1,l w ps,l . . . . . . . . . . . . . . .</formula><p>. . . situation is when u contains angle attributes of polar coordinates. Using closed B-spline approximation in the angle dimension naturally enforces the angle 0 to be evaluated to the same weight as the angle 2π or, for higher m, the kernel function to be continuously differentiable at those points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select Compute</head><p>The proposed kernels can easily be modified so that they use closed approximation in an arbitrary subset of the d dimensions, by mapping different p ∈ P to the same trainable control value w p,l . This leads to a reduction of trainable parameters and B-spline basis functions. Referring to <ref type="figure" target="#fig_2">Figure 3</ref>, this approach can be interpreted as periodic repetition of the function surface along the corresponding axis.</p><p>Root nodes. Up to now, we did not consider the node i of neighborhood N (i) in our convolution operator. It is not aggregated together with all j ∈ N (i), like it would be the case in traditional CNNs. If Cartesian coordinates are used, we can simply define N (i) to include i. However, when using polar/spherical pseudo-coordinates, problems arise since the point with zero radius is not well defined. Therefore, we introduce an additional trainable parameter for each feature of the root node and add the product of this parameter and the corresponding feature to the result.</p><p>Relation to traditional CNNs. Except for a normalization factor, our spline-based convolution operator is a generalization of the traditional convolutional layer in CNNs with odd filter size in each dimension. For example, if we assume to have a two-dimensional grid-graph with diagonal, horizontal and vertical edges to be the input, B-spline degree m = 1, kernel size <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>, and the vectors u to contain Cartesian relations between adjacent nodes, then our convolution operator is equivalent to a discrete convolution of an image with a kernel of size 3 × 3. This also holds for larger discrete kernels if the neighborhoods of the gridgraph are modified accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GPGPU algorithm</head><p>For the spline-based convolutional layer defined in the last section, we introduce a GPU algorithm which allows efficient training and inference with SplineCNNs. For simplicity, we use a tensor indexing notation with, e.g., A[x, y, z] describing the element at position (x, y, z) of a tensor A with rank three. Our forward operation of our convolution operator is outlined in Algorithm 1.</p><p>We achieve parallelization over the edges E by first gathering edge-wise input features F E in ∈ R E×Min from the input matrix F in ∈ R N ×Min , using the target node of each edge as index. Then, we compute edge-wise output features F E out ∈ R E×Mout , as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, before scatteradding them back to node-wise features F out ∈ R N ×Mout , performing the actual neighborhood aggregation. Our algorithm has a parallel time complexity of O(s · M in ), with small s, using O(E·M out ) processors, assuming that scatteradd is a parallel operation with constant time complexity.</p><p>Computing B-spline bases. We achieve independence from the number of trainable weights by computing matrices P ∈ N E×s and B ∈ R E×s . P contains the indices of parameters with B p = 0 while B contains the basis products B p for these parameters. B and P can be preprocessed for a given graph structure or can be computed directly in the kernel. For the GPU evaluation of the basis functions required for B we use explicit low-degree polynomial formulations of those functions for each m. For further details </p><formula xml:id="formula_8">r ← 0 for each i ∈ {1, . . . , M in } do for each p ∈ {1, . . . , s} do w ← W[P[e, p], i, o] r ← r + (F E in [e, i] · w · B[e, p]) end for end for F E out [e, o] ← r Scatter-add F E</formula><p>out to F out based on origin nodes of edges Return F out we refer to our PyTorch implementation, which is available on GitHub.</p><p>Mini-batch handling. For batch learning, parallelization over a mini-batch can be achieved by creating sparse block diagonal matrices out of all U of one batch and concatenating matrices F in in the node dimension. For matrices F E in , B and P, this results in example-wise concatenation in the edge dimension. Note that this composition allows differing number of nodes and edges over examples in one batch without introducing redundant computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We perform experiments with different SplineCNN architectures on three distinct tasks from the fields of image graph classification (Section 5.1), graph node classification (Section 5.2) and shape correspondence on meshes (Section 5.3). For each of the tasks, we create a SplineCNN using the spline-based convolution operator which we denote as SConv(k, M in , M out ) for a convolutional layer with kernel size k, M in input feature maps and M out output feature maps. In addition, we denote fully connected layers as FC(o), with o as number of output neurons.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image graph classification</head><p>For validation on two-dimensional regular and irregular structured input data, we apply our method on the widelyknown MNIST dataset <ref type="bibr" target="#b13">[14]</ref> of 60,000 training and 10,000 test images containing grayscale, handwritten digits from 10 different classes. We conduct two different experiments on MNIST. For both experiments, we strictly follow the experimental setup of Defferrard et al. and Monti et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> to provide comparability. For the first experiment, the MNIST images are represented as a set of equal grid graphs, where each node corresponds to one pixel in the original image, resulting in grids of size 28 × 28 with N = 28 2 = 784 nodes. For the second experiment, the MNIST superpixel dataset of Monti et al. <ref type="bibr" target="#b17">[18]</ref> is used, where each image is represented as an embedded graph of 75 nodes defining the centroids of superpixels, c.f . <ref type="figure" target="#fig_5">Figure 5a</ref>, with each graph having different node positions and connectivities. This experiment is an ideal choice to validate the capabilities of our approach on irregular structured, image-based data.</p><p>Pooling. Our SplineCNN architectures use a pooling operator based on the Graclus method <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. The pooling operation is able to obtain a coarsened graph by deriving a clustering on the graph nodes, aggregating nodes in one cluster and computing new pseudo-coordinates for each of those new nodes. We denote a max-pooling layer using this algorithm with MaxP(c), with c being the cluster size (and approximate downscaling factor).</p><p>Architectures and parameters. For the grid graph experiments, Cartesian coordinates and a B-spline basis degree of m = 1 are used to reach equivalence to the traditional convolution operator in CNNs, c.f . Section 3.3. In contrast, we compare all configurations of m and possible pseudocoordinates against each other on the superpixel dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>LeNet5 <ref type="bibr" target="#b13">[14]</ref> MoNet <ref type="bibr">[</ref> For classification on the grid data, we make use of a LeNet5-like network architecture <ref type="bibr" target="#b13">[14]</ref>: SConv((5, 5),1,32) → MaxP(4) → SConv((5, 5),32,64) → MaxP(4) → FC(512) → FC <ref type="bibr" target="#b9">(10)</ref>. The initial learning rate was chosen as 10 −3 and dropout probability as 0.5. Note that we used neighborhoods of size 5 × 5 from the grid graph, to mirror the LeNet5 architecture with its 5 × 5 filters.</p><p>The superpixel dataset is evaluated using the SplineCNN architecture SConv((k 1 , k 2 ),1,32) → MaxP(4) → SConv((k 1 , k 2 ),32,64) → MaxP(4) → AvgP → FC(128) → FC(10), where AvgP denotes a layer that averages features in the node dimension. We use the Exponential Linear Unit (ELU) as non-linearity after each SConv layer and the first FC layer. For Cartesian coordinates, we choose the kernel size to be k 1 = k 2 = 4 + m and for polar coordinates k 1 = 1 + m and k 2 = 8. Training was done for 20 epochs with a batch size of 64, initial learning rate 0.01 and dropout probability 0.5. Both networks were trained for 30 epochs using the Adam method <ref type="bibr" target="#b10">[11]</ref>.</p><p>Discussion. All results of the MNIST experiments are shown in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_5">Figure 5b</ref>. The grid graph experiment results in approximately the same accuracy as LeNet5 and the MoNet method. For the superpixel dataset, we improve previous results by 4.11 percentage points in accuracy. Since we are using a similar architecture and the same input data as MoNet, the better results are an indication that our operator is able to capture more relevant information in the structure of the input. This can be explained by the fact that, in contrast to the MoNet kernels, our kernel function has individual trainable weights for each combination of input and output feature maps, just like the filters in traditional CNNs.</p><p>Results for different configurations are shown in <ref type="figure" target="#fig_5">Figure 5b</ref>. We only notice small differences in accuracy for varying m and pseudo-coordinates. However, lower m and using Cartesian coordinates performs slightly better than the other configurations.</p><p>In addition, we visualized the 32 learned kernels of the first SConv layers from the grid and superpixel experiments in <ref type="figure" target="#fig_6">Figure 6</ref>. It can be observed that edge detecting patterns are learned in both approaches, whether being trained on regular or irregular structured data. ChebNet <ref type="bibr" target="#b5">[6]</ref> GCN <ref type="bibr" target="#b11">[12]</ref> CayleyNet <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Graph node classification</head><p>As second experiment, we address the problem of graph node classification using the Cora citation graph <ref type="bibr" target="#b20">[21]</ref>. We validate that our method also performs strongly on datasets, where no Euclidean relations are given. Cora consists of 2,708 nodes and 5,429 undirected unweighted edges, representing scientific publications and citation links respectively. Each document is represented individually by a 1,433 dimensional sparse binary bag-of-words feature vector and is labeled to exactly one out of 7 classes. Similar to the experimental setup in Levi et al. <ref type="bibr" target="#b14">[15]</ref>, we split the dataset into 1,708 nodes for training and 500 nodes for testing, to simulate labeled and unlabeled information.</p><p>Architecture and parameters. We use a SplineCNN similar to the network architecture introduced in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>: SConv((2),1433,16) → SConv((2),16,7), with ELU activation after the first SConv layer and m = 1.</p><p>For pseudo-coordinates, we choose the globally normalized degree of the target nodes u(i, j) = (deg(j)/ max v∈V deg(v)), leading to filtering based on the number of cites of neighboring publications. Training was done using the Adam optimization method <ref type="bibr" target="#b10">[11]</ref> for 200 epochs with learning rate 0.01, dropout probability 0.5 and L2 regularization 0.005. As loss function, the cross entropy between the network's softmax output and a one-hot target distribution was used.</p><p>Discussion. Results of our and related methods are shown in <ref type="table" target="#tab_3">Table 2</ref> and report the mean classification accuracy averaged over 100 experiments. It can be seen that SplineCNNs improve the state-of-the-art in this experiment by approximately 1.58 percentage points. We contribute this improvement to the filtering based on u, which contains node degrees as additional information to learn more complex kernel functions. This indicates that SplineCNNs can be successfully applied to irregular but non-geometric data and that they are able to improve previous results in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Shape correspondence</head><p>As our last and largest experiment, we validate our method on a collection of three-dimensional meshes solving the task of shape correspondence similar to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. Shape correspondence refers to the task of labeling each node of a given shape to the corresponding node of a reference shape <ref type="bibr" target="#b16">[17]</ref>. We use the FAUST dataset <ref type="bibr" target="#b0">[1]</ref>, containing 10 scanned human shapes in 10 different poses, resulting in a total of 100 non-watertight meshes with 6,890 nodes each. The first 80 subjects in FAUST were used for training and the remaining 20 subjects for testing, following the dataset splits introduced in <ref type="bibr" target="#b17">[18]</ref>. Ground truth correspondence of FAUST meshes are given implicitly, where nodes are sorted in the exact same order for every example. Correspondence quality is measured according to the Princeton benchmark protocol <ref type="bibr" target="#b9">[10]</ref>, counting the percentage of derived correspondences that lie within a geodesic radius r around the correct node.</p><p>In contrast to similar approaches, e.g. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>, we go without handcrafted feature descriptors as inputs, like the local histogram of normal vectors known as SHOT descriptors <ref type="bibr" target="#b23">[24]</ref>, and force the network to learn from the geometry (i.e. spatial relations encoded in U) itself. Therefore, input features are trivially given by 1 ∈ R N ×1 . Also, we validate our method on three-dimensional meshes as inputs instead of generating two-dimensional geodesic patches for each node. These simplifications reduce the computation time and memory consumption that are required to preprocess the data by a wide margin, making training and inference completely end-to-end and very efficient.</p><p>Architecture and parameters. We apply a Spline-CNN architecture with 6 convolutional layers: SConv((k 1 , k 2 , k 3 ),1,32) → SConv((k 1 , k 2 , k 3 ),32,64) → 4× SConv((k 1 , k 2 , k 3 ),64,64) → Lin(256) → Lin(6890), where Lin(o) denotes a 1 × 1 convolutional layer to o output features per node. As non-linear activation function, ELU is used after each SConv and the first Lin layer. For Cartesian coordinates we choose the kernel size to be k 1 = k 2 = k 3 = 4 + m and for polar coordinates k 1 = k 3 = 4 + m and k 2 = 8. We evaluate our method on multiple choices of m = {1, 2, 3}. Training was done for 100 epochs with a batch size of 1, initial learning rate 0.01 and dropout probability 0.5, using the Adam optimizer <ref type="bibr" target="#b10">[11]</ref> and cross entropy loss.</p><p>Discussion. Obtained accuracies for different geodesic errors are plotted in <ref type="figure" target="#fig_7">Figure 7</ref>. The results for different SplineCNN parameters match the observations from before, where only small differences could be seen but using Cartesian coordinates and small B-spline degrees seemed to be slightly better. Our SplineCNN outperforms all other approaches with 99.20% of predictions on the test set having zero geodesic error. However, the global behavior over larger geodesic error bounds is slightly worse in comparison to FMNet <ref type="bibr" target="#b15">[16]</ref>. In <ref type="figure" target="#fig_7">Figure 7c</ref> it can be seen that most nodes are classified correctly but that the few false classifications have a high geodesic error. We contribute this differences to the varying loss formulations. While we train against a one-hot binary vector using the cross entropy loss, FMNet trains using a specialized soft error loss, which is a more geometrically meaningful criterion that punishes geodesically far-away predictions stronger than predictions near the correct node <ref type="bibr" target="#b15">[16]</ref>. However, it is worth highlighting that we do not use SHOT descriptors as input features, like all other approaches we compare against. Instead, we train only on the geometric structure of the meshes.</p><p>Performance We report an average forward step runtime of 0.043 seconds for a single FAUST example processed by the suggested SplineCNN architecture (k 1 = k 2 = k 3 = 5, m = 1) on a single NVIDIA GTX 1080 Ti. We train this network in approximately 40 minutes. Regarding scalability, we are able to stack up to 160 SConv((5, 5, 5),64,64) layers before running out of memory on the mentioned GPU, while the runtime scales linearly with the number of layers. However, for this task we do not observe significant improvement in accuracy when using deeper networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced SplineCNN, a spline-based convolutional neural network with a novel trainable convolution operator, which learns on irregular structured, geometric input data. Our convolution filter operates in the spatial domain and aggregates local features, applying a trainable continuous kernel function parametrized by trainable B-spline control values. We showed that SplineCNN is able to improve stateof-the-art results in several benchmark tasks, including image graph classification, graph node classification and shape correspondence on meshes, while allowing very fast training and inference computation. To conclude, SplineCNN is the first architecture that allows deep end-to-end learning directly from geometric data while providing strong results. Due to missing preprocessing, this allows for even faster processing of data.</p><p>In the future we plan to enhance SplineCNNs by concepts known from traditional CNNs, namely recurrent neurons for geometric, spatio-temporal data or dynamic graphs, and un-pooling layers to allow encoder-decoder or generative architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>https://github.com/rusty1s/pytorch_geometric (a) Filtering of graphs (b) Filtering of meshes Examples for spatial aggregation in geometric deep learning with trainable, continuous kernel functions, showing methods for (a) image graph representations and (b) meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>j) = (x, y) u(i, j) = (x, y, z) u(i, j) = (ρ, θ) u(i, j) = (ρ, θ, ϕ) Possibilities for pseudo-coordinates u: two-and three-dimensional Cartesian, polar and spherical coordinates. Values for scaling and translation of the coordinates u to interval [0, 1] d are omitted.Input node features. Let f : V → R Min , with f (i) ∈ R Min ,denote a vector of M in input features for each node i ∈ V. For each 1 ≤ l ≤ M in we reference the set {f l (i) | i ∈ V} as input feature map. B-spline basis functions. In addition to the input graph and node features, let ((N m 1,i ) 1≤i≤k1 , . . . , (N m d,i ) 1≤i≤k d ) denote d open B-spline bases of degree m, based on uniform, i.e. equidistant, knot vectors (c.f . Piegl et al. [19]), with k = (k 1 , . . . , k d ) defining our d-dimensional kernel size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of our continuous convolution kernel for B-spline basis degrees (a) m = 1 and (b) m = 2 for kernel dimensionality d = 2. The heights of the red dots are the trainable parameters for a single input feature map. They are multiplied by the elements of the B-spline tensor product basis before influencing the kernel value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Forward computation scheme of the proposed convolution operation. During the backward step of the backpropagation algorithm, the gradient flows along the inverted solid arrows, reaching inputs from W and f l (i).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Geometric convolution with B-spline kernels Input: N : Number of nodes M in : Number of input features per node M out : Number of output features per node s = (m + 1) d : Number of non-zero B p for one edge W ∈ R K×Min×Mout : Trainable weights B ∈ R E×s : Basis products of s weights for each edge P ∈ N E×s : Indices of s weights in W for each edge F in ∈ R N ×Min : Input features for each node Output: F out ∈ R N ×Mout : Output features for each node -----------------------Gather F E in from F in based on target nodes of edges Parallelize over e ∈ {1, . . . , E}, o ∈ {1, . . . , M out }:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>MNIST 75 superpixels (a) example and (b) classification accuracy of SplineCNN using varying pseudocoordinates and B-spline base degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualizations of the 32 kernels from the first spline-based convolutional layers, trained on the MNIST (a) grid and (b) superpixels datasets, with kernel size (5, 5) and B-spline base degree m = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Results for different SplineCNNs (c) Geodesic error of test examples Geodesic error plots of the shape correspondence experiments with (a) SplineCNN and related approaches and (b) different SplineCNN experiments. The x-axis displays the geodesic distance in % of diameter and the y-axis the percentage of correspondences that lie within a given geodesic radius around the correct node. Our SplineCNN achieves the highest accuracy for low geodesic error and significantly outperforms other general approaches like MoNet, GCNN and ACNN.InFigure (c), three examples of the FAUST test dataset with geodesic errors of SplineCNN predictions for each node are presented. We show the best (left), the median (middle) and worst (right) test example, sorted by average geodesic error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Graph node classification on the Cora dataset for different learning methods (ChebNet, GCN, CayleyNet and SplineCNN). The presented accuracy means and standard deviations are computed over 100 experiments, where for each experiment the network was trained for 200 epochs.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Research Association (DFG) within the Collaborative Research Center SFB 876, Providing Information by Resource-Constrained Analysis, projects B2 and A6. We also thank Pascal Libuschewski for proofreading and helpful advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3794" to="3801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors: A multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blended intrinsic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno>79:1-79:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intrinsic shape context descriptors for deformable shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cay-leyNets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>abs/1705.07664</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep functional maps: Structured prediction for dense shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5660" to="5668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The NURBS Book</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Piegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SchNet: A continuousfilter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 11th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
