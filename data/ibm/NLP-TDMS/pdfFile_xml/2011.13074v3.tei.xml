<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omni-GAN: On the Secrets of cGANs and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
							<email>zhoupengcv@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Geng</surname></persName>
							<email>gengcong@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Omni-GAN: On the Secrets of cGANs and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the Ima-geNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to ×60+ higher resolution. Code will be available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Generative Adversarial Network (GAN) <ref type="bibr" target="#b18">[19]</ref> is a powerful tool for image generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b46">46]</ref> and domain adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b71">70]</ref>. The big family of GAN can be roughly divided into two parts, i.e., the unconditional GANs <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref> and conditional GANs (cGANs) <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b5">6]</ref>, differing from each other in whether the class labels (e.g., cat, car, flower, etc.) are used for image generation. In practice, cGAN often suffers severe collapse when the number of categories is large. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, all of BigGAN <ref type="bibr" target="#b5">[6]</ref>, Multi-hinge GAN <ref type="bibr" target="#b34">[34]</ref>, and AC-GAN <ref type="bibr" target="#b49">[49]</ref> achieve high Inception scores, but the curves drop dramatically at some point of training. This makes the cGAN training procedure unstable and thus early termination trick is used by the com- munity <ref type="bibr" target="#b5">[6]</ref>. It has been noticed <ref type="bibr" target="#b31">[31]</ref> that the instability of the training procedure is highly related to the discriminator, i.e., the module that outputs a value indicating the reality of the generated image. The existing cGAN discriminators are roughly categorized into two types, namely, the projectionbased <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b5">6]</ref> and classification-based <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b34">34]</ref> ones, according to whether the discriminator is required to output an explicit class label for each image. We find that, although the former choice (i.e., a projection-based, with a weaker, implicit discriminator) is inferior to the latter in terms of the Inception score, the latter is prone to collapse (e.g., in <ref type="figure" target="#fig_0">Fig. 1</ref>, Multi-hinge GAN achieves a high Inception score but collapses earlier). This paper investigates the reason behind this phenomenon. We formulate the classification-based and projection-based discriminator into a multi-label classification framework, which offers us an opportunity to observe the advantages and disadvantages of them. As a result, we find that combining strong supervision (classification loss) and moderate regularization (to prevent it from quickly memorizing the training image set) is the best choice, where the GAN model enjoys high quality in image generation yet has a low risk of mode collapse. In practice, we use weight decay as the choice of regularization, and our algorithm, named Omni-GAN, is easily implemented in any deep learning framework (adding a few lines of code beyond BigGAN). To show that our discovery generalizes to a wide range of cGAN models, we further integrate implicit neural representation (INR) <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b10">11]</ref>, an off-the-shelf encoding method, into Omni-GAN. This not only improves the generation quality of Omni-GAN, but also enables it to generate images of any aspect ratio and any resolution, facilitating its application to downstream tasks.</p><p>We validate the advantages of our approach with extensive experiments on image generation and restoration. The image generation task is performed on CIFAR10, CI-FAR100 <ref type="bibr" target="#b35">[35]</ref>, and ImageNet <ref type="bibr" target="#b14">[15]</ref>, three popular datasets. Omni-GAN surpasses the baselines in terms of the Fréchet Inception distance <ref type="bibr" target="#b21">[22]</ref> and Inception score <ref type="bibr" target="#b60">[60]</ref>. In particular, in generating 128 × 128 and 256 × 256 images on ImageNet, Omni-GAN achieves surprising Inception scores of 262.85 and 343.22, respectively, both of which surpassing the previous records by more than 100 points. The image restoration part involves colorization and single image super-resolution, where Omni-INR-GAN is more flexible than BigGAN and significantly outperforms other restoration methods like DIP <ref type="bibr" target="#b72">[71]</ref> and LIIF <ref type="bibr" target="#b9">[10]</ref>, arguably because the prior learned by the generator is stronger.</p><p>We highlight the contributions of this paper as follows:</p><p>• The core discovery is that combining strong supervision and moderate regularization is the key to cGAN optimization. We achieve this goal easily with the proposed Omni-GAN framework.</p><p>• We integrate Omni-GAN into a recently published encoding named INR to validate its generalized ability and extend its range of applications.</p><p>• Omni-GAN achieves the state-of-the-art on the task of image generation on ImageNet, surpassing the prior best results by significant margins. It also requires fewer computational costs to gain the ability of highquality image generation.</p><p>We will release the code to the public. We hope that our discovery can inspire the community in studying the principle of generative models and designing powerful algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditional GANs</head><p>Conditional GAN (cGAN) <ref type="bibr" target="#b45">[45]</ref> adds conditional information to the generator and discriminator of GANs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b63">63]</ref>. There are some ways to incorporate class information into the generator, such as conditional batch normalization (CBN) <ref type="bibr" target="#b13">[14]</ref>, conditional instance normalization (CIN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, classmodulated convolution (CMConv) <ref type="bibr" target="#b83">[82]</ref>, etc. There are also many ways to add class information to the discriminator. A simple way is to directly concatenate the class information with the input or features from some middle layers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b59">59]</ref>. Next, we expound on several slightly complicated methods. AC-GAN Auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b49">[49]</ref> uses an auxiliary classifier to enhance the standard GAN model (see <ref type="figure" target="#fig_1">Fig. 2a</ref>). In particular, the objective function consists of tow parts: the GAN loss, L GAN , and the classification loss, L cls :</p><formula xml:id="formula_0">L GAN =E [log P (g = real | x real )] + E [log P (g = fake | x fake )] ,<label>(1)</label></formula><formula xml:id="formula_1">L cls = E [log P (g = c | x real )] + E [log P (g = c | x fake )] ,<label>(2)</label></formula><p>where g denotes the label of x. x real and x fake represent a real image and a generated image respectively. The discriminator D of AC-GAN is trained to maximize L GAN + L cls , and the generator is trained to maximize L cls − L GAN . We will show that the discriminator loss of AC-GAN is not optimal (see Sec. 3.3). Projection Discriminator Projection discriminator <ref type="bibr" target="#b47">[47]</ref> incorporates class information into the discriminator of GANs in a projection-based way (see <ref type="figure" target="#fig_1">Fig. 2b</ref>). The mathematical form of the projection discriminator is given by</p><formula xml:id="formula_2">D(x, y) = y T V f 1 (x; θ 1 ) + f 2 (f 1 (x; θ 1 ) ; θ 2 ) , (3)</formula><p>where x and y denote the input image and one-hot label vector respectively. V is a class embedding matrix, f 1 (·; θ 1 ) is a vector function, and f 2 (·; θ 2 ) is a scalar function. V , θ 1 , θ 2 are learned parameters of D. The discriminator D only outputs a scalar for each pair of x and y. Multi-hinge GAN Multi-hinge GAN <ref type="bibr" target="#b34">[34]</ref> belongs to classification-based cGANs. It uses a C + 1 dimensional classifier as the discriminator, which is trained by a multiclass hinge loss (see <ref type="figure" target="#fig_1">Fig. 2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implicit Neural Representation</head><p>Images are usually represented by a set of pixels with fixed resolution. A popular method named implicit neural representation (INR) is prevalent in the 3D field <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b24">25]</ref>. Recently, people introduced the INR method to 2D images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b67">66]</ref>. The INR of an image directly maps (x, y) coordinates to image's RGB pixel values. Since the coordinates are continuous, once we get the INR of an image, we can get images of arbitrary resolutions by sampling different numbers of coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Unified Loss for Feature Learning</head><p>There is a unified perspective for classification tasks. We denote the positive score set as S pos = {s (p) 1 , · · · , s (p) K }, and negative score set as S neg = {s (n) 1 , · · · , s (n) L }, respectively. Sun et al. <ref type="bibr" target="#b69">[68]</ref> proposed a unified loss to maximize s (p) as well as to minimize s (n) . The loss is defined as</p><formula xml:id="formula_3">L uni = log   1 + s (n) i ∈Sneg s (p) j ∈Spos e γ s (n) i −s (p) j +m    = log   1 + s (n) i ∈Sneg e γ s (n) i +m s (p) j ∈Spos e γ −s (p) j    ,<label>(4)</label></formula><p>where γ stands for a scale factor, and m for a margin between positive and negative scores. Eq. (4) can be converted into triplet loss <ref type="bibr" target="#b62">[62]</ref> or softmax with the cross-entropy loss <ref type="bibr" target="#b69">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Omni-GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Omni-GAN and One-sided Omni-GAN</head><p>We commence from defining the omni-loss. Based on this loss, we design two versions of cGANs: Omni-GAN, being a classification-based cGAN, and one-sided Omni-GAN, being a projection-based cGAN. These two cGANs enable us to fairly and intuitively explore the secrets behind classification-based cGANs and projection-based cGANs. Let x and y denote an image and its multi-label vector respectively. S is a classifier. Suppose that there are K positive labels and L negative labels. Then s = S(x) is a K + L dimensional score vector. The omni-loss is defined as</p><formula xml:id="formula_4">L omni (x, y) = log   1 + i∈Ineg e si(x)   + log   1 + j∈Ipos e −sj (x)   ,<label>(5)</label></formula><p>where I neg is a set consisting of indexes of negative scores (i.e., |I neg | = L), and I pos consists of indexes of positive scores (i.e., |I pos | = K). s k (x) represents the element k of vector s. <ref type="bibr" target="#b68">[67]</ref> shows that Eq. (5) is a special case of Eq. (4). We provide a detailed derivation from Eq. (4) to Eq. (5) in Appendix A. Next, we introduce two versions of Omni-GAN by setting different labels for the omni-loss.</p><p>• The classification-based Omni-GAN.</p><p>We first elucidate the loss of the discriminator for Omni-GAN. The discriminator loss consists of two parts, one for x real (drawn from the training data), and the other for x fake (drawn from the generator). For x real , its multi-label vector is given by</p><formula xml:id="formula_5">y real = [−1, . . . , 1 gt , . . . , −1 C , 1 real , −1 2 ],<label>(6)</label></formula><p>whose dimension is C + 2, with C being the number of classes of the training dataset. 1 gt is 1 if its index in the vector is equal to the ground truth label of x real , otherwise −1. We use 1 to denote the corresponding score belongs to the positive set, and −1 to the negative set. The multi-label vector of x fake is also a C + 2 dimensional vector:</p><formula xml:id="formula_6">y fake = [−1, . . . , −1, . . . , −1 C , −1, 1 fake 2 ],<label>(7)</label></formula><p>where only the last element is 1. According to Eq. (5), <ref type="bibr" target="#b5">(6)</ref>, and <ref type="formula" target="#formula_6">(7)</ref>, we define the discriminator loss as</p><formula xml:id="formula_7">L D =E xreal∼pd [L omni (x real , y real )] + E xfake∼pg [L omni (x fake , y fake )] ,<label>(8)</label></formula><p>where p d is the training data distribution, and p g is the generated data distribution. It is obvious that the discriminator D actually acts as a multi-label classifier, which takes as input x, and outputs a score vector s = D(x).</p><p>The generator attempts to fool the discriminator into believing its samples are real. To this end, its multi-label is set to be</p><formula xml:id="formula_8">y (G) fake = [−1, . . . , 1 G , . . . , −1 C , 1 real , −1 2 ],<label>(9)</label></formula><p>which is the same as y real defined in Eq. (6). 1 G is 1 if its index in the vector is equal to the label adopted by the generator to generate x fake , otherwise −1. The generator loss is then given by</p><formula xml:id="formula_9">L G = E xfake∼pg L omni x fake , y (G) fake .<label>(10)</label></formula><p>• The projection-based (one-sided) Omni-GAN.</p><p>We imitate the way how the projection-based discriminator <ref type="bibr" target="#b47">[47]</ref> utilizes class labels (see Eq. (3)), and design a projection-based variant of Omni-GAN, named one-sided Omni-GAN, which does not fully utilize the class supervision.</p><p>It is easy to implement one-sided Omni-GAN: only slightly modify the multi-label vector, y. Following the setting above, the multi-label vector for x real is set to be</p><formula xml:id="formula_10">y real = [0, . . . , 1 gt , . . . , 0 C , 1, 0 2 ],<label>(11)</label></formula><p>where 1 gt is 1 if its index in the vector is equal to the ground truth label of x real , otherwise 0. And 0 means that the corresponding score will be ignored when calculating the omniloss. The multi-label vector for x fake is given by</p><formula xml:id="formula_11">y fake = [0, . . . , −1 G , . . . , 0 C , −1, 0 2 ],<label>(12)</label></formula><p>where −1 G is −1 if its index in the vector is equal to the label adopted by the generator to generate x fake , otherwise 0. The discriminator loss is the same as that defined in Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><p>For generator, its multi-label vector for x fake is</p><formula xml:id="formula_12">y (G) fake = [0, . . . , 1 G , . . . , 0 C , 1, 0 2 ],<label>(13)</label></formula><p>where 1 G is 1 if its index in the vector is equal to the label adopted by the generator to generate x fake , otherwise 0. The generator loss is the same as that defined in Eq. <ref type="bibr" target="#b9">(10)</ref>. In summary, we introduce two versions of Omni-GAN by setting different multi-label vector for the omni-loss (defined in Eq. (5)). It is easy to implement these two GANs in practice: as shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>, first, let the discriminator output a vector instead of a scalar; second, apply the omni-loss to the output vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Devil Lies in Combination of Strong Supervision and Moderate Regularization</head><p>We conducted control experiments on CIFAR100 <ref type="bibr" target="#b35">[35]</ref>, and compared Omni-GAN and one-sided Omni-GAN with a projection-based cGAN, namely BigGAN <ref type="bibr" target="#b5">[6]</ref>. As shown in <ref type="figure">Fig. 3</ref>, One-sided Omni-GAN is on par with BigGAN in terms of IS, indicating that one-sided Omni-GAN is indeed a projection-based cGAN. We can see from Eq. (11),  <ref type="figure">Figure 3</ref>: IS on CIFAR100. "wd" stands for weight decay. The combination of strong supervision and weight decay is crucial. Weight decay effectively alleviates the collapse problem of strongly supervised cGANs (Omni-GAN) so that the cGANs enjoy superior performance from strong supervision. On the other hand, weight decay may slightly impair the performance of weakly supervised cGANs (e.g., projection-based BigGAN, one-sided Omni-GAN).</p><p>(12) and (13) that one-sided Omni-GAN does not make full use of the class label's supervision. Those elements with label 0 in the output vector of the discriminator are ignored (i.e., they are not used to calculate the omni-loss, meaning these elements will not get gradients when backpropagation). Therefore, we think that the projection-based cGAN is an implicit and weaker cGAN in the sense that it does not make full use of the supervision of the class label.</p><p>On the other hand, as a classification-based cGAN, Omni-GAN makes full use of class supervision (strong supervision). However, it suffers a severe collapse in the initial stages of training. As shown in <ref type="figure">Fig. 3</ref>, the IS of Omni-GAN shows a significant upward compared to the projection-based cGAN but drops dramatically when about 1M real images (20 epoch) are shown to the discriminator.</p><p>Our core discovery is that a moderate regularization effectively prevents the early collapse of classification-based cGANs. In practice, we use weight decay <ref type="bibr" target="#b36">[36]</ref> as the choice of regularization. We call weight decay moderate regularization in that it does not introduce considerable computational overhead as other regularizations do, such as gradient penalty <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b75">74]</ref>. Therefore, Omni-GAN can be trained efficiently on large-scale datasets such as ImageNet. As shown in <ref type="figure">Fig. 3</ref>, combined with weight decay, Omni-GAN has greatly improved its IS compared to BigGAN. Moreover, we observe the projection-based cGAN, Big-GAN, also collapses after long training. Weight decay is also effective for alleviating the collapse of BigGAN.</p><p>Note that we are not the first to use weight decay in GANs. <ref type="bibr" target="#b82">[81]</ref> applys weight decay to unconditional GANs. However, our main contribution is to emphasize that the combination of strong supervision and weight decay is the key to cGANs. In fact, combining weight decay with weakly supervised cGANs even hurts performance. As shown in <ref type="figure">Fig. 3</ref>, both projection-based BigGAN and one-sided Omni-GAN suffer performance degradation after combined with weight decay.</p><p>In summary, we claim that the combination of strong supervision and weight decay is critical for cGANs. Strong supervision helps boost the performance of cGANs but causes severe early collapse. Weight decay effectively alleviates early collapse, so that cGANs can fully enjoy the benefits of strong supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison to Previous Approaches</head><p>We study another well-known classification-based cGAN, AC-GAN <ref type="bibr" target="#b49">[49]</ref>, and show its results in <ref type="figure" target="#fig_3">Fig. 4</ref>. AC-GAN also suffers severe early collapse like Omni-GAN does. One possible explanation for the early collapse of classification-based cGANs is that the discriminator overfits the training data <ref type="bibr" target="#b31">[31]</ref>. Therefore, we study whether data augmentation is effective to alleviate the early collapse. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, AC-GAN combined with differentiable data augmentation (DiffAug) <ref type="bibr" target="#b80">[79]</ref> 2 still cannot avoid early collapse. However, weight decay is still very effective in alleviating the early collapse of AC-GAN.</p><p>We emphasize that weight decay prevents overfitting of the discriminator at the model level, and data augmentation does at the data level. We will show in experiments that both methods are effective for improving the performance of cGANs. However, we empirically find that weight decay is almost 100% effective in preventing the early collapse of classification-based cGANs, but data augmentation is not always effective.</p><p>Next, we investigate whether spectral normalization (SN) <ref type="bibr" target="#b46">[46]</ref> and gradient penalty <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b75">74]</ref> will alleviate early collapse. Because the network architectures of the generator and discriminator we used in our experiments employ SN by default, Omni-GAN and AC-GAN still suffer severe early collapse, indicating that SN cannot alleviate the collapse problem of classification-based cGANs. In addition, we have empirically found that gradient penalty cannot help classification-based cGANs avoid early collapse (refer to Appendix B for experimental results).</p><p>Finally, by comparing AC-GAN and Omni-GAN's loss functions, we found that the original AC-GAN still has room for improvement. Due to space limitations, we put the details in Appendix C. We name the improved AC-GAN ImAC-GAN. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the performance of ImAC-GAN is significantly better than that of AC-GAN, and is comparable to that of Omni-GAN (refer to Sec. 4.1).</p><p>To sum up, our results reveals that fully utilizing the supervision can improve performance of cGANs, but at the risk of early collapse. This work offers a practical way <ref type="bibr" target="#b1">2</ref> We did not choose ADA augmentation because ADA needs to design different overfitting heuristics for different losses. (weight decay) to overcome the collapse issue, so that the trained model enjoys both superior performance and safe optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Omni-INR-GAN: Being Friendly to Downstream Tasks</head><p>Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods. We derive Omni-INR-GAN by integrating implicit neural representation (INR) <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b10">11]</ref> into Omni-GAN. Omni-INR-GAN employs INR to enhance the generator's output layer. Due to limited space, we detail technical details in Appendix D.</p><p>Omni-INR-GAN has the ability to output images with any aspect ratio and any resolution. Thus it is friendly to downstream tasks like image restoration. <ref type="figure" target="#fig_4">Fig. 5</ref> shows an example of combining Omni-INR-GAN with DGP <ref type="bibr" target="#b50">[50]</ref> for colorization. As shown in <ref type="figure" target="#fig_4">Fig. 5 (d)</ref>, the DGP with BigGAN model only colorize a square patch in the original image. This is because BigGAN only generates fixed-size images, which is inflexible for downstream tasks. On the other hand, Omni-INR-GAN is able to generate images with any aspect ratio and any resolution easily so as to directly handle the entire degraded image. Because the generator has seen considerable numbers of natural images, it owns a wealth of prior knowledge. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref> (e), utilizing the generator prior helps get a plausible color image.</p><p>Another benefit is that just adding a simple INR network, whose parameters can be neglected compared to the backbone of the generator, greatly boost Omni-GAN's performance on ImageNet. For example, Omni-INR-GAN improves the IS of Omni-GAN from 190.94 to 262.85 on Im-ageNet 128 × 128, and from 304.05 to 343.22 on ImageNet 256 × 256, respectively (refer to experiments for details).   <ref type="table">Table 1</ref>: FID and IS on CIFAR10 and CIFAR100. † indicates quoted from the paper. "wd" stands for applying weight decay, and "wd" for not applying weight decay. Dif-fAug means differentiable data augmentation. FID and IS are computed using 50K training and 50K generated images with the TensorFlow-based pre-trained Inception-V3 model. Please refer to Sec. 4.1 for detailed analysis.</p><formula xml:id="formula_13">No. Method CIFAR10 CIFAR100 FID ↓ IS ↑ Collapse? FID ↓ IS ↑ Collapse? - FQ-GAN [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on CIFAR</head><p>We compare a projection-based cGAN, BigGAN, and several classification-based cGANs, including AC-GAN, ImAC-GAN, Omni-GAN, and Omni-INR-GAN. We also study the effects of data augmentation and weight decay on these methods. Results are summarized in <ref type="table">Table 1</ref>.</p><p>First, let us focus on the projection-based cGAN, Big-GAN. We found that data augmentation helps improve the performance of BigGAN, but weight decay cannot, even being harmful. This is reasonable because BigGAN belongs to projection-based cGANs, whose discriminator is essentially a weak implicit classifier. Weight decay constrains the fitting ability of the discriminator (i.e., a weak network). Moreover, the supervision for the discriminator is too weak (i.e., a weak supervision). As such, the performance is naturally not good.</p><p>Next, we compare AC-GAN and ImAC-GAN, which belong to classification-based cGANs. ImAC-GAN has a clear and consistent improvement over AC-GAN in all experiments. This improvement is achieved by slightly modifying the loss function of AC-GAN to be consistent with Omni-GAN (refer to Appendix C for technical details). Third, ImAC-GAN and Omni-GAN are both superior classification-based cGANs, and their performance is comparable. ImAC-GAN uses cross-entropy as the classification loss, so it only supports single-label classification. However, the omni-loss used by Omni-GAN naturally supports multi-label classification. Therefore when the image owns multiple positive labels, Omni-GAN is more flexible than ImAC-GAN. We give an example of using Omni-GAN to generate images with multiple positive labels in Appendix E.</p><p>We find that training collapse is more likely to appear on CIFAR100 that owns more classes. For example, all classification-based cGANs, including AC-GAN, ImAC-GAN, Omni-GAN, and Omni-INR-GAN, collapsed on CI-FAR100 (shown in <ref type="table">Table 1</ref>, No. #4, #8, #12, #16). Even if equipped with data augmentation, they still collapsed (No. #6, #10, #14, #18). However, weight decay effectively alleviates the training collapse of these methods. On CI-FAR10, data augmentation can alleviate the collapse issue of classification-based cGANs (No. #6, #10, #14). We think a possible reason is that CIFAR10 has a relatively small number of categories, and each category has 5, 000 training images. Combining with data augmentation can effectively prevent the discriminator from overfitting the training data. An anomaly is that data augmentation cannot prevent the collapse of Omni-INR-GAN on CIFAR10 (#18). We found in our experiments that data augmentation seems to be somewhat exclusive with Omni-INR-GAN. We guess that one possible reason is that some random shift operations in data augmentation destroy some prior information to be learned by Omni-INR-GAN from the coordinates.</p><p>Last but not least, we have tried to combine Omni-GAN with the network architecture of StyleGAN2 <ref type="bibr" target="#b33">[33]</ref>, but failed. StyleGAN2 employs some technologies like equalized learning rate <ref type="bibr" target="#b30">[30]</ref> (explicitly scales the weights at runtime) during the training process. We found that these techniques seem to conflict with weight decay, which is the key for classification-based cGANs to avoid early collapse.  Real images shown to D  avoiding early collapse needs further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining strong classification loss with StyleGANs while</head><formula xml:id="formula_14">Method ImageNet 128 × 128 ImageNet 256 × 256 FID (train) ↓ FID (val) ↓ IS ↑ G Params FID (train) ↓ FID (val) ↓ IS ↑ G Params CR-BigGAN † [77] 6.66 - - - - - - - S3GAN † [40] 7.70 - 83.10 - - - - - BigGAN ‡ [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on ImageNet</head><p>ImageNet <ref type="bibr" target="#b14">[15]</ref> is a large dataset with 1000 number of classes and approximate 1.2M training data. We trained BigGAN 3 , Omni-GAN, and Omni-INR-GAN on ImageNet 128×128 and 256×256, respectively. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>, and the convergence curves are shown in <ref type="figure" target="#fig_6">Fig. 6</ref> (please refer to Appendix G for more results).</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>   <ref type="table">Table 3</ref>: Image reconstruction results using pre-trained GAN models. High performance shows the potential to be applied to downstream tasks.</p><p>the improvement does not lie in the number of parameters. We think the possible reason is that Omni-INR-GAN introduces coordinates as input, which helps the generator learn some prior knowledge of the natural images (for example, the sky often appears in the upper part of an image, and the grass often appears in the lower part on the contrary).</p><p>In summary, the significant improvement of Omni-GAN lies in the combination of strong supervision and weight decay. Strong supervision helps boost the performance of cGANs, but it causes the training to collapse earlier. Weight decay effectively alleviates the collapse problem, so that cGANs fully enjoy the benefits from strong supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Application to Image-to-Image Translation</head><p>We improve the mIoU of SPADE <ref type="bibr" target="#b52">[52]</ref> from 62.21 to 65.07 by only using Omni-GAN loss and weight decay for the discriminator (please refer to Appendix H for details). We believe that the improvement comes from the improved ability of the discriminator in distinguishing different classes, so that the generator receives better guidance and thus produces images with richer semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Application to Downstream Tasks</head><p>Colorization. <ref type="figure">Fig. 7</ref> shows an example of using the pretrained BigGAN and Omni-INR-GAN to colorize images, respectively. See Appendix I for technical details. Omni-INR-GAN directly colorizes the entire image because it has the ability to output images of any resolution. However, BigGAN cannot do this. Another interesting phenomenon is that Omni-INR-GAN gradually overlays colors on the corresponding objects, indicating that the finetuning process is mining GAN's prior information. Prior Enhanced Super-Resolution <ref type="figure">Fig. 8</ref> shows an example of using pre-trained Omni-INR-GAN for superresolution. We deliver two messages. First, Omni-INR-GAN can extrapolate low-resolution images to any resolution. <ref type="figure">Fig. 8 (f)</ref> shows the results with upsampling scales of ×1, ×4.6, and even ×63.5. Second, Omni-INR-GAN has a wealth of prior knowledge, which helps complement the missing semantics of the input. For example, for the extremely low-resolution zebra image shown in <ref type="figure">Fig. 8 (b)</ref>, the image details are severely missing. In this case, although LIIF <ref type="bibr" target="#b9">[10]</ref> has the ability to up-sample images at any scale, it cannot fill in the missing stripes of the zebra, as shown in <ref type="figure">Fig. 8 (c)</ref>.</p><p>In <ref type="figure">Fig. 8 (d)</ref>, DIP <ref type="bibr" target="#b72">[71]</ref> also fails. DIP only leverages the input image and the structure of a ConvNet as the image prior. It cannot work when the input image's resolution is too low. DGP <ref type="bibr" target="#b50">[50]</ref> with BigGAN can only handle fixed-size images, limiting its practical application ( <ref type="figure">Fig. 8 (e)</ref>). However, DGP with Omni-INR-GAN can super-resolve the entire image and extrapolate the input at any scale. In <ref type="figure">Fig. 8</ref> (f), by mining Omni-INR-GAN's prior, the missing zebra stripes are complemented, and a clear foreground and background are obtained. Even the shadow of the zebra is clearly visible.</p><p>In <ref type="table">Table 3</ref>, we quantitatively compare different pretrained GAN models for image reconstruction. Please refer to Appendix I for details. Omni-INR-GAN outperforms BigGAN and Omni-GAN by a large margin, showing its potential for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents an elegant and practical solution to training effective conditional GAN models. The key discovery is that strong supervision can largely improve the upper-bound of image generation quality, but it also makes the model collapse earlier. We design the Omni-GAN algorithm that equips the classification-based loss with regularization (in particular, weight decay) to alleviate collapse. Our algorithm achieves notable performance gain in various scenarios including image generation and restoration. Our research implies that there may be more 'secrets' in optimizing cGAN models. We look forward to applying the proposed algorithm and pre-trained models to more scenarios and investigating further properties to improve cGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Omni-GAN and Omni-INR-GAN Supplementary Material</head><p>Part I A. Derivation from Unified Loss to Omni-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Derivation of Omni-loss</head><p>The unified loss <ref type="bibr" target="#b69">[68]</ref> is defined as The omni-loss is defined as</p><formula xml:id="formula_15">L uni = log   1 + s (n) i ∈Sneg s (p) j ∈Spos e γ s (n) i −s (p) j +m    = log   1 + s (n) i ∈Sneg e γ s (n) i +m s (p) j ∈Spos e γ −s (p) j    ,<label>(14)</label></formula><formula xml:id="formula_16">L omni (x, y) = log   1 + i∈Ineg e si(x)   + log   1 + j∈Ipos e −sj (x)   ,<label>(15)</label></formula><p>where I neg is a set consisting of indexes of negative scores (|I neg | = L), and I pos consists of indexes of positive scores (|I pos | = K).</p><p>Eq. <ref type="formula" target="#formula_0">(15)</ref> is a special case of Eq. <ref type="formula" target="#formula_0">(14)</ref>, which has been proved by <ref type="bibr" target="#b68">[67]</ref>. For the convenience of readers in the English community, we provide our proof here. Let γ be 1 and m be 0, then where [·] + means max(·, 0).</p><formula xml:id="formula_17">L uni = log   1 + s (n) i ∈Sneg e s (n) i s (p) j ∈Spos e −s (p) j    (16) = log   1 + e log s (n) i ∈Sneg e s (n) i s (p) j ∈Spos e −s (p) j   =softplus   log    s (n) i ∈Sneg e s (n) i s (p) j ∈Spos e −s (p) j       =softplus   log    s (n) i ∈Sneg s (p)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>According to log</head><formula xml:id="formula_18">n i=1 e xi ≈ max(x 1 , x 2 , . . . , x n ), we get L uni ≈ max s (n) i ∈Sneg,s (p) j ∈Spos s (n) i − s (p) j + ,<label>(17)</label></formula><p>where minimizing Eq. (17) makes the smallest s </p><formula xml:id="formula_19">uni = log   1 + s (n) i ∈S (1) neg e s (n) i s (p) j ∈{0} e −s (p) j    = log   1 + s (n) i ∈S (1) neg e s (n) i e 0    = log   1 + s (n) i ∈S (1) neg e s (n) i    ,<label>(18)</label></formula><p>where from Eq. (17) we know that minimizing Eq. (18) makes s </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Gradient Analysis</head><p>The gradients of omni-loss have two properties: on one hand, the gradients w.r.t. s (n) and s (p) are independent; on the other hand, the gradients w.r.t. s k ), {k = 0, 1, . . . }, are automatically balanced. To illustrate these properties, we visualize the gradients of omni-loss. <ref type="figure" target="#fig_9">Fig. 9a</ref> shows a case that only contains one s (n) and one s (p) . A, B, and C have the same s (p) , which is 0, but different s (n) (i.e., 4, 0, −4, respectively). As a result, the gradients w.r.t. s (p) at these three points are the same (i.e., 0.5). Nevertheless, the gradients w.r.t. s (n) at these three points are different. For example, the gradient w.r.t. s (n) at A is largest (equal to 0.98). The reason for this is that the objective of omni-loss is to minimize s (n) . Thus the larger the s (n) , the larger the gradient w.r.t. s (n) . (b) IS on CIFAR100 <ref type="figure" target="#fig_0">Figure 10</ref>: FID and IS on CIFAR100. We test three gradient penalty methods (i.e., WGAN-GP, WGAN-div, and R1 regularization), none of which can alleviate the collapse issue of AC-GAN.</p><p>In <ref type="figure" target="#fig_9">Fig. 9b</ref>, we show the ability of omni-loss to automatically balance gradients. We consider a case with only two positive labels, namely s </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradient Penalty for Classification-based cGANs</head><p>We investigate whether gradient penalty will alleviate early collapse. We chose AC-GAN <ref type="bibr" target="#b49">[49]</ref>, the currently widely known classification-based cGAN, as the testbed, and evaluate three gradient penalty methods: WGAN-GP <ref type="bibr" target="#b20">[21]</ref>, WGAN-div <ref type="bibr" target="#b42">[42]</ref>, and R1 regularization <ref type="bibr" target="#b75">[74]</ref>. Because cGANs are more likely to collapse when the number of categories is large, we evaluate them on CIFAR100 instead of CIFAR10. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, none of the three gradient penalty methods can prevent AC-GAN from collapsing. We emphasize that computing gradient penalties will introduce additional computational overhead during GAN's training, which is very unfriendly to large-scale datasets such as ImageNet. However, weight decay effectively alleviates the collapse problem without adding any additional training overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Improved AC-GAN (ImAC-GAN)</head><p>Auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b49">[49]</ref> uses an auxiliary classifier to enhance the standard GAN model. Its objective function consists of tow parts: the GAN loss, L GAN , and the classification loss, L cls :</p><formula xml:id="formula_20">L GAN =E [log P (g = real | x real )] + E [log P (g = fake | x fake )] ,<label>(21)</label></formula><formula xml:id="formula_21">L cls = E [log P (g = c | x real )] + E [log P (g = c | x fake )] ,<label>(22)</label></formula><p>where g is a random variable denoting the class label and c is the ground truth label of x. x real and x fake represent a real image and a generated image respectively. The discriminator D of AC-GAN is trained to maximize L GAN + L cls , and the generator is trained to maximize L cls − L GAN .</p><p>The discriminator loss of AC-GAN is not optimal. We give a slightly modified version below. Suppose the dataset owns C categories, then the discriminator is trained to maximize</p><formula xml:id="formula_22">L D =L GAN + E [log P (g = c | x real )] + E [log P (g = C | x fake )] ,<label>(23)</label></formula><p>where c ∈ {0, 1, . . . , C − 1} is the ground truth class label of x real , and g = C means that x fake belongs to the fake class. To sum up, we use an additional class to represent the generated image. In practice, this is achieved by setting the dimension of the fully connected layer of the auxiliary classification layer to be C + 1 rather than C.</p><p>The objective function of the generator is consistent with that of the original AC-GAN, i.e., maximizing</p><formula xml:id="formula_23">L G = −L GAN + E [log P (g = c fake | x fake )] ,<label>(24)</label></formula><p>where c fake is the class label used by the generator to generate x fake . We name this improved version of AC-GAN ImAC-GAN. As shown in the paper, ImAC-GAN is comparable to Omni-GAN, both of which achieve superior performance compared to projection-based cGANs. However, because ImAC-GAN uses cross-entropy as the loss function for classification, it can only handle the case where the sample has a positive label. Omni-GAN uses omni-loss, essentially a multi-label classification loss, which naturally supports handling samples with one positive label or multiple positive labels. We will give an example of generating images with multiple positive labels in Sec. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Technical Details of Omni-INR-GAN</head><p>Learning image prior model is helpful for image restoration and manipulation, such as denoising, inpainting, and harmonizing. Deep generative prior (DGP) <ref type="bibr" target="#b50">[50]</ref> showed the potential of employing the generator prior captured by a pre-trained GAN model (i.e., a BigGAN model trained on a large-scale image dataset, ImageNet). However, BigGAN can only output images with a fixed aspect ratio, limiting the practical application of DGP. To make the pre-trained GAN model more flexible for downstream tasks, we propose a new GAN named Omni-INR-GAN, which can output images with any aspect ratio and any resolution.</p><p>Images are usually represented by a set of pixels with fixed resolution. A popular method named implicit neural representation (INR) is prevalent in the 3D field <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Recently, people introduced the INR method to 2D images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b65">65]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 11 (a)</ref>, the INR of an image directly maps (x, y) coordinates to image's RGB pixel values. Since the coordinates are continuous, once we get the INR of an image, we can get images of arbitrary resolutions by sampling different numbers of coordinates. Inspired by the local implicit image function (LIIF) <ref type="bibr" target="#b9">[10]</ref>, we use INR to enhance Omni-GAN, with the goal of enabling the generator to output images with any aspect ratios and any resolution. We name our method Omni-INR-GAN. As shown in <ref type="figure" target="#fig_0">Fig. 11 (b)</ref>, we keep the backbone of the generator network unchanged and employ an INR network for the output layer. Let M ∈ R C×H×W represent the output feature map of the backbone, f θ be the implicit neural function. Then the RGB signal at (x, y) coordinate is given by s = f θ (M x,y , x, y), where M x,y stands for the feature vector at (x, y). Note that since x and y can be any real numbers, M x,y may not exist in M . In such a case, we adopt the bilinear interpolation of the four feature vectors near (x, y) as the feature at (x, y).</p><p>Omni-INR-GAN can generate images with any aspect ratio, so as to be more friendly to downstream tasks like image restoration and manipulation. After trained on the largescale dataset ImageNet, Omni-INR-GAN can be combined with DGP to do restoration tasks. Omni-INR-GAN eliminates cropping operations before image restoration, making it possible to repair the entire image directly. Since the generator has seen considerable natural images, utilizing the generator prior can facilitate downstream tasks significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. An Example of Multi-label Discriminator</head><p>Omni-loss is essentially a multi-label classification loss and naturally supports classification with multiple positive labels. To verify the ability of Omni-GAN for generating samples with multiple positive labels, we construct a mixed dataset containing images of digits from two distinct domains, namely MNIST <ref type="bibr" target="#b38">[38]</ref> of handwritten digits and SVHN <ref type="bibr" target="#b48">[48]</ref> of house numbers. Some example images from the datasets are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. In this setting, the discriminator needs to predict three attributes, class (recognizing digits), domain, and reality.</p><p>Let us take images of MNIST as an example, and show how to set the loss for the discriminator. As for SVHN, the case is analogous. Suppose x real is an image sampled from MNIST, its multi-label vector is given by <ref type="bibr" target="#b24">(25)</ref> where −1 means the corresponding score belongs to the negative set, and 1 to the positive set. As can be seen, y real possesses three positive labels. The multi-label vector for x fake is then given by <ref type="bibr" target="#b25">(26)</ref> which is a one-hot vector with the last element being 1. The discriminator loss is given by</p><formula xml:id="formula_24">y real = [−1, . . . , 1 gt , . . . , −1 class , 1 mnist , −1 domain , 1 real , −1 reality ],</formula><formula xml:id="formula_25">y fake = [−1, . . . , −1, . . . , −1 class , −1, −1 domain , −1, 1 fake reality ],</formula><formula xml:id="formula_26">L D =E xreal∼pd [L omni (x real , y real )] + E xfake∼pg [L omni (x fake , y fake )] .<label>(27)</label></formula><p>For generator, its goal is to cheat the discriminator. The multi-label vector for x fake is given by where 1 G is 1 if its index in the vector is equal to the label adopted by the generator to generate x fake , otherwise −1. The generator loss is given by.</p><formula xml:id="formula_27">L G = E xfake∼pg L omni x fake , y (G) fake .<label>(29)</label></formula><p>We experimentally found that this multi-label discriminator can instruct the generator to generate images from different domains. Some generated images are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. We must emphasize that this is only a preliminary experiment to verify the function of the multi-label discriminator. We look forward to applying the multi-label discriminator to other tasks in the future, such as translation between images in different domains, domain adaptation, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Results on CIFAR F.1. Over-fitting of the Discriminator</head><p>Karras et al. <ref type="bibr" target="#b31">[31]</ref> found that the discriminator overfits the training dataset, which will lead to incorrect gradients provided to the generator. Thus the training diverges. To verify   <ref type="bibr" target="#b31">[31]</ref>.</p><p>that the collapse of the projection-based cGAN is due to the over-fitting of the discriminator, we plotted the scalar output of the discriminator, D(x), over the course of training. We utilized the test set of CIFAR100 containing 10, 000 images as the verification set, which was not used in the training. As shown in <ref type="figure" target="#fig_0">Fig. 14a</ref>, obviously, as training progresses, the D(x) of the validation set tends to that of the generated images, substantiating that the discriminator overfits the training data. We also plotted the FID curve in the same <ref type="figure">figure.</ref> We can see that the training commences diverging when showing about 20M real images (i.e., around 400 epoch) to the discriminator. The best FID is obtained when approximately 15M real images are shown to the discriminator.</p><p>In <ref type="figure" target="#fig_0">Fig. 14b</ref>, we show the D(x) and FID after applying weight decay to the projection-based discriminator. We can find that although the discriminator still overfits the training data, the training dose not collapse during the whole training process (the minimum FID, 9.74, is reached at the end of the training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Comparison of One-sided Omni-GAN and</head><p>Projection-based GAN on CIFAR10</p><p>We provide the results of one-sided Omni-GAN and projection-based BigGAN on CIFAR10. As shown in <ref type="figure" target="#fig_0">Fig. 15</ref>, one-sided Omni-GAN is comparable to the projection-based BigGAN in terms of both FID and IS. This proves that one-sided Omni-GAN indeed belongs to projection-based cGANs. Both one-sided Omni-GAN and projection-based BigGAN are inferior to Omni-GAN. Because the only difference between one-sided Omni-GAN and Omni-GAN is whether the supervision is fully utilized, we conclude that the superiority of Omni-GAN lies in the full use of supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Comparison with Multi-hinge GAN</head><p>Multi-hinge GAN belongs to classification-based cGANs, and also suffers from the early collapse issue. We study whether weight decay is effective for Multi-hinge GAN. As shown in <ref type="figure" target="#fig_0">Fig. 16</ref>, original Multi-hinge GAN suffers a severe early collapse issue. After equipped with weight decay, Multi-hinge GAN enjoys a safe optimization and its FID is even comparable to that of Omni-GAN. However, its IS is worse than that of Omni-GAN.</p><p>Multi-hinge GAN combined with weight decay does not  always perform well. The results on CIFAR10 are shown in <ref type="figure" target="#fig_0">Fig. 17</ref>. Weight decay deteriorates Multi-hinge GAN in terms of both FID and IS. However, Omni-GAN outperforms Multi-hinge GAN. In addition, omni-loss is more flexible than multi-hinge loss. It supports implementing a multi-label discriminator. As a result, we suggest first considering using Omni-GAN when choosing cGANs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Applying Weight Decay to the Generator</head><p>We found empirically that applying weight decay also to the generator can make training more stable. As shown in <ref type="figure" target="#fig_0">Fig. 18</ref>, although only applying weight decay to the discriminator can avoid the risk of collapse earlier, the IS has a trend of gradually decreasing as the training progresses. Fortunately, applying weight decay (set to be 0.001 in our most experiments) to the generator can solve this problem. This phenomenon seems to indicate that the generator is also at a risk of over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5. How to Set the Weight Decay?</head><p>We did a grid search for the weight decay on CIFAR and found that its value is related to the size of the training dataset. For CIFAR100, there are only 500 images per class, and the weight decay is set to be 0.0005. For CI-FAR10, there are 5000 images per class, and the weight decay is set to be 0.0001. For ImageNet, it is a large dataset with a considerable number of training data (approximate 1.2M). The weight decay is set to 0.00001. The conclusion is that the smaller the dataset, the higher the risk of overfitting for the discriminator. Then weight decay should be larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Results on ImageNet</head><p>We provide convergence curves on ImageNet 256 × 256. As shown in <ref type="figure" target="#fig_0">Fig. 19</ref>, both Omni-GAN and Omni-INR-GAN converge faster than BigGAN, proving the effectiveness of combining strong supervision and weight decay. Omni-INR-GAN clearly outperforms Omni-GAN, showing its significant potential for future applications. In <ref type="figure" target="#fig_1">Fig. 20</ref>, we show the tradeoff curve of these methods using the truncation trick on ImageNet 256 × 256. Omni-INR-GAN is consistently superior to Omni-GAN and BigGAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Application to Image-to-Image Translation</head><p>Omni-GAN can be used for image-to-image translation tasks. We verify the effectiveness of Omni-GAN on semantic image synthesis <ref type="bibr" target="#b73">[72,</ref><ref type="bibr" target="#b56">56]</ref>. In particular, we replace the GAN loss of SPADE <ref type="bibr" target="#b52">[52]</ref> with Omni-GAN's loss, and keep other hyper-parameters unchanged. The discriminator is a fully convolutional network, which is widely adopted by image-to-image translation tasks <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b74">73]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 21</ref>, the discriminator takes images as input and outputs feature maps with the number of channels being C + 2. C represents the number of classes which is analogous to that of the semantic segmentation task. 2 indicates there are two extra feature maps representing to what extent the input image is real or fake. We adopt nearest neighbor downsampling to downsample the label map to the same resolution as the output feature maps of the discriminator. Then we use the downsampled label map as the ground truth label, and apply a per-pixel omni-loss to the output feature maps of the discriminator.</p><p>We use Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref> as a testbed, and train models on the training set with size of 2, 975. The images is resized to 256 × 512. Models are evaluated by the mIoU of the generated images on the test set with 500 images. We use a pre-trained DRN-D-105 <ref type="bibr" target="#b76">[75]</ref> as the segmentation model for the sake of evaluation. As shown in <ref type="table" target="#tab_6">Table 4</ref>, Omni-GAN improves the mIoU score of SPADE from 62.21 to 65.07, substantiating that the synthesized images possess more semantic information. We believe that the improvement comes from the improved ability of the discriminator in distinguishing different classes, so that the generator receives better guidance and thus produces images with richer semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>C+2 channels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Discriminator</head><p>Label Map</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Neighbor Downsampling</head><p>Label Map</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Neighbor Downsampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-pixel Omni-loss</head><p>Per-pixel Omni-loss <ref type="figure" target="#fig_0">Figure 21</ref>: Combine omni-loss with a fully convolutional discriminator whose outputs are feature maps. In the figure, the green and red feature maps represent scores that the input images are real and fake, respectively. Omni-loss is applied to the output feature maps pixel-by-pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Application to Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Colorization and Super-resolution</head><p>Deep generative prior (DGP) <ref type="bibr" target="#b50">[50]</ref> showed the potential of employing image prior captured by a pre-trained GAN model. Our colorization and super-resolution schemes are based on DGP. We first introduce the preliminary knowledge of DGP.</p><p>Suppose x is a natural image and φ is a degradation function, e.g., gray transform for colorization and downsampling for super-resolution. Thenx = φ(x) represents the degraded image, i.e., a partial observation of the original image, x. The goal of image restoration is to recover x from x with the help of some statistical image prior of x. DGP proposes employing the image prior stored in a pre-trained GAN's generator. The objective is defined as</p><formula xml:id="formula_28">θ * , z * = arg min θ,z L(x, φ(G(z; θ))),<label>(30)</label></formula><p>where z is a noise vector. G represents the generator in GAN and is parameterized by θ.</p><p>L is a discriminator-based distance metric: L (x 1 , x 2 ) = i∈I D (x 1 , i) , D (x 2 , i) 1 . D is the discriminator coupled with G. I is a index set for feature maps of different blocks of D. Note that both G and D have been trained on a large-scale natural image dataset. DGP employs the prior of G by fine-tuning θ and z. After fine-tuning, we get the restored image x * = G(z * ; θ * ).</p><p>Although DGP has achieved noteworthy results in image restoration and manipulation, it has limitations due to the inflexibility of the pre-trained <ref type="bibr">GAN</ref>    <ref type="formula" target="#formula_28">(30)</ref> is the objective. For colorization,x is a grayscale image, and for super-resolution,x is a low-resolution image. We resize the input image's short edge to 256 and keep the aspect ratio of the image unchanged. After fine-tuning, x * = G(z * ; θ * ) is the restored image. Because G(z * ; θ * ) represents x * in the INR form, we can get the restored image at any resolution through G(z * ; θ * ). Therefore, Omni-INR-GAN is more friendly to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. Reconstruction</head><p>We compare pre-trained GAN models for image reconstruction tasks. Specifically, we finetune the parameters of the generator to make it reconstruct given images. Note that we do not use mse or L1 loss, because these loss functions make it easy for the generator to overfit the given image, as long as the training iterations are enough. Instead, we only use the discriminator feature loss, because it has been proven to be very effective for utilizing the prior of the generator. For the dataset, we use 1k images sampled from the ImageNet validation set, which is the same as DGP's choice. Note that these data have not been used in GAN's training. We adopt the progressive reconstruction strategy of DGP <ref type="bibr" target="#b50">[50]</ref>, and finetune each GAN model for the same number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Implementation Details</head><p>We adopt BigGAN architectures of 128 × 128 and 256 × 256 in our experiments. For 128 × 128 experiments, the generator and discriminator use non-local block at 64 × 64 resolution. The generator is updated once every time the discriminator is updated. The weight decay of the generator and discriminator are set to 0.001 and 0.00001, respectively. For Omni-INR-GAN, we removed the non-local block at 64 × 64 resolution of the discriminator. Because when Omni-INR-GAN is used for downstream tasks, the input image of the discriminator may be of any size, so the middle layer of the discriminator may not output 64 × 64 resolution features. Moreover, although Omni-INR-GAN can generate images of any resolution, we did not adopt a multi-scale training strategy. We found that multi-scale training led to training collapse. We think that a possible reason is that multi-scale training enhances the discriminator, resulting in the ability of the generator and the discriminator to be out of balance. Thus we only generate 128 × 128 images during training, and the real images are also resized to 128 × 128.</p><p>For 256 × 256 experiments, the weight decay of the generator and discriminator are set to 0.0001 and 0.00001, respectively. The generator is updated once every time the discriminator is updated twice. We have found experimentally that this will make training more stable. The generator and discriminator use non-local block at 64 × 64 resolution rather than 128 × 128 due to limited GPU memory. For Omni-INR-GAN, in order to support downstream tasks friendly, we do not use non-local block in the discriminator. Moreover, due to GPU memory limitation, we reduce the batch size to 128 and accumulate the gradient twice to approximate the gradient when the batch size is 256. We did not adopt a multi-scale training strategy. Only 256 × 256 images are generated during training, and the real images are also resized to 256 × 256.    (c) LIIF <ref type="bibr" target="#b9">[10]</ref> can extrapolate the input image to any scale, but it cannot add semantic details, so the result is still blurred. (d) DIP <ref type="bibr" target="#b72">[71]</ref> also failed because the input image resolution is too low. (e) DGP <ref type="bibr" target="#b50">[50]</ref> with BigGAN must crop the input and upsamples the cropped patch to a fixed size, which is inflexible. (f) Omni-INR-GAN has the ability to upsample the input image to any scale and also adds rich semantic details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>Ground Truth SPADE Ours <ref type="figure" target="#fig_3">Figure 43</ref>: Results of semantic image synthesis on Cityscapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>https://github.com/PeterouZh/Omni-GAN-PyTorch (BigGAN) Classification-based (Multi-hinge GAN) Classification-based (AC-GAN) Classification-based (AC-GAN w/ DiffAug) Classification-based (Omni-GAN) (a) The trend of Inception scores along the training procedure on CIFAR100, showing that Omni-GAN enjoys both high performance and a lower risk of mode collapse. (b) The tradeoff curves using the truncation trick to generate 128×128 images on ImageNet, where Omni-GAN and Omni-INR-GAN outperform BigGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Different discriminator models for training a cGAN. Omni-loss supports to implement a classification-based cGAN or a projection-based cGAN, enabling us to fairly and intuitively explore the secrets behind them. Please refer to the texts in Sec. 2.1 and Sec. 3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>BigGAN (w/o wd) Projection-based BigGAN (w/ wd) One-sided Omni-GAN (w/o wd) One-sided Omni-GAN (w/ wd) Omni-GAN (w/o wd) Omni-GAN (w/ wd)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>AC-GAN suffers a severe collapse in the initial stage of training. "wd" stands for weight decay, DiffAug for differentiable data augmentation. Weight decay effectively alleviates early collapse, but data augmentation does not. ImAC-GAN means an improved version of AC-GAN. Please refer to Sec. 3.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Colorization. (a) degraded input. (b) and (c) automatic colorization algorithms. (d) and (e) GAN inversion-based methods. BigGAN only colorizes a square image patch. Omni-INR-GAN directly colorizes the entire image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>by the author) BigGAN (reproduced by us) Omni-GAN Omni-INR-GAN (a) FID on ImageNet 128 × 128 by the author) BigGAN (reproduced by us) Omni-GAN Omni-INR-GAN 14 days (8*v100 GPUs) 1 day (8*v100 GPUs) (b) IS on ImageNet 128 × 128</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Convergence curves on ImgeNet. Both Omni-GAN and Omni-INR-GAN converge faster than the projection-based cGAN, BigGAN. In particular, Omni-GAN only took one day to reach the IS of BigGAN trained for 14 days. Omni-INR-GAN consistently outperforms BigGAN and Omni-GAN. Its IS is 2.5 times higher than that of BigGAN (namely 262.85 vs. 104.57).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Colorization example. Top: DGP with BigGAN. Bottom: DGP with Omni-INR-GAN. BigGAN only colorizes a square image patch. Omni-INR-GAN directly colorizes the entire image. Omni-INR-GAN gradually overlays colors on the corresponding objects, indicating that the finetuning process is mining GAN's prior information.Size (256 × 384) Size (32 × 48) ×1 (32 × 48) ×4.6 (147 × 220) ×63.5 (2032 × 3048) (a) Ground truth (b) Input (c) LIIF ×8 (256 × 384) ×8 (256 × 256) ×1 (32 × 48)×4.6 (147 × 220) ×63.5 (2032 × 3048) (d) DIP (e) DGP w/ BigGAN (f) DGP w/ Omni-INR-GAN (ours) Super-resolution using Omni-INR-GAN's prior, at any scale (×1-×60+). (b) input image with low resolution. (c) LIIF [10] can extrapolate the input image to any scale, but it cannot add semantic details, so the result is still blurred. (d) DIP [71] also failed because the input image resolution is too low. (e) DGP<ref type="bibr" target="#b50">[50]</ref> with BigGAN must crop the input and upsamples the cropped patch to a fixed size, which is inflexible. (f) Omni-INR-GAN has the ability to upsample the input image to any scale and also adds rich semantic details (clearer foreground and background compared to other methods). Please see the video demo in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where γ stands for a scale factor, and m for a margin between positive and negative scores. S pos = {s (p) 1 , · · · , s (p) K } and S neg = {s (n) 1 , · · · , s (n) L } denote positive score set and negative score set, respectively. Eq. (14) aims to maximize s (p) and to minimize s (n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Gradients of the omni-loss. (a) Gradients w.r.t. s (n) and s (p) are independent. (b) Gradients w.r.t. s (p) k , {k = 0, 1, . . . }, are automatically balanced. Please see the text in Sec. A.2 for details. This figure is inspired by [68].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 ,</head><label>1</label><figDesc>· · · , s (p) K } and S (2) neg = {0}. According to Eq. (16), we get L where minimizing Eq. (20) makes s (n) i less than 0 and s (p) j greater than 0. We finish the derivation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 .1</head><label>1</label><figDesc>We can observe that for A, its s (p) 0 is smaller than s(p) 1 (i.e., -2 vs. 0). As a result, the gradients w.r.t. s (p) 0 is larger than that w.r.t. s(p) 1 (i.e., 0.79 vs. 0.11), meaning that the omni-loss try to increase s (p) 0 with higher superiority. A similar analysis applies to C as well. For B, since s are equal, the gradients of them are also equal (0.33).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>(a) An example of an image represented in INR form. A fully connected network maps coordinates (x, y) to pixel values (r, g, b). (b) Using an INR network to enhance the generator so that the generator can output images with any resolution and any aspect ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>1 class, 1 mnist , − 1 domain, 1</head><label>1111</label><figDesc>fake = [−1, . . . , 1 G , . . . , −real , −1 reality ],<ref type="bibr" target="#b27">(28)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Real images sampled from the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Images generated by a generator which is guided by a multi-label discriminator. The raw logits of D(x) and the corresponding FID score of a projection-based cGAN are plotted in the same figure. The black dashed line indicates where the minimum FID is reached. (a) training without weight decay. (b) training with weight decay. The figures of D(x) are inspired by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 :</head><label>15</label><figDesc>One-sided Omni-GAN is on par with projectionbased BigGAN on CIFAR10, proving that one-sided Omni-GAN indeed belongs to projection-based cGANs. Both of them are inferior to the classification-based cGAN, Omni-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>FID and IS on CIFAR100. Weight decay can eliminate the early collapse problem of Multi-hinge GAN on CIFAR100. FID and IS on CIFAR10. Weight decay deteriorates Multi-hinge GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 18 :</head><label>18</label><figDesc>Applying weight decay to the generator. Applying weight decay to the discriminator helps alleviate the collapse issue, but the IS gradually decreases as the training progresses. Applying weight decay to the generator simultaneously solves this problem. Experiments are conducted on CIFAR100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 19 :</head><label>19</label><figDesc>IS on ImageNet 256 × 256 FID and IS on ImgeNet 256 × 256. Omni-GAN and Omni-INR-GAN converge faster than the projection-based BigGAN. Omni-INR-GAN clearly outperforms Omni-GAN, showing its significant potential for future applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 20 :</head><label>20</label><figDesc>Tradeoff curves using truncation trick on Ima-geNet 256×256. We show truncation values from σ = 0.05 to σ = 1 with step being 0.05. Omni-INR-GAN outperforms Omni-GAN and BigGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 22 :Figure 23 :</head><label>2223</label><figDesc>Randomly generated image by Omni-GAN for CIFAR10 Randomly generated image by Omni-GAN for CIFAR100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 41 :Figure 42 :</head><label>4142</label><figDesc>Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution. Size (256 × 336) Size (32 × 42) ×1 (32 × 42) ×4.6 (147 × 193) ×63.5 (2032 × 2667) (a) Ground truth (b) Input (c) LIIF ×8 (256 × 320) ×8 (256 × 256) ×1 (32 × 42) ×4.6 (147 × 193) ×63.5 (2032 × 2667) (d) DIP (e) DGP w/ BigGAN (f) DGP w/ Omni-INR-GAN (ours) Super-resolution using Omni-INR-GAN's prior, at any scale (×1-×60+). (b) input image with low resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID and IS on ImageNet dataset. Omni-GAN achieves consistent improvements in terms of FID and IS compared to BigGAN. Omni-INR-GAN improves the IS to 2.5 times compared with BigGAN on ImageNet 128 × 128, with almost the same number of parameters. † stands for quoting from the paper, ‡ for using the model provided by the author, and for reproducing BigGAN by us. FID and IS are computed using 50K generated images. The training and validation data are utilized as the reference distribution for the computing of FID, respectively.</figDesc><table><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and Fig. 6, Omni-GAN shows significant advantages over BigGAN, both in terms of convergence speed and final performance. For example, Omni-GAN only took one day to reach the IS of BigGAN trained for 14 days on ImageNet 128×128. Besides, its IS is almost twice that of BigGAN, namely 190.94 vs. 104.57. Omni-INR-GAN consistently outperforms BigGAN and Omni-GAN. As shown in Table 2, on ImageNet 128 × 128, the IS of Omni-INR-GAN is 2.5 times that of BigGAN. We also show the number of parameters of the generator in Table 2. The number of parameters of Omni-INR-GAN is on par with that of BigGAN and Omni-GAN, indicating that 3 https://github.com/ajbrock/BigGAN-PyTorch.</figDesc><table><row><cell></cell><cell cols="3">BigGAN Omni-GAN Omni-INR-GAN</cell></row><row><cell>PSNR↑</cell><cell>25.68</cell><cell>26.35</cell><cell>29.36</cell></row><row><cell>SSIM↑</cell><cell>85.17</cell><cell>89.36</cell><cell>92.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table of</head><label>of</label><figDesc>Contents A. Derivation from Unified Loss to Omni-loss. 12 A.1. Derivation of Omni-loss . . . . . . . 12 A.2. Gradient Analysis . . . . . . . . . . . 13 Reconstruction . . . . . . . . . . . . 19 Generated Images on CIFAR . . . . . 20 K.2. Generated Images on ImageNet . . . . 20 K.3. Results of Semantic Image Synthesis . 20</figDesc><table><row><cell>B. Gradient Penalty for Classification-based</cell><cell></cell></row><row><cell>cGANs</cell><cell>14</cell></row><row><cell>C. Improved AC-GAN (ImAC-GAN)</cell><cell>14</cell></row><row><cell>G. Additional Results on ImageNet</cell><cell>17</cell></row><row><cell cols="2">H. Application to Image-to-Image Translation 18</cell></row><row><cell>I . Application to Downstream Tasks</cell><cell>18</cell></row><row><cell cols="2">I.1 . Colorization and Super-resolution . . 18</cell></row><row><cell>I.2 . J. Implementation Details</cell><cell>19</cell></row><row><cell>K. Additional Results</cell><cell>20</cell></row><row><cell>K.1.</cell><cell></cell></row></table><note>D. Technical Details of Omni-INR-GAN 14 E. An Example of Multi-label Discriminator 15 F. Additional Results on CIFAR 15 F.1. Over-fitting of the Discriminator . . . 15 F.2. Comparison of One-sided Omni-GAN and Projection-based GAN on CI- FAR10 . . . . . . . . . . . . . . . . 16 F.3. Comparison with Multi-hinge GAN . 16 F.4. Applying Weight Decay to the Generator 17 F.5. How to Set the Weight Decay? . . . . 17</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>model. For example, if DGP adopts a 128 × 128 BigGAN model, DGP must first crop the original image into image patch of size 128 × 128 before restoration, restricting its practical application. However, because Omni-INR-GAN can output im-</figDesc><table><row><cell></cell><cell cols="4">road sidewalk building wall</cell><cell>fence</cell><cell cols="5">pole traffic light traffic sign vegetation terrain</cell></row><row><cell>SPADE [52]</cell><cell>97.44</cell><cell>79.89</cell><cell>87.86</cell><cell cols="3">50.57 47.21 35.90</cell><cell>38.97</cell><cell>44.67</cell><cell>88.15</cell><cell>66.14</cell></row><row><cell></cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell></cell><cell>91.61</cell><cell>62.27</cell><cell>38.67</cell><cell cols="3">88.68 64.96 70.17</cell><cell>41.42</cell><cell>28.58</cell><cell>58.86</cell><cell>62.21</cell></row><row><cell></cell><cell cols="4">road sidewalk building wall</cell><cell>fence</cell><cell cols="5">pole traffic light traffic sign vegetation terrain</cell></row><row><cell>+ Omni-GAN</cell><cell>97.57</cell><cell>81.62</cell><cell>88.58</cell><cell cols="3">53.39 50.47 35.88</cell><cell>41.08</cell><cell>46.75</cell><cell>89.31</cell><cell>67.00</cell></row><row><cell></cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell></cell><cell>92.14</cell><cell>63.97</cell><cell>41.99</cell><cell cols="3">89.91 71.06 74.21</cell><cell>56.16</cell><cell>33.99</cell><cell>61.23</cell><cell>65.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Semantic image synthesis using SPADE. Replacing the GAN used by SPADE with Omni-GAN can improve the quality of synthesized images.ages of any resolution, combining it with DGP can directly restore the original image.We use Omni-INR-GAN pre-trained on ImageNet 256 × 256 for colorization and super-resolution. Eq.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>, 6, 7, and 8 show the architectural details. Each experiment is conducted on eight v100 GPUs. Training Omni-GAN on ImageNet 128 × 128 and 256×256 took 25 days and 60 days, respectively. Training Omni-INR-GAN on ImageNet 128×128 and 256×256 took 27 days and 87 days, respectively. No collapse occurred during the entire training process. We have found experimentally that classification-based cGANs cannot set a large batch size like projection-based BigGAN. For all experiments of Omni-GAN and Omni-INR-GAN, the batch size is set to 256. We adopt the ADAM optimizer in all experiments, with betas being 0 and 0.999. The learning rates of the generator and discriminator are set to 0.0001 and 0.0004, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Omni-INR-GAN architecture on Imagenet 256 × 256. ch is set to be 96. items in the test set of Cityscapes dataset, without cherrypicking.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure 24: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 25: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 26: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 27: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 28: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 29: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 30: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 31: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 32: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 33: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 34: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 35: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 36: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 37: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 38: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 39: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.Figure 40: Samples generated by our Omni-INR-GAN 256 × 256 model. Omni-INR-GAN has the ability to generate images of any resolution.</figDesc><table><row><cell>32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32</cell><cell>64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64</cell><cell>128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128</cell><cell>256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256</cell></row><row><cell>512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512</cell><cell></cell><cell>1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024</cell><cell></cell></row><row><cell>32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32 32 × 32</cell><cell>64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64 64 × 64</cell><cell>128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128 128 × 128</cell><cell>256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256 256 × 256</cell></row><row><cell>512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512 512 × 512</cell><cell></cell><cell>1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024 1024 × 1024</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1. Generated Images on CIFAR</head><p>In <ref type="figure">Fig. 22</ref> and 23, we show generated images from Omni-GAN on CIFAR10, CIFAR100 respectively. Due to limited space, we only show images of some categories on CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2. Generated Images on ImageNet</head><p>Omni-INR-GAN inherently supports generating images of arbitrary resolution. We adopt the Omni-INR-GAN 256 × 256 model to generate some images with different resolutions, e.g., <ref type="figure">Fig. 24, 25</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.3. Results of Semantic Image Synthesis</head><p>In <ref type="figure">Fig. 43</ref>, we show several results of Omni-GAN as well as those of SPADE for semantic image synthesis. The label maps and the ground truth images are from the first ten  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<title level="m">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10414</idno>
		<idno>2020. 3</idno>
		<title level="m">Sign Agnostic Learning of Shapes from Raw Data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<title level="m">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Bemana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. X-Fields</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00450[cs],2020.3</idno>
		<title level="m">Implicit Neural View-, Light-and Time-Image Interpolation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10983[cs],2020.3</idno>
		<title level="m">Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial-Learned Loss for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ultra-Data-Efficient GAN Training: Drawing A Lottery Ticket First</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00397[cs],2021.2</idno>
	</analytic>
	<monogr>
		<title level="m">Then Training It Toughly</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Continuous Image Representation with Local Implicit Image Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09161</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning Implicit Fields for Generative Shape Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02822</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05751</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Learned Representation For Artistic Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AutoGAN: Neural Architecture Search for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Yariv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10099</idno>
		<idno>2020. 3</idno>
		<title level="m">Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit Geometric Regularization for Learning Shapes</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08981[cs],2020.3</idno>
		<title level="m">Local Implicit Grid Representations for 3D Scenes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: A key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ContraGAN: Contrastive learning for conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Msg-Gan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multi-Scale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06048</idno>
		<title level="m">Gradient GAN for Stable Image Synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
	</analytic>
	<monogr>
		<title level="m">Stability, and Variation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat], 2020. 1, 5, 6, 15</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04216</idno>
		<title level="m">cGANs with Multi-Hinge Loss</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Simple Weight Decay Can Improve Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Representations for Automatic Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<idno>1998. 15</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02271</idno>
		<title level="m">High-Fidelity Image Generation With Fewer Labels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Least Squares Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually Converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Numerics of GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03828</idno>
		<title level="m">Sebastian Nowozin, and Andreas Geiger. Occupancy Networks: Learning 3D Reconstruction in Function Space</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral Normalization for Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">cGANs with Projection Discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional Image Synthesis With Auxiliary Classifier GANs</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepsdf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05103</idno>
		<title level="m">Learning Continuous Signed Distance Functions for Shape Representation</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic Image Synthesis with Spatially-Adaptive Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04618[cs],2020.3</idno>
		<title level="m">Convolutional Occupancy Networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible Conditional GANs for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06264</idno>
		<title level="m">Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semi-parametric Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative Adversarial Text to Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Stabilizing Training of Generative Adversarial Networks through Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal Generative Adversarial Nets with Singular Value Clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A U-Net Based Discriminator for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">CircleGAN: Generative Adversarial Learning across Spherical Circles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohyeon</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Implicit Neural Representations with Periodic Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<title level="m">Savva Ignatyev, and Mohamed Elhoseiny</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<idno type="arXiv">arXiv:2011.12026</idno>
		<title level="m">Adversarial Generation of Continuous Images</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Compositional pattern producing networks: A novel abstraction of development. Genetic Programming and Evolvable Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Extending Cross-Entropy of Softmax to Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<ptr target="https://kexue.fm/archives/7359,2020" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Circle Loss: A Unified Perspective of Pair Similarity Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guinan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09134</idno>
		<title level="m">AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05464</idno>
		<title level="m">Adversarial Discriminative Domain Adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<title level="m">Deep Image Prior</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat]</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Video-to-Video Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Wasserstein divergence for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janine</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dilated Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Stack-GAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027</idno>
		<idno>2020. 7</idno>
		<title level="m">Consistency Regularization for Generative Adversarial Networks</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08511</idno>
		<title level="m">Colorful Image Colorization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Differentiable Augmentation for Data-Efficient GAN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738[cs],2020.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02088</idno>
	</analytic>
	<monogr>
		<title level="j">Feature Quantization Improves GAN Training</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Don&apos;t let your Discriminator be fooled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Searching towards Class-Aware Generators for Conditional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14208[cs],2020.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
