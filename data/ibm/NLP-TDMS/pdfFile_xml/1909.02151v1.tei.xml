<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><forename type="middle">Yuchen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KA GNE T, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for BERT-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning. We open-source our code 1 to the community for future research in knowledge-aware commonsense reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human beings are rational and a major component of rationality is the ability to reason. Reasoning is the process of combining facts and beliefs to make new decisions <ref type="bibr" target="#b8">(Johnson-Laird, 1980)</ref>, as well as the ability to manipulate knowledge to draw inferences <ref type="bibr" target="#b7">(Hudson and Manning, 2018)</ref>. Commonsense reasoning utilizes the basic knowledge that reflects our natural understanding of the world and human behaviors, which is common to all humans. Empowering machines with the ability to perform commonsense reasoning has been seen as the bottleneck of artificial general intelligence <ref type="bibr" target="#b4">(Davis and Marcus, 2015)</ref>. Recently, there have been a few emerging large-scale datasets for testing machine commonsense with various focuses <ref type="bibr" target="#b41">(Zellers et al., 2018;</ref><ref type="bibr" target="#b24">Sap et al., 2019b;</ref><ref type="bibr" target="#b40">Zellers et al., 2019)</ref>. In a typical dataset, CommonsenseQA <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>, given a question like "Where do adults use glue sticks?", with the answer choices being {classroom(), office (), desk drawer ()}, a commonsense reasoner is expected to differentiate the correct choice from other "distractive" candidates. False choices are usually highly related to the question context, but just less possible in realworld scenarios, making the task even more challenging. This paper aims to tackle the research question of how we can teach machines to make such commonsense inferences, particularly in the question-answering setting.</p><p>It has been shown that simply fine-tuning large, pre-trained language models such as GPT <ref type="bibr" target="#b19">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> arXiv:1909.02151v1 [cs.CL] 4 Sep 2019 can be a very strong baseline method. However, there still exists a large gap between performance of said baselines and human performance. Reasoning with neural models is also lacking in transparency and interpretability. There is no clear way as to how they manage to answer commonsense questions, thus making their inferences dubious.</p><p>Merely relying on pre-training large language models on corpora cannot provide well-defined or reusable structures for explainable commonsense reasoning. We argue that it would be more beneficial to propose reasoners that can exploit commonsense knowledge bases <ref type="bibr" target="#b26">(Speer et al., 2017;</ref><ref type="bibr" target="#b28">Tandon et al., 2017;</ref><ref type="bibr" target="#b23">Sap et al., 2019a)</ref>. Knowledgeaware models can explicitly incorporate external knowledge as relational inductive biases <ref type="bibr">(Battaglia et al., 2018)</ref> to enhance their reasoning capacity, as well as to increase the transparency of model behaviors for more interpretable results. Furthermore, a knowledge-centric approach is extensible through commonsense knowledge acquisition techniques <ref type="bibr" target="#b14">(Li et al., 2016;</ref><ref type="bibr" target="#b36">Xu et al., 2018)</ref>.</p><p>We propose a knowledge-aware reasoning framework for learning to answer commonsense questions, which has two major steps: schema graph grounding ( §3) and graph modeling for inference ( §4). As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, for each pair of question and answer candidate, we retrieve a graph from external knowledge graphs (e.g. ConceptNet) in order to capture the relevant knowledge for determining the plausibility of a given answer choice. The graphs are named "schema graphs" inspired by the schema theory proposed by Gestalt psychologists <ref type="bibr" target="#b1">(Axelrod, 1973)</ref>. The grounded schema graphs are usually much more complicated and noisier, unlike the ideal case shown in the figure.</p><p>Therefore, we propose a knowledge-aware graph network module to further effectively model schema graphs. Our model KA GNE T is a combination of graph convolutional networks <ref type="bibr" target="#b11">(Kipf and Welling, 2017)</ref> and LSTMs, with a hierarchical path-based attention mechanism, which forms a GCN-LSTM-HPA architecture for path-based relational graph representation. Experiments show that our framework achieved a new state-of-the-art performance 2 on the CommonsenseQA dataset. Our model also works better then other methods with limited supervision, and provides human- <ref type="bibr">2</ref> The highest score on the leaderboard as of the time when we submitted the paper (May 2019). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>In this section, we first formalize the commonsense question answering problem in a knowledge-aware setting, and then introduce the overall workflow of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem statement</head><p>Given a commonsense-required natural language question q and a set of N candidate answers {a i }, the task is to choose one answer from the set. From a knowledge-aware perspective, we additionally assume that the question q and choices {a i } can be grounded as a schema graph (denoted as g) extracted from a large external knowledge graph G, which is helpful for measuring the plausibility of answer candidates. The knowledge graph G = (V, E) can be defined as a fixed set of concepts V , and typed edges E describing semantic relations between concepts. Therefore, our goal is to effectively ground and model schema graphs to improve the reasoning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reasoning Workflow</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our framework accepts a pair of question and answer (QA-pair) denoted as q and a. It first recognizes the mentioned concepts within them respectively from the concept set V of the knowledge graph. We then algorithmically construct the schema graph g by finding paths between pairs of mentioned concepts ( §3). The grounded schema graph is further encoded with our proposed knowledge-aware graph network module ( §4). We first use a model-agnostic language encoder, which can either be trainable or a fixed feature extractor, to represent the QA-pair as a statement vector. The statement vector serves as an additional input to a GCN-LSTM-HPA architecture for path-based attentive graph modeling to obtain a graph vector. The graph vector is finally fed into a simple multi-layer perceptron to score this QA-pair into a scalar ranging from 0 to 1, representing the plausibility of the inference. The answer candidate with the maximum plausibility score to the same question becomes the final choice of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Schema Graph Grounding</head><p>The grounding stage is three-fold: recognizing concepts mentioned in text ( §3.1), constructing schema graphs by retrieving paths in the knowledge graph ( §3.2), and pruning noisy paths ( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Recognition</head><p>We match tokens in questions and answers to sets of mentioned concepts (C q and C a respectively) from the knowledge graph G (for this paper we chose to use ConceptNet due to its generality).</p><p>A naive approach to mentioned concept recognition is to exactly match n-grams in sentences with the surface tokens of concepts in V . For example, in the question "Sitting too close to watch tv can cause what sort of pain?", the exact matching result C q would be {sitting, close, watch tv, watch, tv, sort, pain, etc.}. We are aware of the fact that such retrieved mentioned concepts are not always perfect (e.g. "sort" is not a semantically related concept, "close" is a polysemous concept). How to efficiently retrieve contextually-related knowledge from noisy knowledge resources is still an open research question by itself <ref type="bibr" target="#b35">(Weissenborn et al., 2017;</ref><ref type="bibr" target="#b9">Khashabi et al., 2017)</ref>, and thus most prior works choose to stop here <ref type="bibr" target="#b43">(Zhong et al., 2018;</ref><ref type="bibr" target="#b32">Wang et al., 2019b)</ref>. We enhance this straightforward approach with some rules, such as soft matching with lemmatization and filtering of stop words, and further deal with noise by pruning paths ( §3.3) and reducing their importance with attention mechanisms ( §4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Schema Graph Construction</head><p>ConceptNet. Before diving into the construction of schema graphs, we would like to briefly introduce our target knowledge graph ConceptNet. ConceptNet can be seen as a large set of triples of the form (h, r, t), like (ice, HasProperty, cold), where h and t represent head and tail con-cepts in the concept set V and r is a certain relation type from the pre-defined set R. We delete and merge the original 42 relation types into 17 types, in order to increase the density of the knowledge graph 3 for grounding and modeling.</p><p>Sub-graph Matching via Path Finding. We define a schema graph as a sub-graph g of the whole knowledge graph G, which represents the related knowledge for reasoning a given questionanswer pair with minimal additional concepts and edges. One may want to find a minimal spanning sub-graph covering all the question and answer concepts, which is actually the NP-complete "Steiner tree problem" in graphs <ref type="bibr" target="#b6">(Garey and Johnson, 1977)</ref>. Due to the incompleteness and tremendous size of ConceptNet, we find that it is impractical to retrieve a comprehensive but helpful set of knowledge facts this way. Therefore, we propose a straightforward yet effective graph construction algorithm via path finding among mentioned concepts (C q ∪ C a ).</p><p>Specifically, for each question concept c i ∈ C q and answer concept c j ∈ C a , we can efficiently find paths between them that are shorter than k concepts 4 . Then, we add edges, if any, between the concept pairs within C q or C a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Path Pruning via KG Embedding</head><p>To prune irrelevant paths from potentially noisy schema graphs, we first utilize knowledge graph embedding (KGE) techniques, like TransE <ref type="bibr" target="#b34">(Wang et al., 2014)</ref>, to pre-train concept embeddings V and relation type embeddings R, which are also used as initialization for KA GNE T ( §4). In order to measure the quality of a path, we decompose it into a set of triples, the confidence of which can be directly measured by the scoring function of the KGE method (i.e. the confidence of triple classification). Thus, we score a path with the multiplication product of the scores of each triple in the path, and then we empirically set a threshold for pruning ( §5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding Unlabeled</head><p>Schema </p><formula xml:id="formula_0">v 6 B f X j U V n E q C W 2 R m M e y G 2 B F O R O 0 p Z n m t J t I i q O A 0 0 4 w v s / 9 z o R K x W L R 1 N O E e h E e C h Y y g r W R f N v u R 1 i P g j B r z v y M X T z N f L v q 1 J w 5 0 C p x C 1 K F A g 3 f / u o P Y p J G V G j C s V I 9 1 0 m 0 l 2 G p G e F 0 V u m n i i a Y j P G Q 9 g w V O K L K y + b J Z + j M K A M U x t I 8 o d F c / b 2 R 4 U i p a R S Y y T y n W v Z y 8 T + v l + r w 1 s u Y S F J N B V k c C l O O d I z y G t C A S U o 0 n x q C i W Q m K y I j L D H R p q y K K c F d / v I q a V / W 3 K v a 5 e N 1 t X 5 X 1 F G G E z i F c 3 D h B u r w A A 1 o A Y E J P M M r v F m Z 9 W K 9 W x + L 0 Z J V 7 B z D H 1 i f P 8 a M k 7 8 = &lt; / l a t e x i t &gt; P i,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w j g Z Q H V C e 1 9 R l Y 4 R d t 9 v O h d q X / Q = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B g 5 R E B T 0 W v X i s Y D + g D W W z n b R r N 5 u w u x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t 1 O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l V r 2 X 8 b P H S a 9 c c a v u D G S Z e D m p Q I 5 6 r / z V 7 c c s j V A a J q j W H c 9 N j J 9 R Z T g T O C l 1 U 4 0 J Z S M 6 w I 6 l k k a o / W x 2 7 o S c W K V P w l j Z k o b M 1 N 8 T G Y 2 0 H k e B 7 Y y o G e p F b y r + 5 3 V S E 1 7 7 G Z d J a l C y + a I w F c T E Z P o 7 6 X O F z I i x J Z Q p b m 8 l b E g V Z c Y m V L I h e I s v L 5 P m e d W 7 q J 7 f X 1 Z q N 3 k c R T i C Y z g F D 6 6 g B n d Q h w Y w G M E z v M K b k z g v z r v z M W 8 t O P n M I f y B 8 / k D G q S P a g = = &lt; / l a t e x i t &gt; P i,j [k]</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t e q K / C</p><formula xml:id="formula_1">N C d O X u G A w 7 k u D k n x G t Z Z M = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u i x 6 M V j B f u B 2 6 V k 0 2 w b m 0 2 W J C u U p f / C i w d F v P p v v P l v T N s 9 a O u D g c d 7 M 8 z M C x P O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W l q m i t A m k V y q T o g 1 5 U z Q p m G G 0 0 6 i K I 5 D T t v h 6 G b q t 5 + o 0 k y K e z N O a B D j g W A R I 9 h Y 6 a H R y 9 j Z 4 8 Q f B b 1 y x a 2 6 M 6 B l 4 u W k A j k a v f J X t y 9 J G l N h C M d a + 5 6 b m C D D y j D C 6 a T U T T V N M B n h A f U t F T i m O s h m F 0 / Q i V X 6 K J L K l j B o p v 6 e y H C s 9 T g O b W e M z V A v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n 7 q M 8 U J Y a P L c F E M X s r I k O s M D E 2 p J I N w V t 8 e Z m 0 a l X v v F q 7 u 6 j U r / M 4 i n A E x 3 A K H l x C H W 6 h A U 0 g I O A Z X u H N 0 c 6 L 8 + 5 8 z F s L T j 5 z C H / g f P 4 A S 7 e Q q w = = &lt; / l a t e x i t &gt; ↵ (i,j,k) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x t P E 3 J N D u X x 2 1 O q z T C U 6 w s 7 I 8 C 8 = " &gt; A A A B + X i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S J U K C W p g h 6 L X j x W s B / Q h j D Z b t q 1 m 0 3 Y 3 R R K 6 D / x 4 k E R r / 4 T b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p R 3 n 2 1 p b 3 9 j c 2 i 7 s F H f 3 9 g 8 O 7 a P j l o p T S W i T x D y W n Q A U 5 U z Q p m a a 0 0 4 i K U Q B p + 1 g d D f z 2 2 M q F Y v F o 5 4 k 1 I t g I F j I C G g j + b b d A 5 4 M w c / K r P J U G V 1 M f b v k V J 0 5 8 C p x c 1 J C O R q + / d X r x y S N q N C E g 1 J d 1 0 m 0 l 4 H U j H A 6 L f Z S R R M g I x j Q r q E C I q q 8 b H 7 5 F J 8 b p Y / D W J o S G s / V 3 x M Z R E p N o s B 0 R q C H a t m b i f 9 5 3 V S H N 1 7 G R J J q K s h i U Z h y r G M 8 i w H 3 m a R E 8 4 k h Q C Q z t 2 I y B A l E m 7 C K J g R 3 + e V V 0 q p V 3 c t q 7 e G q V L / N 4 y i g U 3 S G y s h F 1 6 i O 7 l E D N R F B Y / S M X t G b l V k v 1 r v 1 s W h d s / K Z E / Q H 1 u c P h 0 K S 7 w = = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-level Attention</head><p>ConceptPair-level Attention.  cept embeddings in their particular context within schema graphs. It then utilizes LSTMs to encode the paths between C q and C a , capturing multihop relational information ( §4.2). Finally, we apply a hierarchical path-based attention mechanism ( §4.3) to complete the GCN-LSTM-HPA architecture, which models relational schema graphs with respect to the paths between question and answer concepts.</p><formula xml:id="formula_2">) P i,j [k] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t e q K / C N C d O X u G A w 7 k u D k n x G t Z Z M = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u i x 6 M V j B f u B 2 6 V k 0 2 w b m 0 2 W J C u U p f / C i w d F v P p v v P l v T N s 9 a O u D g c d 7 M 8 z M C x P O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W l q m i t A m k V y q T o g 1 5 U z Q p m G G 0 0 6 i K I 5 D T t v h 6 G b q t 5 + o 0 k y K e z N O a B D j g W A R I 9 h Y 6 a H R y 9 j Z 4 8 Q f B b 1 y x a 2 6 M 6 B l 4 u W k A j k a v f J X t y 9 J G l N h C M d a + 5 6 b m C D D y j D C 6 a T U T T V N M B n h A f U t F T i m O s h m F 0 / Q i V X 6 K J L K l j B o p v 6 e y H C s 9 T g O b W e M z V A v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n 7 q M 8 U J Y a P L c F E M X s r I k O s M D E 2 p J I N w V t 8 e Z m 0 a l X v v F q 7 u 6 j U r / M 4 i n A E x 3 A K H l x C H W 6 h A U 0 g I O A Z X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Convolutional Networks</head><p>Graph convolutional networks (GCNs) encode graph-structured data by updating node vectors via pooling features of their adjacent nodes (Kipf and Welling, 2017). Our intuition for applying GCNs to schema graphs is to 1) contextually refine the concept vectors and 2) capture structural patterns of schema graphs for generalization. Although we have obtained concept vectors by pre-training ( §3.3), the representations of concepts still need to be further accommodated to their specific schema graphs context. Think of polysemous concepts such as "close" ( §3.1), which can either be a verb concept like in "close the door" or an adjective concept meaning "a short distance apart". Using GCNs to update the concept vector with their neighbors is thus helpful for disambiguation and contextualized concept embedding. Also, the pattern of schema graph structures provides potentially valuable information for reasoning. For instance, shorter and denser connections between question and answer concepts could mean higher plausibility under specific contexts.</p><p>As many works show <ref type="bibr" target="#b16">(Marcheggiani and Titov, 2017;</ref><ref type="bibr" target="#b42">Zhang et al., 2018)</ref>, relational GCNs (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information. We thus apply GCNs on the plain version (unlabeled, nondirectional) of schema graphs, ignoring relation types on the edges. Specifically, the vector for concept c i ∈ V g in the schema graph g is initialized by their pre-trained embeddings at first (h (0) i = V i ). Then, we update them at the (l + 1)th layer by pooling features of their neighboring nodes (N i ) and their own at the l-th layer with an non-linear activation function σ:</p><formula xml:id="formula_3">h (l+1) i = σ(W (l) self h (l) i + j∈N i 1 |N i | W (l) h (l) j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relational Path Encoding</head><p>In order to capture the relational information in schema graphs, we propose an LSTM-based path encoder on top of the outputs of GCNs. Recall that our graph representation has a special purpose: "to measure the plausibility of a candidate answer to a given question". Thus, we propose to represent graphs with respect to the paths between question concepts C q and answer concepts C a . We denote the k-th path between i-th question concept c (q) i ∈ C q and j-th answer concept c (a) j ∈ C a as P i,j <ref type="bibr">[k]</ref>, which is a sequence of triples:</p><formula xml:id="formula_4">P i,j [k] = [(c (q) i , r 0 , t 0 ), ..., (t n−1 , r n , c (a) j )]</formula><p>Note that the relations are represented with trainable relation vectors (initialized with pretrained relation embeddings), and concept vectors are the GCNs' outputs (h (l) ). Thus, each triple can be represented by the concatenation of the three corresponding vectors. We employ LSTM networks to encode these paths as sequences of triple vectors, taking the concatenation of the first and the last hidden states:</p><formula xml:id="formula_5">R i,j = 1 |P i,j | k LSTM(P i,j [k])</formula><p>The above R i,j can be viewed as the latent relation between the question concept c  j , for which we aggregate the representations of all the paths between them in the schema graph. Now we can finalize the vector representation of a schema graph g by aggregating all vectors in the matrix R using mean pooling:</p><formula xml:id="formula_6">T i,j = MLP([s ; c (i) q ; c (j) a ]) g = i,j [R i,j ; T i,j ] |C q | × |C a | , where [· ;</formula><p>·] means concatenation of two vectors. The statement vector s in the above equation is obtained from a certain language encoder, which can either be a trainable sequence encoder like LSTM or features extracted from pre-trained universal language encoders like GPT/BERT). To encode a question-answer pair with universal language encoders, we simply create a sentence combining the question and the answer with a special token ("question+ [sep] + answer"), and then use the vector of '[cls]' as suggested by prior works <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>..</p><p>We concatenate R i,j with an additional vector T i,j before doing average pooling. The T i,j is inspired from the Relation Network <ref type="bibr">(Santoro et al., 2017)</ref>, which also encodes the latent relational information yet from the context in the statement s instead of the schema graph g. Simply put, we want to combine the relational representations of a pair of question/answer concepts from both the schema graph side (symbolic space) and the language side (semantic space).</p><p>Finally, the plausibility score of the answer candidate a to the question q can be computed as score(q, a) = sigmoid(MLP(g)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hierarchical Attention Mechanism</head><p>A natural argument against the above GCN-LSTM-mean architecture is that mean pooling over the path vectors does not always make sense, since some paths are more important than others for reasoning. Also, it is usually not the case that all pairs of question and answer concepts equally contribute to the reasoning. Therefore, we propose a hierarchical path-based attention mechanism to selectively aggregate important path vectors and then more important question-answer concept pairs. This core idea is similar to the work of <ref type="bibr" target="#b39">Yang et al. (2016)</ref>, which proposes a document encoder that has two levels of attention mechanisms applied at the word-and sentence-level. In our case, we have path-level and concept-pair-level attention for learning to contextually model graph representations. We learn a parameter matrix W 1 for path-level attention scores, and the importance of the path P i,j [k] is denoted asα <ref type="bibr">(i,j,·)</ref> .</p><formula xml:id="formula_7">α (i,j,k) = T i,j W 1 LSTM(P i,j [k]), α (i,j,·) = SoftMax(α (i,j,·) ), R i,j = kα (i,j,k) · LSTM(P i,j [k]).</formula><p>Afterwards, we similarly obtain the attention over concept-pairs.</p><formula xml:id="formula_8">β (i,j) = s W 2 T i,ĵ β (·,·) = SoftMax(β (·,·) ) g = i,jβ (i,j) [R i,j ; T i,j ]</formula><p>The whole GCN-LSTM-HPA architecture is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. To sum up, we claim that the KA GNE T is a graph neural network module with the GCN-LSTM-HPA architecture that models relational graphs for relational reasoning under the context of both knowledge symbolic space and language semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We introduce our setups of the CommonsenseQA dataset <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>, present the baseline methods, and finally analyze experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Experiment Setup</head><p>The CommonsenseQA dataset consists of 12,102 (v1.11) natural language questions in total that require human commonsense reasoning ability to answer, where each question has five candidate answers (hard mode). The authors also release an easy version of the dataset by picking two random terms/phrases for sanity check.</p><p>CommonsenseQA is directly gathered from real human annotators and covers a broad range of  types of commonsense, including spatial, social, causal, physical, temporal, etc. To the best of our knowledge, CommonsenseQA may be the most suitable choice for us to evaluate supervised learning models for question answering.</p><p>For the comparisons with the reported results in the CommonsenseQA's paper and leaderboard, we use the official split (9,741/1,221/1,140) named (OFtrain/OFdev/OFtest). Note that the performance on OFtest can only be tested by submitting predictions to the organizers. To efficiently test other baseline methods and ablation studies, we choose to use randomly selected 1,241 examples from the training data as our in-house data, forming an (8,500/1,221/1,241) split denoted as (IHtrain/IHdev/IHtest). All experiments are using the random-split setting as the authors suggested, and three or more random states are tested on development sets to pick the best-performing one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Methods</head><p>We consider two different kinds of baseline methods as follows:</p><p>• Knowledge-agnostic Methods. These methods either use no external resources or only use unstructured textual corpora as additional information, including gathering textual snippets from search engine or large pre-trained language models like BERT-LARGE. QABILINEAR, QACOM-PARE, ESIM are three supervised learning models for natural language inference that can be equipped with different word embeddings including GloVe and ELMO. BIDAF++ utilizes Google web snippets as context and is further augmented with a self-attention layer while using ELMO as input features. GPT/BERT-LARGE are fine-tuning methods with an additional linear layer for classification as the authors suggested. They both add a special token '[sep]' to the input and use the hidden state of the '[cls]' as the input to the linear layer. More details about them can be found in the dataset paper <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>.</p><p>• Knowledge-aware Methods. We also adopt some recently proposed methods of incorporating knowledge graphs for question answering. KV-MEM <ref type="bibr" target="#b17">(Mihaylov and Frank, 2018</ref>) is a method that incorporates retrieved triples from ConceptNet at the word-level, which uses a key-valued memory module to improve the representation of each token individually by learning an attentive aggregation of related triple vectors. CBPT <ref type="bibr" target="#b43">(Zhong et al., 2018</ref>) is a plug-in method of assembling the predictions of any models with a straightforward method of utilizing pre-trained concept embeddings from ConceptNet. TEXTGRAPH-CAT <ref type="bibr" target="#b33">(Wang et al., 2019c)</ref> concatenates the graphbased and text-based representations of the statement and then feed it into a classifier. We create sentence template for generating sentences and then feed retrieved triples as additional text inputs as a baseline method TRIPLESTRING. <ref type="bibr" target="#b20">Rajani et al. (2019)</ref> propose to collect human explanations for commonsense reasoning from annotators as additional knowledge (COS-E), and then train a language model based on such human annotations for improving the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details of KagNet</head><p>Our best (tested on OFdev) settings of KA GNE T have two GCN layers (100 dim, 50dim respectively), and one bidirectional LSTMs (128dim) . We pre-train KGE using TransE (100 dimension) initialized with GloVe embeddings. The statement encoder in use is BERT-LARGE, which works as a pre-trained sentence encoder to obtain fixed features for each pair of question and answer candidate. The paths are pruned with path-score threshold set to 0.15, keeping 67.21% of the original Human Performance -88.9 <ref type="table">Table 2</ref>: Comparison with official benchmark baseline methods using the official split on the leaderboard.</p><p>paths. We did not conduct pruning on concept pairs with less than three paths. For very few pairs with none path,R (i,j) will be a randomly sampled vector. We learn our KA GNE T models with Adam optimizers <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref>. In our experiments, we found that the recall of ConceptNet on commonsense questions and answers is very high (over 98% of QA-pairs have more than one grounded concepts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Comparisons and Analysis</head><p>Comparison with standard baselines. As shown in <ref type="table">Table 2</ref>, we first use the official split to compare our model with the baseline methods reported on the paper and leaderboard. BERT and GPT-based pre-training methods are much higher than other baseline methods, demonstrating the ability of language models to store commonsense knowledge in an implicit way. This presumption is also investigated by Trinh and Le <ref type="formula">(2019)</ref> and <ref type="bibr">Wang et al. (2019)</ref>. Our proposed framework achieves an absolute increment of 2.2% in accuracy on the test data, a state-of-the-art performance.</p><p>We conduct the experiments with our in-house splits to investigate whether our KA GNE T can also work well on other universal language encoders (GPT and BERT-BASE), particularly with different fractions of the dataset (say 10%, 50%, 100% of the training data). <ref type="table">Table 1</ref> shows that our KA GNE T-based methods using fixed pre-trained language encoders outperform fine-tuning themselves in all settings. Furthermore, we find that the improvements in a small data situation (10%) is relatively limited, and we believe an important future research direction is thus few-shot learning  <ref type="table">Table 3</ref>: Comparisons with knowledge-aware baseline methods using the in-house split (both easy and hard mode) on top of BLSTM as the sentence encoder.</p><p>for commonsense reasoning.</p><p>Comparison with knowledge-aware baselines.</p><p>To compare our model with other adopted baseline methods that also incorporate ConceptNet, we set up a bidirectional LSTM networks-based model for our in-house dataset. Then, we add baseline methods and KA GNE T onto the BLSTMs to compare their abilities to utilize external knowledge 5 . <ref type="table">Table 3</ref> shows the comparisons under both easy mode and hard mode, and our methods outperform all knowledge-aware baseline methods by a large margin in terms of accuracy. Note that we compare our model and the CoS-E in Table 2. Although CoS-E also achieves better result than only fine-tuning BERT by training with human-generated explanations, we argue that our proposed KagNet does not utilize any additional human efforts to provide more supervision. Ablation study on model components.</p><p>To better understand the effectiveness of each component of our method, we have done ablation study as shown in <ref type="table" target="#tab_4">Table 4</ref>. We find that replacing our GCN-LSTM-HPA architecture with traditional relational GCNs, which uses separate weight matrices for different relation types, results in worse performance, due to its overparameterization. The attention mechanisms matters almost equally in two levels, and pruning also effectively filters noisy paths. Error analysis.</p><p>In the failed cases, there are three kinds of hard problems that KA GNE T is still not good at.</p><p>• negative reasoning: the grounding stage is not sensitive to the negation words, and thus can choose exactly opposite answers. • comparative reasoning strategy: For the  The dataset gives the answer as "exhilarating" instead of "exhausting", which we think is more like a personalized subjective inference instead of common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study on Interpretibility</head><p>Our framework enjoys the merit of being more transparent, and thus provides more interpretable inference process. We can understand our model behaviors by analyzing the hierarchical attention scores on the question-answer concept pairs and path between them. <ref type="figure" target="#fig_8">Figure 4</ref> shows an example how we can analyze our KA GNE T framework through both pairlevel and path-level attention scores. We first select the concept-pairs with highest attention scores and then look at the (one or two) top-ranked paths for each selected pair. We find that paths located in this way are highly related to the inference process and also shows that noisy concepts like 'fountain' will be diminished while modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Model Transferability.</head><p>We study the transferability of a model that is trained on CommonsenseQA (CSQA) by directly testing it with another task while fixing its parameters. Recall that we have obtained a BERT-LARGE model and a KA GNE T model trained on CSQA. Now we denoted them as CSQA-BL and CSQA-KN to suggest that they are not trainable anymore.</p><p>In order to investigate their transferability, we separately test them on SWAG <ref type="bibr" target="#b41">(Zellers et al., 2018)</ref> WhatJdoJyouJfill withJink toJwrite on an A4 paper?J and WSC <ref type="bibr" target="#b13">(Levesque, 2011)</ref> datasets. We first test them the 20k validation examples in SWAG.</p><p>CSQA-BL has an accuracy of 56.53%, while our fixed CSQA-KN model achieves 59.01%. Similarly, we also test both models on the WSC-QA, which is converted from the WSC pronoun resolution to a multi-choice QA task. The CSQA-BL achieves an accuracy of 51.23%, while our model CSQA-KN scores 53.51%. These two comparisons further support our assumption that KA GNE T, as a knowledge-centric model, is more extensible in commonsense reasoning. As we expect for a good knowledge-aware frameworks to behave, our KA GNE T indeed enjoys better transferablity than only fine-tuning large language encoders like BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Recent methods on the leaderboard.</head><p>We argue that the KA GNE T utilizes the ConceptNet as the only external resource and other methods are improving their performance in orthogonal directions: 1) we find that most of the other recent submissions (as of Aug. 2019) with public information on the leaderboard utilize larger additional textual corpora (e.g. top 10 matched sentences in full Wikipedia via information retrieval tools), and fine-tuning on larger pre-trained encoders, such as XLNet <ref type="bibr" target="#b38">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019)</ref>. 2) there are also models using multi-task learning to transfer knowledge from other reading comprehension datasets, such as RACE <ref type="bibr" target="#b12">(Lai et al., 2017)</ref> and</p><p>OpenBookQA .</p><p>An interesting fact is that the best performance on the OFtest set is still achieved the original fine-tuned RoBERTa model, which is pre-trained with copora much larger than BERT. All other RoBERTa-extended methods have negative improvements. We also use statement vectors from RoBERTa as the input vectors for KA GNE T, and find that the performance on OFdev marginally improves from 77.47% to 77.56%. Based on our above-mentioned failed cases in error analysis, we believe fine-tuning RoBERTa has achieved the limit due to the annotator biases of the dataset and the lack of comparative reasoning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Commonsense knowledge and reasoning. There is a recent surge of novel large-scale datasets for testing machine commonsense with various focuses, such as situation prediction (SWAG) <ref type="bibr" target="#b41">(Zellers et al., 2018)</ref>, social behavior understanding <ref type="bibr">(Sap et al., 2019a,b)</ref>, visual scene comprehension <ref type="bibr" target="#b40">(Zellers et al., 2019)</ref>, and general commonsense reasoning <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>, which encourages the study of supervised learning methods for commonsense reasoning. Trinh and Le (2018) find that large language models show promising results in WSC resolution task <ref type="bibr" target="#b13">(Levesque, 2011)</ref>, but this approach can hardly be applied in a more general question answering setting and also not provide explicit knowledge used in inference. A unique merit of our KA GNE T method is that it provides grounded explicit knowledge triples and paths with scores, such that users can better understand and put trust in the behaviors and inferences of the model.</p><p>Injecting external knowledge for NLU. Our work also lies in the general context of using external knowledge to encode sentences or answer questions. <ref type="bibr" target="#b37">Yang and Mitchell (2017)</ref> are the among first ones to propose to encode sentences by keeping retrieving related entities from knowledge bases and then merging their embeddings into LSTM networks computations, to achieve a better performance on entity/event extraction tasks. <ref type="bibr" target="#b35">Weissenborn et al. (2017)</ref>, <ref type="bibr" target="#b17">Mihaylov and Frank (2018)</ref>, and <ref type="bibr" target="#b0">Annervaz et al. (2018)</ref> follow this line of works to incorporate the embeddings of related knowledge triples at the word-level and improve the performance of natural language understanding tasks. In contrast to our work, they do not explicitly impose graph-structured knowledge into models , but limit its potential within transforming word embeddings to concept embeddings. Some other recent attempts <ref type="bibr" target="#b43">(Zhong et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019c)</ref> to use ConceptNet graph embeddings are adopted and compared in our experiments ( §5). <ref type="bibr" target="#b20">Rajani et al. (2019)</ref> propose to manually collect more human explanations for correct answers as additional supervision for auxiliary training. KA GNE T-based framework focuses on injecting external knowledge as an explicit graph structure, and enjoys the relational reasoning capacity over the graphs. Relational reasoning. KA GNE T can be seen as a knowledge-augmented Relation Network module (RN) <ref type="bibr">(Santoro et al., 2017)</ref>, which is proposed for the visual question answering task requiring relational reasoning (i.e. questions about the relations between multiple 3D-objects in an image). We view the concepts in the questions and answers as objects and effectively utilize external knowledge graphs to model their relations from both semantic and symbolic spaces ( §4.2), while prior methods mainly work on the semantic one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a knowledge-aware framework for learning to answer commonsense questions. The framework first constructs schema graphs to represent relevant commonsense knowledge, and then model the graphs with our KA GNE T module. The module is based on a GCN-LSTM-HPA architecture, which effectively represent graphs for relational reasoning purpose in a transparent, interpretable way, yielding a new state-of-the-art results on a large-scale general dataset for testing machine commonsense. Future directions include better question parsing methods to deal with negation and comparative question answering, as well as incorporating knowledge to visual reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of using external commonsense knowledge ( symbolic space) for inference in natural language commonsense questions (semantic space ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall workflow of the proposed framework with knowledge-aware graph network module. readable results via intermediate attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Graphs Statement Vector s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l I L m n d i f Y l X Y Y d 3 T B D e i C V 0 k J u w = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z d F N y 4 r 2 A e 2 Q 8 m k d 9 r Q T G Z I M k I Z + h d u X C j i 1 r 9 x 5 9 + Y t r P Q 1 g O B w z n 3 k n N P k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z 0 2 I u o G Q V h p q f 9 c s W t u n O Q V e L l p A I 5 G v 3 y V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 1 E s 1 J p S N 6 R C 7 l k o a o f a z e e I p O b P K g I S x s k 8 a M l d / b 2 Q 0 0 n o S B X Z y l l A v e z P x P 6 + b m v D a z 7 h M U o O S L T 4 K U 0 F M T G b n k w F X y I y Y W E K Z 4 j Y r Y S O q K D O 2 p J I t w V s + e Z W 0 a l X v o l q 7 v 6 z U b / I 6 i n A C p 3 A O H l x B H e 6 g A U 1 g I O E Z X u H N 0 c 6 L 8 + 5 8 L E Y L T r 5 z D H / g f P 4 A 9 u 6 R G w = = &lt; / l a t e x i t &gt; C q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 O M P A + 8 6 t R P 1 y S u 0Q 8 D e E i Y V z T U = " &gt; A A A B 9 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Z k q 6 L L Y j c s K 9 g H t W D J p p g 3 N J G O S U c r Q / 3 D j Q h G 3 / o s 7 / 8 Z M O w t t P R A 4 n H M v 9 + Q E M W f a u O 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 5 b W i a K 0 C a R X K p O g D X l T N C m Y Y b T T q w o j g J O 2 8 G 4 n v n t R 6 o 0 k + L O T G L q R 3 g o W M g I N l a 6 7 0 X Y j A j m aX 3 a f y j 2 S 2 W 3 4 s 6 A l o m X k z L k a P R L X 7 2 B J E l E h S E c a 9 3 1 3 N j 4 K V a G E U 6 n x V 6 i a Y z J G A 9 p 1 1 K B I 6 r 9 d J Z 6 i k 6 t M k C h V P Y J g 2 b q 7 4 0 U R 1 p P o s B O Z i n 1 o p e J / 3 n d x I R X f s p E n B g q y P x Q m H B k J M o q Q A O m K D F 8 Y g k m i t m s i I y w w s T Y o r I S v M U v L 5 N W t e K d V 6 q 3 F + X a d V 5 H A Y 7 h B M 7 A g 0 u o w Q 0 0 o A k E F D z D K 7 w 5 T 8 6 L 8 + 5 8 z E d X n H z n C P 7 A + f w B P J y S V Q = = &lt; / l a t e x i t &gt; C a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d Q Q H t 0 / 0 P B O F o 0 b E C + a w h y k I r o Q = " &gt; A A A B 9 H i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V Z I q 6 L L Y j c s K 9 g F t K D f T S T t 0 M o k z k 0 I J / Q 4 3 L h R x 6 8 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s 8 c P + Z M a c f 5 t g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r a J E E t o i E Y 9 k 1 0 d F O R O 0 p Z n m t B t L i q H P a c e f N D K / M 6 V S s U g 8 6 l l M v R B H g g W M o D a S 1 w 9 R j w n y t D E f 4 K B c c a r O A v Y 6 c X N S g R z N Q f m r P 4 x I E l K h C U e l e q 4 T a y 9 F q R n h d F 7 q J 4 r G S C Y 4 o j 1 D B Y Z U e e k i 9 N y + M M r Q D i J p n t D 2 Q v 2 9 k W K o 1 C z 0 z W Q W U q 1 6 m f i f 1 0 t 0 c O u l T M S J p o I s D w U J t 3 V k Z w 3 Y Q y Y p 0 X x m C B L J T F a b j F E i 0 a a n k i n B X f 3 y O m n X q u 5 V t f Z w X a n f 5 X U U 4 Q z O 4 R J c u I E 6 3 E M T W k D g C Z 7 h F d 6 s q f V i v V s f y 9 G C l e + c w h 9 Y n z / q 6 J I x &lt; / l a t e x i t &gt; R i,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I o E u 3 A 5 j 4 r u Z 0 1 p K r Q Y y O C / 3 r y 0 = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g Q k q i g i 6 L b l x W s Q 9 o Q 5 h M J + 3 Y y S T M T A o l 9 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z g o Q z p R 3 n 2 y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 9 v 5 B S 8 W p J L R J Y h 7 L T o A V 5 U z Q p m a a 0 0 4 i K Y 4 C T t v B 6 D b 3 2 2 M q F Y v F o 5 4 k 1 I v w Q L C Q E a y N 5 N t 2 L 8 J 6 G I T Z w 9 T P 2 N n T 1 L e r T s 2 Z A S 0 T t y B V K N D w 7 a 9 e P y Z p R I U m H C v V d Z 1 E e x m W m h F O p 5 V e q m i C y Q g P a N d Q g S O q v G y W f I p O j N J H Y S z N E x r N 1 N 8 b G Y 6 U m k S B m c x z q k U v F / / z u q k O r 7 2 M i S T V V J D 5 o T D l S M c o r w H 1 m a R E 8 4 k h m E h m s i I y x B I T b c q q m B L c x S 8 v k 9 Z 5 z b 2 o n d 9 f V u s 3 R R 1 l O I J j O A U X r q A O d 9 C A J h A Y w z O 8 w p u V W S / W u / U x H y 1 Z x c 4 h / I H 1 + Q P D d p O 9 &lt; / l a t e x i t &gt; LSTM Path Encoder T i,j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 8 O 8 d s 2 U 1 Y g P D U b k t j j n w F X W k B o = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g Q k q i g i 6 L b l x W 6 A v a E C b T S T t 2 M g k z k 0 I J / R M 3 L h R x 6 5 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I O F M a c f 5 t k p r 6 x u b W + X t y s 7 u 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 S a l F p A G / x Z a M w a i Q h Z w s L y y D W 4 = " &gt; A A A B 9 H i c b V D L S g N B E J y N r x h f U Y 9 e B o M Q Q c J u F P Q Y 9 O I x g n l A s o T Z S W 8 y Z v b h T G 8 g L P k O L x 4 U 8 e r H e P N v n C R 7 0 G h B Q 1 H V T X e X F 0 u h 0 b a / r N z K 6 t r 6 R n 6 z s L W 9 s 7 t X 3 D 9 o 6 i h R H B o 8 k p F q e 0 y D F C E 0 U K C E d q y A B Z 6 E l j e 6 m f m t M S g t o v A e J z G 4 A R u E w h e c o Z H c r g f I e m l Z n D 2 c T n v F k l 2 x 5 6 B / i Z O R E s l Q 7 x U / u / 2 I J w G E y C X T u u P Y M b o p U y i 4 h G m h m 2 i I G R + x A X Q M D V k A 2 k 3 n R 0 / p i V H 6 1 I + U q R D p X P 0 5 k b J A 6 0 n g m c 6 A 4 V A v e z P x P 6 + T o H / l p i K M E 4 S Q L x b 5 i a Q Y 0 V k C t C 8 U c J Q T Q x h X w t x K + Z A p x t H k V D A h O M s v / y X N a s U 5 r 1 T v L k q 1 6 y y O P D k i x 6 R M H H J J a u S W 1 E m D c P J I n s g L e b X G 1 r P 1 Z r 0 v W n N W N n N I f s H 6 + A Y L W p G f &lt; / l a t e x i t &gt; LSTM(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>u H N 0 c 6 L 8 + 5 8 z F s L T j 5 z C H / g f P 4 A S 7 e Q q w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " C F e n P O x U j 3 C M v 4 U A O f u u X h c u T T U = " &gt; A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q K u i y 4 c V n B P r A N Z T K 9 a Y d O J m F m I p T Q v 3 D j Q h G 3 / o 0 7 / 8 Z J m 4 W 2 H h g 4 n H M v c + 4 J E s G 1 c d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 d Z w q h i 0 W i 1 h 1 A 6 p R c I k t w 4 3 A b q K Q R o H A T j C 5 z f 3 O E y r N Y / l g p g n 6 E R 1 J H n J G j Z U e + x E 1 4 y D M R r N B p e r W 3 D n I K v E K U o U C z U H l q z + M W R q h N E x Q r X u e m x g / o 8 p w J n B W 7 q c a E 8 o m d I Q 9 S y W N U P v Z P P G M n F t l S M J Y 2 S c N m a u / N z I a a T 2 N A j u Z J 9 T L X i 7 + 5 / V S E 9 7 4 G Z d J a l C y x U d h K o i J S X 4 + G X K F z I i p J Z Q p b r M S N q a K M m N L K t s S v O W T V 0 m 7 X v M u a / X 7 q 2 r D L e o o w S m c w Q V 4 c A 0 N u I M m t I C B h G d 4 h T d H O y / O u / O x G F 1 z i p 0 T + A P n 8 w f f S J D 9 &lt; / l a t e x i t &gt; Modeling Relational Paths between R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r L s u W M D M F W j z + i a U w 0 9 l j 4 O V G 4 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 + t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j m 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l 5 l 2 / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x N e + x m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S d q 3 q X V R r z c t K 3 c 3 j K M I J n M I 5 e H A F d b i F B r S A A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P q I W M y A = = &lt; / l a t e x i t &gt; T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K c C k Q 8 D r 2 D P V F N e c f O X j V 2 4 o J 5 Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 r G F f k E b y m Y 7 a d d u N m F 3 I 5 T Q X + D F g y J e / U n e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u Y F i e D a u O 6 3 s 7 G 5 t b 2 z W 9 g r 7 h 8 c H h 2 X T k 7 b O k 4 V w x a L R a y 6 A d U o u M S W 4 U Z g N 1 F I o 0 B g J 5 j c z / 3 O E y r N Y 9 k 0 0 w T 9 i I 4 k D z m j x k q N 5 q B U d i v u A m S d e D k p Q 4 7 6 o P T V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 2 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 I x c W m V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 9 7 5 G Z d J a l C y 5 a I w F c T E Z P 4 1 G X K F z I i p J Z Q p b m 8 l b E w V Z c Z m U 7 Q h e K s v r 5 N 2 t e J d V 6 q N m 3 L N z e M o w D l c w B V 4 c A s 1 e I A 6 t I A B w j O 8 w p v z 6 L w 4 7 8 7 H s n X D y W f O 4 A + c z x + r j Y z K &lt; / l a t e x i t &gt; W 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L J R h c N B T U KL I m / q d f 0 c G H F U w / U U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R V 0 G X B j c s K 9 g F N K Z P p T T t 0 M g k z E 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W Nz a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 L v e 7 T 6 g 0 j + W j m S U 4 i O h Y 8 p A z a q z k + x E 1 k y D M u v O h N 6 z W 3 L q 7 A F k n X k F q U K A 1 r H 7 5 o 5 i l E U r D B N W 6 7 7 m J G W R U G c 4 E z i t + q j G h b E r H 2 L d U 0 g j 1 I F t k n p M L q 4 x I G C v 7 p C E L 9 f d G R i O t Z 1 F g J / O M e t X L x f + 8 f m r C 2 0 H G Z Z I a l G x 5 K E w F M T H J C y A j r p A Z M b O E M s V t V s I m V F F m b E 0 V W 4 K 3 + u V 1 0 m n U v a t 6 4 + G 6 1 n S L O s p w B u d w C R 7 c Q B P u o Q V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c L T n F z i n 8 g f P 5 A / P O k Z E = &lt; / l a t e x i t &gt; W 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l z k h 1 6 e y o / L R n d r m 7 y y J 5 f 1 A Z B U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R V 0 G X B j c s K 9 g F N K Z P p T T t 0 M g k z E 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k A i u j e t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S 0 X G q G L Z Z L G L V C 6 h G w S W 2 D T c C e 4 l C G g U C u 8 H 0 L v e 7 T 6 g 0 j + W j m S U 4 i O h Y 8 p A z a q z k + x E 1 k y D M u v N h Y 1 i t u X V 3 A b J O v I L U o E B r W P 3 y R z F L I 5 S G C a p 1 3 3 M T M 8 i o M p w J n F f 8 V G N C 2 Z S O s W + p p B H q Q b b I P C c X V h m R M F b 2 S U M W 6 u + N j E Z a z 6 L A T u Y Z 9 a q X i / 9 5 / d S E t 4 O M y y Q 1 K N n y U J g K Y m K S F 0 B G X C E z Y m Y J Z Y r b r I R N q K L M 2 J o q t g R v 9 c v r p N O o e 1 f 1 x s N 1 r e k W d Z T h D M 7 h E j y 4 g S b c Q w v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B / V S k Z I = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " 6 j x 4 y / 5 x i l U N L m S m 8 V O 3 Z u D e H t U = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R a h X s p u K + i x 4 M V j B f s h 7 V q y a b a N T b J L k h X K 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H L R 0 l i t A m i X i k O g H W l D N J m 4 Y Z T j u x o l g E n L a D 8 f X M b z 9 R p V k k 7 8 w k p r 7 A Q 8 l C R r C x 0 j 3 p P z 6 k Z X w + 7 R d L b s W d A 6 0 S L y M l y N D o F 7 9 6 g 4 g k g k p D O N a 6 6 7 m x 8 V O s D C O c T g u 9 R N M Y k z E e 0 q 6 l E g u q / X R + 8 B S d W W W A w k j Z k g b N 1 d 8 T K R Z a T 0 R g O w U 2 I 7 3 s z c T / v G 5 i w i s / Z T J O D J V k s S h M O D I R m n 2 P B k x R Y v j E E k w U s 7 c i M s I K E 2 M z K t g Q v O W X V 0 m r W v F q l e r t R a n u Z n H k 4 Q R O o Q w e X E I d b q A B T S A g 4 B l e 4 c 1 R z o v z 7 n w s W n N O N n M M f + B 8 / g A 8 E Y / 6 &lt; / l a t e x i t &gt; c (q) i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c F p 0 l m / J q J A A b x C N Y i E L R w y F I Y 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R a h X s p u K + i x 4 M V j B f s h 7 V q y a b Y N T b J r k h X K 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H L R 0 l i t A m i X i k O g H W l D N J m 4 Y Z T j u x o l g E n L a D 8 f X M b z 9 R p V k k 7 8 w k p r 7 A Q 8 l C R r C x 0 j 3 p s 4 e 0 / H g + 7 R d L b s W d A 6 0 S L y M l y N D o F 7 9 6 g 4 g k g k p D O N a 6 6 7 m x 8 V O s D C O c T g u 9 R N M Y k z E e 0 q 6 l E g u q / X R + 8 B S d W W W A w k j Z k g b N 1 d 8 T K R Z a T 0 R g O w U 2 I 7 3 s z c T / v G 5 i w i s / Z T J O D J V k s S h M O D I R m n 2 P B k x R Y v j E E k w U s 7 c i M s I K E 2 M z K t g Q v O W X V 0 m r W v F q l e r t R a n u Z n H k 4 Q R O o Q w e X E I d b q A B T S A g 4 B l e 4 c 1 R z o v z 7 n w s W n N O N n M M f + B 8 / g B S 5 5 A J &lt; / l a t e x i t &gt; and g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 0 n N B Q a C B g Z Q w V T Q r R x M M m G a Q K 4 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 r G K / Y A 2 l M 1 2 0 y 7 d b M L u R C i h / 8 C L B 0 W 8 + o + 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l a P O E 2 4 H 9 G R E q F g F K 3 0 M C o P K l W 3 5 s 5 B V o l X k C o U a A 4 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z u Z 8 a n l A 2 o S P e s 1 T R i B s / m 1 8 6 I + d W G Z I w 1 r Y U k r n 6 e y K j k T H T K L C d E c W x W f Z y 8 T + v l 2 J 4 4 2 d C J S l y x R a L w l Q S j E n + N h k K z R n K q S W U a W F v J W x M N W V o w 8 l D 8 J Z f X i X t e s 2 7 r N X v r 6 o N t 4 i j B K d w B h f g w T U 0 4 A 6 a 0 A I G I T z D K 7 w 5 E + f F e X c + F q 1 r T j F z A n / g f P 4 A / M y M 8 Q = = &lt; / l a t e x i t &gt; Illustration of the GCN-LSTM-HPA architecture for the proposed KA GNE T module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.(%) IHtest-Acc.(%) IHdev-Acc.(%) IHtest-Acc.(%) IHdev-Acc.(%) IHtest-Acc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>-&gt; container &lt;-IsA-fountain_pen fill &lt;-HasSubEvent-ink &lt;-AtLocation-fountain_pen fill -RelatedTo-&gt; container &lt;-IsA-fountain_pen write &lt;-UsedFor-pen write &lt;-UsedFor-pen &lt;-IsA-fountain_pen paper &lt;-RelatedTo-write &lt;-UsedFor-fountain_pen ….. 2. Ranking via path-level attn. 1. select concept pairs of high att. scores KagNet An example of interpreting model behaviors by hierarchical attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: Comparisons with large pre-trained language model fine-tuning with different amount of training data.</figDesc><table><row><cell>GPT-FINETUNING</cell><cell>27.55</cell><cell>26.51</cell><cell>32.46</cell><cell>31.28</cell><cell>47.35</cell><cell>45.58</cell></row><row><cell>GPT-KAGNET</cell><cell>28.13</cell><cell>26.98</cell><cell>33.72</cell><cell>32.33</cell><cell>48.95</cell><cell>46.79</cell></row><row><cell>BERT-BASE-FINETUNING</cell><cell>30.11</cell><cell>29.78</cell><cell>38.66</cell><cell>36.83</cell><cell>53.48</cell><cell>53.26</cell></row><row><cell>BERT-BASE-KAGNET</cell><cell>31.05</cell><cell>30.94</cell><cell>40.32</cell><cell>39.01</cell><cell>55.57</cell><cell>56.19</cell></row><row><cell>BERT-LARGE-FINETUNING</cell><cell>35.71</cell><cell>32.88</cell><cell>55.45</cell><cell>49.88</cell><cell>60.61</cell><cell>55.84</cell></row><row><cell>BERT-LARGE-KAGNET</cell><cell>36.82</cell><cell>33.91</cell><cell>58.73</cell><cell>51.13</cell><cell>62.35</cell><cell>57.16</cell></row><row><cell>Human Performance</cell><cell>-</cell><cell>88.9</cell><cell>-</cell><cell>88.9</cell><cell>-</cell><cell>88.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the KA GNE T framework.</figDesc><table><row><cell>questions with more than one highly plau-</cell></row><row><cell>sible answers, the commonsense reasoner</cell></row><row><cell>should benefit from explicitly investigating</cell></row><row><cell>the difference between different answer can-</cell></row><row><cell>didates, while KA GNE T training method is</cell></row><row><cell>not capable of doing so.</cell></row><row><cell>• subjective reasoning: Many answers actu-ally depend on the "personality" of the rea-</cell></row><row><cell>soner. For instance, "Traveling from new</cell></row><row><cell>place to new place is likely to be what?"</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/INK-USC/KagNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We do LSTM-based setup because it is non-trivial to apply token-level knowledge-aware baseline methods for complicated pre-trained encoders like BERT.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported in part by National Science Foundation SMA 18-29268, DARPA MCS and GAILA, IARPA BETTER, Schmidt Family Foundation, Amazon Faculty Award, Google Research Award, Snapchat Gift and JP Morgan AI Research Award. We would like to thank all the collaborators in the INK research lab for their constructive feedback on the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning beyond datasets: Knowledge graph augmented neural networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Annervaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somnath</forename><surname>Basu Roy Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Schema theory: An information processing model of perception and cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Axelrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American political science review</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1248" to="1266" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinícius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Flores Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Aglar Gülehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kelsey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<idno>abs/1806.01261</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Commonsense reasoning and commonsense knowledge in artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="92" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The rectilinear steiner tree problem is np-complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="826" to="834" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mental models in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip N Johnson-Laird</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="115" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning what is essential in questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Commonsense knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aynaz</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><forename type="middle">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzvetan</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Socialiqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno>abs/1904.09728</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Semantic Web Conference</title>
		<meeting>of European Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Webchild 2.0 : Fine-grained commonsense knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Do language models have common sense? OpenReview, ICLR submissions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Does it make sense? and why? a pilot study for sense making and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving natural language inference using external knowledge in the science questions domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassem</forename><surname>Makni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving natural language inference using external knowledge in the science questions domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassem</forename><surname>Makni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Nicholas Mattei, and Michael Witbrock</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic integration of background knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02596</idno>
	</analytic>
	<monogr>
		<title level="m">neural nlu systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic extraction of commonsense locatednear knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Michael</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1906.08237</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving question answering by commonsense-based pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno>abs/1809.03568</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
