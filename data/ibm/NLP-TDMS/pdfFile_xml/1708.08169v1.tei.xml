<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chain-erCV: a Library for Deep Learning in Computer Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-23">2017. October 23-27, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Niitani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chain-erCV: a Library for Deep Learning in Computer Vision</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Reference format</title>
						<meeting>MM &apos;17 <address><addrLine>Mountain View, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2017-10-23">2017. October 23-27, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3123266.3129395</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Open source software</term>
					<term>• Comput- ing methodologies → Image segmentation</term>
					<term>Object detection</term>
					<term>KEYWORDS Open Source</term>
					<term>Computer Vision</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant progress of deep learning in the field of computer vision, there has not been a software library that covers these methods in a unifying manner. We introduce ChainerCV, a software library that is intended to fill this gap. ChainerCV supports numerous neural network models as well as software components needed to conduct research in computer vision. These implementations emphasize simplicity, flexibility and good software engineering practices. The library is designed to perform on par with the results reported in published papers and its tools can be used as a baseline for future research in computer vision. Our implementation includes sophisticated models like Faster R-CNN and SSD, and covers tasks such as object detection and semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, the computer vision community has witnessed rapid progress thanks to deep learning methods in areas including image classification <ref type="bibr" target="#b2">[2]</ref>, object detection <ref type="bibr" target="#b12">[13]</ref> and semantic segmentation <ref type="bibr" target="#b3">[3]</ref>. High quality software tools are essential to keep up the rapid pace of innovation in deep learning research. The quality of a software library is hugely influenced by traditional software quality metrics such as consistent coding conventions and coverage of tests and documentation. In addition to that, a deep learning library, specifically a library that hosts implementations of deep Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM '17, October 23-27, 2017, Mountain View, CA, USA © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4906-2/17/10. . . $15.00 https://doi.org/10.1145/3123266.3129395 learning models, needs to guarantee quality during the training phase. Training a machine learning model to have a good performance is difficult due to numerous details that can hinder it from achieving its full potential. This makes it all the more important that a library hosts implementations of high performance training code so that it can give guidance to developers and researchers who want to extend and develop further from these implementations.</p><p>We think that training code should perform on par with the performance reported by the paper that the implementation is based on. On top of providing a high quality implementation for training a model, we also aim at making it more accessible, especially for users with limited experience, to run inference on sophisticated computer vision models such as Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>.</p><p>The rapid progress of deep learning research has been enabled by a number of frameworks including Chainer <ref type="bibr" target="#b14">[15]</ref>, TensorFlow <ref type="bibr" target="#b0">[1]</ref> and PyTorch 1 . These frameworks have supported fundamental components of deep learning software such as automatic differentiation and effective parallelization using GPUs. However, they are intended to target general usage, and do not aim to provide complete implementations of vision algorithms.</p><p>Our software, ChainerCV, supports algorithms to solve tasks in the computer vision field such as object detection, while considering usability and predictable performance as the top priorities. This makes it perfect to be used as a building block in larger software projects such as robotic software systems even by developers who are not computer vision experts. Recently there has been a growing trend of building new neural network models using existing architectures as building blocks. Examples can be seen in tasks such as instance segmentation <ref type="bibr" target="#b8">[8]</ref> and scene graph generation <ref type="bibr" target="#b15">[16]</ref>, which depend on object detection algorithms to localize objects in images. ChainerCV's algorithms can be used as components to construct software that can solve complex computer vision problems.</p><p>Training a network is a critical part of a machine learning algorithm, and ChainerCV is designed to make this process easy. In many use cases, users need a machine learning model to perform well on a particular dataset they have. Often a pretrained model is not sufficient for the users' tasks, and so they must re-train the model using their datasets. To make training a model easier in such cases, ChainerCV provides reference implementations to train models, which can be used as a baseline to write new training code. In addition to that, pretrained models can be used together with the users dataset to fine-tune the model. ChainerCV also provides set of tools for training a model including dataset loader, prediction evaluator and visualization tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility in machine learning and computer vision is one of the most important factors affecting the quality of the research.</head><p>ChainerCV aims at easing the process of reproducing the published results by providing training code that is guaranteed to perform on par with them. These algorithms would serve as baselines to find a new idea through refinement and as a tool to compare a new approach against existing approaches.</p><p>To summarize, ChainerCV offers the following two contributions:</p><p>• High quality implementations of deep learning-based computer vision algorithms to solve problems with emphasis on usability. • Reference code and tools to train models, which is guaranteed to perform on par with the published results.</p><p>Our code is released at https://github.com/pfnet/chainercv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep learning frameworks such as Chainer <ref type="bibr" target="#b14">[15]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref> play a fundamental role in deep learning software. However, these software packages focus on fundamental components such as automatic differentiation and GPU support. Keras <ref type="bibr" target="#b6">[6]</ref> is a high-level deep learning API that is intended to enable fast experimentation. While ChainerCV shares a similar goal with Keras to enable fast prototyping, our software provides more thorough coverage of software components for the computer vision tasks. In addition to that, Keras does not provide high performance training code for sophisticated vision models like Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>. OpenCV [10] is a prominent example of computer vision software libraries supporting numerous highly tuned implementations. The library supports wide range of algorithms including some deep learning-based algorithms, which emphasize on running inference on a cross platform environment. Different from their work, ChainerCV aims at acceralating research in this field in a more comprehensive manner by providing high quality training code on top of implementations to conduct inference.</p><p>Orthogonal to our open source work, there are several proprietary software solutions that support computer vision algorithms based on deep learning. These include Computer Vision System Toolbox by MATLAB and Google Cloud Vision API by Google Cloud Platform.</p><p>Model Zoo hosts a number of open source implementations and their trained models. Algorithms for a wide range of tasks are hosted on their website, but they are not provided as a library that organizes code in some standardized manner. There are open source implementations released by the authors of papers and thirdparty implementations released by open source developers. The primarily aim of these works is to make a prototype of a research idea. Unlike them, one of our goals is to develop an implementation that follows a good software engineering practices so that it is readable and easily extendable to other projects. We assure the quality by developing through peer review process, and thorough coverage of documentations and tests.</p><p>More closely related to our work is pytorch/vision, which is a computer vision library that uses PyTorch as its backend. Similar to our work, it hosts pretrained models to let users use high performance convolutional neural networks off-the-shelf. At the time of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMPLEMENTATION 3.1 Task Specific Models</head><p>Currently, ChainerCV supports networks for object detection and semantic segmentation <ref type="figure" target="#fig_1">(Figure 1</ref>). Object detection is the task of finding objects in an image and classifying them. Semantic segmentation is the task of segmenting an image into pieces and assigning object labels to them.</p><p>We implemented our detection algorithms in a unifying manner by exploiting the fact that many of the leading state of the art architectures have converged on a similar structure <ref type="bibr" target="#b9">[9]</ref>. All of these architectures use convolutional neural networks to extract features and use sliding windows to predict localization and classification. Our implementation includes architectures that can be grouped by Faster R-CNN <ref type="bibr" target="#b12">[13]</ref> and Single Shot Multibox Detector (SSD) <ref type="bibr" target="#b10">[11]</ref> meta-architectures. Faster R-CNN takes a crop proposed by an external neural network called Region Proposal Networks and carry out classification on the crop of the input image. SSD tries to alleviate the extra time running Region Proposal Networks by directly predicting classes and coordinates of bounding boxes. These meta-architectures are instantiated into more concrete networks that have different feature extractors or different head architectures. These different implementations inherit from the base class for each meta-architecture using our flexible class design.</p><p>Our implementation of semantic segmentation models includes SegNet <ref type="bibr" target="#b3">[3]</ref>. The architecture follows an encoder-decoder style. We have separated a module to calculate loss from a network that predicts a probability map. This design makes the loss reusable in other implementation of semantic segmentation models, which we are planning to add in the future. Models for a certain task are designed to have a common interface. For example, detection models support a predict method that takes images and outputs bounding boxes around regions where objects are predicted to be located. The common interface allows users to swap different models easily inside their code. On top of that, the common interface is necessary to build functions that interact with neural network models by passing input images and receiving predictions. For instance, thanks to this interface, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>In order to train and evaluate deep learning models, datasets are needed. ChainerCV provides an interface to datasets commonly used in computer vision tasks, such as datasets from the Pascal VOC Challenge <ref type="bibr" target="#b7">[7]</ref>. The datasets object downloads data from the Internet if necessary, and returns requested contents with an arraylike interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transforms</head><p>A transform is a function that takes an image and annotations as inputs and applies a modification to the inputs such as image resizing. These functions are composed together to create a custom data preprocessing pipeline. ChainerCV uses TransformDataset to compose different transforms. This is a class that wraps around a dataset by applying a function to a sample retrieved from the underlying dataset, which is often prepared to simply load data from a file system without any modifications.</p><p>We found that extending a dataset with an arbitrary function is effective especially in the case where multiple objects are processed in an interdependent manner. Such interdependence of transforms happen in a scenario when an image is randomly flipped horizontally to augment a dataset and coordinates of bounding boxes are altered depending on whether the image is flipped or not. See <ref type="figure" target="#fig_2">Figure 2</ref> for the code to carry out the data preprocessing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visualizations and Evaluation Metrics</head><p>ChainerCV supports a set of functions for visualization and evaluation, which are important for conducting research in computer vision. These functions can be used across different models by enforcing a consistent data representation for each type of data. For example, an image array is assumed to be RGB and shaped as (C, H, W), where the elements of the tuple are channel size, height and width of the image. The evaluations and visualizations in Section 4 are carried out using the functions in ChainerCV. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GPU Arrays</head><p>As done in Chainer <ref type="bibr" target="#b14">[15]</ref>, ChainerCV uses cupy.ndarray to represent arrays stored in GPU memory and numpy.ndarray to represent arrays stored in CPU memory. CuPy 2 is a NumPy like multidimensional array library with GPU acceleration. Many functions in ChainerCV support both types as arguments, and returns the output with the same type as the input. These functions include non-maximum suppression <ref type="bibr" target="#b11">[12]</ref>, which is efficiently implemented using CUDA parallelization. It is often complicated to set up a library to call CUDA kernels from a python module because the installation needs to consider a variety of machine configurations. ChainerCV relies on CuPy when calling CUDA kernels, and its installation procedure is quite simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We report performance of the implemented training code, and verify that the scores are on par with the ones reported in the original papers. Note that due to randomness in training, it is inevitable to produce slightly different scores from the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Faster R-CNN</head><p>We evaluated the performance of our implementation of Faster R-CNN, and compared it to the performance reported in the original paper <ref type="bibr" target="#b12">[13]</ref>. We experimented with a model that uses VGG-16 model <ref type="bibr" target="#b13">[14]</ref> as a feature extractor. The model is trained on the PASCAL VOC detection dataset. The model is trained on the 2007 trainval and evaluated on 2007 test using our training code. Some detection results of this trained model are shown in <ref type="figure" target="#fig_3">Figure 3a</ref>.</p><p>The performance is compared against the original implementation using mean average precision in <ref type="table" target="#tab_0">Table 1</ref>. Due to the stochastic training process, it is known that the final performance fluctuates <ref type="bibr" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Single Shot Multibox Detector (SSD)</head><p>We evaluated the performance of our implementations of SSD300 and SSD512, and compared them to the performance reported in <ref type="bibr" target="#b5">[5]</ref>. We trained these models with the trainval splits of PASCAL VOC 2007 and 2012 for training. The performance is compared against the original implementation using mean average precision in <ref type="table" target="#tab_0">Table 1</ref>. Note that we changed the train batchsize of SSD512 from 32 to 24 due to the GPU memory limitation. We also show some detection results of SSD512 in <ref type="figure" target="#fig_3">Figure 3b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SegNet</head><p>We evaluated the performance of our SegNet implementation, and compared it to the performance reported in the journal version of the original paper <ref type="bibr" target="#b3">[3]</ref>. It is trained on the train split of CamVid <ref type="bibr" target="#b3">[3]</ref>, and evaluated on the test split. The performance is measured by  pixel accuracy, mean pixel accuracy and mean IoU, which are the metrics used in <ref type="bibr" target="#b3">[3]</ref>. The score is shown in <ref type="table" target="#tab_1">Table 2</ref> and an example result is shown in <ref type="figure" target="#fig_3">Figure 3c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this article we have introduced a new computer vision software library that focuses on deep learning-based methods. Our software lowers the barrier of entry to use deep learning-based computer vision algorithms by providing a convenient and unified interface. It also provides evaluation and visualization tools to aid research and development in the field. Our implementation achieves performance on par with the reported results, and we expect it to be used as a baseline to be extended with new ideas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and demo codes for networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Directory structure of ChainerCV writing, its support for pretrained model and data preparation are limited only to classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>from c h a i n e r c v . d a t a s e t s import * from c h a i n e r c v . t r a n s f o r m s import * d a t a s e t = V O C D e t e c t i o n D a t a s e t ( ) def f l i p _ t r a n s f o r m ( i n _ d a t a ) : img , bbox , l a b e l = i n _ d a t a img , param = r a n d o m _ f l i p ( img , x _ f l i p = True , r e t u r n _ p a r a m = True ) bbox = f l i p _ b b o x ( bbox , x _ f l i p =param [ ' x _ f l i p ' ] ) return img , bbox , l a b e l n e w _ d a t a s e t = T r a n s f o r m D a t a s e t ( d a t a s e t , f l i p _ t r a n s f o r m ) Code to preprocess data using TransformDataset can write a function that iterates over a dataset and visualizes the predictions of all the samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>SegNet (top: input, bottom: output) Example visualizations. (a, b) Visualizations of object detection and (c) visualizations of semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Object detection mean average precision (%).</figDesc><table><row><cell cols="4">Implementations Faster R-CNN SSD300 SSD512</cell></row><row><cell>Original</cell><cell>69.9 [13]</cell><cell cols="2">77.5 [5] 79.5 [5]</cell></row><row><cell>ChainerCV</cell><cell>70.5</cell><cell>77.5</cell><cell>80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation performance of SegNet.</figDesc><table><row><cell></cell><cell cols="3">pixel accuracy mean pixel accuracy mIoU</cell></row><row><cell>Original [3]</cell><cell>82.7</cell><cell>62.3</cell><cell>46.3</cell></row><row><cell>ChainerCV</cell><cell>82.8</cell><cell>67.1</cell><cell>47.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://pytorch.org arXiv:1708.08169v1 [cs.CV] 28 Aug 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/cupy/cupy</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank Richard Calland for helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<ptr target="tensorflow.org" />
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ilya Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An Implementation of Faster RCNN with Study for Region Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth Ranga Ambrish Tyagi Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">DSSD : Deconvolutional Single Shot Detector. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">François Chollet and others</title>
		<ptr target="Keras.github.com/fchollet/keras." />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollãąr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<title level="m">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<meeting><address><addrLine>Kevin Murphy, and Google Research</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325v2</idno>
		<title level="m">SSD: Single Shot MultiBox Detector</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Work-efficient parallel non-maximum suppression for embedded GPU architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernãąndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497v1</idno>
		<title level="m">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chainer: a Next-Generation Open Source Framework for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems in NIPS</title>
		<meeting>Workshop on Machine Learning Systems in NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
