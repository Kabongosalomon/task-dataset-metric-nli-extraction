<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Self-Training for Gradual Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
							<email>ananya@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyuma@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Self-Training for Gradual Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. We consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data, and suggesting that self-training works particularly well for shifts with small Wasserstein-infinity distance. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset and a realistic Portraits dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning models are typically trained and tested on the same data distribution. However, when a model is deployed in the real world, the data distribution typically evolves over time, leading to a drop in performance. This problem is widespread: sensor measurements drift over time due to sensor aging <ref type="bibr" target="#b0">[1]</ref>, self-driving car vision modules have to deal with evolving road conditions <ref type="bibr" target="#b1">[2]</ref>, and neural signals received by brain-machine interfaces change within the span of a day <ref type="bibr" target="#b2">[3]</ref>. Repeatedly gathering large sets of labeled examples to retrain the model can be impractical, so we would like to leverage unlabeled examples to adapt the model to maintain high accuracy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>In these examples the domain shift doesn't happen at one time, but happens gradually, although this gradual structure is ignored by most domain adaptation methods. Intuitively, it is easier to handle smaller shifts, but for each shift we can incur some error so the more steps, the more degradation-making it unclear whether leveraging the gradual shift structure is better than directly adapting to the target.</p><p>In this paper, we provide the first theoretical analysis showing that gradual domain adaptation provides improvements over the traditional approach of direct domain adaptation. We analyze selftraining (also known as pseudolabeling), a method in the semi-supervised learning literature <ref type="bibr" target="#b4">[5]</ref> that has led to state-of-the-art results on ImageNet <ref type="bibr" target="#b5">[6]</ref> and adversarial robustness on CIFAR-10 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>As a concrete example of our setting, the Portraits dataset <ref type="bibr" target="#b9">[10]</ref> contains photos of high school seniors taken across many years, labeled by gender ( <ref type="figure" target="#fig_1">Figure 1</ref>). We use the first 2000 images (1905 - <ref type="figure" target="#fig_1">Figure 1</ref>: In gradual domain adaptation we are given labeled data from a source domain, and unlabeled data from intermediate domains that shift gradually in distribution towards a target domain. Here, blue = female, red = male, and gray = unlabeled data. 1935) as the source, next 14000 <ref type="bibr">(1935 -1969)</ref> as intermediate domains, and next 2000 images as the target <ref type="bibr">(1969 -1973)</ref>. A model trained on labeled examples from the source gets 98% accuracy on held out examples in the same years, but only 75% accuracy on the target domain. Assuming access to unlabeled images from intermediate domains, our goal is to adapt the model to do well on the target domain. Direct adaptation to the target with self-training only improves the accuracy a little, from 75% to 77%.</p><p>The gradual self-training algorithm begins with a classifier w 0 trained on labeled examples from the source domain <ref type="figure" target="#fig_2">(Figure 2a</ref>). For each successive domain P t , the algorithm generates pseudolabels for unlabeled examples from that domain, and then trains a regularized supervised classifier on the pseudolabeled examples. The intuition, visualized in <ref type="figure" target="#fig_2">Figure 2</ref>, is that after a single gradual shift, most examples are pseudolabeled correctly so self-training learns a good classifier on the shifted data, but the shift from the source to the target can be too large for self-training to correct. We find that gradual self-training on the Portraits dataset improves upon direct target adaptation (77% to 84% accuracy).</p><p>Our results: We analyze gradual domain adaptation in two settings. The key challenge for domain adaptation theory is dealing with source and target domains whose support do not overlap <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, which are typical in the modern high-dimensional regime. The gradual shift structure inherent in many applications provides us with leverage to handle adapting to target distributions with non-overlapping support.</p><p>Our first setting, the margin setting, is distribution-free-we only assume that at every point in time there exists some linear classifier that can classify most of the data correctly with a margin, where the linear classifier may be different at each time step (so this is more general than covariate shift), and that the shifts are small in Wasserstein-infinity distance. A simple example (as in <ref type="figure" target="#fig_2">Figure 2</ref>) shows that a classifier that gets 100% accuracy can get 0% accuracy after a constant number of time steps. Directly adapting to the final target domain also gets 0% accuracy. Gradual self-training does better, letting us bound the error after T steps: err T ≤ e cT (α 0 + O(1/ √ n)), where α 0 is the error of the classifier on the source domain, and n is the number of unlabeled examples in each (a) t = 0 (b) t = 1 (c) t = 2 (d) t = 3 <ref type="figure" target="#fig_2">Figure 2</ref>: The source classifier w 0 gets 100% accuracy on the source domain <ref type="figure" target="#fig_2">(Figure 2a</ref>), where we have labeled data. But after 3 time steps ( <ref type="figure" target="#fig_2">Figure 2d</ref>) the source classifier is stale, classifying most examples incorrectly. Now, we cannot correct the classifier using unlabeled data from the target domain, which corresponds to traditional domain adaptation directly to the target. Given unlabeled data in an intermediate domain <ref type="figure" target="#fig_2">(Figure 2b)</ref> where the shift is gradual, the source classifier pseudolabels most points correctly, and self-training learns an accurate classifier (show in green) that separates the classes. Successively applying self-training learns a good classifier on the target domain (green classifier in <ref type="figure" target="#fig_2">Figure 2d</ref>).</p><p>intermediate domain. While this bound is exponential in T , this bound is non-vacuous for small α 0 , and we show that this bound is tight for gradual self-training.</p><p>In the second setting, stronger distributional assumptions allow us to do better-we assume that P (X | Y = y) is a d-dimensional isotropic Gaussian for each y. Here, we show that if we begin with a classifier w 0 that is nearly Bayes optimal for the initial distribution, we can recover a classifier w T that is Bayes optimal for the target distribution with infinite unlabeled data. This is an idealized setting to understand what properties of the data might allow self-training to do better than the exponential bound.</p><p>Our theory leads to practical insights, showing that regularization-even in the context of infinite data-and label sharpening are essential for gradual self-training. Without regularization, the accuracy of gradual self-training drops from 84% to 77% on Portraits and 88% to 46% on rotating MNIST. Even when we self-train with more examples, the performance gap between regularized and unregularized models stays the same-unlike in supervised learning where the benefit of regularization diminishes as we get more examples.</p><p>Finally, our theory suggests that the gradual shift structure helps when the shift is small in Wasserstein-infinity distance as opposed to other distance metrics like the KL-divergence. For example, one way to interpolate between the source and target domains is to gradually introduce more images from the target, but this shift is large in Wasserstein-infinity distance-we see experimentally that gradual self-training does not help in this setting. We hope this gives practitioners some insight into when gradual self-training can work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>Gradually shifting distributions: Consider a binary classification task of predicting labels y ∈ {−1, 1} from input features x ∈ R d . We have joint distributions over the inputs and labels, R d × {−1, 1}: P 0 , P 1 , . . . , P T , where P 0 is the source domain, P T is the target domain, and P 1 , . . . , P T −1 are intermediate domains. We assume the shift is gradual: for some &gt; 0, ρ(P t , P t+1 ) &lt; for all 0 ≤ t &lt; T , where ρ(P, Q) is some distance function between distributions P and Q. We</p><formula xml:id="formula_0">have n 0 labeled examples S 0 = {x (0) i , y (0) i } n 0 i=1 sampled independently from the source P 0 and n unlabeled examples S t = {x (t) i } n i=1 sampled independently from P t for each 1 ≤ t ≤ T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and objectives:</head><p>We have a model family Θ, where a model M θ : R d → R outputs a score representing its confidence that the label y is 1 for the given example. The model's prediction for an input x is sign(M θ (x)), where sign(r) = 1 if r ≥ 0 and sign(r) = −1 if r &lt; 0. We evaluate models on the fraction of times they make a wrong prediction, also known as the 0-1 loss:</p><formula xml:id="formula_1">Err(θ, P ) = E X,Y ∼P [sign(M θ (X)) = Y ]<label>(1)</label></formula><p>The goal is to find a classifier θ that gets high accuracy on the target domain P T -that is, low Err(θ, P T ). In an online setting we may care about the accuracy at the current P t for every time t, and our analysis works in this setting as well.</p><p>Baseline methods: We select a loss function : R × {−1, 1} → R + which takes a prediction and label, and outputs a non-negative loss value, and we begin by training a source model θ 0 that minimizes the loss on labeled data in the source domain:</p><formula xml:id="formula_2">θ 0 = arg min θ ∈Θ 1 n 0 (x i ,y i )∈S 0 (M θ (x i ), y i )<label>(2)</label></formula><p>The non-adaptive baseline is to use θ 0 on the target domain, which incurs error Err(θ 0 , P T ). Selftraining uses unlabeled data to adapt a model. Given a model θ and unlabeled data S, ST(θ, S) denotes the output of self-training. Self-training pseudolabels each example in S using M θ , and then selects a new model θ that minimizes the loss on this pseudolabeled dataset. Formally,</p><formula xml:id="formula_3">ST(θ, S) = arg min θ ∈Θ 1 |S| x i ∈S (M θ (x i ), sign(M θ (x i )))<label>(3)</label></formula><p>Here, self-training uses "hard" labels: we pseudolabel examples as either −1 or 1, based on the output of the classifier, instead of a probabilistic label based on the model's confidence-we refer to this as label sharpening. In our theoretical analysis, we sometimes want to describe the behavior of self-training when run on infinite unlabeled data from a probability distribution P :</p><formula xml:id="formula_4">ST(θ, P ) = arg min θ ∈Θ E X∼P [ (M θ (X), sign(M θ (X)))]<label>(4)</label></formula><p>The direct adaptation to target baseline takes the source model θ 0 and self-trains on the target data S T , and is denoted by ST(θ 0 , S T ). Prior work often chooses to repeat this process of self-training on the target k times, which we denote by ST k (θ 0 , S T ).</p><p>Gradual self-training: In gradual self-training, we self-train on the finite unlabeled examples from each domain successively. That is, for i ≥ 1, we set:</p><formula xml:id="formula_5">θ i = ST(θ i−1 , S i )<label>(5)</label></formula><p>ST(θ 0 , (S 1 , . . . , S T )) = θ T is the output of gradual self-training, which we evaluate on the target distribution P T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theory for the margin setting</head><p>We show that gradual self-training does better than directly adapting to the target, where we assume that at each time step there exists some linear classifier-which can be different at each step-that can classify most of the data correctly with a margin (a standard assumption in learning theory), and that the shifts are small. Our main result (Theorem 3.2) bounds the error of gradual self-training. We show that our analysis is tight for gradual self-training (Example 3.4), and explain why regularization, label sharpening, and the ramp loss, are key to our bounds. Proofs are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumptions</head><p>Models and losses: We consider regularized linear models that have weights with bounded 2 norm:</p><formula xml:id="formula_6">Θ R = {(w, b) : w ∈ R d , b ∈ R, w 2 ≤ R} for some fixed R &gt; 0. Given (w, b) ∈ Θ R , the model's output is M w,b (x) = w x + b.</formula><p>We consider margin loss functions such as the hinge and ramp losses. Intuitively, a margin loss encourages a model to classify points correctly and confidently-by keeping correctly classified points far from the decision boundary. We consider the hinge function h and ramp function r:</p><formula xml:id="formula_7">h(m) = max(1 − m, 0)<label>(6)</label></formula><p>r(m) = min(h(m), 1)</p><p>The ramp loss is r (ŷ, y) = r(yŷ), whereŷ ∈ R is a model's prediction, and y ∈ {−1, 1} is the true label. The hinge loss is the standard way to enforce margin, but the ramp loss is more robust towards outliers because it is bounded above-no single point contributes too much to the loss. We will see that the ramp loss is key to the theoretical guarantees for gradual self-training because of its robustness. We denote the population ramp loss as:</p><formula xml:id="formula_9">L r (θ, P ) = E X,Y ∼P [ r (M θ (X), Y )]<label>(8)</label></formula><p>Given a finite sample S, the empirical loss is:</p><formula xml:id="formula_10">L r (θ, S) = 1 |S| x,y∈S [ r (M θ (x), y)]<label>(9)</label></formula><p>Distributional distance: Our notion of distance is W ∞ , the Wasserstein-infinity distance. Intuitively, W ∞ moves points from distribution P to Q by distance at most to match the distributions.</p><p>For ease of exposition we consider the Monge form of W ∞ , although the results can be extended to the Kantarovich formulation as well. Formally, given probability measures P, Q on X :</p><formula xml:id="formula_11">W ∞ (P, Q) = inf{ sup x∈R d ||f (x) − x|| 2 : f : R d → R d , f # P = Q}<label>(10)</label></formula><p>As usual, # denotes the push-forward of a measure, that is, for every set</p><formula xml:id="formula_12">A ⊆ R d , f # P (A) = P (f −1 (A)).</formula><p>In our case, we require that the conditional distributions do not shift too much. Given joint probability measures P, Q on the inputs and labels R d × {−1, 1}, the distance is:</p><formula xml:id="formula_13">ρ(P, Q) = max(W ∞ (P X|Y =1 , Q X|Y =1 ), W ∞ (P X|Y =−1 , Q X|Y =−1 )).<label>(11)</label></formula><p>α * -separation assumption: Assume every domain admits a classifier with low loss α * , that is there exists α * ≥ 0 and for every domain P t , there exists some</p><formula xml:id="formula_14">θ t ∈ Θ R with L r (θ t , P t ) ≤ α * .</formula><p>Gradual shift assumption: For some ρ &lt; 1 R , assume ρ(P t , P t+1 ) ≤ ρ for every consecutive domain, where 1 R is the regularization strength of the model class Θ R . γ = 1 R can be interpreted as the geometric margin (distance from decision boundary to data) the model is trying to enforce.</p><p>Bounded data assumption: When dealing with finite samples we need a standard regularity condition: we say that P satisfies the bounded data assumption if the data is not too large on average:</p><formula xml:id="formula_15">EX∼P [||X|| 2 2 ] ≤ B 2 where B &gt; 0.</formula><p>No label shift assumption: Assume that the fraction of Y = 1 labels does not change: P t (Y ) is the same for all t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain shift: baselines fail</head><p>While the distribution shift from P t to P t+1 is small, the distribution shift from the source P 0 to the target P T can be large, as visualized in <ref type="figure" target="#fig_2">Figure 2</ref>. A classifier that gets 100% accuracy on P 0 , might classify every example wrong on P T , even if T ≥ 2. In this case, directly adaptating to P T would not help. The following example formalizes this:</p><formula xml:id="formula_16">Example 3.1.</formula><p>Even under the α * -separation, no label shift, gradual shift, and bounded data assumptions, there exists distributions P 0 , P 1 , P 2 and a source model θ ∈ Θ R that gets 0 loss on the source (L r (θ, P 0 ) = 0), but high loss on the target: L r (θ, P 2 ) = 1. Self-training directly on the target does not help: L r (ST(θ, P 2 ), P 2 ) = 1. This holds true even if every domain is separable, so α * = 0.</p><p>Other methods: Our analysis focuses on self-training, but other bounds do not apply in this setting because they either assume that the density ratio between the target and source exists and is not too small <ref type="bibr" target="#b12">[13]</ref>, or that the source and target are similar enough that we cannot discriminate between them <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gradual self-training improves error</head><p>We show that gradual self-training helps over direct adaptation. For intuition, consider a simple example where α * = 0 and θ 0 classifies every example in P 0 correctly with geometric margin γ = 1 R . If each point shifts by distance &lt; γ, θ 0 gets every example in the new domain P 1 correct. If we had infinite unlabeled data from P 1 , we can learn a model θ that classifies every example in the new domain P 1 correctly with margin γ since α * = 0. Repeating the process for P 2 , . . . , P T , we get every example in P T correct.</p><p>But what happens when we start with a model that has some error, for example because the data cannot be perfectly separated, and have only finite unlabeled samples? We show that self-training still does better than adapting to the target domain directly, or using the non-adaptive source classifier.</p><p>The first main result of the paper says that if we have a model θ that gets low loss and the distribution shifts slightly, self-training gives us a model θ that does not do too badly on the new distribution.</p><p>Theorem 3.2. Given P, Q with ρ(P, Q) = ρ &lt; 1 R and marginals on Y are the same so P (Y ) = Q(Y ). Suppose P, Q satisfy the bounded data assumption, and we have initial model θ, and n unlabeled samples S from Q, and we set θ = ST(θ, S). Then with probability at least 1 − δ over the sampling of S, letting α * = min θ * ∈Θ R L r (θ * , Q):</p><formula xml:id="formula_17">L r (θ , Q) ≤ 2 1 − ρR L r (θ, P ) + α * + 4BR + 2 log 2/δ √ n<label>(12)</label></formula><p>The proof of this result is in Appendix A, but we give a high level sketch here. There exists some classifier that gets accuracy α * on Q, so if we had access to n labeled examples from Q then empirical risk minimization gives us a classifier that is accurate on the population-from a Rademacher complexity argument we get a classifier θ with loss at most α * + O(1/ √ n), the second and third term in the RHS of the bound.</p><p>Since we only have unlabeled examples from Q, self-training uses θ to pseudolabel these n examples and then trains on this generated dataset. Now, if the distribution shift ρ is small relative to the geometric margin γ = 1 R , then we can show that the original model θ labels most examples in the new distribution Q correctly-that is, Err(θ, Q) is small if L r (θ, P ) is small. Finally, if most examples are labeled correctly we show that because there exists some classifier θ * with low margin loss, self-training will also learn a classifier θ with low margin loss L r (θ , Q), which completes the proof.</p><p>We apply this argument inductively to show that after T time steps, the error of gradual self-training is exp(cT )α 0 for some constant c, if the original error is α 0 . Corollary 3.3. Under the α * -separation, no label shift, gradual shift, and bounded data assumptions, if the source model θ 0 has low loss α 0 ≥ α * on P 0 (i.e. L r (θ 0 , P 0 ) ≤ α 0 ) and θ is the result of gradual self-training: θ = ST(θ 0 , (S 1 , . . . , S n )), letting β = 2 1−ρR :</p><formula xml:id="formula_18">L r (θ, P T ) ≤ β T +1 α 0 + 4BR + 2 log 2T /δ √ n .<label>(13)</label></formula><p>Corrollary 3.3 says that the gradual structure allows some control of the error unlike direct adaptation where the accuracy on the target domain can be 0% if T ≥ 2. Note that if the classes are separable and we have infinite data, then gradual self-training maintains 0 error.</p><p>Our next example shows that our analysis for gradual self-training in this setting is tight-if we start with a model with loss α 0 , then the error can in fact increase exponentially even with infinite unlabeled examples. Intuitively, at each step of self-training the loss can increase by a constant factor, which leads to an exponential growth in the error.</p><p>Example 3.4. Even under the α * -separation, no label shift, gradual shift, and bounded data assumptions, given 0 &lt; α 0 ≤ 1 4 , for every T there exists distributions P 0 , . . . , P 2T , and</p><formula xml:id="formula_19">θ 0 ∈ Θ R with L r (θ 0 , P 0 ) ≤ α 0 , but if θ = ST(θ 0 , (P 1 , . . . , P 2T )) then L r (θ , P 2T ) ≥ min(0.5, 1 2 2 T α 0 ). Note that L r is always in [0, 1].</formula><p>This suggests that if we want sub-exponential bounds we either need to make additional assumptions on the data distributions, or devise alternative algorithms to achieve better bounds (which we believe is unlikely).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Essential ingredients for gradual self-training</head><p>In this section, we explain why regularization, label sharpening, and the ramp loss are essential to bounding the error of gradual self-training (Theorem 3.2).</p><p>Regularization: Without regularization there is no incentive for the model to change when selftraining-if we self-train without regularization an optimal thing to do is to output the original model. The intuition is that since the model θ = (w, b) is used to pseudolabel examples, θ gets every pseudolabeled example correct. The scaled classifier θ = (αw, αb) for large α then gets optimal loss, but θ and θ make the same predictions for every example. We use ST (θ, S) to denote the set of possible θ that minimize the loss on the pseudolabeled distribution (Equation <ref type="formula" target="#formula_3">(3)</ref>):</p><formula xml:id="formula_20">Example 3.5. Given a model θ ∈ Θ ∞ and unlabeled examples S where for all x ∈ S, M θ (x) = 0, there exists θ ∈ ST (θ, S) such that for all x ∈ R d , M θ (x) = M θ (x).</formula><p>More specific to our setting, our bounds require regularized models because regularized models classify the data correctly with a margin, so even after a mild distribution shift we get most new examples correct. Note that in traditional supervised learning, regularization is usually required when we have few examples for better generalization to the population, whereas in our setting regularization is important for maintaining a margin even with infinite data.</p><p>Label sharpening: When self-training, we pseudolabel examples as −1 or 1, based on the output of the classifier. Prior work sometimes uses "soft" labels <ref type="bibr" target="#b8">[9]</ref>, where for each example they assign a probability of the label being −1 or 1, and train using a logistic loss. The loss on the softpseudolabeled distribution is defined as:</p><formula xml:id="formula_21">L σ,θ (θ ) = E X∼P [ll(σ(M θ (X)), σ(M θ (X)))]<label>(14)</label></formula><p>, where σ is the sigmoid function, and ll is the log loss:</p><formula xml:id="formula_22">ll(p, p ) = p log p + (1 − p) log (1 − p )<label>(15)</label></formula><p>Self-training then picks θ ∈ Θ minimizing L σ,θ (θ ). A simple example shows that this form of self-training may never update the parameters because θ minimizes L σ,θ :</p><formula xml:id="formula_23">Example 3.6. For all θ ∈ Θ, θ is a minimizer of L σ,θ , that is, for all θ ∈ Θ, L σ,θ (θ) ≤ L σ,θ (θ ).</formula><p>This suggests that we "sharpen" the soft labels to encourage the model to update its parameters. Note that this is true even on finite data: set P to be the empirical distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ramp versus hinge loss:</head><p>We use the ramp loss, but does the more popular hinge loss L h work? Unfortunately, the next example shows that we cannot control the error of gradual self-training with the hinge loss even if we had infinite examples, so the ramp loss is important for Theorem 3.2.</p><p>Example 3.7. Even under the α * -separation, no label shift, and gradual shift assumptions, given</p><formula xml:id="formula_24">α 0 &gt; 0, there exists distributions P 0 , P 1 , P 2 and θ 0 ∈ Θ R with L h (θ 0 , P 0 ) ≤ α, but if θ = ST(θ 0 , (P 1 , P 2 )) then L h (θ , P 2 ) ≥ Err(θ , P 2 ) = 1 (θ gets every example in P 2 wrong), where</formula><p>we use the hinge loss in self-training.</p><p>We only analyzed the statistical effects here-the hinge loss tends to work better in practice because it is much easier to optimize and is convex for linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Self-training without domain shift</head><p>Example 3.4 showed that when the distribution shifts, the loss of gradual self-training can grow exponentially (though the non-adaptive baseline has unbounded error). Here we show that if we have no distribution shift, the error can only grow linearly: if P 0 = . . . = P T , given a classifier with loss α 0 , if we do gradual self-training the loss is at most α 0 T .</p><formula xml:id="formula_25">Proposition 3.8. Given α 0 &gt; 0, distributions P 0 = = P T , and model θ 0 ∈ Θ R with L r (θ 0 , P 0 ) ≤ α 0 , L r (θ , P T ) ≤ α 0 (T + 1) where θ = ST(θ 0 , (P 1 , . . . , P T ))</formula><p>In Appendix A, we show that self-training can indeed hurt without domain shift: given a classifier with loss α on P , self-training on P can increase the classifier's loss on P to 2α, but here the non-adaptive baseline has error α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theory for the Gaussian setting</head><p>In this section we study an idealized Gaussian setting to understand conditions under which selftraining can have better than exponential error bounds: we show that if we begin with a good classifier, the distribution shifts are not too large, and we have infinite unlabeled data, then gradual self-training maintains a good classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting</head><p>We assume P t (X | Y = y) is an isotropic Gaussian in d-dimensions for each y ∈ {−1, 1}. We can shift the data to have mean 0, so we suppose:</p><formula xml:id="formula_26">P t (X|Y = y) = N (yµ t , σ 2 t I)<label>(16)</label></formula><p>Where µ t ∈ R d and σ t &gt; 0 for each t.</p><p>As usual, we assume the shifts are gradual: for some</p><formula xml:id="formula_27">B &gt; 0, µ t+1 − µ t 2 ≤ B 4 .</formula><p>We assume that the means of the two classes do not get closer than the shift, or else it would be impossible to distinguish between no shift, and the distributions of the two classes swapping: so µ t 2 ≥ B for all t. We assume infinite unlabeled data (access to P t (X)) in our analysis.</p><p>Given labeled data in the source, we use the objective:</p><formula xml:id="formula_28">L(w, P ) = E X,Y ∼P [φ(Y (w X))]<label>(17)</label></formula><p>For unlabeled data, self-training performs descent steps on an underlying objective function <ref type="bibr" target="#b14">[15]</ref>, which we focus on:</p><formula xml:id="formula_29">U (w, P ) = E X∼P [φ(|w X|)]<label>(18)</label></formula><p>We assume φ : R → R + is a continuous, non-increasing function which is strictly decreasing on [0, 1]: these are regularity conditions which the hinge, ramp, and logistic losses satisfy. If w = ST(w, P ) then U (w , P ) ≤ U (w, P ) <ref type="bibr" target="#b14">[15]</ref>.</p><p>The algorithm we analyze begins by choosing w 0 from labeled data in P 0 , and then updates the parameters with unlabeled data from P t for 1 ≤ t ≤ T :</p><formula xml:id="formula_30">w t = arg min w 2 ≤1, w−w t−1 2 ≤ 1 2 U (w, P t )<label>(19)</label></formula><p>Note that we do not show that self-training actually converges to the constrained minimum of U in Equation <ref type="formula" target="#formula_1">(19)</ref> and prior work only shows that self-training descends on U -we leave this optimization analysis to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Let w * (µ) = µ µ 2 where µ ≥ B &gt; 0. Note that w * (µ t ) minimizes the 0-1 error on P t . Our main theorem says that if we start with a regularized classifier w 0 that is near w * (µ 0 ), which we can learn from labeled data, and the distribution shifts µ t+1 − µ t 2 are not too large, then we recover the optimal w T = w * (µ T ). The key challenge is that the unlabeled loss U in d dimensions is non-convex, with multiple local minima, so directly minimizing U does not guarantee a solution that minimizes the labeled loss L.</p><formula xml:id="formula_31">Theorem 4.1. Assuming the Gaussian setting, if w 0 −w * (µ 0 ) 2 ≤ 1 4 , then we recover w T = w * (µ T ).</formula><p>Proving this reduces to proving the single-step case. At each step t + 1, if we have a classifier w t that was close to w * (µ t ), then we will recover w t+1 = w * (µ t+1 ). We give intuition here and the formal proof in Appendix B.</p><p>We first show that if µ changes by a small amount, the optimal parameters (for the labeled loss) does not change too much. Then since w t is close to w * (µ t ), w t is not too far away from w * (µ t+1 ).</p><p>The key step in our argument is showing that the unique minimum of the unlabeled loss U (w, P µ t+1 ) in the neighborhood of w t , is w * (µ t )-looking for a minimum nearby is important because if we deviate too far we might select other "bad" minima. We consider arbitrary w near w * (µ t+1 ) and construct a pairing of points (a, b) in R d , using a convexity argument to show that (a, b) contributes more to the loss of w than w * (µ t+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our theory leads to practical insights-we show that regularization and label sharpening are important for gradual self-training, that leveraging the gradual shift structure improves target accuracy, and give intuition for when the gradual shift assumption may not help. We run experiments on three datasets (see Appendix C for more details):</p><p>Gaussian: Synthetic dataset where the distribution P t (X|Y ) for each of two classes is a ddimensional Gaussian, where d = 100. The means and covariances of each class vary over time. The model gets 500 labeled samples from the source domain, and 500 unlabeled samples from each of 10 intermediate domains. This dataset resembles our Gaussian setting but the covariance matrices are not isotropic, and the number of labeled and unlabeled samples is finite and on the order of the dimension d.</p><p>Rotating MNIST: Rotating MNIST is a semi-synthetic dataset where we rotate each MNIST image by an angle between 0 and 60 degrees. We split the 50,000 MNIST training set images into a source domain (images rotated between 0 and 5 degrees), intermediate domain (rotations between 5 and 60 degrees), and a target domain (rotations between 55 degrees and 60 degrees). Note that each image is seen at exactly one angle, so the training procedure cannot track a single image across different angles.</p><p>Portraits: A real dataset comprising photos of high school seniors across years <ref type="bibr" target="#b9">[10]</ref>. The model's goal is to classify gender. We split the data into a source domain (first 2000 images), intermediate domain (next 14000 images), and target domain (next 2000 images). For rotating MNIST and Portraits we used a 3-layer convolutional network with dropout(0.5) and batchnorm on the last layer, that was able to achieve 97% − 98% accuracy on held out examples in the source domain. For the Gaussian dataset we used a logistic regression classifier with l 2 regularization. For each step of self-training, we filter out the 10% of images where the model's prediction was least confident-Appendix C shows similar findings without this filtering. To account for variance in initialization and optimization, we ran each method 5 times and give 90% confidence intervals. More experimental details are in Appendix C. <ref type="table" target="#tab_0">Table 1</ref> shows that leveraging the gradual structure leads to improvements over the baselines on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Important ingredients for gradual self-training</head><p>Our theory suggests that regularization and label sharpening are important for gradual self-training, because without regularization and label sharpening there is no incentive for the model to change (Section 3.4). However, prior work suggests that overparameterized neural networks trained with stochastic gradient methods have strong implicit regularization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>-in the supervised setting they perform well without explicit regularization even though the number of parameters is much larger than the number of data points-is this implicit regularization enough for gradual self-training?</p><p>In our experiments, we see that even without explicit regularization, or with 'soft' probabilistic labels, gradual self-training does slightly better than the non-adaptive source classifier, suggesting that this implicit regularization may have some effect. However, explicit regularization and 'hard' labeling gives a much larger accuracy boost.</p><p>Regularization is important: We repeat the same experiment as Section 5.1, comparing gradual self-training with or without regularization-that is, disabling dropout and batchnorm <ref type="bibr" target="#b17">[18]</ref> in the neural network experiments. In both cases, we first train an unregularized model on labeled examples in the source domain. Then, we either turn on regularization during self-training, or keep the model unregularized. We control the original model to be the same in both cases to see if regularization helps in the self-training process, as opposed to in learning a better supervised classifier. <ref type="table" target="#tab_1">Table 2</ref> shows that accuracies are significantly better with regularization, even though unregularized performance is still better than the non-adaptive source classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft labeling hurts:</head><p>We ran the same experiment as Section 5.1, comparing gradual self-training with hard labeling versus using probabilistic labels output by the model. <ref type="table" target="#tab_1">Table 2</ref> shows that accuracies are better with hard labels.</p><p>Regularization is still important with more data: In supervised learning, the importance of regularization diminishes as we have more training examples-if we had access to infinite data (the population), we don't need regularization. On the other hand, for gradual domain adaptation, the theory says regularization is needed to adapt to the dataset shift even with infinite data, and predicts that regularization remains important even if we increase the sample size.</p><p>To test this hypothesis, we construct a rotating MNIST dataset where we increase the sample sizes. The source domain P 0 consists of N ∈ {2000, 5000, 20000} images on MNIST. P t then consists of these same N images, rotated by angle 3t, for 0 ≤ t ≤ 20. The goal is to get high accuracy on P 20 : these images rotated by 60 degrees-the model doesn't have to generalize to unseen images, but to seen images at different angles. We compare using regularization versus not using regularization during gradual self-training. <ref type="table" target="#tab_2">Table 3</ref> shows that regularization is still important here, and the gap between regularized and unregularized gradual self-training does not shrink much with more data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">When does gradual shift help?</head><p>Our theory in Section 3 says that gradual self-training works well if the shift between domains is small in Wasserstein-infinity distance, but it may not be enough for the total variation or KL-divergence between P and Q to be small.</p><p>To test this, we run an experiment on a modified version of the rotating MNIST dataset. We keep the source and target domains the same as before, but change the intermediate domains.</p><p>In <ref type="table" target="#tab_0">Table 1</ref> we Here the total-variation distance between successive domains is small, but intuitively the Wasserstein distance is large because each image undergoes a large (≈ 55 degrees) rotation.</p><p>As the theory suggests, here gradual self-training does not outperform directly self-training on the target-gradual self-training gets 33.5 ± 1.5% accuracy on the target, while direct adaptation to the target gets 33.0 ± 2.2% over 5 runs. We hope this gives practitioners some insight into not just the strengths of gradual self-training, but also its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Self-training is a popular method in semi-supervised learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, and is related to entropy minimization <ref type="bibr" target="#b23">[24]</ref>. Theory in semi-supervised learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> analyzes when unlabeled data can help, but does not show bounds for particular algorithms. Recent work shows that a robust variant of self-training can mitigate the tradeoff between standard and adversarial accuracy <ref type="bibr" target="#b27">[28]</ref>. Related to self-training is co-training <ref type="bibr" target="#b28">[29]</ref>, which assumes that the input features can be split into two or more views that are conditionally independent on the label.</p><p>Unsupervised domain adaptation, where the goal is to directly adapt from a labeled source domain to an unlabeled target domain, is widely studied <ref type="bibr" target="#b29">[30]</ref>. The key challenge for domain adaptation theory is when the source and target supports do not overlap <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, which are typical in the modern high-dimensional regime. Importance weighting based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> assume the domains overlap, with bounds depending on the expected density ratios between the source and target. Even if the domains overlap, the density ratio often scales exponentially in the dimension. These methods also assume that P (Y | X) is the same for the source and target.</p><p>The theory of H∆H-divergence <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref> gives conditions for when a model trained on the source does well on the target without any adaptation. Empirical methods aim to learn domain invariant representations <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> but there are no theoretical guarantees for these methods <ref type="bibr" target="#b10">[11]</ref>. These methods require additional heuristics <ref type="bibr" target="#b36">[37]</ref>, and work well on some tasks but not others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>. Our work suggests that the structure from gradual shifts, which appears often in applications, can be a way to build theory and algorithms for regimes where the source and target are very different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs for Section 3</head><p>Restatement of Example 3.1. Even under the α * -separation, no label shift, gradual shift, and bounded data assumptions, there exists distributions P 0 , P 1 , P 2 and a source model θ ∈ Θ R that gets 0 loss on the source (L r (θ, P 0 ) = 0), but high loss on the target: L r (θ, P 2 ) = 1. Self-training directly on the target does not help: L r (ST(θ, P 2 ), P 2 ) = 1. This holds true even if every domain is separable, so α * = 0.</p><p>Proof. We construct an example in 2-D, where we consider the set of regularized linear models Θ R , where R = 1. Such a classifier is parametrized by</p><formula xml:id="formula_32">(w, b) where w ∈ R 2 with ||w|| 2 ≤ 1, and b ∈ R. The output of the model is M w,b (x) = w T x + b, and the predicted label is sign(w T x + b).</formula><p>We first define the source distribution P 0 :</p><formula xml:id="formula_33">P 0 (X = (1, 1) ∧ Y = 1) = 0.5 (20) P 0 (X = (−1, −1) ∧ Y = −1) = 0.5<label>(21)</label></formula><p>Consider the source classifier w 0 = (0, 1). The classifier classifies all examples correctly, in particular sign(w T 0 (1, 1)) = 1, and sign(w T 0 (−1, −1)) = −1. In addition, the ramp loss is 0, that is:</p><formula xml:id="formula_34">E X,Y ∼P 0 [r(Y (w T 0 X))] = 0<label>(22)</label></formula><p>We now construct distributions P 1 and P 2 :</p><formula xml:id="formula_35">P 1 (X = (1, 1/3) ∧ Y = 1) = 0.5<label>(23)</label></formula><formula xml:id="formula_36">P 1 (X = (−1, −1/3) ∧ Y = −1) = 0.5<label>(24)</label></formula><formula xml:id="formula_37">P 2 (X = (1, −1/3) ∧ Y = 1) = 0.5<label>(25)</label></formula><formula xml:id="formula_38">P 2 (X = (−1, 1/3) ∧ Y = −1) = 0.5<label>(26)</label></formula><p>Basically, the second-coordinate starts at 1 and decreases over time when the label is Y = 1, and starts at −1 and increases over time when the label is Y = −1. We note that ρ(P 0 , P 1 ) = ρ(P 1 , P 2 ) = 2 3 ≤ 1 R . Now, w 0 , b 0 classifies everything incorrectly in P 2 . sign(w T 0 (1, −1/3)) = −1, and sign(w T 0 (−1, 1/3)) = 1 but the corresponding labels in P 2 are 1 and −1 respectively. Accordingly, the ramp loss L r (M w 0 ,b 0 , P 2 ) = 1.</p><p>Self-traning on P 2 cannot fix the problem. w 0 , b 0 gets every example incorrect, so all the pseudolabels are incorrect. In particular, let Y be the pseudolabels produced using w 0 , b 0 -we have, Y | [X = (1, −1/3)] = −1 and Y | [X = (−1, 1/3)] = 1. Self-training on this is now a convex optimization problem, which attains 0 loss, for example using the classifier w = (−1, 0), b = 0, but any such classifier also gets all the examples incorrect. Note that the max-margin classifier on the source also exhibits the same issue (that is, it can get all the examples wrong after the dataset shift), from a simple extension of this example.</p><p>Finally, the classifier w * = (1, 0), b * = 0, gets every label correct in all distributions, P 0 , P 1 , P 2 .</p><p>Restatement of Theorem 3.2. Given P, Q with ρ(P, Q) = ρ &lt; 1 R and marginals on Y are the same so P (Y ) = Q(Y ). Suppose P, Q satisfy the bounded data assumption, and we have initial model θ, and n unlabeled samples S from Q, and we set θ = ST(θ, S). Then with probability at least 1 − δ over the sampling of S, letting α * = min θ * ∈Θ R L r (θ * , Q):</p><formula xml:id="formula_39">L r (θ , Q) ≤ 2 1 − ρR L r (θ, P ) + α * + 4BR + 2 log 2/δ √ n<label>(27)</label></formula><p>We begin by stating and proving some lemmas that formalize the proof outline in the main paper. We begin with a standard lemma that says if we learn a regularized linear classifier from n labeled examples from a distribution P , then the classifier is almost as good as the optimal regularized linear classifier on P , and the classifier gets closer to optimal as n increases. We bound the error of the classifier using the Rademacher complexity of regularized linear models Θ R . Then with probability at least 1 − δ,</p><formula xml:id="formula_40">L r (f ) − L r (f * ) ≤ 4BR + 2 log 2/δ √ n<label>(30)</label></formula><p>Proof. We begin with a standard bound (see e.g. Theorem 9, page 70 in <ref type="bibr" target="#b46">[47]</ref>), where the generalization error on the left is bounded by the Rademacher complexity:</p><formula xml:id="formula_41">L r (f ) − L r (f * ) ≤ 4R n (A) + 2 log 2/δ n<label>(31)</label></formula><p>Here, A = {(x, y) → r (M θ (x), y) : θ ∈ Θ} is the composition of the loss with the set of regularized linear models, and R n is the Rademacher complexity. It now suffices to bound R n .</p><p>We first use Talagrand's lemma, which says that if φ : R → R + is an L-Lipschitz function (that is, |φ(b) − φ(a)| ≤ L|b − a| for all a, b), then:</p><formula xml:id="formula_42">R n (φ • F ) ≤ LR n (F )<label>(32)</label></formula><p>In our case, we let F = {(x, y) → yM θ (x) : θ ∈ Θ}, in which case A = r • F where r is the ramp loss. The Lipschitz constant of the ramp loss r is 1, so R n (A) ≤ R n (F ).</p><p>Finally, we need to bound R n (F ), the Rademacher complexity of 2 -regularized linear models. This is a standard argument (e.g. see Theorem 11, page 82 in <ref type="bibr" target="#b46">[47]</ref>) and we get:</p><formula xml:id="formula_43">R n (F ) ≤ BR √ n<label>(33)</label></formula><p>The next lemma shows that the error (0-1 loss) of M θ is low on Q, even though the margin loss may be high. Intuitively, M θ classifies most points in P correctly with geoemtric margin 1 R , so after a small distribution shift &lt; 1 R , these points are still correctly classified since the margin acts as a 'buffer' protecting us from misclassification. Intuitively, if the ramp loss for a regularized linear model is low, then most points are classified correctly with high geometric margin (distance to decision boundary). Formally, we first show (using basically Markov's inequality) that P (Y (w T X + b) ≤ ρR) ≤ 1 1−ρR L r (θ, P ), where we recall that r : R → [0, 1] is the ramp loss which is bounded between 0 and 1:</p><formula xml:id="formula_44">L r (θ, P ) = E X,Y ∼P [r(Y (w T X + b))] ≥ E X,Y ∼P [r(Y (w T X + b))I Y (w T X+b)≤ρR ] ≥ E X,Y ∼P [(1 − ρR)I Y (w T X+b)≤ρR ] = (1 − ρR)P (Y (w T X + b) ≤ ρR)</formula><p>Here, the inequality on the third line follows because if Y (w T X + b) ≤ ρR where 0 &lt; ρR ≤ 1, then r(Y (w T X + b)) ≥ 1 − ρR, from the definition of the ramp loss. This gives us:</p><formula xml:id="formula_45">P (Y (w T X + b) ≤ ρR) ≤ 1 1 − ρR L r (θ, P )<label>(34)</label></formula><p>The high level intuition of the next step is that since the shift is small, only points x, y with y(w T x + b) ≤ ρR can be misclassified after the distribution shift, and from the previous step since there aren't too many of these the error of θ on Q is small.</p><p>Formally, fix &gt; 0 with ρ + &lt; 1 R , and let f y : R d → R d be a mapping such that for all measurable</p><formula xml:id="formula_46">A ⊆ R d , P (f −1 (A) | Y = y) = Q(A|Y = y), with sup x∈R d ||f y (x) − x|| 2 ≤ ρ + for y ∈ {−1, 1} 1 ,</formula><p>then we have:</p><formula xml:id="formula_47">Err(θ, Q) = Q(Y = sign(w T (X + b))) = Q(Y (w T (X + b)) ≤ 0) = Q(Y = 1)Q(w T X + b ≤ 0 | Y = 1) + Q(Y = −1)Q(w T X + b ≥ 0 | Y = −1) = P (Y = 1)P (w T f 1 (X) + b ≤ 0 | Y = 1) + P (Y = −1)P (w T f −1 (X) + b ≥ 0 | Y = −1) ≤ P (Y = 1)P (w T X + b ≤ (ρ + )R | Y = 1) + P (Y = −1)P (w T X + b ≥ −(ρ + )R | Y = −1) = P (Y (w T X + b) ≤ (ρ + )R)</formula><p>Where the inequality follows from Cauchy-Schwarz:</p><formula xml:id="formula_48">|w T X − w T f y (X)| = |w T (X − f y (X))| ≤ ||w|| 2 ||X − f y (X)|| 2 ≤ R(ρ + )</formula><p>Combining this with Equation <ref type="formula" target="#formula_3">(34)</ref>, this gives us:</p><formula xml:id="formula_49">Err(θ, Q) ≤ 1 1 − (ρ + )R L r (θ, P )<label>(35)</label></formula><p>Since &gt; 0 was arbitrary, by taking the infimum over all &gt; 0, we get:</p><formula xml:id="formula_50">Err(θ, Q) ≤ 1 1 − ρR L r (θ, P )<label>(36)</label></formula><p>Which was what we wanted to show.</p><p>From the previous lemma, M θ has low error on Q, or in other words only occasionally mislabels examples from Q. The next lemma says that if we minimize the ramp loss on a distribution where the points are only occasionally mislabeled, then we learn a classifier with low (good) ramp loss as well.</p><p>Lemma A.3. Given random variables X, Y, Y (defined on the same measure space) with joint distribution P , where X denotes the distribution over inputs, and Y, Y denote distinct distributions over labels. If P (Y = Y ) ≤ β then for any θ, L r (θ, P X P Y |X ) ≤ L r (θ, P X P Y |X ) + β. Here P X P Y |X denotes the distribution where the input X is sampled from P X and then the label is sampled from P Y |X .</p><p>Proof. Let θ = (w, b). The proof is by algebra, where we recall that r : R → [0, 1] is the ramp loss which is bounded between 0 and 1:</p><formula xml:id="formula_51">L r (θ, P X P Y |X ) = E r Y (w T X + b) = E r Y (w T X + b) I Y =Y + E r Y (w T X + b) I Y =Y ≤ E r Y (w T X + b) I Y =Y + E I Y =Y = E r Y (w T X + b) I Y =Y + β = E r Y (w T X + b) I Y =Y + β ≤ E r Y (w T X + b) + β = L r (θ, P X P Y |X ) + β</formula><p>Proof of Theorem 3.2. We begin by noting that there is some θ * ∈ Θ R that gets low loss α * on Q:</p><formula xml:id="formula_52">L r (M θ * , Q) = α * = min θ * ∈Θ R L r (M θ * , Q)<label>(37)</label></formula><p>In self-training, we do not have access to labels from Q so we use M θ to pseudolabel examples X from Q, so let w, b = θ and let Y | X = sign(w T X + b) be the pseudolabel distribution Q Y |X .</p><p>However, our pseudolabels are mostly correct. That is, let β = 2 1−ρR L r (M θ , P ). Since the conditions of Lemma A.2 are satisfied, Err(M θ , Q) ≤ β. This means that the pseudolabels from M θ and the true labels on Q mostly agree: Q(Y = Y ) ≤ β. So by Lemma A.3, θ * , which attained low loss α * on Q, also does fairly well on the pseudolabeled distribution Q X Q Y |X , which denotes the distribution where the input X is sampled from Q X and then the label is sampled from Q Y |X :</p><formula xml:id="formula_53">L r (M θ * , Q X Q Y |X ) ≤ L r (M θ * , Q) + β ≤ α * + β<label>(38)</label></formula><p>Since we have n examples from Q X Q Y |X , from Lemma A.1 the empirical risk minimizer θ on the n examples satisfies:</p><formula xml:id="formula_54">L r (θ , Q X Q Y |X ) ≤ min θ∈Θ R L r (θ, Q X Q Y |X ) + 4BR + 2 log 2/δ √ n<label>(39)</label></formula><p>But minimizing the loss on Q X Q Y |X explicitly gives us a lower loss than θ * gets on Q X Q Y |X (recall that θ * is the minimizer of the loss on Q which is different):</p><formula xml:id="formula_55">min θ∈Θ R L r (θ, Q X Q Y |X ) ≤ L r (M θ * , Q X Q Y |X ) ≤ α * + β<label>(40)</label></formula><p>Combining Equations <ref type="formula" target="#formula_3">(39)</ref> and <ref type="formula" target="#formula_4">(40)</ref>, we get:</p><formula xml:id="formula_56">L r (θ , Q X Q Y |X ) ≤ α * + β + 4BR + 2 log 2/δ √ n<label>(41)</label></formula><p>This bounds the ramp loss of θ on the pseudolabeled distribution Q X Q Y |X -to convert this back to Q we apply Lemma A.3 again which we can since Q(Y = Y ) ≤ β, which gives us:</p><formula xml:id="formula_57">L r (θ , Q) ≤ α * + 2β + 4BR + 2 log 2/δ √ n<label>(42)</label></formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restatement of Corollary 3.3.</head><p>Under the α * -separation, no label shift, gradual shift, and bounded data assumptions, if the source model θ 0 has low loss α 0 ≥ α * on P 0 (i.e. L r (θ 0 , P 0 ) ≤ α 0 ) and θ is the result of gradual self-training: θ = ST(θ 0 , (S 1 , . . . , S n )), letting β = 2 1−ρR :</p><formula xml:id="formula_58">L r (θ, P T ) ≤ β T +1 α 0 + 4BR + 2 log 2T /δ √ n .<label>(43)</label></formula><p>Proof. We begin with a classifier with loss α 0 . Applying Theorem 3.2 for each subsequent step of self-training, letting β = 2 1−ρR , we get:</p><formula xml:id="formula_59">L r (M θ i+1 , P i+1 ) ≤βL r (M θ i , P i ) + α * + 4BR + 2 log 2T /δ √ n<label>(44)</label></formula><p>Expanding, this becomes the sum of a geometric series. Noting that α * ≤ α 0 , by using the formula for the sum of geometric series, we get:</p><formula xml:id="formula_60">L r (M θ T , P T ) ≤ β T +1 α 0 + 4BR + 2 log 2T /δ √ n<label>(45)</label></formula><p>Restatement of Example 3.4. Even under the α * -separation, no label shift, gradual shift, and bounded data assumptions, given 0 &lt; α 0 ≤ 1 4 , for every T there exists distributions P 0 , . . . , P 2T , and θ 0 ∈ Θ R with L r (θ 0 , P 0 ) ≤ α 0 , but if θ = ST(θ 0 , (P 1 , . . . , P 2T )) then L r (θ , P 2T ) ≥ min(0.5, 1 2 2 T α 0 ). Note that L r is always in [0, 1].</p><p>Proof. The construction works even in 1-D. We will consider regularized linear models Θ R with R = 1, so ρ &lt; 1 R . Such a model in 1D can be parametrized by 2 parameters, w, b ∈ R with |w| ≤ 1, where the output of the linear model for an input x ∈ R is wx + b, and the label is sign(wx + b).</p><p>First we give intuition, and then we dive into the formal details of the construction.</p><p>We start with a classifier θ 0 = (w 0 , b 0 ) = (1, 0). We will construct the distributions so that the classifier θ t = θ 0 for all t, that is, gradual self-training will not update the classifier. In the initial distribution P 0 , all the negative examples will be located at x = −10, so the classifier gets them correct and incurs 0 loss on them. α 0 fraction of the positive examples will be at x = −0.1, these examples are misclassified so the classifier incurs loss α 0 . The rest of the positive examples will be at x = 1, and the classifier incurs 0 loss on them.</p><p>In distribution P 1 , 0.5α 0 fraction of the positive examples will move from x = 1 to x = 0.5, but everything else stays the same as in P 0 . After pseudolabeling and self-training, the classifier still stays the same, that is θ 1 = θ 0 . This is because the α 0 fraction of examples at x = −0.1 will be pseudolabeled negative, the 0.5α 0 fraction of examples at x = 0.5 pseudolabeled positive, and the remaining positive examples at x = 1 will be pseudolabeled positive. Training on this pseudolabeled distribution gives us θ 1 = (w 1 , b 1 ) = (1, 0) as the optimal parameters.</p><p>In P 2 , the 0.5α 0 fracton of points at x = 0.5 moves to x = −0.1. After pseudolabeling and selftraining, we still get θ 2 = θ 0 . At this point the classifier incurs loss 1.5α 0 . We repeat this process, except for P 3 , α 0 fraction of the positive examples move from x = 1 to x = 0.5, and then the next time in P 5 , 2α 0 fraction of the positive examples move from x = 1 to x = 0.5, etc. So in this way the loss grows exponentially.</p><p>We now give the formal construction, which works even in just 1 dimension. First, we choose S to be the maximum integer such that <ref type="bibr">(</ref></p><formula xml:id="formula_61">2 S−1 + 1 2 )α 0 &lt; 1 2 . We have S ≥ 1, because (2 1−1 + 1 2 )α 0 = 3 2 α 0 ≤ 3 2 1 4 &lt; 1 2 .</formula><p>We now define a sequence of weights, which represents the fraction of points we move in each step as in the sketch above. For 0 ≤ i ≤ S − 1, let w i = 1 2 2 i α 0 , and let w S = 1 2 − (2 S−1 + 1 2 )α 0 . From the sum of geometric series, we can verify that each of these weights are positive, and the weights sum up to <ref type="bibr">1 2</ref> .</p><p>We now define the distributions at each step, we case on whether the step is odd or even since as in the above high level sketch, it takes 2 steps to move a point from x = 1 across to the other side of the decision boundary. One subtlety is that unlike the sketch above, since we use the Monge form of the Wasserstein distance, we cannot have all the points exactly at x = 1 but keep them separated by a small distance δ = 1 10S . This is a technical detail, so on a first reading the reader may just pretend δ = 0 to work through the structure of the proof.</p><p>(Odd case) For 0 ≤ t &lt; min(T, S + 1), P 2t+1 is given by:</p><formula xml:id="formula_62">P 2t+1 (x = −10 ∧ Y = −1) = 0.5 (46) P 2t+1 (x = −0.1 ∧ Y = 1) = α 0 + t−1 i=0 w i (47) P 2t+1 (x = 0.5 ∧ Y = 1) = w t (48) P 2t+1 (x = 1 + iδ ∧ Y = 1) = w i ∀t &lt; i ≤ S<label>(49)</label></formula><p>(Even case) For 0 ≤ t ≤ min(T, S + 1), P 2t is given by:</p><formula xml:id="formula_63">P 2t (x = −10 ∧ Y = −1) = 0.5 (50) P 2t (x = −0.1 ∧ Y = 1) = α 0 + t−1 i=0 w i (51) P 2t (x = 1 + iδ ∧ Y = 1) = w i ∀t ≤ i ≤ S<label>(52)</label></formula><p>If T ≥ S + 1, then for 2S + 2 ≤ i ≤ 2T , we set P i = P 2S+2 (by this step the classifier will have reached ramp loss and error 0.5).</p><p>We can check that if t &lt; 2S, then the classifier obtained from gradual self-training is w t = (1, 0) (the classifier does not change after self-training). When t = 2S, w t = (1, −0.9), and finally if 2S &lt; t then w t = (1, −1.5). The edge case is because at the end all positive points are to the left of the classifier, so the classifier moves to the right.</p><p>Next we examine the loss values. If t ≤ S, the fraction of examples the classifier w 2t gets wrong on P 2t is:</p><formula xml:id="formula_64">α 0 + t−1 i=0 w i = (2 t−1 + 1 2 )α 0 ≥ 1 2 2 t α 0 (53)</formula><p>If t &gt; S, the fraction of examples the classifier w 2t gets wrong on P 2t is 0.5. The ramp loss is bounded below by the error rate, which means:</p><formula xml:id="formula_65">L r (θ , P 2T ) ≥ min(0.5, 1 2 2 T α 0 )<label>(54)</label></formula><p>As desired.</p><p>We can verify that for every i, W ∞ (P i , P i+1 ) ≤ 0.6 &lt; 1 = 1 R , so these distributions satisfy the gradual shift assumption. P i (Y = 1) = P i (Y = −1) = 0.5 for all i, so the distributions satisfy the no label shift assumption. The classifier (w, b) = (1, 5) gets 0 loss on all P i , so the distributions satisfy the α * -separation assumption with α * = 0. Finally, the data is all bounded in a constant region, between x = −10 and x = 2, so the distributions satisfy the bounded data assumption. Proof. The proof is straightforward: scaling up the parameters of the original model θ gives us a θ that gets 0 loss (ramp or hinge) on the pseudolabeled distribution, but does not change the model predictions. For simplicity, we focus on the ramp loss but the proof applies to the hinge loss as well.</p><formula xml:id="formula_66">Suppose θ = (w, b), where w ∈ R d and b ∈ R.</formula><p>We choose our new parameters to be θ = (αw, αb), where α ≥ 1 is a scaling factor we will choose.</p><p>Then we can write L(θ ), the loss of θ on the pseudolabeled examples S as:</p><formula xml:id="formula_67">L(θ ) = 1 |S| x∈S r (M θ (x), sign(M θ (x))) = 1 |S| x∈S r(sign(w T x + b)(αw T x + αb)) = 1 |S| x∈S r(α|w T x + b|)<label>(55)</label></formula><p>Now, we can choose large enough α so that the term inside the r in the last line above is always ≥ 1:</p><formula xml:id="formula_68">α = 1 min x∈S |w T x + b| (56) So now, |(w ) T x + b | = α|(w T x + b)| ≥ 1 for all x ∈ S.</formula><p>This gives us that L(θ ) = 0, since r(m) = 0 for m ≥ 1. Note that this is true for the hinge loss as well, h(m) = 0 for m ≥ 1. Since L is bounded below by 0, θ is a minimizer of the loss on the pseudolabeled distribution (which is what self-training minimizes, see Equation <ref type="formula" target="#formula_3">(3)</ref>).</p><p>Since θ is just a scaled up version of θ, it does not change the predictions:</p><formula xml:id="formula_69">sign(αw T x + αb) = sign(w T x + b)<label>(57)</label></formula><p>Restatement of Example 3.6. For all θ ∈ Θ, θ is a minimizer of L σ,θ , that is, for all θ ∈ Θ, L σ,θ (θ) ≤ L σ,θ (θ ).</p><p>Proof. The reason for this is that the logistic loss is a proper scoring loss-if we fix p, the loss of ll(p, p ) is minimized when p = p. That is, if 0 ≤ p, p ≤ 1:</p><formula xml:id="formula_70">ll(p, p) ≤ ll(p, p )<label>(58)</label></formula><p>So we have:</p><formula xml:id="formula_71">L σ,θ (θ ) = E [ll(σ(M θ (X)), σ(M θ (X)))] ≥ E [ll(σ(M θ (X)), σ(M θ (X)))] = L σ,θ (θ)<label>(59)</label></formula><p>Restatement of Example 3.7. Even under the α * -separation, no label shift, and gradual shift assumptions, given α 0 &gt; 0, there exists distributions P 0 , P 1 , P 2 and θ 0 ∈ Θ R with L h (θ 0 , P 0 ) ≤ α, but if θ = ST(θ 0 , (P 1 , P 2 )) then L h (θ , P 2 ) ≥ Err(θ , P 2 ) = 1 (θ gets every example in P 2 wrong), where we use the hinge loss in self-training.</p><p>Proof. We construct an example in 2D. We consider the set of regularized linear models Θ R , where R = 1. Such a classifier is parametrized by (w, b) where w ∈ R 2 with ||w|| 2 ≤ 1, and b ∈ R. The output of the model is M w,b (x) = w T x + b, and the predicted label is sign(w T x + b).</p><p>Set α 0 = min( 1 2 , 2α 3 ). We will construct an example where the initial hinge error is ≤ α 0 , but it increases to over 1 and gets every example wrong, in 2 distribution shifts, even though there exists a single classifier with 0 hinge loss across all the distributions. Let w 0 = (1, 0) and b 0 = 0. Consider a distribution Q δ , for δ ∈ R, defined as follows:</p><formula xml:id="formula_72">Q δ (Y = 1 ∧ X = (δ, 1)) = 1 − α 0 2 [Point 1] Q δ (Y = 1 ∧ X = (− 1 2 , 1 − α 0 α 0 )) = α 0 2 [Point 2] Q δ (Y = −1 ∧ X = (−δ, −1)) = 1 − α 0 2 [Point 3] Q δ (Y = −1 ∧ X = ( 1 2 , − 1 − α 0 α 0 )) = α 0 2 [Point 4]</formula><p>We will set P 0 = Q 1 , P 1 = Q 1/3 , and P 2 = Q −1/3 . First, we note that the Wasserstein-infinity distance between any consecutive one of these is at most 2/3 &lt; 1.</p><p>Next, we can verify that L h (w 0 , P 0 ) = 3 2 α 0 ≤ α. In particular, w 0 gets points 2 and 4 incorrect, and points 1 and 3 correct with margin 1. Computing the expectation of the loss, we get 3 2 α 0 .</p><p>Now the algorithm self-trains on P 1 : w 0 pseudolabels points 1 and 4 positive (y = 1), and pseudolabels points 2 and 3 negative (y = −1), again getting points 2 and 4 incorrect. From the KKT conditions, we can verify that the minimizer of the hinge loss on these pseudolabeled points is w 1 = w 0 , and b 2 = 0.</p><p>Finally, the algorithm self-trains on P 2 : here w 0 pseudolabels points 3 and 4 positive, and 1 and 2 negative. That is, it gets all the examples wrong. Self-training on these pseudolabels, the model still gets every example wrong (one solution is w 2 = (0, −1) and b 2 = 0). So Err(w 2 , P 2 ) = 1, and the hinge loss is lower bounded by the error with L h (w 2 , P 2 ) ≥ Err(w 2 , P 2 ).</p><p>On the other hand, the classifier w * = (0, 1) and b * = 0, gets hinge loss 0 on P 1 , P 2 , P 3 .</p><p>Restatement of Proposition 3.8. Given α 0 &gt; 0, distributions P 0 = = P T , and model θ 0 ∈ Θ R with L r (θ 0 , P 0 ) ≤ α 0 , L r (θ , P T ) ≤ α 0 (T + 1) where θ = ST(θ 0 , (P 1 , . . . , P T ))</p><p>We give intuition for our argument, and then dive into the formal proof. Suppose we start out with a model that has ramp loss α 0 on P = P 0 = · · · = P T . After a single step of self-training, the loss can increase to 2α 0 on P . So a naive argument leads to an exponential bound (since the loss is now 2α 0 , it can increase to 2 · 2α 0 after another round of self-training, etc, so after T steps the loss on P is bounded by 2 T α 0 ). Showing a linear upper bound requires a more subtle argument that tracks some other invariants, and not just the loss value.</p><p>Roughly speaking, if the initial loss is below α 0 , there cannot be more than α 0 fraction of points near the decision boundary. We show that this invariant is maintained by self-training: the 'number' of points near the decision boundary decreases, so it always stays below the initial value α 0 . Finally, we show that if there are α 0 points near the decision boundary, then self-training cannot increase the loss by more than α 0 no matter what the current loss is. This shows that at each step the loss can only increase by α 0 . Compare this with Example 3.4, where we do have distribution shift-in this case the 'number' of points near the decision boundary can keep increasing which can lead to an exponential growth in the loss.</p><p>We now dive into the formal proof-we begin by making some definitions and stating and proving lemmas that formalize the above intuition.</p><p>In self-training, we pseudolabel an example x with label sign(M θ (x)). We define the corresponding distribution on the pseudolabels P Y |x,θ by Y | x, θ = sign(M θ (x)).</p><p>Recall that the loss of θ on labeled data is (where r is the ramp loss):</p><formula xml:id="formula_73">L r (θ, P ) = E X,Y ∼P [ r (M θ (X), Y )] = E X,Y ∼P [r(Y M θ (X))]<label>(60)</label></formula><p>We define a loss on unlabeled data which corresponds to the loss of θ if every example was labeled by M θ . This roughly corresponds to the 'number' of points near the decision boundary, since points far from the decision boundary incur 0 loss, but points near the decision boundary incur a loss between 0 and 1. Note that the unlabeled loss does not use the labels Y . Letting P X denote the marginal distribution of P on X, and P X P Y |X denote the distribution where X is sampled from P X and Y is sampled from P Y |X , the unlabeled loss U r is:</p><formula xml:id="formula_74">U r (θ, P ) = L r (θ, P X P Y |X,θ ) = E X∼P [ r (M θ (X), sign(M θ (X)))] = E X∼P [r(|M θ (X)|)]<label>(61)</label></formula><p>The unlabeled loss U r and labeled loss L r are always defined since the ramp loss is bounded below by 0. A straightforward lemma shows that the unlabeled loss lower bounds the labeled loss.</p><p>Lemma A.4 (Lower bounds labeled loss). The unlabeled loss lower bounds the labeled loss:</p><formula xml:id="formula_75">U r (θ, P ) ≤ L r (θ, P ). Proof. Since Y ∈ {−1, 1}, |M θ (X)| = |Y M θ (X)| ≥ Y M θ (X)<label>(62)</label></formula><p>Now, since r is a non-increasing function, we have:</p><formula xml:id="formula_76">r(|M θ (X)|) ≤ r(Y M θ (X))<label>(63)</label></formula><p>Taking expectations on both sides:</p><formula xml:id="formula_77">U r (θ, P ) ≤ L r (θ, P )<label>(64)</label></formula><p>The next lemma shows that each step of self-training decreases the unlabeled loss.</p><p>Lemma A.5 (Unlabeled loss decreases). If θ, θ ∈ Θ and θ = ST(θ, P ), then U r (θ , P ) ≤ U r (θ, P ).</p><p>Proof. Since the unlabeled loss does not depend on the labels, we have:</p><formula xml:id="formula_78">U r (θ , P ) = U r (θ , P X P Y |X,θ )<label>(65)</label></formula><p>From Lemma A.4, the unlabeled loss lower bounds the labeled loss:</p><formula xml:id="formula_79">U r (θ , P X P Y |X,θ ) ≤ L r (θ , P X P Y |X,θ )<label>(66)</label></formula><p>But P X P Y |X,θ is the distribution of pseudolabels produced by model θ, which is exactly what self-training (θ = ST(θ, P )) minimizes (recall the definition of self-training in Equation <ref type="formula" target="#formula_4">(4)</ref>), so θ has lower loss than θ on the pseudolabeled distribution:</p><formula xml:id="formula_80">L r (θ , P X P Y |X,θ ) ≤ L r (θ, P X P Y |X,θ ) = U r (θ, P )<label>(67)</label></formula><p>Which means that:</p><formula xml:id="formula_81">U r (θ , P ) ≤ U r (θ, P )<label>(68)</label></formula><p>We now show a type of triangle inequality for the loss, which says that the loss of θ on P is upper bounded by the loss of θ on pseudolabels from θ plus the loss of θ on P .</p><formula xml:id="formula_82">Lemma A.6 (Triangle Inequality). L r (θ , P ) ≤ L r (θ , P X P Y |X,θ ) + L r (θ, P )</formula><p>Proof. We will first show that for any x, y, θ, θ :</p><formula xml:id="formula_83">r (M θ (x), y) ≤ max( r (M θ (x), sign(M θ (x))), r (M θ (x), y))<label>(69)</label></formula><p>We can prove this by casing. If M θ (x) and M θ (x) have different signs, or M θ (x) and y have different signs, then the RHS is 1. But the ramp loss is bounded above by 1, so the LHS has loss at most 1, which makes this statement true. Otherwise, suppose M θ (x), M θ (x), and y all have the same signs-but then sign(M θ (x)) = y, so r (M θ (x), y) = r (M θ (x), sign(M θ (x))).</p><p>With this in hand, the result follows with some algebra:</p><formula xml:id="formula_84">L r (θ , P ) = E [ r (M θ (X), Y )] ≤ E [max( r (M θ (X), sign(M θ (X))), r (M θ (X), Y ))] ≤ E [ r (M θ (X), sign(M θ (X))) + r (M θ (X), Y )] =L r (θ , P X P Y |X,θ ) + L r (θ, P )<label>(70)</label></formula><p>Next, we show that if the unlabeled loss of θ is less than α, then self-training cannot increase the loss by more than α.</p><p>Lemma A.7 (Upper bounding loss growth). Suppose U r (θ, P ) ≤ α and let θ = ST(θ, P ). Then:</p><formula xml:id="formula_85">L r (θ , P ) ≤ L r (θ, P ) + α</formula><p>Proof. From Lemma A.6, it suffices to show that L r (θ , P X P Y |X,θ ) ≤ α. But as in Equation <ref type="formula" target="#formula_7">(67)</ref>, this is simply because θ minimizes the pseudolabeled loss so we have:</p><formula xml:id="formula_86">L r (θ , P X P Y |X,θ ) ≤ U r (θ, P ) ≤ α<label>(71)</label></formula><p>The proof of Proposition 3.8 now simply inductively applies Lemma A.5 and Lemma A.7.</p><p>Proof of Proposition 3.8. Let θ t = ST(θ t−1 , P t ) for 1 ≤ t ≤ T . The unlabeled loss lower bounds the labeled loss: that is, since L r (θ 0 , P 0 ) ≤ α 0 , from Lemma A.4, U r (θ 0 , P 0 ) ≤ α 0 . The unlabeled loss can only decrease with self-training: that is, inductively applying Lemma A.5, we get that for all t, U r (θ t , P t ) ≤ α 0 . Then from Lemma A.7, the loss can only increase by α 0 at each step of self-training, so L r (θ T , P T ) ≤ L r (θ 0 , P 0 ) + α 0 T ≤ α 0 (T + 1).</p><p>The next Example shows that even without distribution shift, self-training can increase the loss of a model from α 0 to nearly 2α 0 . Let δ = /3 and a = α 0 /(1 + δ). Let the data distribution P be given by:</p><formula xml:id="formula_87">P (X = −10 ∧ Y = −1) = 0.5<label>(72)</label></formula><p>P (X = 0 ∧ Y = 1) = a (73)</p><formula xml:id="formula_88">P (X = 1 ∧ Y = 1) = a − δ<label>(74)</label></formula><formula xml:id="formula_89">P (X = 10 ∧ Y = 1) = 0.5 − 2a + δ<label>(75)</label></formula><p>Note that the probabilities are all non-negative and add up to 1 and the data is bounded between x = −10 and x = 10.</p><p>Let the initial model be w 0 = 1 and b 0 = −δ. The initial loss is L r ((w 0 , b 0 ), P ) = a + (a − δ)δ = α 0 − δ 2 ≤ α 0 . We can check that after self-training, the updated parameters are w 1 = 1 and b 1 = 1. The final loss is L r <ref type="figure" target="#fig_1">((w 1 , b 1</ref> </p><formula xml:id="formula_90">), P ) = 2a − δ ≥ 2α 0 (1 − δ) − δ ≥ 2α 0 − 3δ = 2α 0 − .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs for Section 4</head><p>We prove Theorem 4.1 in Section 3, following the sketch described in the paper. Our first lemma shows that if µ does not change too much, then the optimal parameters w * (µ) do not change too much either.</p><formula xml:id="formula_91">Lemma B.1. w * is 1 B -Lipschitz, that is if ||µ|| 2 , ||µ || 2 ≥ B &gt; 0, then: ||w * (µ ) − w * (µ)|| 2 ≤ 1 B ||µ − µ|| 2<label>(76)</label></formula><p>Proof. Recall that w * (µ) = µ/||µ|| 2 , which is well defined since ||µ|| 2 &gt; 0. We will first prove that if ||v || 2 ≥ ||v|| 2 = 1, then the claim holds, that is:</p><formula xml:id="formula_92">|| v ||v || 2 − v|| 2 2 ≤ ||v − v|| 2 2<label>(77)</label></formula><p>Expanding both sides, this is equivalent to showing:</p><formula xml:id="formula_93">1 + ||v|| 2 2 − 2v T v ||v || 2 ≤ ||v || 2 2 + ||v|| 2 2 − 2v T v<label>(78)</label></formula><p>Subtracting both sides by ||v|| 2 2 , it suffices to show:</p><formula xml:id="formula_94">1 − 2v T v ||v || 2 ≤ ||v || 2 2 − 2v T v<label>(79)</label></formula><p>But since ||v || ≥ 1, we can bound the LHS above if we multiply by ||v || 2 :</p><formula xml:id="formula_95">1 − 2v T v ||v || 2 ≤ ||v || 2 − 2v T v ≤ ||v || 2 2 − 2v T v<label>(80)</label></formula><p>So Equation <ref type="formula" target="#formula_8">(77)</ref> is true. Now we prove the main claim. Without loss of generality, suppose ||µ || 2 ≥ ||µ|| 2 , otherwise we can swap µ and µ . Then we can scale µ and reduce to the previous case:</p><formula xml:id="formula_96">||w * (µ ) − w * (µ)|| 2 = || µ ||µ || 2 − µ ||µ|| 2 || 2 = || µ /||µ|| 2 ||µ || 2 /||µ|| 2 − µ ||µ|| 2 || 2 = || (µ /||µ|| 2 ) ||(µ /||µ|| 2 )|| 2 − µ ||µ|| 2 || 2 ≤ || µ ||µ|| 2 − µ ||µ|| 2 || 2 = 1 ||µ|| 2 ||µ − µ|| 2 ≤ 1 B ||µ − µ|| 2<label>(81)</label></formula><p>Where in the inequality on the 4th line we applied Equation (77). This completes the proof.</p><p>We now state a standard lemma in measure theory, which says that if f (x) ≥ g(x) for all x, and the inequality is strict on a set of non-zero measure (volume), then the integral of f is strictly greater than the integral of g.</p><formula xml:id="formula_97">Lemma B.2. Let µ be a measure on R d , and C be measurable with µ(C) &gt; 0. Suppose f (x) &gt; g(x) if x ∈ C, and f (x) ≥ g(x)</formula><p>for all x ∈ R d , where f and g are measurable functions with finite integrals. Then:</p><formula xml:id="formula_98">R d f (X)dµ &gt; R d g(X)dµ<label>(82)</label></formula><p>Our next lemma is the key step of the proof. We show that w * (µ) is a strict local minimizer of U (w, P µ,σ ), that is it has lower loss than any other w nearby.</p><p>Lemma B.3. For all w ∈ R d with ||w|| 2 ≤ 1 and ||w − w * (µ)|| 2 &lt; 1, with w = w * (µ), we have:</p><formula xml:id="formula_99">U (w * (µ), P µ,σ ) &lt; U (w, P µ,σ )<label>(83)</label></formula><p>Proof. Denote w * (µ) as w * . By Cauchy-Schwarz, since w * 2 = 1 and w − w * 2 &lt; 1, we have w · w * &gt; 0, and w 2 &gt; 0. This is because</p><formula xml:id="formula_100">w · w * = w * 2 + (w − w * ) · w * ≥ 1 − w − w * 2 w * 2 &gt; 0.</formula><p>Since the dot product is non-zero, neither vector can be 0.</p><p>We begin by noting that U (w, P µ,σ ) is well-defined and finite: because φ(|X|) is between 0 and 1 so the expectation is well-defined with finite, non-negative value.</p><p>Step 1 (Scaling Parameters): First, we show that scaling up the parameters decreases the loss: for any w and λ &gt; 1, U (λw, P µ,σ ) &lt; U (w, P µ,σ ).</p><p>Since φ is non-increasing, φ(|λw T x|) ≥ φ(|w T x|). Now, let C = {x ∈ R d : 0 &lt; λw T x &lt; 1}. Since φ is strictly decreasing on [0, 1], for x ∈ C, φ(|λw T x|) &lt; φ(|w T x|). P µ,σ (C) &gt; 0 (the Gaussian mixture distribution assigns positive probability to any set with non-zero volume / Lebesgue measure). Then from Lemma B.2:</p><formula xml:id="formula_101">E X∼Pµ,σ [φ(|λw T X|)] &lt; E X∼Pµ,σ [φ(|w T X|)]<label>(84)</label></formula><p>Which is precisely saying U (λw, P µ,σ ) &lt; U (w, P µ,σ ).</p><p>This lets us assume, without loss of generality, that ||w|| 2 = 1 since scaling up w strictly decreases the loss, and the theorem statement assumes ||w|| 2 ≤ 1.</p><p>Step 2 (Rotating parameters): Note that rotating the entire space does not change the loss values, formally if A is a rotation matrix then:</p><formula xml:id="formula_102">U (Aw, P Aµ,σ ) = U (w, P µ,σ )<label>(85)</label></formula><p>So without loss of generality, we rotate the setup so that w and w * lie on the XY plane (except for the first two coordinates, all coordinates are 0). Let v be the unit bisector of w and w * , given by v = (w + w * )/||w + w * || 2 . Without loss of generality, rotate the setup so that v is along the positive Y axis (the second coordinate is 1, and all other coordinates are 0), and the first two coordinates of w * are positive. Let µ = (r, s, 0) where 0 ∈ R d−2 , we then have that r, s &gt; 0 since w * and µ are in the same direction.</p><p>Step 3 (Symmetry argument): Now consider any point u = (x, y, z) with z ∈ R d−2 , with x, y &gt; 0. Consider its reflection point around v, u = (−x, y, z). Let ∆(w, w , u) = φ(|(w ) T u|) − φ(|w T u|) denote the increase in loss on x from using classifier w instead of w. Now, from the way we constructed u , w T u = (w * ) T u , and (w * ) T u = w T u . So ∆(w, w * , u) = −∆(w * , w, u ). That is, as per our sketch, the loss for u decreases when using w * instead of w, but increases for u when using w * instead of w, but the magnitudes of these two quantities are equal.</p><p>Next, we will show that the probability density is higher for u than u . Let p denote the density of P µ,σ . P µ,σ is the mixture of two Gaussians, so for normalizing constant k &gt; 0, we have:</p><formula xml:id="formula_103">p(u) = k exp − 1 2σ ((r − x) 2 + (s − y) 2 + z 2 ) + exp − 1 2σ ((r + x) 2 + (s + y) 2 + z 2 ) (86) p(u ) = k exp − 1 2σ ((r + x) 2 + (s − y) 2 + z 2 ) + exp − 1 2σ ((r − x) 2 + (s + y) 2 + z 2 )<label>(87)</label></formula><p>We now use strict convexity of exp(−x) to show that p(u) &gt; p(u ). Let a = (r − x) 2 + (s − y) 2 + z 2 , b = (r + x) 2 + (s + y) 2 + z 2 , c = (r + x) 2 − (r − x) 2 = 4rx. Since, x, y, r, s &gt; 0, we have 0 &lt; a &lt; b and 0 &lt; c &lt; b − a. Letting f (x) = exp(−x/(2σ)) we can rewrite the above probabilities as:</p><formula xml:id="formula_104">p(u) = k f (a) + f (b) (88) p(u ) = k f (a + c) + f (b − c)<label>(89)</label></formula><p>Finally, we use strict convexity of f (x) = exp(−x) to show the desired result. Since a &lt; a + c &lt; a + b, for some α ∈ (0, 1), we can write:</p><formula xml:id="formula_105">a + c = αa + (1 − α)b (90) b − c = (1 − α)a + αb<label>(91)</label></formula><p>Then, from strict convexity, we have:</p><formula xml:id="formula_106">f (a + c) &lt; αf (a) + (1 − α)f (b) (92) f (b − c) &lt; (1 − α)f (a) + αf (b)<label>(93)</label></formula><p>Adding both of these, we get:</p><formula xml:id="formula_107">f (a + c) + f (b − c) &lt; f (a) + f (b) (94)</formula><p>That is, we have shown p(u) &gt; p(u ).</p><p>The case when u = (−x, −y, z), where z ∈ R d−2 and x, y &gt; 0 is symmetric. We ignore points {(x, y, z) : x = 0 ∨ y = 0} since this has measure 0.</p><p>Step 4 (Expectation): We give intuition and then dive into the math. For every pair of points in our pairing in Step 3, the contribution to the loss of w * is at most as high as the contribution to the loss of w. So this trivially gives us L(w * , P µ,σ ) ≤ L(w, P µ,σ ), but we want a strict inequality. However, we can find a set of points with non-zero volume (Lebesgue measure) where the contribution to the loss for w * is strictly less than for w, which completes the proof.</p><p>Formally, letting S + = {(x, y, z) : z ∈ R d−2 , x &gt; 0, y &gt; 0}, we can write (where we defined ∆ in</p><p>Step 3):</p><formula xml:id="formula_108">L(w, P µ,σ ) − L(w * , P µ,σ ) = 2 S + p(u)∆(w * , w, u) + p(u )∆(w * , w, u )<label>(95)</label></formula><p>Where the 2 comes from the fact that the case when x, y &lt; 0 is symmetric and gives the same integral. Now, let C = {(x, y, z) : x &gt; 0, y &gt; 0, x 2 + y 2 ≤ 1, z ∈ R d−2 , w * } be a quarter cylinder.</p><p>The volume of C is &gt; 0, and C ⊆ S + . Further, for all x ∈ C, we have:</p><formula xml:id="formula_109">p(u)∆(w * , w, u) + p(u )∆(w * , w, u ) &gt; 0<label>(96)</label></formula><p>So applying Lemma B.2 again, we get:</p><formula xml:id="formula_110">L(w, P µ,σ ) − L(w * , P µ,σ ) &gt; 0<label>(97)</label></formula><p>Which completes the proof.</p><p>With these key lemmas, the proof of Theorem 4.1 is straightforward.</p><p>Restatement of Theorem 4.1. Assuming the Gaussian setting, if w 0 − w * (µ 0 ) 2 ≤ 1 4 , then we recover w T = w * (µ T ).</p><p>Proof. The proof reduces to showing the one-step case: for</p><formula xml:id="formula_111">0 &lt; t ≤ T , if w t−1 − w * (µ t−1 ) 2 ≤ 1 4 then w t = w * (µ t ),</formula><p>where the w t is selected according to Equation <ref type="bibr" target="#b18">(19)</ref>. Applying this one-step result inductively gives us the desired result, that w T = w * (µ T ).</p><p>For the one-step case, from Lemma B.1,</p><formula xml:id="formula_112">since ||µ t−1 || 2 , ||µ t || 2 ≥ B &gt; 0, ||w * (µ t ) − w * (µ t−1 )|| 2 ≤ 1 B ||µ t − µ t−1 || 2 ≤ 1 B B 4 = 1 4</formula><p>. Then by triangle inequality, since ||w t−1 − w * (µ t−1 )|| 2 ≤ 1 4 , we have ||w t−1 − w * (µ t )|| 2 ≤ 1 2 . Further, ||w * (µ t )|| 2 ≤ 1, and by Lemma B.3, any other w satisfying ||w t−1 − w|| 2 ≤ 1 2 &lt; 1, ||w|| 2 ≤ 1, and w = w * (µ t ) satisfies U (w * (µ t ), P µt,σt ) &lt; U (w, P µt,σt ). So w * (µ t ) is the unique minimizer in the constrained set, which means w t = w * (µ t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental details for Section 5</head><p>In this section, we provide additional experimental details, and give results for ablations for the experiments in Section 5.1. An advantage of gradual self-training is that it has a very small number of hyperparameters and we show that our findings are robust to different choices of these parameters-even if we do not do confidence thresholding, train every method for more iterations, and use a smaller window size, gradual self-training does better than self-training directly to the target and the other baselines. For reproducibility, we provide all code but we also describe our datasets and models here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Portraits:</head><p>A more realistic dataset where we do not control the structure of the shift, consisting of photos of high school seniors taken across many years. Additionally, there is label shift, that is the proportions of males and females, P(Y ), changes over time (see <ref type="figure">Figure 3</ref>), unlike our theory which assumes that the probability of each label stays constant. We use the first 2000 images as source images. We shuffle these, and use 1000 for training, and 1000 for validation. We use the next 14000 images as unlabeled intermediate examples. Finally, we use the next 2000 images as unseen target examples. We downsample the images to 32x32 but do no other preprocessing. We reserve images at the end of the dataset as held-out examples for future work, and so that we can test how the method extrapolates past the point we validate on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Algorithm and baselines</head><p>Next, we describe the gradual self-training algorithm and parameters in more detail. Algorithm 1 shows pseudocode for gradual self-training. filter low confidence filters out the α fraction of examples where the model is least confident, where confidence is measured as the maximum of the softmax output of the classifier. This filtering is standard in many instances of self-training <ref type="bibr" target="#b5">[6]</ref>. For the baselines-for target self-train, we self-train multiple times (iteratively) on the target. Each round of self-training uses the current model M to pseudolabel examples in the target, and then trains on these pseudolabeled examples. Specifically, to make comparisons fair we self-train |I|/W times on the target, so that the total number of self-training steps performed by the target self-train baseline and gradual self-training are the same. Similarly, when we self-train to all examples, we self-train multiple times on all unlabeled data, self-training |I|/W times. Here W is the window size in Algorithm 1 which is the number of examples in each intermediate domain.</p><p>Note that in the synthetic datasets (rotating MNIST and Gaussian) we ensure that target self-train gets access to the same number of unlabeled examples as gradual self-training does in total, to ensure that the improvements are not simply because gradual self-training consumes more unlabeled data (accumulated over all of the intermediate domains). For the real dataset (Portraits), we cannot generate additional examples for target self-train. However, this is why we also compare against self-training directly to all the unlabeled data, which gets access to exactly the same data that gradual self-training does but does not leverage the gradual structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Models and parameter settings</head><p>Next, we describe the models and parameter settings we used: 1. Models: For the Gaussian dataset we use a logistic regression classifier, with l2 regularization 0.02. For the MNIST and Portraits dataset, we use a 3 layer convolutional network. For each conv layer we use a filter size of 5x5, stride of 2x2, 32 output channels, and relu activation. We added dropout(0.5) after the final conv layer, and batchnorm after dropout. We flatten the final layer, and then apply a single linear layer to output logits (the number of logits is the number of classes in the dataset which is 10 for rotating MNIST and 2 for Portraits). We then take the softmax of the logits, and optimize the cross-entropy loss. We did not tune the model architecture for our experiments, however we checked that adding an extra layer, changing the number of output channels, and using a different architecture with an extra fully connected layer on top, have little impact on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parameters:</head><p>For the window size, we use W = 500 for the Gaussian dataset, and W = 2000 for the rotating MNIST and Portraits dataset. We use a smaller window for the Gaussian dataset because the data is lower dimensional and we have less unlabeled data. We train the model for 10 epochs, 20 epochs, and 100 epochs in each round for the rotating MNIST, Portraits, and Gaussian dataset respectively. These numbers were chosen on validation data on the source without examining the intermediate or target data, and we show an ablation which suggests that the results are not sensitive to these choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Confidence thresholding:</head><p>We chose α = 0.1 to filter out the 10% least confident examples, since these are examples the model is not confident on, so the predicted label is less likely to be correct. We run an ablation without this filtering and see that all methods perform slightly worse, but the relative ordering is similar-gradual self-training is still significantly better than all the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Ablations</head><p>We run ablations which suggest that the results in Section 5.1 are robust to the choice of algorithm hyperparameters.</p><p>Confidence thresholding: <ref type="table" target="#tab_4">Table 4</ref> shows the results for rotating MNIST and Portraits without confidence thresholding. All methods do worse without confidence thresholding but gradual selftraining does significantly better than the other methods.</p><p>Window sizes: <ref type="table" target="#tab_5">Table 5</ref> shows the results for rotating MNIST and Portraits if we use smaller window sizes (from 2000 to 1000). Gradual self-training still does significantly better than the other methods.</p><p>Additional ablations for Portraits: We ran two additional ablations, focusing on Portraits. In the first ablation, we trained every method of self-training for 50% more epochs. Over 5 trials, gradual self-training got an accuracy of 83.9 ± 0.4%, target self-train got an accuracy of 80.7 ± 1.1%, </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>saw that gradual self-training works well if we have intermediate images rotated by gradually increasing rotation angles. Another type of gradual transformation is to gradually introduce more examples rotated by 55 to 60 degrees. That is, in the i-th domain, (20 − i)/20 fraction of the examples are MNIST images rotated by 0 to 5 degrees, and i/20 of the examples are MNIST images rotated by 55 to 60 degrees, where 1 ≤ i ≤ 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma A. 1 .</head><label>1</label><figDesc>Given n samples S from a joint distribution P over inputs R d and labels {−1, 1}, and suppose EX∼P [||X|| 2 2 ] ≤ B 2 . Letf and f be the empirical and population minimizers of the ramp loss respectively:f = arg min f ∈Θ R L r (f, S) (28) f * = arg min f ∈Θ R L r (f, P ) (29)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma A. 2 .</head><label>2</label><figDesc>If θ ∈ Θ R , ρ(P, Q) = ρ &lt; 1 R , and the marginals on Y are the same so P (Y ) = Q(Y ), then Err(M θ , Q) ≤ 2 1−ρR L r (M θ , P )Proof. Let θ = (w, b) be the weights and bias of the regularized linear model, with ||w|| 2 ≤ R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Restatement of Example 3 . 5 .</head><label>35</label><figDesc>Given a model θ ∈ Θ ∞ and unlabeled examples S where for all x ∈ S, M θ (x) = 0, there exists θ ∈ ST (θ, S) such that for all x ∈ R d , M θ (x) = M θ (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Example A. 8 .</head><label>8</label><figDesc>Even under the α * -separation and bounded data assumptions, for every 0.25 &gt; α 0 &gt; &gt; 0, there exists a model θ 0 and distribution P withL r (θ 0 , P ) ≤ α 0 but L r (ST(θ 0 , P ), P ) ≥ 2α 0 − .Proof. We give an example in 1D, where a linear model can be parametrized by 2 parameters, w, b ∈ R with |w| ≤ 1, where the output of the linear model for an input x ∈ R is wx + b, and the label is sign(wx + b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 : 2 .</head><label>32</label><figDesc>The plot shows a rolling average of the fraction of images that are female, over a window size of 1000, with 90% confidence intervals. The plot suggests that the proportion of males and females changes over time, and is not constant-this label shift might make the task more challenging for self-training methods. Rotating MNIST : We split the training data, consisting of 50,000 images, using the firstN src = 5000 images as the source training set, next N val = 1000 images as source validation set, next N inter = 42000 images as unlabeled intermediate examples, and the final N trg = 2000 images as unseen target examples. We rotate each source image by an angle uniformly selected between 0 and 5 degrees. The i-th intermediate example is rotated by angle 5 + 55i/N inter degrees. Each target image is rotated by an angle uniformly selected between 55 degrees and 60 degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies for gradual self-train (ST) and baselines on 3 datasets, with 90% confidence intervals for the mean over 5 runs. Gradual ST does better than self-training directly on the target or self-training on all the unlabeled data pooled together.Our goal is to see if adapting to the gradual shift sequentially helps compared to directly adapting to the target. We evaluate four methods:Source: simply train a classifier on the labeled source examples. Target self-train: repeatedly self-train on the unlabeled target examples ignoring the intermediate examples. All self-train: pool all the unlabeled examples from the intermediate and target domains, and repeatedly self-train on this pooled dataset to adapt the initial source classifier. Gradual self-train: sequentially use self-training on unlabeled data in each successive intermediate domain, and finally self-train on unlabeled data on the target domain, to adapt the initial source classifier. For the Gaussian and MNIST datasets, we ensured that the target self-train method sees as many unlabeled target examples as gradual self-train sees across all the intermediate examples. Since portraits is a real dataset we cannot synthesize more examples from the target, so target self-train uses fewer unlabeled examples here.</figDesc><table><row><cell></cell><cell cols="3">Gaussian Rot MNIST Portraits</cell></row><row><cell>Source</cell><cell>47.7±0.3</cell><cell>31.9±1.7</cell><cell>75.3±1.6</cell></row><row><cell>Target ST</cell><cell>49.6±0.0</cell><cell>33.0±2.2</cell><cell>76.9±2.1</cell></row><row><cell>All ST</cell><cell>92.5±0.1</cell><cell>38.0±1.6</cell><cell>78.9±3.0</cell></row><row><cell cols="2">Gradual ST 98.8±0.0</cell><cell>87.9±1.2</cell><cell>83.8±0.8</cell></row><row><cell cols="3">5.1 Does the gradual shift assumption help?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies for gradual self-train with explicit regularization and hard labels (Gradual ST), without regularization but with hard labels (No Reg), and with regularization but with soft labels (Soft Labels). Gradual self-train does best with explicit regularization and hard labels, as our theory suggests, even for neural networks with implicit regularization.</figDesc><table><row><cell></cell><cell cols="3">Gaussian Rot MNIST Portraits</cell></row><row><cell cols="2">Soft Labels 90.5±1.9</cell><cell>44.1±2.3</cell><cell>80.1±1.8</cell></row><row><cell>No Reg</cell><cell>84.6±1.1</cell><cell>45.8±2.5</cell><cell>76.5±1.0</cell></row><row><cell cols="2">Gradual ST 99.3±0.0</cell><cell>83.8±2.5</cell><cell>82.6±0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracies for gradual self-train on rotating MNIST as we vary the number of samples. Unlike in previous experiments, here the same N samples are rotated, so the models do not have to generalize to unseen images, but seen images at different angles. The gap between regularized and unregularized gradual self-training does not shrink much with more data.</figDesc><table><row><cell></cell><cell>N=2000</cell><cell>N=5000 N=20,000</cell></row><row><cell cols="3">Source 28.3±1.4 29.9±2.5</cell><cell>33.9±2.6</cell></row><row><cell cols="3">No Reg 55.7±3.9 53.6±4.0</cell><cell>55.1±3.9</cell></row><row><cell>Reg</cell><cell cols="2">93.1±0.8 91.7±2.4 87.4±3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 Gradual Self-Training Input: Labeled source examples S, Intermediate unlabeled examples I, Window size W , Confidence threshold α ∈ (0, 1), Number of Epochs n, Regularized model M Assume: W divides |I| Train M on S for n epochs for t = 1 to |I|/W do cur xs= I[(t − 1)W : tW ] pseudolabeled ys= M .predict labels(cur xs) confident idxs=filter low confidence(M , cur xs, α) filtered xs = cur xs[confident idxs] filtered ys = pseudolabeled ys[confident idxs] Train M on filtered xs, filtered ys for n epochs end for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracies for gradual self-train (ST) and baselines without confidence thresholding/filtering, with 90% confidence intervals for the mean over 5 runs. All methods do worse without confidence thresholding but gradual self-training does significantly better than the other methods.</figDesc><table><row><cell></cell><cell cols="2">Rot MNIST Portraits</cell></row><row><cell>Source</cell><cell>30.5±1.0</cell><cell>76.2±0.5</cell></row><row><cell>Target ST</cell><cell>31.1±1.4</cell><cell>76.9±1.3</cell></row><row><cell>All ST</cell><cell>32.6±1.3</cell><cell>77.1±0.5</cell></row><row><cell>Gradual ST</cell><cell>80.3±1.4</cell><cell>81.7±1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies for gradual self-train (ST) and baselines with smaller window sizes, with 90% confidence intervals for the mean over 5 runs. Gradual self-training still does significantly better than the other methods. and self-training to all unlabeled examples got an accuracy of 79.6 ± 2.2%. The non-adaptive baseline got an accuracy of 77.3 ± 1.0%.We also ran an experiment on Portraits where we extrapolate further in time. Here we use the first 2000 images as source, next 20,000 images as unlabeled intermediate examples, and next 2000 images as the target. Here the accuracy of gradual self-training is 60.6 ± 1.4%, self-training on the target directly is 56.5 ± 1.4%, and self-training on all unlabeled data is 57.4 ± 0.3%. Gradual self-training still does better, but all methods do quite poorly-developing and analyzing new techniques for gradual domain adaptation is an exciting avenue for future work.</figDesc><table><row><cell></cell><cell cols="2">Rot MNIST Portraits</cell></row><row><cell>Source</cell><cell>35.6±1.7</cell><cell>74.1±1.4</cell></row><row><cell>Target ST</cell><cell>36.0±1.5</cell><cell>77.9±1.4</cell></row><row><cell>All ST</cell><cell>38.5±2.6</cell><cell>76.3±2.2</cell></row><row><cell>Gradual ST</cell><cell>90.4±2.0</cell><cell>83.8±0.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Hoffman et al.<ref type="bibr" target="#b38">[39]</ref>, Michael et al.<ref type="bibr" target="#b39">[40]</ref>, Markus et al.<ref type="bibr" target="#b40">[41]</ref>, Bobu et al.<ref type="bibr" target="#b1">[2]</ref> among others propose approaches for gradual domain adaptation. This setting differs from online learning<ref type="bibr" target="#b41">[42]</ref>, lifelong learning<ref type="bibr" target="#b42">[43]</ref>, and concept drift<ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>, since we only have unlabeled data from shifted distributions. To the best of our knowledge, we are the first to develop a theory for gradual domain adaptation, and investigate when and why the gradual structure helps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We need the here because a mapping with exactly the W∞ distance may not exist, although if they P and Q have densities then such a mapping does exist.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank the Open Philantropy Project and the Stanford Graduate Fellowship program for funding. This work is also partially supported by the Stanford Data Science Initiative and the Stanford Artificial Intelligence Laboratory.</p><p>We are grateful to Stephen Mussman, Robin Jia, Csaba Szepesvari, Shai Ben-David, Lin Yang, Rui Shu, Michael Xie, Aditi Raghunathan, Yining Chen, Colin Wei, Pang Wei Koh, Fereshte Khani, Shengjia Zhao, and Albert Gu for insightful discussions.</p><p>Reproducibility. Our code is at https://github.com/p-lambda/gradual_domain_adaptation. Code, data, and experiments will be available on CodaLab soon.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Datasets</head><p>We ran experiments on 3 datasets: 1. Gaussian in d = 100 dimensions: We randomly select an initial mean and covariance for each of the two classes, and a final mean and covariance for each class, all in d dimensions.</p><p>Note that unlike in the theory in Section 4, each class can have a different (non diagonal) covariance matrix. The initial and final covariance matrices can also be different. The marginal probability of each class is the same, 0.5. We get labeled data sampled from a gaussian with the initial mean and covariance. For the intermediate domains, we linearly interpolate the means and covariances for each class between the initial and final, and sample points from a gaussian with the corresponding mean and covariance matrices. The number of labeled and unlabeled samples is on the order of d (as opposed to exponential in d, which importance weighting approaches would need). We provide more details next.</p><p>Details: We sample µ</p><p>independently from N (0, I) in d dimensions. Since d is high, these are all nearly orthogonal to each other. We then sample covariance matrices Σ</p><p>independently by sampling a diagonal matrix and rotation matrix (since the covariance matrices are PSD they decompose into U DU for rotation matrix U and diagonal matrix D). We first sample a diagonal matrix D in d dimensions where each entry is uniformly random and independently sampled between min var and max var. Then, we sample a rotation matrix U from the Haar distribution (which is a standard way to sample random orthogonal matrices), and then compose these to get U DU . T . We then sample y t ∼ Bern(0.5), and</p><p>t )-the model only gets to see x t but not y t . The unseen target images are sampled from the final means and covariances for each class, and we measure accuracy on these held out examples.</p><p>We use N = 500 (500 labeled examples from the source), T = 5000 (so 5000 unlabeled examples in total), and use min var=0.05, max var=0.1 (the standard-deviation is the square root of these). We sample 1000 target examples to check accuracy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chemical gas sensor drift compensation using classifier ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huerta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="320" to="329" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adapting to continuously shifting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for stable brain-machine interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farshchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the reliable detection of concept drift from streaming unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantardzic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="77" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Self-training with noisy student improves imagenet classification. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are labels required for improving adversarial robustness?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unlabeled data improves adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robustness to adversarial perturbations in learning from incomplete data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A century of portraits: A visual historical record of american high school yearbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On learning invariant representation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A DIRT-T approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiayuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with explicit misclassification modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<title level="m">Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09822</idno>
		<title level="m">Confidence regularized self-training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization error bounds in semi-supervised classification under the cluster assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1369" to="1392" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Understanding and mitigating the tradeoff between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the loglikelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradual domain adaptation for segmenting whole slide images showing pathological variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Mara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ingmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<title level="m">Online Learning: Theory, Algorithms, and Applications</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>The Hebrew University of Jerusalem</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lifelong machine learning systems: Beyond learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning despite distribution shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models Summer School</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning with a slowly changing distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning changing concepts by exploiting the structure of change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/class/cs229t/notes.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
