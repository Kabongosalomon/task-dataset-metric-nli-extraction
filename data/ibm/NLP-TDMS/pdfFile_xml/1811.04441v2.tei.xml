<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shang</surname></persName>
							<email>chao.shang@uconn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<settlement>Storrs</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
							<email>yun.tang@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
							<email>jing.huang@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
							<email>jinbo.bi@uconn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<settlement>Storrs</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<email>xiaodong.he@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>bowen.zhou@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, DistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over the state-of-theart ConvE in terms of HITS@1, HITS@3 and HITS@10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Over the recent years, large-scale knowledge bases (KBs), such as Freebase <ref type="bibr" target="#b1">(Bollacker et al. 2008)</ref>, DBpedia <ref type="bibr" target="#b0">(Auer et al. 2007</ref>), NELL <ref type="bibr" target="#b4">(Carlson et al. 2010</ref>) and YAGO3 (Mahdisoltani, Biega, and Suchanek 2013), have been built to store structured information about common facts. KBs are multirelational graphs whose nodes represent entities and edges represent relationships between entities, and the edges are labeled with different relations. The relationships are organized in the forms of (s, r, o) triplets (e.g. entity s = Abraham Lincoln, relation r = DateOfBirth, entity o = 02-12-1809). These KBs are extensively used for web search, rec-ommendation and question answering. Although these KBs have already contained millions of entities and triplets, they are far from complete compared to existing facts and newly added knowledge of the real world. Therefore knowledge base completion is important in order to predict new triplets based on existing ones and thus further expand KBs.</p><p>One of the recent active research areas for knowledge base completion is knowledge graph embedding: it encodes the semantics of entities and relations in a continuous lowdimensional vector space (called embeddings). These embeddings are then used for predicting new relations. Started from a simple and effective approach called TransE (Bordes et al. 2013), many knowledge graph embedding methods have been proposed, such as TransH <ref type="bibr" target="#b25">(Wang et al. 2014)</ref>, TransR <ref type="bibr" target="#b14">(Lin et al. 2015)</ref>, DistMult <ref type="bibr" target="#b27">(Yang et al. 2014)</ref>, TransD <ref type="bibr" target="#b10">(Ji et al. 2015)</ref>, ComplEx <ref type="bibr" target="#b24">(Trouillon et al. 2016)</ref>, STransE <ref type="bibr" target="#b17">(Nguyen et al. 2016)</ref>. Some surveys <ref type="bibr" target="#b18">(Nguyen 2017;</ref><ref type="bibr" target="#b26">Wang et al. 2017)</ref> give details and comparisons of these embedding methods.</p><p>The most recent ConvE <ref type="bibr" target="#b6">(Dettmers et al. 2017</ref>) model uses 2D convolution over embeddings and multiple layers of nonlinear features, and achieves the state-of-the-art performance on common benchmark datasets for knowledge graph link prediction. In ConvE, the embeddings of s and r are reshaped and concatenated into an input matrix and fed to the convolution layer. Convolutional filters of n × n are used to output feature maps that are across different dimensional embedding entries. Thus ConvE does not keep the translational property as TransE which is an additive embedding vector operation: e s + e r ≈ e o ( ). In this paper, we remove the reshape step of ConvE and operate convolutional filters directly in the same dimensions of s and r. This modification gives better performance compared with the original ConvE, and has an intuitive interpretation which keeps the global learning metric the same for s, r, and o in an embedding triple (e s , e r , e o ). We name this embedding as Conv-TransE.</p><p>ConvE also does not incorporate connectivity structure in the knowledge graph into the embedding space. In contrast, graph convolutional network (GCN) has been an effective tool to create node embeddings which aggregate local information in the graph neighborhood for each node <ref type="bibr" target="#b13">(Kipf and Welling 2016b;</ref><ref type="bibr" target="#b7">Hamilton, Ying, and Leskovec 2017a;</ref><ref type="bibr" target="#b12">Kipf and Welling 2016a;</ref><ref type="bibr" target="#b19">Pham et al. 2017;</ref><ref type="bibr" target="#b21">Shang et al. 2018</ref>). GCN models have additional benefits <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017b)</ref>, such as leveraging the attributes associated with nodes. They can also impose the same aggregation scheme when computing the convolution for each node, which can be considered a method of regularization, and improves efficiency. Although scalability is originally an issue for GCN models, the latest data-efficient GCN, <ref type="bibr">Pin-Sage (Ying et al. 2018)</ref>, is able to handle billions of nodes and edges.</p><p>In this paper, we propose an end-to-end graph Structure-Aware Convolutional Networks (SACN) that take all benefits of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and relation types. It has learnable weights to determine the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes are added to WGCN as additional for easy integration. The output of WGCN becomes the input of the decoder Conv-TransE. Conv-TransE is similar to ConvE but with the difference that Conv-TransE keeps the translational characteristic between entities and relations. We show that Conv-TransE performs better than ConvE, and our SACN improves further on top of Conv-TransE in the standard benchmark datasets. The code for our model and experiments is publicly available 1 .</p><p>Our contributions are summarized as follows:</p><p>• We present an end-to-end network learning framework SACN that takes benefit of both GCN and Conv-TransE. The encoder GCN model leverages graph structure and attributes of graph nodes. The decoder Conv-TransE simplifies ConvE with special convolutions and keeps the translational property of TransE and the prediction performance of ConvE;</p><p>• We demonstrate the effectiveness of our proposed SACN on the standard FB15k-237 and WN18RR datasets, and show about 10% relative improvement over the stateof-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Knowledge graph embedding learning has been an active research area with applications directly in knowledge base completion (i.e. link prediction) and relation extractions. TransE <ref type="bibr" target="#b2">(Bordes et al. 2013</ref>) started this line of work by projecting both entities and relations into the same embedding vector space, with translational constraint of e s + e r ≈ e o . Later works enhanced KG embedding models such as TransH <ref type="bibr" target="#b25">(Wang et al. 2014)</ref>, TransR <ref type="bibr" target="#b14">(Lin et al. 2015)</ref>, and TransD <ref type="bibr" target="#b10">(Ji et al. 2015)</ref> introduced new representations of relational translation and thus increased model complexity. These models were categorized as translational distance models <ref type="bibr" target="#b26">(Wang et al. 2017)</ref> or additive models, while DistMult <ref type="bibr" target="#b27">(Yang et al. 2014)</ref> and ComplEx <ref type="bibr" target="#b24">(Trouillon et al. 2016</ref>) are multiplicative models <ref type="bibr">(Sharma, Talukdar, and others 2018)</ref>, due to the multiplicative score functions used for computing entity-relation-entity triplet likelihood. The most recent KG embedding models are ConvE <ref type="bibr" target="#b6">(Dettmers et al. 2017)</ref> and ConvKB .</p><p>ConvE was the first model using 2D convolutions over embeddings of different embedding dimensions, with the hope of extracting more feature interactions. ConvKB replaced 2D convolutions in ConvE with 1D convolutions, which constrains the convolutions to be the same embedding dimensions and keeps the translational property of TransE. Con-vKB can be considered as a special case of Conv-TransE that only uses filters with width equal to 1. Although Con-vKB was shown to be better than ConvE, the results on two datasets (FB15k-237 and WN18RR) were not consistent, so we leave these results out of our comparison table. The other major difference of ConvE and ConvKB is on the loss functions used in the models. ConvE used the cross-entropy loss that could be sped up with 1-N scoring in the decoder, while ConvKB used a hinge loss that was computed from positive examples and sampled negative examples. We take the decoder from ConvE because we can easily integrate the encoder of GCN and the decoder of ConvE into an end-to-end training framework, while ConvKB is not suitable for our approach.</p><p>These embedding models achieved good performance for knowledge base completion in terms of efficiency and scalability. However, these approaches only modeled relational triplets, while ignoring a large number of attributes associated with graph nodes, e.g., ages of people or release region of music. Furthermore, these models do not enforce any large-scale connectivity structure in the embedding space, and totally ignore the knowledge graph structure. The proposed (SACN) handles these two problems in an end-to-end training framework, by using a variant of graph convolutional network (GCN) as the encoder, and a variant of ConvE as the decoder.</p><p>GCNs were first proposed in <ref type="bibr" target="#b3">(Bruna et al. 2013)</ref> where graph convolutional operations were defined in the Fourier domain. The eigendecomposition of the graph Laplacian caused intense computation. Later, smooth parametric spectral filters <ref type="bibr" target="#b9">(Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b5">Defferrard, Bresson, and Vandergheynst 2016)</ref> were introduced to achieve localization in the spatial domain and improve computational efficiency. Recently, Kipf et al. <ref type="bibr" target="#b13">(Kipf and Welling 2016b</ref>) simplified these spectral methods by a first-order approximation with the Chebyshev polynomials. The spatial graph convolution approaches (Hamilton, Ying, and Leskovec 2017a) define convolutions directly on graph, which sum up node features over all spatial neighbors using adjacency matrix.</p><p>GCN models were mostly criticized for its huge memory requirement to scale to massive graphs. However, <ref type="bibr" target="#b27">(Ying et al. 2018</ref>) developed a data efficient GCN algorithm called PinSage, which combined efficient random walks and graph convolutions to generate embeddings of nodes that incorporated both graph structure as well as node features. The experiments on Pinterest data were the largest application of deep graph embeddings to date with 3 billion nodes and 18 billion edges <ref type="bibr" target="#b27">(Ying et al. 2018)</ref>. This success paves the way for a new generation of web-scale recommender systems based on GCNs. Therefore we believe that our proposed model could take advantage of huge graph structures and high computational efficiency of Conv-TransE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we describe the proposed end-to-end SACN. The encoder WGCN is focused on representing entities by aggregating connected entities as specified by the relations in the KB. With node embeddings as the input, the decoder Conv-TransE network aims to represent the relations more accurately by recovering the original triplets in the KB. Both encoder and decoder are trained jointly by minimizing the discrepancy (cross-entropy) between the embeddings e s +e r and e o to preserve the translational property e s + e r ≈ e o . We consider an undirected graph G = (V, E) throughout this section, where V is a set of nodes with |V | = N , and</p><formula xml:id="formula_0">E ⊆ V × V is a set of edges with |E| = M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Graph Convolutional Layer</head><p>The WGCN is an extension of classic GCN <ref type="bibr" target="#b13">(Kipf and Welling 2016b)</ref> in the way that it weighs the different types of relations differently when aggregating and the weights are adaptively learned during the training of the network. By this adaptation, the WGCN can control the amount of information from neighboring nodes used in aggregation. Roughly speaking, the WGCN treats a multi-relational KB graph as multiple single-relational subgraphs where each subgraph entails a specific type of relations. The WGCN determines how much weights to give to each subgraph when combining the GCN embeddings for a node.</p><p>The l-th WGCN layer takes the output vector of length F l for each node from the previous layer as inputs and generates a new representation comprising F l+1 elements. Let h l i represent the input (row) vector of the node v i in the l-th layer, and thus H l ∈ R N ×F l be the input matrix for this layer. The initial embedding H 1 is randomly drawn from Gaussian. If there are a total of L layers in the WGCN, the output H L+1 of the L-th layer is the final embedding. Let the total number of edge types be T in a multi-relational KB graph with E edges. The interaction strength between two adjacent nodes is determined by their relation type and this strength is specified by a parameter {α t , 1 ≤ t ≤ T } for each edge type, which is automatically learned in the neural network. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the entire process of SACN. In this example, the WGCN layers of the network compute the embeddings for the red node in the middle graph. These layers aggregate the embeddings of neighboring entity nodes as specified in the KB relations. Three colors (blue, yellow and green) of the edges indicate three different relation types in the graph. The corresponding three entity nodes are summed up with different weights according to α t in this layer to obtain the embedding of the red node. The edges with the same color (same relation type) use the same α t . Each layer has its own set of relation weights α l t . Hence, the output of the l-th layer for the node v i can be written as follows:</p><formula xml:id="formula_1">h l+1 i = σ   j∈N i α l t g(h l i , h l j )   ,<label>(1)</label></formula><p>where h l j ∈ R F l is the input for node v j , and v j is a node in the neighbor N i of node v i . The g function specifies how to incorporate neighboring information. Note that the activation function σ here is applied to every component of its input vector. Although any function g suitable for a KB embedding can be used in conjunction with the proposed framework, we implement the following g function:</p><formula xml:id="formula_2">g(h l i , h l j ) = h l j W l ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W l ∈ R F l ×F l+1 is the connection coefficient matrix and used to linearly transform h l i to h l+1 i ∈ R F l+1 . In Eq.</formula><p>(1), the input vectors of all neighboring nodes are summed up but not the node v i itself, hence self-loops are enforced in the network. For node v i , the propagation process is defined as:</p><formula xml:id="formula_4">h l+1 i = σ( j∈N i α l t h l j W l + h l i W l ).<label>(3)</label></formula><p>The output of the layer l is a node feature matrix: H l+1 ∈ R N ×F l+1 , and h l+1 i is the i-th row of H l+1 , which represents features of the node v i in the (l + 1)-th layer.</p><p>The above process can be organized as a matrix multiplication as shown in <ref type="figure" target="#fig_1">Figure 2</ref> to simultaneously compute embeddings for all nodes through an adjacency matrix. For each relation (edge) type, an adjacency matrix A t is a binary matrix whose ij-th entry is 1 if an edge connecting v i and v j exists or 0 otherwise. The final adjacency matrix is written as follows:</p><formula xml:id="formula_5">A l = T t=1 (α l t A t ) + I,<label>(4)</label></formula><p>where I is the identity matrix of size N × N . Basically, the A l is the weighted sum of the adjacency matrices of subgraphs plus self-connections. In our implementation, we consider all first-order neighbors in the linear transformation for each layer as shown in <ref type="figure" target="#fig_1">Figure 2</ref>:</p><formula xml:id="formula_6">H l+1 = σ(A l H l W l ).<label>(5)</label></formula><p>Node Attributes. In a KB graph, nodes are often associated with several attributes in the form of (entity, relation, attribute). For example, (s = Tom, r = people.person.gender, a = male) is an instance where gender is an attribute associated with a person. If a vector representation is used for node attributes, there would be two potential problems. First, the number of attributes for each node is usually small, and differs from one to another.</p><p>Hence, the attribute vector would be very sparse. Second, the value of zero in the attribute vectors may have ambiguous meanings: the node does not have the specific attribute, or the node misses the value for this attribute. These zeros would affect the accuracy of the embedding. In this work, the entity attributes in the knowledge graph are represented by another set of nodes in the network called attribute nodes. Attribute nodes act as the "bridges" to link the related entities. The entity embeddings can be transported over these "bridges" to incorporate the entity's attribute into its embedding. Because these attributes exhibit in triplets, we represent the attributes similarly to the representation of the entity o in relation triplets. Note that each type of attribute corresponds to a node. For instance, in our example, gender is represented by a single node rather than two nodes for "male" and "female". In this way, the WGCN not only utilizes the graph connectivity structure (relations and relation types), but also leverages the node attributes (a kind of graph structure) effectively. That is why we name our WGCN as a structure-aware convolution network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-TransE</head><p>We develop the Conv-TransE model as a decoder that is based on ConvE but with the translational property of TransE: e s + e r ≈ e o . The key difference of our approach from ConvE is that there is no reshaping after stacking e s and e r . Filters (or kernels) of size 2 × k , k ∈ {1, 2, 3, ...}, are used in the convolution. The example in <ref type="figure" target="#fig_0">Figure 1</ref> uses 2 × 3 kernels to compute 2D convolutions. We experimented with several of such settings in our empirical study.</p><p>Note that in the encoder of SACN, the dimension of the relation embedding is commonly chosen to be the same as the dimension of the entity embedding, so in other words, is equal to F L . Hence, the two embeddings can be stacked. For the decoder, the inputs are two embedding matrices: one R N ×F L from WGCN for all entity nodes, and the other R M ×F L for relation embedding matrix which is trained as well. Because we use a mini-batch stochastic training algorithm, the first step of the decoder performs a look-up operation upon the embedding matrices to retrieve the input e s and e r for the triplets in the mini-batch.</p><p>More precisely, given C different kernels where the c-th kernel is parameterized by ω c , the convolution in the decoder is computed as follows:</p><formula xml:id="formula_7">m c (e s , e r , n) = K−1 τ =0 ω c (τ, 0)ê s (n + τ ) + ω c (τ, 1)ê r (n + τ ),<label>(6)</label></formula><p>where K is the kernel width, n indexes the entries in the output vector and n ∈ [0, F L − 1], and the kernel parameters ω c are trainable.ê s andê r are padding version of e s and e r respectively. If the dimension s of kernel is odd, the first K/2 and last K/2 components are filled with 0. Here value returns the floor of value. Otherwise, the first K/2 − 1 and last K/2 components are filled with 0. Other components are copied from e s and e r directly. As shown in Eq. (6) the convolution operation amounts to  </p><p>where W ∈ R CF L ×F L is a matrix for the linear transformation, and f denotes a non-linear function. The feature map matrix is reshaped into a vector vec(M) ∈ R CF L and projected into a F L dimensional space using W for linear transformation. Then the calculated embedding is matched to e o by an appropriate distance metric. During the training in our experiments, we apply the logistic sigmoid function to the scoring:</p><p>p(e s , e r , e o ) = σ(ψ(e s , e o )). (8) In <ref type="table" target="#tab_0">Table 1</ref>, we summarize the scoring functions used by several state of the art models. The vector e s and e o are the subject and object embedding respectively, e r is the relation embedding, "concat" means concatenates the inputs, and "*" denotes the convolution operator.</p><p>In summary, the proposed SACN model takes advantage of knowledge graph node connectivity, node attributes and relation types. The learnable weights in WGCN help to collect adaptive amount of information from neighboring graph nodes. The entity attributes are added as additional nodes in the network and are easily integrated into the WGCN. Conv-TransE keeps the translational property between entities and relations to learn node embeddings for the link prediction. We also emphasize that our SACN has significant improvements over ConvE with or without the use of node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Benchmark Datasets</head><p>Three benchmark datasets (FB15k-237, WN18RR and FB15k-237-Attr) are utilized in this study to evaluate the performance of link prediction.</p><p>FB15k-237. The FB15k-237 <ref type="bibr" target="#b23">(Toutanova and Chen 2015)</ref> dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs, as used in the work published in <ref type="bibr" target="#b23">(Toutanova and Chen 2015)</ref>. The knowledge base triples are a subset of the FB15K <ref type="bibr" target="#b2">(Bordes et al. 2013</ref>), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Construction</head><p>Most of the previous methods only model the entities and relations, and ignore the abundant entity attributes. Our method can easily model a large number of entity attribute triples. In order to prove the efficiency, we extract the attribute triples from the FB24k <ref type="bibr" target="#b15">(Lin, Liu, and Sun 2016)</ref> dataset to build the evaluation dataset called FB15k-237-Attr.</p><p>FB24k. FB24k <ref type="bibr" target="#b15">(Lin, Liu, and Sun 2016)</ref> is built based on Freebase dataset. FB24k only selects the entities and relations which constitute at least 30 triples. The number of entities is 23,634, and the number of relations is 673. In addition, the reversed relations are removed from the original dataset. In the FB24k datasets, the attribute triples are provided. FB24k contains 207,151 attribute triples and 314 attributes. FB15k-237-Attr. We extract the attribute triples of entities in FB15k-237 from FB24k. During the mapping, there are 7,589 nodes from the original 14,541 entities which have the node attributes. Finally, we extract 78,334 attribute triples from FB24k. These triples include 203 attributes and 247 relations. Based on these triples, we create the "FB15k-237-Attr" dataset, which includes 14,541 entity nodes, 203 attribute nodes, 484 relation types. All the 78,334 attribute triples are combined with the training set of FB15k-237.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>The  Here all the models use the WGCN with two layers. For different datasets, we have found that the following settings work well: for FB15k-237, set the dropout to 0.2, number of kernels to 100, learning rate to 0.003 and embedding size to 200 for SACN; for WN18RR dataset, set dropout to 0.2, number of kernels to 300, learning rate to 0.003, and embedding size to 200 for SACN. When using the Conv-TransEalone model, these settings still work well.</p><p>Each dataset is split into three sets for: training, validation and testing, which is same with the setting of the original ConvE. We use the adaptive moment (Adam) algorithm <ref type="bibr" target="#b11">(Kingma and Ba 2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Evaluation Protocol Our experiments use the the proportion of correct entities ranked in top 1,3 and 10 (Hits@1, Hits@3, Hits@10) and the mean reciprocal rank (MRR) as the metrics. In addition, since some corrupted triples exist in the knowledge graphs, we use the filtered setting <ref type="bibr" target="#b2">(Bordes et al. 2013)</ref>, i.e. we filter out all valid triples before ranking.</p><p>Link Prediction Our results on the standard FB15k-237, WN18RR and FB15k-237-Attr are shown in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="table" target="#tab_3">Table 3</ref> reports Hits@10, Hits@3, Hits@1 and MRR results of four different baseline models and two our models on three knowledge graphs datasets. The FB15k-237-Attr dataset is used to prove the efficiency of node attributes. So we run our SACN in FB15k-237-Attr to do the comparison with SACN using FB15k-237.</p><p>We first compare our Conv-TransE model with the four baseline models. ConvE has the best performance comparing all baselines. In FB15k-237 dataset, our Conv-TransE model improves upon ConvE's Hits@10 by a margin of 4.1% , and upon ConvE's Hits@3 by a margin of 5.7% for the test. In WN18RR dataset, Conv-TransE improves upon ConvE's Hits@10 by a margin of 8.3% , and upon ConvE's Hits@3 by a margin of 9.3% for the test. For these results, we conclude that Conv-TransE using neural network keeps the translational characteristic between entities and relations and achieve better performance.</p><p>Second, the structure information is added into our SACN model. In <ref type="table" target="#tab_3">Table 3</ref>, SACN also get the best performances in the test dataset comparing all baseline methods. In FB15k-237, comparing ConvE, our SACN model improves Hits@10 value by a margin of 10.2%, Hits@3 value by a margin of 11.4%, Hits@1 value by a margin of 8.3% and MRR value by a margin of 9.4% for the test. In WN18RR dataset, comparing ConvE, our SACN model improves Hits@10 value by a margin of 12.5%, Hits@3 value by a margin of 11.6%, Hits@1 value by a margin of 10.3% and MRR value by a margin of 2.2% for the test. So our method has significant improvements over ConvE without attributes.</p><p>Third, we add node attributes into our SACN model, i.e. we use the FB15k-237-Attr to train SACN. Note that SACN has significant improvements over ConvE without attributes. Adding attributes improves performance again. Our model using attributes improves upon ConvE's Hits@10 by a margin of 12.2% , Hits@3 by a margin of 14.3%, Hits@1 by a margin of 12.5% and MRR by a margin of 12.5%. In addition, our SACN using attributes improved Hits@10 by a margin of 1.9% , Hits@3 by a margin of 2.6%, Hits@1 by a margin of 3.8% and MRR by a margin of 2.9% comparing with SACN without attributes.</p><p>In order to better compare with ConvE, we also use the attributes into ConvE. Here the attributes will be treated as the entity triplets. Following the official ConvE code with default setting, the test result in FB15k-237-Attr was: 0.46 (Hits@10), 0.33 (Hits@3), 0.22 (Hits@1) and 0.30 (MRR). Comparing to the performance without the attributes, adding the attributes into the ConvE didn't improve performance. <ref type="figure" target="#fig_4">Figure 3</ref> shows the convergence of the three models. We can see that the SACN (the red line) is always better than Conv-TransE (the yellow line) after several epochs. And the performance of SACN keeps increasing after around 120 epochs. However, the Conv-TransE has achieved the best performance after around 120 epochs. The gap between these two models proves the usefulness of structural information. When using the FB15k-237-Attr dataset, the performance of "SACN + Attr" is better than   <ref type="table" target="#tab_4">Table 4</ref>, different kernel sizes are examined in our models. The kernel of "2 × 1" means the knowledge or information translating between one attribute of entity vector and the corresponding attribute of relation vector. If we increase the kernel size to "2 × k" where k = {3, 5}, the information is translated between a combination of s attributes in entity vector and a combination of k attributes in relation vector. The larger view to collect attribute information can help to increase the performance as shown in <ref type="table" target="#tab_4">Table 4</ref>. All the values of Hits@1, Hits@3, Hits@10 and MRR can be improved by increasing the kernel size in the FB15k-237 and FB15k-237-Attr datasets. However, the optimal kernel size may be task dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence Analysis</head><p>Node Indegree Analysis The indegree of the node in knowledge graph is the number of edges connected to the node. The node with larger degree means it have more neighboring nodes, and this kind of nodes can receive more information from neighboring nodes than other nodes with smaller degree. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we present the results for different sets of nodes with different indegree scopes. The average Hits@10 and Hits@3 scores are calculated. Along the increasing of indegree scope, the average value of Hits@10 and Hits@3 will be increased. First for a node with small indegree, it benefits from aggregation of neighbor information from the WGCN layers of SACN. Its embedding can be estimated robustly. Second for a node with high indegree, it means that a lot more information is aggregated through GCN, and the estimation of its embedding is substantially smoothed among neighbors. Thus the embedding learned from SACN is worse than that from Conv-TransE. One solution to this problem would be neighbor selection as in <ref type="bibr" target="#b27">(Ying et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>We have introduced an end-to-end structure-aware convolutional network (SACN). The encoding network is a weighted graph convolutional network, utilizing knowledge graph connectivity structure, node attributes and relation types. WGCN with learnable weights has the benefit of collecting adaptive amount of information from neighboring graph nodes. In addition, the entity attributes are added as the nodes in the network so that attributes are transformed into knowledge structure information, which is easily integrated into the node embedding. The scoring network of SACN is a convolutional neural model, called Conv-TransE. It uses a convolutional network to model the relationship as the translation operation and capture the translational characteristic between entities and relations. We also prove that Conv-TransE alone has already achieved the state of the art performance. The performance of SACN achieves overall about 10% improvement than the state of the art such as ConvE.</p><p>In the future, we would like to incorporate the neighbor selection idea into our training framework, such as, importance pooling in <ref type="bibr" target="#b27">(Ying et al. 2018</ref>) which takes into account the importance of neighbors when aggregating the vector representations of neighbors. We would also like to extend our model to be scalable with larger knowledge graphs encouraged by the results in <ref type="bibr" target="#b27">(Ying et al. 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of our end-to-end Structure-Aware Convolutional Networks model. For encoder, a stack of multiple WGCN layers builds an entity/node embedding matrix. For decoder, e s and e r are fed into Conv-TransE. The output embeddings are vectorized and projected, and matched with all candidate e o embeddings via inner products. A logistic sigmoid function is used to get the scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A weighted graph convolutional network (WGCN) for entity embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SACN f (vec(M(e s , e r ))W )e o a sum of e s and e r after the one-dimensional convolution. Hence, it preserves the translational property of the embeddings of e s , e r . The output forms a vector M c (e s , e r ) = [m c (e s , e r , 0), ..., m c (e s , e r , F L − 1)]. Aligning the output vectors from the convolution with all kernels yield a matrix M(e s , e r ) ∈ R C×F L . Finally, the scoring function for the Conv-TransE method after the nonlinear convolution is defined as below: ψ(e s , e o ) = f (vec(M(e s , e r ))W )e o ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for training the model. Our models are implemented by PyTorch and run on NVIDIA Tesla P40 Graphics Processing Units. For the FB15k-237 dataset, the computation time of SACN for each epoch is about 1 minute. For the WN18RR, the computation time of SACN for one epoch is about 1.5 minutes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The convergence study of SACN, Conv-TransE models in FB15k-237 and SACN in FB15k-237-Attr (SACN + Attr) using the validation set. Due to the page limitation, only the results of Hits@1 and MRR are reported here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scoring function ψ(e s , e o ). Hereē s andē r denote a 2D reshaping of e s and e r . Model Scoring Function ψ(e s , e o ) TransE ||e s + e r − e o || p DistMult &lt; e s , e r , e o &gt; ComplEx &lt; e s , e r , e o &gt;</figDesc><table><row><cell>ConvE</cell><cell>f (vec(f (concat(ē s ,ē r )  *  ω))W )e o</cell></row><row><cell>ConvKB</cell><cell>concat(g([e</cell></row></table><note>s , e r , e o ] * ω))β</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">FB15k-237 WN18RR FB15k-237-Attr</cell></row><row><cell>Entities</cell><cell>14,541</cell><cell>40,943</cell><cell>14,744</cell></row><row><cell>Relations</cell><cell>237</cell><cell>11</cell><cell>484</cell></row><row><cell>Train Edges</cell><cell>272,115</cell><cell>86,835</cell><cell>350,449</cell></row><row><cell>Val. Edges</cell><cell>17,535</cell><cell>3,034</cell><cell>17,535</cell></row><row><cell>Test Edges</cell><cell>20,466</cell><cell>3,134</cell><cell>20,466</cell></row><row><cell>Attributes Triples</cell><cell>-</cell><cell>-</cell><cell>78,334</cell></row><row><cell>Attributes</cell><cell>-</cell><cell>-</cell><cell>203</cell></row><row><cell cols="4">originally derived from Freebase. The inverse relations are</cell></row><row><cell cols="2">removed in FB15k-237.</cell><cell></cell><cell></cell></row><row><cell cols="4">WN18RR. WN18RR (Dettmers et al. 2017) is created from</cell></row><row><cell cols="4">WN18 (Bordes et al. 2013), which is a subset of WordNet.</cell></row><row><cell cols="4">WN18 consists of 18 relations and 40,943 entities. However,</cell></row><row><cell cols="4">many text triples obtained by inverting triples from the</cell></row><row><cell cols="4">training set. Thus WN18RR dataset (Dettmers et al. 2017)</cell></row><row><cell cols="4">is created to ensure that the evaluation dataset does not have</cell></row><row><cell cols="4">inverse relation test leakage. In summary, WN18RR dataset</cell></row><row><cell cols="4">contains 93,003 triples with 40,943 entities and 11 relation</cell></row><row><cell>types.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Link prediction for FB15k-237, WN18RR and FB15k-237-Attr datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>@10</cell><cell>@3</cell><cell>@1</cell><cell>MRR</cell><cell>@10</cell><cell>@3</cell><cell>@1</cell><cell>MRR</cell></row><row><cell>DistMult (Yang et al. 2014)</cell><cell>0.42</cell><cell>0.26</cell><cell>0.16</cell><cell>0.24</cell><cell>0.49</cell><cell>0.44</cell><cell>0.39</cell><cell>0.43</cell></row><row><cell>ComplEx (Trouillon et al. 2016)</cell><cell>0.43</cell><cell>0.28</cell><cell>0.16</cell><cell>0.25</cell><cell>0.51</cell><cell>0.46</cell><cell>0.41</cell><cell>0.44</cell></row><row><cell>R-GCN (Schlichtkrull et al. 2018)</cell><cell>0.42</cell><cell>0.26</cell><cell>0.15</cell><cell>0.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvE (Dettmers et al. 2017)</cell><cell>0.49</cell><cell>0.35</cell><cell>0.24</cell><cell>0.32</cell><cell>0.48</cell><cell>0.43</cell><cell>0.39</cell><cell>0.46</cell></row><row><cell>Conv-TransE</cell><cell>0.51</cell><cell>0.37</cell><cell>0.24</cell><cell>0.33</cell><cell>0.52</cell><cell>0.47</cell><cell>0.43</cell><cell>0.46</cell></row><row><cell>SACN</cell><cell>0.54</cell><cell>0.39</cell><cell>0.26</cell><cell>0.35</cell><cell>0.54</cell><cell>0.48</cell><cell>0.43</cell><cell>0.47</cell></row><row><cell>SACN using FB15k-237-Attr</cell><cell>0.55</cell><cell>0.40</cell><cell>0.27</cell><cell>0.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Performance Improvement</cell><cell>12.2%</cell><cell>14.3%</cell><cell>12.5%</cell><cell>12.5%</cell><cell>12.5%</cell><cell>11.6%</cell><cell>10.3%</cell><cell>2.2%</cell></row><row><cell>and kernel size {2 × 1, 2 × 3, 2 × 5}.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Kernel size analysis for FB15k-237 and FB15k-237-Attr datasets. "SACN+Attr" means the SACN using FB15k-237-Attr dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15k-237</cell></row><row><cell></cell><cell></cell><cell>Hits</cell></row><row><cell>Model</cell><cell cols="2">Kernel Size @10 @3</cell><cell>@1 MRR</cell></row><row><cell>Conv-TransE</cell><cell>2 × 1</cell><cell cols="2">0.504 0.357 0.234 0.324</cell></row><row><cell>Conv-TransE</cell><cell>2 × 3</cell><cell cols="2">0.513 0.365 0.240 0.331</cell></row><row><cell>Conv-TransE</cell><cell>2 × 5</cell><cell cols="2">0.512 0.361 0.239 0.329</cell></row><row><cell>SACN</cell><cell>2 × 1</cell><cell cols="2">0.527 0.379 0.255 0.345</cell></row><row><cell>SACN</cell><cell>2 × 3</cell><cell cols="2">0.536 0.384 0.260 0.351</cell></row><row><cell>SACN</cell><cell>2 × 5</cell><cell cols="2">0.536 0.385 0.261 0.352</cell></row><row><cell>SACN+Attr</cell><cell>2 × 1</cell><cell cols="2">0.535 0.384 0.260 0.351</cell></row><row><cell>SACN+Attr</cell><cell>2 × 3</cell><cell cols="2">0.543 0.394 0.268 0.360</cell></row><row><cell>SACN+Attr</cell><cell>2 × 5</cell><cell cols="2">0.547 0.396 0.268 0.360</cell></row><row><cell>"SACN" model.</cell><cell></cell><cell></cell></row></table><note>Kernel Size Analysis In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Node indegree study using FB15k-237 dataset.</figDesc><table><row><cell></cell><cell cols="2">Conv-TransE</cell><cell cols="2">SACN</cell></row><row><cell></cell><cell cols="2">Average Hits</cell><cell cols="2">Average Hits</cell></row><row><cell>Indegree Scope</cell><cell>@10</cell><cell>@3</cell><cell>@10</cell><cell>@3</cell></row><row><cell>[0,100]</cell><cell cols="4">0.192 0.125 0.195 0.134</cell></row><row><cell>[100,200]</cell><cell cols="4">0.441 0.245 0.441 0.253</cell></row><row><cell>[200,300]</cell><cell cols="4">0.696 0.446 0.705 0.429</cell></row><row><cell>[300,400]</cell><cell cols="4">0.829 0.558 0.806 0.577</cell></row><row><cell>[400,500]</cell><cell cols="4">0.894 0.661 0.868 0.663</cell></row><row><cell>[500,1000]</cell><cell cols="4">0.918 0.767 0.891 0.695</cell></row><row><cell cols="5">[1000, maximum] 0.992 0.941 0.981 0.922</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/JD-AI-Research-Silicon-Valley/SACN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by NSF grants CCF-1514357 and IIS-1718738, as well as NIH grants R01DA037349 and K02DA043063 to Jinbo Bi.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational graph autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems,Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge representation learning with entities, attributes and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ethnicity</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08140</idno>
		<idno>arXiv:1712.02121</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Column networks for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Edge attention-based multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04944</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards understanding the geometry of knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph convolutional neural networks for web-scale recommender systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
