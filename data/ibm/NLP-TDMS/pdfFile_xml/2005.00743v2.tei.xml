<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SYNTHESIZER: RETHINKING SELF-ATTENTION FOR TRANSFORMER MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<email>yitay@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
							<email>dbahri@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<email>metzler@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
							<email>zhezhao@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
							<email>chezheng@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SYNTHESIZER: RETHINKING SELF-ATTENTION FOR TRANSFORMER MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dot product self-attention is known to be central and indispensable to stateof-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose SYNTHESIZER, a model that learns synthetic attention weights without token-token interactions.</p><p>In our experiments, we first show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we find that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only 60% faster but also improves perplexity by a relative 3.5%. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformer models <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref> have demonstrated success across a wide range of tasks. This has resulted in Transformers largely displacing once popular auto-regressive and recurrent models in recent years. At the heart of Transformer models lies the query-key-value dot product attention. The success of Transformer models is widely attributed to this self-attention mechanism since fully connected token graphs, which are able to model long-range dependencies, provide a robust inductive bias.</p><p>But is the dot product self-attention really so important? Do we need it? Is it necessary to learn attention weights via pairwise dot products? This paper seeks to develop a deeper understanding of the role that the dot product self-attention mechanism plays in Transformer models.</p><p>The fundamental role of dot product self-attention is to learn self-alignment, i.e., to determine the relative importance of a single token with respect to all other tokens in the sequence. To this end, there have been memory metaphors and analogies constructed to support this claim. Indeed, the terms query, keys, and values imply that self-attention emulates a content-based retrieval process which leverages pairwise interactions at its very core.</p><p>Moving against convention, this paper postulates that we cannot only do without dot product self-attention but also content-based memory-like self-attention altogether. Traditionally, attention weights are learned at the instance or sample level, where weights are produced by instance-level pairwise interactions. As a result, these instance-specific interactions often fluctuate freely across different instances as they lack a consistent global context. This paper proposes SYNTHESIZER, a new model that learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products. We propose a diverse suite of synthesizing functions and extensively evaluate them. We characterize the source information that these synthe-Preprint sizing functions receive, i.e., whether they receive information from individual tokens, token-token interactions, and/or global task information. Intuitively, different source inputs to the synthesizing functions should capture diverse views, which may be useful when employed in conjunction.</p><p>Aside from generalizing the standard Transformer model, we show that it is possible to achieve competitive results with fully global attention weights that do not consider token-token interactions or any instance-level (local) information at all. More specifically, a random matrix SYNTHESIZER model achieves a 27.27 BLEU score on WMT 2014 English-German 1 . Via a set of rigorous experiments, we observe that the popular and well-established dot-product content-based attention can be approximated with simpler variants such as random matrices or dense layers without sacrificing much performance in some cases.</p><p>In our experiments, we also show that our relatively simple Synthesizer models also outperform Dynamic Convolutions <ref type="bibr" target="#b23">(Wu et al., 2019)</ref> with a +3.5% relative improvement in perplexity while being 60% faster. On encoding tasks, our factorized Synthesizers can outperform other low-rank efficient Transformer models such as Linformers <ref type="bibr" target="#b20">(Wang et al., 2020)</ref>. While simple Synthesizer models are able to perform competitively, our experiments show that the pairwise dot product is still ultimately helpful. When composing our synthesizing functions with dot products, we find that they consistently improve the performance of Transformers. In general, we believe our findings will spur further investigation and discussion about the true role and utility of the self-attention mechanism in Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions</head><p>Our key contributions are described as follows:</p><p>• We propose Synthetic Attention, a new way of learning to attend without explicitly attending (i.e., without dot product attention or content-based attention). Instead, we generate the alignment matrix independent of token-token dependencies and explore a potpourri of parameterized functions for synthesizing attention matrices.</p><p>• We propose SYNTHESIZER, a new model that leverages Synthetic Attention. The model performs competitive to state-of-the-art Transformer models on a wide range of language tasks, including machine translation and language modeling.</p><p>• Moreover, we show that (1) random learnable alignment matrices perform competitively and (2) token-token dependencies are not necessary to achieve good performance with Transformer models on certain tasks.</p><p>• On large-scale masked language modeling on the C4 dataset <ref type="bibr" target="#b13">(Raffel et al., 2019)</ref>, we show that simple random Synthesizers can outperform/match Lightweight Dynamic convolutions <ref type="bibr" target="#b23">(Wu et al., 2019)</ref> along with outperforming Transformers and Universal Transformers <ref type="bibr" target="#b6">(Dehghani et al., 2018)</ref>. On two encoding tasks, factorized random Synthesizers outperform low-rank Linformers <ref type="bibr" target="#b20">(Wang et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Attention-based models are used across a wide spectrum of problem domains. Such models are especially popular, due to their effectiveness, in the language and vision domains. Attention models can be traced back to the machine translation models of <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> and <ref type="bibr" target="#b10">(Luong et al., 2015)</ref>, where attention is employed to learn soft word alignments between language pairs. The intuition behind the attention mechanism is deeply-rooted in the notion of memory-based retrieval <ref type="bibr" target="#b7">(Graves et al., 2014;</ref><ref type="bibr" target="#b22">Weston et al., 2014)</ref>, in which soft differentiable addressing of memory was initially proposed.</p><p>The paradigm of learning self-alignments, also known as self-attention, has been largely popularized by Transformer models <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>. This technical narrative has also been explored by a number of other recent studies, including those on intra-attention <ref type="bibr" target="#b12">(Parikh et al., 2016)</ref>, selfmatching networks <ref type="bibr" target="#b21">(Wang et al., 2017)</ref>, and LSTMN <ref type="bibr" target="#b2">(Cheng et al., 2016)</ref>. To this end, Transformer models, which function primarily based on self-attention and feed-forward layers, generally serve as a reliable replacement for autoregressive recurrent models.</p><p>The self-attention layer itself has been the subject of many recent technical innovations. For example, recent studies have investigated improving the layer's overall efficiency via sparsification and reducing the complexity of computing the alignment matrix <ref type="bibr" target="#b3">(Child et al., 2019;</ref><ref type="bibr" target="#b9">Kitaev et al., 2020;</ref><ref type="bibr" target="#b8">Huang et al., 2018;</ref><ref type="bibr" target="#b18">Tay et al., 2020;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020)</ref>. These methods are tightly coupled with the query-key-value paradigm, employing a form of memory-based content retrieval as an attention mechanism. On the other end of the spectrum, there have been studies that advocate for replacing self-attention with convolution <ref type="bibr" target="#b23">(Wu et al., 2019)</ref>. The recent surge in interest in simplifying the attention mechanism raises important questions about the role and utility of the pairwise dot products, which are one the defining characteristics of self-attention models. Meanwhile, in the image domain, <ref type="bibr" target="#b4">(Cordonnier et al., 2019)</ref> shows connection of Transformers with CNNs.</p><p>Our work is a new take on the self-attention mechanism in Transformer models. We delve deeper, starting with replacing the pairwise dot products with what we call synthesizing functions that learn attention matrices that may or may not depend on the input tokens. The most closely related work is <ref type="bibr" target="#b14">((Raganato et al., 2020)</ref>), in which the authors propose using fixed (i.e., not learned) attention patterns in Transformer encoders. However, the scope of their work is limited to encoders and relies on manually defined handcrafted patterns that seem to work well. Our work takes this intuition further and expands on this narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>This section introduces our proposed SYNTHESIZER model. At its core, our model is essentially a Transformer model with self-attention modules replaced with our Synthetic Attention modules. <ref type="figure">Figure 3</ref>.1 illustrates the key ideas behind (a) Transformer (b) Dense Synthesizers and (c) Random Synthesizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SYNTHESIZER MODEL</head><p>This section introduces Synthetic Attention, our proposed self-attention module. Our model removes the notion of query-key-values in the self-attention module and directly synthesizes the alignment matrix instead.</p><p>Dense Synthesizer Let us consider the simplest variation of the SYNTHESIZER model which is conditioned on each input token. Overall, our method accepts an input X ∈ R ×d and produces an output of Y ∈ R ×d . Here, refers to the sequence length and d refers to the dimensionality of the model. We first adopt F (.), a parameterized function, for projecting input X i from d dimensions to dimensions.</p><formula xml:id="formula_0">B i = F (X i )<label>(1)</label></formula><p>where F (.) is a parameterized function that maps R d to R and i is the i-th token of X. Intuitively, this can be interpreted as learning a token-wise projection to the sequence length . Essentially, with this model, each token predicts weights for each token in the input sequence. In practice, we adopt a simple two layered feed-forward layer with ReLU activations for F (.):</p><formula xml:id="formula_1">F (X) = W 2 (σ R (W 1 (X) + b)) + b<label>(2)</label></formula><p>where σ R is the ReLU activation function and W 1 ∈ R d×d and W 2 ∈ R d× . Hence, B is now of R × . Given B, we now compute:</p><formula xml:id="formula_2">Y = Softmax(B)G(X).<label>(3)</label></formula><p>where G(.) is another parameterized function of X that is analogous to V (value) in the standard Transformer model.</p><p>This approach eliminates the dot product altogether by replacing QK in standard Transformers with the synthesizing function F (.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Synthesizer</head><p>The previous variant learns synthetic attention by conditioning on each input of X and projecting to dimensions. Hence, the Dense Synthesizer conditions on each token  <ref type="figure">Figure 1</ref>: Our proposed SYNTHESIZER model architecture.</p><p>independently, as opposed to pairwise token interactions in the vanilla Transformer model. We consider another variation of SYNTHESIZER where the attention weights are not conditioned on any input tokens. Instead, the attention weights are initialized to random values. These values can then either be trainable or kept fixed (denoted as Fixed).</p><p>Let R be a randomly initialized matrix. The Random Synthesizer is defined as:</p><formula xml:id="formula_3">Y = Softmax(R)G(X).<label>(4)</label></formula><p>where R ∈ R × . Notably, each head adds 2 parameters to the network. The basic idea 2 of the Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples. This is a direct generalization of the recently proposed fixed self-attention patterns <ref type="bibr" target="#b14">Raganato et al. (2020)</ref>.</p><p>Factorized Models The Dense Synthesizer adds d × parameters to the network. On the other hand, the Random Synthesizer adds × parameters. Here, note that we omit the Q, K projections in the standard Transformer which results in further parameter savings. Despite these savings, synthesized models can be cumbersome to learn when is large. Hence, we propose factorized variations of the SYNTHESIZER models and show that these variants perform comparably in practice.</p><p>Factorized Dense Synthesizer Factorized outputs not only slightly reduce the parameter cost of the SYNTHESIZER but also aid in preventing overfitting. The factorized variant of the dense synthesizer can be expressed as follows:</p><formula xml:id="formula_4">A, B = F A (X i ), F B (X i ) (5) where F A (.) projects input X i into a dimensions, F B (.) projects X i to b dimensions, and a × b = .</formula><p>The output of the factorized module is now written as:</p><formula xml:id="formula_5">Y = Softmax(C)G(X).<label>(6)</label></formula><p>where Factorized Random Synthesizer Similar to Factorized Synthesizers, we are also able to factorize</p><formula xml:id="formula_6">C = H A (A) * H B (B) where H A ,</formula><formula xml:id="formula_7">R into low rank matrices R 1 , R 2 ∈ R ×k . Y = Softmax(R 1 R 2 )G(X).<label>(7)</label></formula><p>Therefore, it is easy to see that, for each head, this reduces the parameter costs from 2 to 2( k) where k &lt;&lt; and hence helps prevent overfitting. In practice, we use a small value of k = 8.</p><p>Mixture of Synthesizers Finally, we note that all of the proposed synthetic attention variants can be mixed in an additive fashion. This can be expressed as:</p><formula xml:id="formula_8">Y = Softmax(α 1 S 1 (X) + · · · α N S N (X))G(X).<label>(8)</label></formula><p>where S(.) is a parameterized synthesizing function and the α (where α = 1) are learnable weights. In the case of mixing Random Factorized with standard Dense Synthesizers, this is expressed as:</p><formula xml:id="formula_9">Y = Softmax(α 1 R 1 R 2 + α 2 F (X))G(X).<label>(9)</label></formula><p>We investigate several Mixture of Synthesizers variants in our experiments.</p><p>On Parameters Depending on Sequence Length Random and dense Synthesizers both rely on parameters that depend on length . In general, we define a maximum length and dynamically truncate to the actual length of each batch. We note that this is in similar spirit to trainable positional encodings which have been common practice in Transformer models. Hence, we do not forsee any issue here. In the case that this is really a problem, one potential solution is to project to a smaller value b and tile b to the maximum sequence length. We leave this exploration to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DISCUSSION</head><p>This paper asks fundamental questions about the attention matrix A and whether it is possible to synthesize A by alternate means other than pairwise attention. It is worth noting that the regular dot product attention can also be subsumed by our SYNTHESIZER framework, i.e., SYNTHESIZER generalizes the Transformer model. In the case of the Transformer, the synthesizing function in question is</p><formula xml:id="formula_10">S(X) = F Q (X)F K (X) . Model S(X) Condition On Sample Interact |θ| Dot Product FQ(X)FK (Xi) Xj ∀j Local Yes 2d 2 Random R N/A Global No . 2 Fac. Random R1R 2 N/A Global No 2 k Dense F1σ(F2(Xi)) Xi Local No d 2 + d Fac. Dense</formula><p>HA(FA(Xi))) * HB(FB(Xi))) Xi    <ref type="table" target="#tab_6">Tables 4 and 5</ref> report results on the GLUE and SuperGLUE benchmarks. We note that the (R) and (D) variants of SYNTHESIZER do not achieve reasonable performance. This can be largely attributed to the fact that the encoder self-attention in the T5 setting also functions as a cross-sentence attention. For example, in the entailment or reading comprehension tasks, the premise and hypothesis are concatenated together and self-attention effectively acts as cross-sentence attention 4 . On datasets like SST, a straightforward sentiment classification task, this cross sentence attention is not necessary and therefore Syn (R) and Syn (D) both perform com-petitively. Optimistically, we observe that the mixture model Syn (R+V) outperforms the T5 model by a substantial margin (+1.9 points on SuperGLUE and +0.6 points on GLUE).   <ref type="table">Table 5</ref>: Experimental results (dev scores) on multi-task language understanding (SuperGLUE benchmark) for small model and en-mix mixture. Note: This task has been co-trained with GLUE.</p><formula xml:id="formula_11">Local No d 2 + d(k1 + k2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on GLUE and SuperGLUE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARING SYNTHESIZERS WITH DYNAMIC CONVOLUTIONS</head><p>To ascertain the competitiveness of Synthesizers, we compare them with Dynamic convolutions <ref type="bibr" target="#b23">(Wu et al., 2019)</ref>. We also benchmark the speed of these models. In order to do so, we conduct additional experiments on the T5 adaptation of masked language modeling on the C4 dataset <ref type="bibr" target="#b13">(Raffel et al., 2019)</ref> by comparing against lightweight dynamic convolutions <ref type="bibr" target="#b23">(Wu et al., 2019</ref>) on a masked language modeling task. We also take this chance to benchmark the speed of Synthesizers compared with Transformers. Experiments are conducted on Mesh Tensorflow     <ref type="bibr" target="#b13">(Raffel et al., 2019)</ref>. All models are at approximately similar parameterization. <ref type="table" target="#tab_8">Table 6</ref> reports the validation set log perplexity on masked language modeling 5 . We observe that Synthesizers (R) can outperform Dynamic Convolutions by a relative +3.5% while being +60% faster. Against Lightweight Dynamic Convolutions, we match the performance while being +5% faster. Given that this is the simple random Synthesizer baseline, we find this extremely interesting how it is able to outperform dynamic convolutions, a relatively complex model. The Random Synthesizer also has less FLOPS compared to both convolution models. On the other hand, the Mixture Synthesizer models that use the dot product attention improves the performance of the base Transformer model with relatively an equal model speed. Finally, similar to the earlier results, we see a consistent performance gain of Synthesizer (D+V) and Synthesizer (R+V) outperforming the base Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MLM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARING SYNTHESIZERS WITH LINFORMERS</head><p>We conduct more experiments comparing factorized random Synthesizers with Linformers. Since Linformer cannot be used to decode, we compare them on two encoding tasks from tensorflow datasets (AGnews <ref type="bibr" target="#b24">(Zhang et al., 2015)</ref> and movie reviews <ref type="bibr" target="#b11">(Maas et al., 2011)</ref>). We use k=32 for both factorized models. We also benchmark Transformers on this task. Note we do not use contextualized embeddings so results are not comparable with other work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We notice that factorized Synthesizers (FR) are competitive with Linformers and Transformers on this task. The accuracy of Syn (FR) is competitive with Linformers while Syn (FR+V) outperforms both Transformers and Linformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">OVERALL SUMMARY OF QUANTITATIVE RESULTS</head><p>This section summarizes our overall findings.</p><p>• Synthetic Attention is competitive even without Dot Product Attention On all evaluated tasks, we showed that synthesized attention functions competitively, i.e., it achieves performance reasonably close to the dot product self-attention. On one task (dialogue generation), the dot product self-attention is found to actually degrade performance. Amongst the other tasks, machine translation is the least affected by the removal of the vanilla dot product. These findings allow us to introspect about whether pairwise comparisons for self-attention are even necessary. On the multi-task language understanding benchmark, the self-attention functions as a form of cross-attention by concatenating sentence pairs. Hence, synthesize attention performance is considerably worse than vanilla Transformers.</p><p>• Synthetic Attention and Dot Product Attention are highly complementary Overall, we also observe that the dot product attention is very helpful. To this end, synthetic attention is highly complementary to the pairwise dot product attention. While Synthetic Attention can usually achieve competitive and fast performance on its own, synthetic attention boosts performs, composing multiple synthetic attention (and dot product attention) together shows gains on almost all tasks that we have investigated. Hence, we believe this to be a robust finding.</p><p>The simplest Synthesizers such as Random Synthesizers are fast competitive baselines Finally, we note that simple random Synthesizers are competitive with dynamic convolutions and Linformers, which are recently proposed models. On two encoding task and a large-scale masked language modeling task, we show that random (or factorized random) Synthesizers remain competitive to other fast or efficient Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposed SYNTHESIZER, a new Transformer model that employs Synthetic Attention.</p><p>We conducted a principled study to better understand and evaluate the utility of global alignment and local, instance-wise alignment (e.g., independent token and token-token based) in self-attention. We show that, on multiple tasks such as machine translation, language modeling, dialogue generation, masked language modeling and document classification, synthetic attention demonstrates competitive performance compared to vanilla self-attention. Moreover, for the dialogue generation task, pairwise interactions actually hurt performance. Notably, we reemphasize that this study refers to self-attention. We found that we are not able to replace cross-attention with simpler variants in most cases. Via a set of additional large-scale experiments, also find that Synthesizers can outperform or match Dynamic Convolutions and Factorized Synthesizers can outperform other low rank Linformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 DETAILED SETUP FOR EXPERIMENTS</head><p>Machine Translation We implement our models in Tensor2Tensor, using the standard base hyperparameter settings. Specifically, we use byte-pair encoding (BPE), 6-layered Transformer networks with hidden size 512, filter size of 2048 and 8 heads. We use label smoothing of 0.1. The maximum sequence length is set to 256. Training is performed using 8 x V100 GPUs. We train all models for 250K steps and report results at the last checkpoint. We use a length penalty of 0.6 and beam size of 4 following the default settings. We also compare with standard Transformer models. In the interest of keeping a consistent, fair evaluation across all model settings, we do not use checkpoint averaging or tune the decoding hyperparameters although this generally leads to better performance. We evaluate BLEU scores using sacrebleu.</p><p>Language Modeling We implement our models in Tensor2Tensor using the packed TPU setup of sequence length 256. We train our models on 300K steps on 16 TPU V2 chips. We use the lmx base model setting for fair comparison across all model variations. The model has 6 layers and 8 heads, along with a filter width of 2048 and hidden size of 512. We used conv relu for the positional feed-forward layers across all baselines since we find them to perform slightly better. We report results (subword level perplexity scores) on the test set at the final checkpoint.</p><p>Summarization For the summarization task, we train all models for 300K steps and a batch size of 128. All models use the base size setting. For the dialogue generation task, due to the smaller dataset size, we train a small model for 20K steps. All results are reported on the test set. For the summarization task, we use the well-established metrics, i.e., Rouge-1, Rouge-2 and Rouge-L. Experiments are conducted using Mesh Tensorflow.</p><p>Dialogue Generation For the dialogue generation task, we train our models on the small size for 20K steps. Experiments are conducted in Tensor2Tensor. We use NLG-Eval 6 <ref type="bibr" target="#b16">(Sharma et al., 2017)</ref> and report BLEU-1, BLEU-4, Rouge-L, Meteor, CIDr and Embedding based similarity scores (Emb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Language Understanding</head><p>Our experiments are based on the T5 repository 7 implemented in Mesh Tensorflow . We pre-train the vanilla T5 models and our models for 524288 steps using the span denoising objective. We then co-train the model on multiple tasks. We co-train on the en mix mixture (SuperGLUE and GLUE) for 100k steps with a constant learning rate of 10 −3 . Embedding and Softmax output layer parameters are kept fixed. The maximum sequence length is set to 512. We evaluate on the en mix mixture as defined in the original codebase which is comprised of training GLUE, SuperGLUE and SQuAD in a single model.</p><p>Pretraining experiments on C4 Experiments are conducted on Mesh Tensorflow. We pretrain for 524288 steps and report the perplexity on the validation set. We use 2x2 TPU V3 chips for our experiments. The sequence length is 512 and optimizer is Adafactor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Document Classification</head><p>We run experiments in JAX/FLAX (https:// github.com/google/flax) with base size models of 8 heads, 6 layers, MLP dimensions of 2048 and a hidden size of 512. We use the Adam optimizer with learning rate 0.05 and 8K steps linear warmup. We train for 10K steps and report evaluation results at 10K step. We use a batch size of 128. We build a new sentencepiece model for each new dataset comprising of 32K tokens.</p><p>No pretraining or contextualized embeddings are used. Experiments are run on 16 TPU v3 chips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ADDITIONAL VARIANTS OF SYNTHESIZER</head><p>We report results of several additional variants of SYNTHESIZER, most of which we found to have marginal or no improvement over the simple dense/random variations.</p><p>• Convolution -Applying a 1D convolution instead of a 2 layer nonlinear network. We vary the filter width in our experiments.</p><p>• Bottleneck -Converting the 2 layered feed forward network to a bottleneck layer, e.g., 512 → 16 → 512. We also experiment with a convolutional variant of bottleneck, i.e., projecting to low dimension space and then projecting back to high dimensions.</p><p>• Gated Linear Units (GLU), applying the GLU units of (  We also investigate the impact of the number of heads on performance. We trained three Random Synthesizer models for the small version of the machine translation tasks using the T5 framework without pretraining. For simplicity, evaluation is done via greedy decoding. We report scores on the development set. We are mainly interested in relative performance and not absolute numbers. <ref type="table" target="#tab_14">Table  9</ref> reports the results on varying the number of heads on performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Weights</head><p>We are interested in investigating how the synthetically generated attention weights differ from the dot product attention weights. <ref type="figure">Figure 2</ref> shows the attention histograms on trained Transformer and SYNTHESIZER models. We report histograms at layers 1, 3, and 5 of a 6 layered (Transformer or SYNTHESIZER) model at 50K steps. We found that the weight distributions remain relatively identical thereafter. <ref type="figure">Figure 3</ref> shows the initialization state. We observe that there are distinct differences in the weight distribution of SYNTHESIZER and Transformer models. The variance of the SYN-THESIZER weights tends to be higher. On the other hand, the weights on the Transformer model tends to gravitate near 0 and have smaller variance. There are also notable differences across the (R) and (D) SYNTHESIZER variants. Specifically, the (D) model in general has greater max values with more values in the 0.1-0.2 range while the values of the R model tends to stay closer to 0. In this section, we perform a deeper analysis of the SYNTHESIZER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesizer weights on LM1B.</head><p>Transformer weights on LM1B.</p><p>Analysis Finally, we are interested to understand what these Synthesizer models are learning. We inspect the random synthetic attention weights for language modeling task LM1B and visualise the differences compared to the vanilla attention. We find that, for the LM task, Synthesizers are capable of learning a local window, emulating the vanilla Transformer quite closely despite starting from completely random. The weights, however, seem smoother and less coarse as compared to the Transformer. This seems to reflect what we expect since the Synthesizer does not benefit from token specific information. We provide additional analysis and visualisation of weights for the Machine Translation task in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 MORE ATTENTION WEIGHTS ANALYSIS</head><p>This section illustrates the attention weights extracted from different variants of Synthesizer on the machine translation (En-De) task. Weights are extracted from lower layers although we do not find any substantial difference in the patterns in early layers and deeper layers. We extract them from Tensorboard midway during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla</head><p>Random Fac. Random Dense Fac. Dense Analysis We first observe that these weights differ a lot from the LM weights shown in the main paper in Section 4.5. This shows that the Synthesizer learns very different weights for different tasks. Next, based on the weighs on MT, we observe a very different pattern in all variants of Synthesizer. For the decoder weights, the main difference seems to be the overall magnitude and distribution values of the weights. However, we can easily observe the cracks and lines of the factorized variants. For the encoder weights, we observe that the Random and Dense variants are more uniform. On the other hand, there appears to be structural/regional clustering of values in the factorized variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 3: Init Decoder weights (Reference)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visual analysis of Synthetic Attention (decoder) on WMT EnDe. Visual analysis of Synthetic Attention (encoder) on WMT EnDe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Overview of all Synthesizing Functions.Table 1lists the different model variants explored within our SYNTHESIZER framework. The 'condition on' column refers to whether the synthesized output is produced as a function of X i or every X i , X j pair. The 'sample' column indicates whether a given variant leverages local or global context. Random Synthesizers are global because they share the same global alignment patterns across all samples. Dense Synthesizers are considered to be local as they are conditioned on X i , which makes the alignment pattern dependent on each individual sample. To this end, it is imperative for synthesized models to have multiple heads to be effective. Specifically, we conduct experiments on (1) machine translation (EnDe, EnFr) (2) autoregressive language modeling (LM1B) (3) text generation (summarization and dialogue modeling and (4) multi-task natural language processing (GLUE/SuperGLUE). Details of each experiments can be found in the appendix.4.1 COMPARING SYNTHESIZER VARIANTS AND TRANSFORMER MODELSThis section dives into a detailed study of multiple Synthesizer variants and the base Transformer model.</figDesc><table><row><cell></cell><cell></cell><cell>NMT (BLEU)</cell><cell cols="2">LM (PPL)</cell></row><row><cell>Model</cell><cell>|θ|</cell><cell>EnDe EnFr</cell><cell>|θ|</cell><cell>LM</cell></row><row><cell cols="3">Trans.  † 67M 27.30 38.10</cell><cell>-</cell><cell>-</cell></row><row><cell>Trans.</cell><cell cols="4">67M 27.67 41.57 70M 38.21</cell></row><row><cell></cell><cell></cell><cell>Synthesizer Models</cell><cell></cell><cell></cell></row><row><cell>Fix</cell><cell cols="4">61M 23.89 38.31 53M 50.52</cell></row><row><cell>R</cell><cell cols="4">67M 27.27 41.12 58M 40.60</cell></row><row><cell>FR</cell><cell cols="4">61M 27.30 41.12 53M 42.40</cell></row><row><cell>D</cell><cell cols="4">62M 27.43 41.39 53M 40.88</cell></row><row><cell>FD</cell><cell cols="4">61M 27.32 41.57 53M 41.20</cell></row><row><cell>R+D</cell><cell cols="4">67M 27.68 41.21 58M 42.35</cell></row><row><cell>D+V</cell><cell cols="4">74M 27.57 41.38 70M 37.27</cell></row><row><cell>R+V</cell><cell cols="4">73M 28.47 41.85 70M 40.05</cell></row></table><note>4 EXPERIMENTS This section outlines our experimental setup and results. We first conduct experiments on five tasks to evaluate the effectiveness 3 of different Synthesizer variants along with how they compare to the vanilla Transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Experimental Results on MT/LM First, we</cell></row><row><cell>observe that our Random Synthesizer baseline</cell></row><row><cell>achieves 27.27 on EnDe and 41.12 on EnFr. The</cell></row><row><cell>non-trainable (i.e., fixed) variant performs sub-</cell></row><row><cell>stantially worse, but still yields surprisingly strong</cell></row><row><cell>≈ 24 BLEU with fixed random attention weights.</cell></row><row><cell>Most other SYNTHESIZER variants achieve com-</cell></row><row><cell>petitive performance, although with slight per-</cell></row><row><cell>formance degradation compared to Transformers.</cell></row></table><note>Experimental Results on WMT'14 English- German, WMT'14 English-French Machine Trans- lation tasks and Language Modeling One Billion (LM1B). † denotes original reported results in (Vaswani et al., 2017).An interesting finding is that the Mixture model of Random + Dense synthesizer performs com- parably to vanilla Transformers on EnDe. When mixing the standard dot product attention, per- formance further increases by +0.8 BLEU points (EnDe). In general, the performance of SYNTHE-SIZER variants are competitive with Transformers for this task. On LM1b, We find that the Random Synthesizers perform within 1-2 PPL points away from the vanilla Transformer model. The best per- forming model is the Synthesizer (D+V), which achieves the best performance on this setting. Results on Text Generation For summarization, we find that the (R) and (D) variants do not outperform Transformers. The performance of the (D) model is ≈ 2 Rouge-L points below Transformers. Hence, we postulate that the local sample-wise pairwise interac- tions are important for the summarization task. On the other hand, the utility of synthesized attention can also be observed, i.e., the (R+V) and (R+D)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Experimental results on Abstrac-</cell></row><row><cell>tive Summarization (CNN/Dailymail) and</cell></row><row><cell>Dialogue Generation (PersonaChat). We re-</cell></row><row><cell>port on RL (Rouge-L), B4 (Bleu-4), Met.</cell></row><row><cell>(Meteor) and CIDr.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results (dev scores) on multi-task language understanding (GLUE benchmark) for small model and en-mix mixture. Note: This task has been co-trained with SuperGLUE.</figDesc><table><row><cell>Model</cell><cell cols="2">SGlue BoolQ</cell><cell>CB</cell><cell cols="3">CoPA MultiRC ReCoRD RTE WiC WSC</cell></row><row><cell>T5 (Base)</cell><cell>70.3</cell><cell>78.2</cell><cell>72.1/83.9</cell><cell>59.0</cell><cell>73.1/32.1 71.1/70.3 77.3 65.8</cell><cell>80.8</cell></row><row><cell>Syn (R)</cell><cell>61.1</cell><cell>69.5</cell><cell>54.6/73.2</cell><cell>60.0</cell><cell>63.0/15.7 58.4/57.4 67.5 64.4</cell><cell>66.3</cell></row><row><cell>Syn (D)</cell><cell>58.5</cell><cell>69.5</cell><cell>51.7/71.4</cell><cell>51.0</cell><cell>66.0/15.8 54.1/53.0 67.5 65.2</cell><cell>58.7</cell></row><row><cell>Syn (D+V)</cell><cell>69.7</cell><cell>79.3</cell><cell>74.3/85.7</cell><cell>64.0</cell><cell>73.8/33.7 69.9/69.2 78.7 64.3</cell><cell>68.3</cell></row><row><cell>Syn (R+V)</cell><cell>72.2</cell><cell>79.3</cell><cell>82.7/91.1</cell><cell>64.0</cell><cell>74.3/34.9 70.8/69.9 82.7 64.6</cell><cell>75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and ran on 2x2 TPU V3 Chips for approximately 524K steps.</figDesc><table><row><cell>Model</cell><cell cols="3">Log PPL Steps/Sec Params</cell><cell>FLOPS</cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell>1.865</cell><cell>3.90</cell><cell>223M</cell><cell>3.70 × 10 12</cell></row><row><cell>Dynamic Conv (Wu et al., 2019)</cell><cell>2.040</cell><cell>2.65</cell><cell>257M</cell><cell>3.93 × 10 12</cell></row><row><cell>Lightweight Conv (Wu et al., 2019)</cell><cell>1.972</cell><cell>4.05</cell><cell>224M</cell><cell>3.50 × 10 12</cell></row><row><cell>Synthesizer (D)</cell><cell>1.965</cell><cell>3.61</cell><cell>224M</cell><cell>3.80 × 10 12</cell></row><row><cell>Synthesizer (R)</cell><cell>1.972</cell><cell>4.26</cell><cell>254M</cell><cell>3.36 × 10 12</cell></row><row><cell>Synthesizer (R+V)</cell><cell>1.849</cell><cell>3.79</cell><cell>292M</cell><cell>4.03 × 10 12</cell></row><row><cell>Synthesizer (D+V)</cell><cell>1.832</cell><cell>3.34</cell><cell>243M</cell><cell>4.20 × 10 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Validation perplexity scores on C4 dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Results on Encoding only tasks (accu-</cell></row><row><cell>racy).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Results for additional SYNTHESIZER variants on WMT EnDe (BLEU scores)A.3 EFFECT OF NUMBER OF HEADS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>HeadsEnDeEnFr EnRo Syn h=2 19.43 34.12 18.67 Syn h=4 20.42 35.26 19.78 Syn h=8 20.88 34.92 20.28 Syn h=16 21.71 35.26 20.43 Syn h=32 21.72 36.01 20.52</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Effect of number of heads on multi-task MT. Increasing the number of heads improves performance.</figDesc><table><row><cell>A.4 ANALYSIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Enc L1</cell><cell>Enc L3</cell><cell>Enc L5</cell><cell>Dec L1</cell><cell>Dec L3</cell><cell>Dec L5</cell></row><row><cell cols="6">Figure 2: Histogram of Encoder and Decoder Attention Weights on MT (WMT EnDe). L denotes</cell></row><row><cell cols="4">the layer number and Enc/Dec denotes encoder or decoder.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The originally reported result is 27.30.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We were not expecting this variation to work at all, but it turns out to be a strong baseline.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that we are primarily interested in making controlled comparisons instead of going for the state-ofthe-art result on each task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">On a related note, the perceived success of pairwise self-attention might also be attributed to the fact that these public benchmarks are bias towards pairwise matching tasks. In reality, this is computationally prohibitive for many practical real-world applications<ref type="bibr" target="#b15">(Seo et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that this follows the sequence transduction style in T5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/Maluuba/nlg-eval.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/google-research/text-to-text-transfer-transformer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Music transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fixed encoder self-attention patterns in transformer-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10260</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Phraseindexed question answering: A new challenge for scalable document comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. CoRR, abs/1706.09799</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.09799" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10414" to="10423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11296</idno>
		<title level="m">Sparse sinkhorn attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
