<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RuDaS: Synthetic Datasets for Rule Learning and Evaluation Tools</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
							<email>veronika.thost@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RuDaS: Synthetic Datasets for Rule Learning and Evaluation Tools</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Logical rules are a popular knowledge representation language in many domains, representing background knowledge and encoding information that can be derived from given facts in a compact form. However, rule formulation is a complex process that requires deep domain expertise, and is further challenged by today's often large, heterogeneous, and incomplete knowledge graphs. Several approaches for learning rules automatically, given a set of input example facts, have been proposed over time, including, more recently, neural systems. Yet, the area is missing adequate datasets and evaluation approaches: existing datasets often resemble toy examples that neither cover the various kinds of dependencies between rules nor allow for testing scalability. We present a tool for generating different kinds of datasets and for evaluating rule learning systems, including new performance measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Logical rules are a popular knowledge representation language in many domains. They represent domain knowledge, encode information that can be derived from given facts in a compact form, and allow for logical reasoning. For example, given facts parent(ann, bob) and parent(bob, dan), the datalog rule <ref type="bibr" target="#b14">(S. Ceri and Tanca 1989)</ref> grandparent(X, Z) :-parent(X, Y ), parent(Y, Z) encodes the fact grandparent(ann, dan) and describes its dependency on the other facts. Moreover, if the data grows and new facts are added, we can automatically derive new knowledge. Since rule formulation is complex and requires domain expertise, rule learning <ref type="bibr" target="#b13">(Raedt 2008;</ref><ref type="bibr" target="#b7">Fürnkranz, Gamberger, and Lavrac 2012)</ref> has been an area of active research in AI for a long time, also under the name inductive logic programming (ILP). It has recently revived with the increasing use of knowledge graphs (KGs), which can be considered as large fact collections. KGs are used in various domains such as in the Semantic Web or with companies such as Google <ref type="bibr" target="#b5">(Dong et al. 2014)</ref> or Amazon <ref type="bibr" target="#b10">(Krishnan 2018)</ref>, and there are large knowledge bases in the medical domain. Useful rules over these knowledge bases would obviously provide various benefits. However, we argue that the evaluations of current ILP systems are insufficient. We demonstrate that the reported results are questionable, especially, in terms of generalization and because the datasets are lacking in various dimensions.</p><p>The evaluation of rule learning has changed over time. While the classical rule learning methods often focused on tricky problems in complex domains <ref type="bibr">(ILP year na;</ref><ref type="bibr" target="#b13">Quinlan 1990</ref>) and proved to be effective in practical applications, current evaluations can be divided into three categories. Some consider very small example problems with usually less than 50 facts and only few rules to be learned <ref type="bibr" target="#b6">(Evans and Grefenstette 2018;</ref><ref type="bibr" target="#b13">Rocktäschel and Riedel 2017;</ref><ref type="bibr" target="#b10">Minervini et al. 2020)</ref>. Often, these problems are completely defined, in the sense that all facts are classified as either true or false, or there are at least some negative examples given. Hence, the systems can be thoroughly evaluated based on classical measures such as accuracy. Other evaluations regard (subsets of) real KGs such as Wikidata 1 or DBpedia 2 , some with millions of facts <ref type="bibr" target="#b7">(Galárraga et al. 2015;</ref><ref type="bibr" target="#b11">Omran, Wang, and Wang 2018;</ref><ref type="bibr" target="#b15">Vaclav Zeman and Svtek 2019)</ref>. Since there are no rules over these KGs, the rule suggestions of the systems are usually evaluated using metrics capturing the precision and coverage of rules (e.g., standard confidence <ref type="bibr" target="#b7">(Galárraga et al. 2015)</ref>) based on the facts contained in the KG. However, since the KGs are generally incomplete, the quality of the rule suggestions is not fully captured in this way. For instance, <ref type="bibr" target="#b11">Omran, Wang, and Wang (2018)</ref> present an illustrative example rule, gender(X, male) :-isCEO(X, Y ), isCompany(Y ), which might well capture the facts in many existing KGs but which is heavily biased and does not extend to the entirety of valid facts beyond them. Furthermore, we cannot assume that the few considered KGs completely capture the variety of existing domains and especially the rules in them. For example, <ref type="bibr" target="#b10">Minervini et al. (2018)</ref> propose rules over Word-Net 3 that are of very simple nature -containing only a small number of the predicates in WordNet and having only a single body atom -and very different from the ones suggested in <ref type="bibr" target="#b7">(Galárraga et al. 2015)</ref> for other KGs.</p><p>Also the evaluation metrics vary, especially considering the intersection between more modern and classic ap-proaches. We will show that most of the standard information retrieval measures used in machine learning are not adequate for a logic context because they neglect important facets like the size of the Herbrand universe (e.g., this may yield a too high accuracy). Some other measures have been used for neural ILP such as Mean Reciprocal Rank, or precision/recall@K, but they can be applied only in specific cases (i.e. the system outputs weighted/probabilistic rules or a ranking of facts). Yet, strict logic measures are not perfect either, since they are based on the assumption that the domain is very small and human understandable. For this reason the community needs to consider several metrics and should define new metrics suitable for both worlds.</p><p>Recently, synthetic datasets have been proposed, but they are very simple and do not cover all characteristics necessary to evaluate an ILP tool properly: 1) <ref type="bibr" target="#b5">Dong et al. (2019)</ref> provide a first synthetic dataset generator for graph reasoning, which can produce an arbitrary number of facts regarding five fixed predicates while the rules are hand written. 2) de Jong and <ref type="bibr" target="#b4">Sha (2019)</ref> argue, in line with us, for more diverse datasets for rule learning. However, their generated datasets are still restricted in several dimensions: e.g. small size and very simple rules (based on five fixed templates). There are well-known ILP competitions 4 in the logic community, but their evaluations are usually based only on test facts and not on rules.</p><p>There are also benchmarks in the database community which are related in that they cover schemata and rules. However, either their use cases and thus the kinds of rule sets considered are rather restricted (e.g., for schema mapping, there are rules from source to target schema, but the rules do not depend on each other in the sense that one is to be applied after the other <ref type="bibr" target="#b0">(Alexe, Tan, and Velegrakis 2008;</ref><ref type="bibr" target="#b1">Arocena et al. 2015)</ref>), or there is a number of fixed test scenarios <ref type="bibr" target="#b2">(Benedikt et al. 2017)</ref>. Further, database data is usually curated, and the benchmarks were developed to evaluate customized algorithms, while our focus is on more arbitrary data, and on learning systems and approximate solutions, needing large amounts of data to learn from (i.e., instead of a few, fixed test sets).</p><p>In summary, we claim that the existing datasets are not sufficient to cover the possible variety of real data and the rules that could be mined from such arbitrary data. However, many existing KGs are large, noisy, heterogeneous, and might embed complex rules. The problem is that we do not know if such embedded rules do not exist or if they are just not learned today because of the restrictions of the current rule learners. Since it is unclear what sort of complexity is required to model the real world, we opted for an artificial but largely random approach that covers different kinds of variety and complexity missing in today's datasets.</p><p>In this paper, we extend the categorization of rule learning datasets beyond the numbers of constants, predicates, and facts. In particular, we propose to consider rather obvious characteristics like the amount of noise (i.e., wrong or missing facts) and (in)completeness (i.e., share of consequences of rules present in the data). 4 for example <ref type="bibr">: 2016, http://ilp16.doc.ic.ac.uk/competition</ref> We present RuDaS (Synthetic Datasets for Rule Learning), a tool for generating synthetic datasets containing both facts and rules, and for evaluating rule learning systems, that overcomes the above mentioned shortcomings of existing datasets and offers proper evaluation methods. RuDaS is parameterizable in the standard and in the new categories, and thus allows for a more fine-grained analysis of rule learning systems. It also supports this analysis by computing classical and more recent metrics, including two new ones that we introduce. Finally, we evaluate representatives of different types of rule learning systems on our datasets demonstrating the necessity of having a diversified portfolio of datasets to help revealing the variety in the capabilities of the systems, and thus also to support and help researchers in developing and optimizing new/existing approaches. Moreover, our experimental shows in detail the benefits of our two novel metrics, and how they complement the existing performance measures. RuDaS is available at &lt;https://github.com/IBM/RuDaS&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Rule Learning Preliminaries</head><p>We assume the reader to be familiar with first-order logic (FOL) and its related concepts (e.g., inference, Herbrand models and universes, etc.). We consider datalog rules <ref type="bibr" target="#b14">(S. Ceri and Tanca 1989)</ref>:</p><formula xml:id="formula_0">α 0 :-α 1 , . . . , α m .<label>(1)</label></formula><p>of length m ≥ 1 where all atoms α j , 0 ≤ j ≤ m, are of the form p(t 1 , . . . , t n ) with a predicate p of arity n ≥ 1 and terms t k , 1 ≤ k ≤ n. A term is either a constant or a variable. α 0 is called the head and the conjunction α 1 , . . . , α m the body of the rule. All variables that occur in the head must occur in the body. A fact is an atom not containing variables. Note that several classical ILP systems also consider more complex function-free Horn rules, which allow for existential quantification in the rule head or negation in the body, but most recent systems focus on datalog rules or restrictions of those <ref type="bibr" target="#b7">(Galárraga et al. 2015;</ref><ref type="bibr" target="#b6">Evans and Grefenstette 2018;</ref><ref type="bibr" target="#b13">Rocktäschel and Riedel 2017)</ref>. In particular, reasoning systems for KGs <ref type="bibr" target="#b16">(Yang, Yang, and Cohen 2017;</ref><ref type="bibr" target="#b11">Omran, Wang, and Wang 2018)</ref> often consider only binary predicates and chain rules of the form:</p><formula xml:id="formula_1">p 0 (X 1 , X m+1 ) :-p 1 (X 1 , X 2 ), . . . , p m (X m , X m+1 ). (2)</formula><p>We define the problem of rule learning in the most general way: given background knowledge in the form of facts, including a set of so-called positive examples (vs. negative or counter-examples), the goal is to learn rules that can be used to infer the positive examples from the background knowledge, based on standard FOL semantics. As it is common, we do not separate the background knowledge into two types of facts but consider a single fact set as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches to Rule Learning</head><p>Classical ILP systems such as FOIL <ref type="bibr" target="#b13">(Quinlan 1990</ref>) and Progol (Muggleton 1995) usually apply exhaustive algorithms to mine rules for the given data and either require false facts as counter-examples or assume a closed world (for an p8(X0, X1) :-p6(X0, X2), p4(X1, X0). p6(X0, X2) :-p0(X3, X0), p9(X4, X2).</p><p>(a) Chain p6(X0, X1) :-p0(X0, X2), p8(X1, X1). p0(X0, X2) :-p4(X0, X3), p2(X2, X0).</p><p>p8(X1, X1) :-p2(X1, X1).</p><formula xml:id="formula_2">(b) Rooted DG (RDG) p5(X0, X1) :-p7(X0, X2), p2(X0, X1). OR p7(X0, X2) :-p1(X2, X3), p0(X3, X0). p7(X0, X2) :-p6(X2, X0). p2(X0, X1) :-p6(X0, X4), p9(X1, X4).</formula><p>(c) Disjunctive Rooted DG (DRDG) <ref type="figure">Figure 1</ref>: Example rule structure generated for the different categories with size S and depth 2.</p><p>overview of classical ILP systems see <ref type="table" target="#tab_4">Table 2</ref> in   <ref type="bibr" target="#b15">(Wang and Li 2015)</ref> as the most prominent representatives, assume the data to be only partially complete and focus on rule learning in the sense of mining patterns that occur frequently in the data. Furthermore, they implement advanced optimization approaches that make them applicable in wider scenarios. In this way, they address already many of the issues that arise with today's knowledge graphs, still maintaining their processing exhaustive.</p><p>Recently, neural rule learning approaches have been proposed: <ref type="bibr" target="#b16">Yang, Yang, and Cohen;</ref><ref type="bibr" target="#b6">Evans and Grefenstette;</ref><ref type="bibr" target="#b10">Minervini et al.;</ref><ref type="bibr" target="#b11">Omran, Wang, and Wang;</ref><ref type="bibr">Campero et al. (2017;</ref>. These methodologies seem a promising alternative considering that deep learning copes with vast amounts of noisy and heterogeneous data. The proposed solutions consider vector or matrix embeddings of symbols, facts and/or rules, and model inference using differentiable operations such as vector addition and matrix composition. However, they are still premature: they only learn certain kinds of rules or lack scalability (e.g., searching the entire rule space) and hence cannot compete with established rule mining systems such as AMIE+ yet, as shown in <ref type="bibr" target="#b11">(Omran, Wang, and Wang 2018)</ref>, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RuDaS Datasets</head><p>RuDaS contains an easy-to-use generator for ILP datasets that generates datasets that vary in many dimensions and is highly parameterizable. While existing datasets are missing more detailed specifications but are described only in terms of size and number of different constants and predicates, we propose a much more detailed set of metrics (parameters in the generator) which can serve as a general classification scheme for ILP datasets and support evaluations. In this section, we give details about these metrics, and thus on the possible shapes of RuDaS datasets. Each dataset contains the rules and the facts in files in standard Prolog format (using the syntax of Rule <ref type="formula" target="#formula_0">(1)</ref>). We also describe example datasets we generated, which can be found in our repository. Observe that our datasets are based on rules, as required for ILP, but can be applied for evaluating the closely related and very popular link prediction systems as well; specifically, auxiliary information provided with the datasets contains the missing consequences which can be used as test facts in that context.</p><p>Symbols. Our datasets are domain independent, which means that we consider synthetic names p i for predicates, c i for constants, and X i for variables with i ≥ 0. While the kinds and numbers of the symbols used is random, it can be controlled by setting the following generator parameters:</p><p>• number of constants and predicates • min/max arity of predicates Observe that these numbers influence the variability and number of generated rules and facts.</p><p>Rules. RuDaS datasets contain datalog rules (see Section 2) of variable structure. The generation is largely at random in terms of which predicates, variables, and constants appear in the rules; that is, in the structure of every single rule. We only require the head to contain some variable.</p><p>To classify a set of rules, we propose four categories depending on the dependencies between rules: Chain, Rooted Directed Graph (DG), Disjunctive Rooted DG, and Mixed. <ref type="figure">Figure 1</ref> shows a generated rule set for each category. The dependencies between the rules are represented as edges in a directed graph where the rules are the nodes. That is, an incoming edge shows that the facts inferred by the child node's rule might be used, during inference with the rule at the parent node. The node at the top is called the root. In the following, we use (rule) graph and DG interchangeably.</p><p>Category Chain. Each rule, except the one at the root, infers facts relevant for exactly one other rule (i.e., every node has at most one parent node) and, for each rule, there is at most one such other rule which might infer facts relevant for the rule (i.e., every node has at most one child node). However, recursive rules (where the predicate in the head occurs also in the body) represent an exception, they are relevant for themselves and for one other rule (i.e., the graph has a small loop at each node representing a recursive rule).</p><p>Category Rooted DG (RDG). It generalizes category Chain in that every rule can be relevant for several others (i.e., each node can have multiple parent nodes). Furthermore, for each rule, there may be several other rules which might infer facts relevant for the rule (i.e., a node may have several child nodes); and at least one such case exists. But, for each predicate occurring in the body of the former rule, there must be at most one other rule with this predicate in the head; that is, there are no alternative rules to derive facts relevant for a rule w.r.t. a specific body atom.</p><p>Category Disjunctive Rooted DG (DRDG). It generalizes category RDG by allowing for the latter alternative rules (represented as children of an "OR" node); and at least one such case exists.</p><p>Category Mixed. A rule graph that contains connected components of different of the above categories. <ref type="figure">Figure 1</ref> illustrates the differences between the categories. In (a), for each rule, there is at most one child node with a rule relevant for its derivations. In (b), there might be multiple children, but each child node contains a different head predicate. In (c), the latter does not hold anymore; for given facts, there may be various derivations.</p><p>The numbers and categories of connected components are selected randomly by default. The shape of RuDaS rule sets can be influenced with the following parameters though:</p><p>• number and maximal length of rules • category of connected components (i.e., one of the above)</p><p>• min/max number of connected components</p><p>• maximal depth of rule graphs (i.e., number of rules nodes in the maximum of the shortest paths between root and leaves)</p><p>Facts. The main advantage of the RuDaS datasets, the availability of the rules, allows for classifying the facts as well. More specifically, facts can be (ir)relevant for inference, depending on if their predicates do (not) occur in a rule body, and they may be consequences of inferences. Such a classification of facts is impossible for all the existing datasets that do not contain rules, but allows for a better evaluation of the rule learners' capabilities (see Section 6).</p><p>RuDaS fact sets vary in the following parameters:</p><p>• dataset size: XS, S, M, L, XL</p><p>• open-world degree n OW ∈ [0, 1]</p><p>• amount of noise in the data n Noise+ , n Noise-∈ [0, 1]</p><p>An XS dataset contains about 50-100 facts, an S dataset about 101-1,000, an M dataset about 1,001-10,000, an L dataset about 10,001-100,000, and an XL dataset about 100,001-500,000. For larger sizes, we suggest meaningful abbreviations in the form of X2L for XXL etc., which allow for extension while being short and easy to understand.</p><p>Since the main purpose of RuDaS is allowing the analysis of the rules learned (vs. scalability), we have however not considered such larger datasets so far. The open-world degree n OW specifies how many of the consequences from an initial set of relevant facts, called support facts, are missing in the dataset (see Section 4 for a detailed description of the generation process). By noise, we mean facts that are not helpful in learning the rules either because they are not relevant for deriving the positive examples (n Noise+ ) or because they are relevant but missing (n Noise-).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Datasets: RuDaS-v0</head><p>For demonstration purposes, we generated RuDaS-v0, a set of datasets which are available to the community (in our repository), and which we also used in our experiments (see Section 6). <ref type="table" target="#tab_2">Table 1</ref> shows statistics about them. The datasets model different possible scenarios, and mainly vary in the structures and sizes of the rule sets and in the sorts and quantities of facts. RuDaS-v0 contains 40 Chain, 78 RDG, and 78 DRDG datasets, of sizes XS and S, and of depths 2 and 3, all evenly distributed. Note that each of the rules sets in RuDaS-v0 consists of exactly one connected component, and that we did not generate rule sets of category Mixed; Mixed datasets with connected components of possibly different categories can be easily created by combining our generated datasets. Further, we constrained both the maximal rule length and arity of atoms to two because several existing rule learning systems require that. All the datasets were generated such that they are missing 20-40% of all consequences, 15-30% of the original support facts, and contain 10-30% facts that are irrelevant for the derivation of positive examples. Since real datasets may strongly vary in the numbers of missing consequences and noise and, in particular, since these numbers are generally unknown, we chose factors seeming reasonable to us. Also note that there is information regarding the accuracy of real fact sets such as YAGO 5 (95%) and NELL 6 (87%), that measures the amount of data correctly extracted from the Web etc. and hence corresponds to 1−n Noise+ in our setting. Thus, our choices in this regard thus seem to be realistic.</p><p>We hence simulated an open-world setting and incorporated noise. While we consider this to be the most realistic training or evaluation scenario, specific rule learning capabilities might be better evaluated in more artificial settings with either consequences or noise missing. For this reason, every dataset mentioned in the table additionally includes files containing the incomplete set of facts without noise (i.e., n OW as in the table; n Noise+ = 0; n Noise-= 0) and the complete fact set (i.e., n OW = 0), with and without noise.  <ref type="table" target="#tab_2">min avg max  min avg  max  min avg max  min avg max   10  CHAIN  XS  2  2  2  2  51  74  95  5  7  9  31  47  71  10  CHAIN  XS  3  3  3  3  49  70  97  7  8  9  31  43  64  10  CHAIN  S  2  2  2  2  168 447  908  9  10  11  97  259 460  10  CHAIN  S  3  3  3  3  120 508  958  8  10  11  52  230 374   22  RDG  XS  2  3  3  3  49  84  122  6  9  11  28  50  84  12  RDG  XS  3  4  5  6  56  104  172  8  10  11  41  55  75  22  RDG  S  2  3  3  3  200 646 1065  6  11  11  71  370 648  22  RDG  S  3  4  5  7  280 613 1107  10  11  11  149 297 612   22  DRDG  XS  2  3  4  5  60  100  181  6  9  11  29  55  82  12  DRDG  XS  3  4  7  11  58  144  573  8  10  11  34  58  89</ref>   Note that the size bounds of our fact sets are not strict, some sizes are slightly larger than expected (e.g., 1065 for size S) because our initial generation needs to take into account that some facts, e.g., consequences, may be removed thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Generation</head><p>In this section, we describe the generation process of the rules and facts in detail, assuming the generator parameters (also configuration) listed in Section 3 to be set.</p><p>Preprocessing. As already mentioned, most parameters are determined randomly in a preprocessing step if they are not fixed in the configuration, such as the symbols that will be used, the numbers of DGs to be generated, and their depths. However, all random selections are within the bounds given in the configuration under consideration; for instance, we ensure that the symbols chosen suffice to generate rule graphs and fact sets of selected size and that at least one graph is of the given maximal depth.</p><p>Rule generation. According to the rule set category specified and graph depths determined, rules (nodes in the graphs) of form (1) are generated top down breadth first, for each of the rule graphs to be constructed. The generation is largely at random, that is, w.r.t. the number of child nodes of a node and which body atom they relate to; the number of atoms in a rule; and the predicates within the latter, including the choice of the target predicate (i.e., the predicate in the head of the root) in the very first step. RuDaS also offers the option that all graphs have the same target predicate. To allow for more derivations, we currently only consider variables as terms in head atoms; the choice of the remaining terms is based on probabilities as described in the following. Given the atoms to be considered (in terms of their number and predicates) and an arbitrary choice of head variables, we first determine a position for each of the latter in the former. Then we populate the other positions one after the other: a head variable is chosen with probability p h = 1 5 ; for one of the variables introduced so far, we have probability p v = (1−p h ) * 3 4 ; for a constant, p c = (1−p h ) * (1−p v ) * 1 10 ; and, for a fresh variable,</p><formula xml:id="formula_3">p f = (1 − p h ) * (1 − p v ) * (1 − p c ).</formula><p>While this conditional scheme might seem rather complex, we found that it works best in terms of the variety it yields; also, these probabilities can be changed easily.</p><p>Fact generation. The fact generation is done in three phases: we first construct a set D of relevant facts in a closedworld setting, consisting of support facts S and their consequences C, and then adapt it according to n OW and n Noise* .</p><p>As it is the (natural) idea, we generate facts by instantiating the rule graphs multiple times, based on the assumption that rule learning systems need positive examples for a rule to learn that rule, and stop the generation when the requested number of facts has been generated. We actually stop later because we need to account for the fact that we subsequently will delete some of them according to n OW . More specifically, we continuously iterate over all rule graphs, for each, select an arbitrary but fresh variable assignment σ, and then iterate over the graph nodes as described in the following, in a bottom-up way. First, we consider each leaf n and corresponding rule of form (1) and generate support facts σ(α 1 ), . . . , σ(α m ). Then, we infer the consequences based on the rules and all facts generated so far. For every node n on the next level and corresponding rule of form (1), we only generate those of the facts σ(α 1 ), . . . , σ(α m ) as support facts which are not among the consequences inferred previously. We then again apply inference, possibly obtaining new consequences, and continue iterating over all nodes in the graph in this way. We further diversify the process based on two integer parameters, n DG and n Skip : in every n DG -th iteration the graph is instantiated exactly in the way described; in the other iterations, we skip the instantiation of a node with probability 1/n Skip and, in the case of DR-DGs, only instantiate a single branch below disjunctive nodes. We implemented this diversification to have more variability in the supports facts, avoiding to have only complete paths from the leaves to the root.</p><p>In the open-world setting, we subsequently construct a set D OW by randomly deleting consequences from D according to the open-world degree given: assuming T ⊆ C to be the set of target facts (i.e., consequences containing the target predicate), we remove n OW % from C \ T, and similarly n OW % from T. In this way, we ensure that the open-world degree is reflected in the target facts. Though, there is the option to have it more arbitrary by removing n OW % from C instead of splitting the deletion into two parts.</p><p>The noise generation is split similarly. Specifically, we construct a set D OW+Noise based on D OW by arbitrarily removing n Noise-% from S, and by adding arbitrary fresh facts that are neither in C (i.e., we do not add facts we have removed in the previous step) nor contain the target predicate such that D OW+Noise \ T contains n Noise+ % of noise. In addition, we add arbitrary fresh facts on the target predicate that are not in T already such that subset of D OW+Noise on that predicate finally contains n Noise+ % of noise.</p><p>Output. The dataset generation produces: the rules; a training set (D OW+Noise ), which is of the requested size, and fulfills n OW , n Noise+ , and n Noise-; and custom fact sets S and C for our evaluation tools generated in the same way as S and C. For further experiments, RuDaS also outputs D, D Noise (an adaptation of D which contains noise but all of C), D OW , S, and C (see also the end of Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Tools</head><p>RuDaS contains also an evaluator (written in Python) that is able to compare the original rules of a dataset to the ones produced by a rule learning system. In the following, we give details about its evaluation approach and about the different measures we consider.</p><p>We focus on three logic(-inspired) distances and four standard information retrieval measures that are relevant to our goal of capturing rule learning performance: 1) Herbrand distance, the traditional distance between Herbrand models; two normalized versions of the Herbrand distance 2) Herbrand accuracy (H-accuracy) and 3) Herbrand score (Hscore), a new metric we propose in this paper; 4) accuracy 5) precision; 6) recall; and 7) F1-score.</p><p>Our test fact sets (both facts and consequences) in the evaluation do not contain noise and all the consequences can be recovered by the original rules applied over the given facts. In line with that, we focused on measures that maintain the closed-world assumption, and did not include in RuDaS measures that focus on the open-world aspect for the evaluation (i.e., PCA in <ref type="figure" target="#fig_2">(Galárraga et al. 2015)</ref>). Although, as it is explained in Section 5.2, F1-score is the best suit metric in RuDaS to deal with an open-world evaluation.</p><p>In what follows, I(R, F ) denotes the set of facts inferred by grounding the rules R over the support facts F excluding the facts in F . We denote an original rule set by R, a learned one by R , and support facts by F. Our evaluation is performed comparing two sets: 1) I(R , F) obtained by the application of the induced rules R to the fact sets F = S (S and C described in Section 4 -Output) using a forwardchaining engine (written in python and available in our tool); 2) C that corresponds to I(R, F): the result of the application of the original rules R to the fact set F = S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Logic Measures</head><p>The Herbrand distance h d between two logic programs (sets of rules), defined over the same set of constants and predicates, is defined as the number of facts that differ between the two minimal Herbrand models of the two programs:</p><formula xml:id="formula_4">h d (R, R , F) := | [I(R, F)∪I(R , F)]\[I(R, F)∩I(R , F)] | .</formula><p>The standard confidence s c <ref type="figure" target="#fig_2">(Galárraga et al. 2015)</ref> is the fraction of correctly inferred facts w.r.t. all facts that can be inferred by the learned rules capturing their precision:</p><formula xml:id="formula_5">s c (R, R , F) := |I(R, F) ∩ I(R , F)| |I(R , F)|</formula><p>In our closed-world setting, this corresponds to the precision of a model, since it is easy to see that |I(R, F) ∩ I(R , F)| are the number of true positive examples and |I(R , F)| corresponds to the union of true and false positive examples. The Herbrant accuracy h r corresponds to the Herbrand distance normalized on the Herbrand universe:</p><formula xml:id="formula_6">h r (R, R , F) := 1− h d u ,</formula><p>where u is the size of the Herbrand universe defined by the original program. We introduce a new metric, the Herbrand score (H-score) defined as:</p><formula xml:id="formula_7">H-score(R, R , F) : = |I(R, F) ∩ I(R , F)| |I(R, F) ∪ I(R , F)| = 1 − h d (R, R , F) |I(R, F) ∪ I(R , F)|</formula><p>H-score provides an advantage over the other metrics since it captures both how many correct facts a set of rules produces and also its completeness (how many of the facts inferred by the original rules R were correctly discovered), while the other measures consider these points only partially.</p><p>Note that Herbrand accuracy is not a significant measure if F or the Herbrand universe is large, because, in these cases, it will be very high (close to 1) disregarding the quality of the rules. This happens because all the facts in F are considered correct predictions, as well as the facts in the Herbrand universe that neither appear in I(R, F) nor in I(R , F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Information Retrieval Measures</head><p>We adapted the main measures used in the machine learning evaluations to our context. We define: Note that the accuracy measure is not a significant measure if F or the Herbrand universe is large, for the same reason reported for Herbrand accuracy above. Moreover, F1score is similar to H-score, with the difference that F1-score gives more priority to the TP examples. We believe that giving uniform priority to FN, TP, and FP is more reasonable in the context of logic; this is in line with standard logic measures like h d . However, F1-score better suits (compared to H-score) open-world settings where some of the consequences could be missing, and thus count as FP (despite being correct). For this reason F1-score would give a better estimate of the quality of the induced rules since it focuses more on the TP examples and give less priority to the generated FP examples.</p><p>We observe that, if I(R, F) = I(R , F), then H-score is equal to precision and both are equal to 1; and, if I(R, F) and I(R , F) are disjoint, then both are 0. Moreover the two measures coincide if I(R, F) ⊆ I(R , F). The main difference between the two measures is highlighted in the case where I(R , F) ⊆ I(R, F). Then, precision = 1 but Hscore is &lt; 1. This property is intentional for our new metric (H-score) because we want to have H-score 1 only if the predicted facts are exactly those produced by the original rules while precision is 1 as soon as all predicted facts are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rule-based Measures</head><p>Several distance metrics between two sets of logic rules have been defined in the literature <ref type="bibr" target="#b6">(Estruch et al. 2005;</ref><ref type="bibr" target="#b6">Estruch et al. 2010;</ref><ref type="bibr" target="#b10">Nienhuys-Cheng 1997;</ref><ref type="bibr" target="#b12">Preda 2006;</ref><ref type="bibr" target="#b14">Seda and Lane 2003)</ref>. However, most of them strongly rely on the parse structure of the formulas and hence are more suitable for more expressive logics.</p><p>For this reason, we propose a new metric, the Rule-score (R-score), which is tailored to rules, in that it calculates a distance d R between the rules that have the same head predicate and, for the latter, computes the pairwise distances d A between the two rules' atoms. Note that, below, we consider datalog rules without constants and with only binary predicates for simplicity, but the definitions can be easily extended to non-binary atoms. An idea of how to deal with constants in addition to variables is suggested in <ref type="bibr" target="#b6">(Estruch et al. 2010</ref>), for example.</p><p>Our distance d A between two non-ground, binary atoms takes into account a specific mapping ω ∈ Ω between their variables so that we can later lift it to rule level; Ω is the set of all possible variable re-namings between two sets of variables names (in our case from two rules). Given two atoms a 1 = p(X 1 , X 2 ) and a 2 = q(Y 1 , Y 2 ) and such a mapping ω, d A (a 1 , a 2 , ω) = 1 if their predicates differ, d A (a 1 , a 2 , ω) = 0 if ω(X 1 ) = Y 1 and ω(X 2 ) = Y 2 , and otherwise:</p><formula xml:id="formula_8">d A (a 1 , a 2 , ω) = 1 4 2 i=1 1 c (ω(X i ) = Y i ) .</formula><p>where the complement indicator function 1 c (e) of an event e is equal to 0 if the event is satisfied and 1 otherwise. Note that d A is based upon the main, and most used, distance metric between two ground atoms, the Nienhuys-Cheng distance (Nienhuys-Cheng and <ref type="bibr" target="#b10">de Wolf 1997)</ref>. In fact, it only differs from the latter in that it is defined for non-ground atoms by considering a variable re-naming. Example 5.1. Consider two rules r 1 and r 2 as follows:</p><p>r 1 : p 1 <ref type="figure">(A, B)</ref> :-p 2 <ref type="figure">(A, A), p 3 (B, B)</ref>, p 4 (A, B). r 2 : p 1 (X, X) :-p 2 (Y, X), p 2 (X, X).</p><p>The re-namings ω 1 = {A : X, B : Y } and ω 1 = {A : Y, B : X} yield: 1) r 1 with ω 1 : p 1 (X, Y ) :-p 2 (X, X), p 3 (Y, Y ), p 4 (X, Y ).</p><p>2) r 1 with ω 2 : p 1 (Y, X) :-p 2 (Y, Y ), p 3 (X, X), p 4 (Y, X).</p><p>Let h i and h 2 denote the head atoms of r 1 and r 2 , respectively. Then, we get d A (h 1 , h 2 , ω 1 ) = 0.25 and d A (h 1 , h 2 , ω 2 ) = 0.25.</p><p>Intuitively, our distance d R between two rules r 1 and r 2 considers matches between the rules, where a match consists of a pairing between their atoms together with a variable renaming, and takes the best match as distance (averaged over the number of atoms). More specifically, such a pairing for r 1 and r 2 is a set of pairs such that the first component is a body atom from r 1 and the second from r 2 . It contains max(|b(r 1 )|, |b(r 2 )|) pairs (|b(r i )| denotes the number of atoms in the body of r i ). To represent a match, we require the atoms to have the same predicate and additionally allow for an empty placeholder atom (denoted by −), extending d A such that it has maximal distance 1 to any other atom. Note that the placeholder also accounts for the case that the rules are of different length.</p><p>Example 5.2. For the rules of Example 5.1, there are two pairings: <ref type="figure">(p 3 (B, B)</ref>, −), (p 4 <ref type="figure">(A, B)</ref> <ref type="figure">(p 3 (B, B)</ref>, −), (p 4 <ref type="figure">(A, B)</ref>, −) } Now, we define the distance d R between rules r 1 and r 2 as:</p><formula xml:id="formula_9">a) c 1 = { (p 2 (A, A), p 2 (Y, X)),</formula><formula xml:id="formula_10">, −) } b) c 2 = { (p 2 (A, A), p 2 (X, X)),</formula><formula xml:id="formula_11">d R (r 1 , r 2 ) = 1 n a min ω∈Ω d A (h 1 , h 2 , ω)+min c∈C (a1,a2)∈c d A (a 1 , a 2 , ω)</formula><p>where n a = max(|b(r 1 )|, |b(r 2 )|)+1 is the number of atoms of the rule with more atoms; h i is the head atom of rule r i ; Ω is the set of all possible variable re-namings between the variable names in the two rules; C is the set of all possible pairings for r 1 and r 2 .</p><p>Example 5.3. For the re-namings ω 1 and ω 2 and pairings c 1 and c 2 from the previous examples, d R represents the minimum of 0.25 + (a1,a2)∈ci d A (a 1 , a 2 , ω j ), with the sums: 1a) 0.25 + 1 + 1 = 2.5 2a) 0.25 + 1 + 1 = 2.5 1b) 0 + 1 + 1 = 2.25 2b) 0.5 + 1 + 1 = 2.75</p><p>Given that n a = 4, we obtain d R (r 1 , r 2 ) = 2.25 4 = 0.5625. Finally, our novel metric Rule-score (R-score) for two logic programs, the original program R and the induced program R (rules of form equation 1), is defined as:</p><formula xml:id="formula_12">R-score(R, R ) = 1− 1 |R| r1∈R min r2∈R [hp(r1)] d R (r 1 , r 2 )</formula><p>where the function hp(r) corresponds to the head predicate of a given rule r; R[p] denotes the rules in R with head predicate p, and |R| denotes the number of rules in R.</p><p>The goal of our experiments is to demonstrate the necessity of having a diversified portfolio of datasets for the evaluation of a rule learning system. The existing datasets are not diverse enough to provide a comprehensive evaluation of ILP methods (e.g., often fall into category Chain). Note that the purpose of our experiments is not to provide an exhaustive analysis of existing ILP systems but to show that this kind of analysis should be done, by pointing out that important aspects of rule learning data that have been ignored so far impact system performance.</p><p>To this purpose we chose a set of representative ILP systems (described below) that covers the different methodologies adopted by researchers during the years to approach the problem of rule learning.</p><p>We compared the following systems (configuration details in the appendix): 1) FOIL <ref type="bibr" target="#b13">(Quinlan 1990</ref>), a traditional ILP system; 2) AMIE+ <ref type="bibr" target="#b7">(Galárraga et al. 2015)</ref>, a rule mining system; 3) Neural-LP <ref type="bibr" target="#b16">(Yang, Yang, and Cohen 2017)</ref>; and 4) NTP (Rocktäschel and Riedel 2017). The latter are both neural approaches. AMIE+, Neural-LP, and NTP output confidence scores for the learned rules. We therefore filtered their output using a system-specific threshold, obtained using grid search over all datasets. Further, to not disadvantage Neural-LP and NTP, which use auxiliary predicates, we ignored the facts produced on these predicates in the computation of the result metrics. It is important to notice that NTP requires additional information in the form of rule templates, obviously representing an advantage.</p><p>We evaluated the systems on the datasets described in Section 3 in four main experiments to understand, respectively, the variety of the performance metrics, and the impact of missing consequences, noise, rule dependencies, and dataset size. We set a time limit of 24h (both for system executions and evaluations 7 ) to make the evaluation feasible since some computations took days. However, we did not penalize the instances that exceeded the limit since we are interested in the rules that can be learned overall. Generally, the runtimes varied greatly over the datasets, were often surprisingly long (AMIE, NTP can take hours; only Neural-LP usually terminated within seconds), and provide not much insight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Results in Different Metrics</head><p>In this experiment, we regard the overall results, reported in <ref type="table" target="#tab_4">Table 2</ref>, in terms of the metrics introduced in Section 5. As expected, the results for F1-score and Herbrand score are very similar, the only difference is that F1-score is a more "optimistic" measure, giving advantage to the methods with a higher number of true positive examples. The results for Rscore are in line with these metrics and, although translated, follow especially close the trend of H-score (see <ref type="figure" target="#fig_2">Figure 2</ref> for a visualization of the comparison). Hence, R-score indeed represents a valid alternative metric with the additional advantage of computational efficiency, since it does not require the computation of the induced facts I(R , F). Also Herbrand accuracy and accuracy provide similar results. Observe that these two measures are not meaningful in our settings since they yield always very high performances. Note that precision and H-score are very close for AMIE+, Neural-LP, and NTP, but not for FOIL. This could be explained by the fact that the training of the former systems maximizes functions that are similar to precision, while FOIL uses heuristics to produce the rules that induce the maximum number of facts in the training set and minimum number of facts not in the training set. The great discrepancy between the two measures with FOIL means that the rules it learns do not produce many false facts but only a subset of the facts induced by the original rules. For AMIE+ instead, since precision and H-score are similar, we have that its rules produce most of the consequences of the original rules and, thanks to the good performance, they do not produce too many false facts. Considering Neural-LP and NTP the two measures are also very similar, but very low: their rules produce most of the positive examples but also a lot of false facts. In the following subsections we report H-score, but the R-score results we computed for comparison showed the same trends.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOIL AMIE+</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Missing Consequences and Noise</head><p>In this experiment, we evaluated the performance of the systems in the presence of complete information, incomplete information, and incomplete information with noise. This was performed analyzing the impact of the different parameters given in RuDaS: n OW , n Noise+ , and n Noise-. The results are reported in <ref type="table" target="#tab_6">Table 3</ref>. The noise parameters are defines as follows 8 : complete datasets n OW = 0, n Noise-= 0, and n Noise+ = 0, incomplete datasets n OW ∈ {0.2, 0.3, 0.4}, n Noise-= 0, and n Noise+ = 0, and incomplete + noise datasets n OW ∈ {0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact of Dependencies Between Rules</head><p>In this experiment, we analyze the impact of the kind of the dependencies between rules. The results are reported in <ref type="table" target="#tab_7">Table 4</ref>. As expected, the systems perform very different depending on the datasets' rule categories. We notice that the systems perform better on the Chain datasets while only learning partially RDG and DRDG rules, meaning that the available rule learning systems are not yet able to capture complex rule set structures. Our results also confirm the system descriptions w.r.t. the rules they support. For instance, AMIE+ does not consider reflexive rules and requires rules to be connected (i.e., every atom must share an argument with each of the other atoms of the rule) and closed (all variables appear at least twice). And, by chance, the Chain datasets in RuDaS-v0 more often satisfy these conditions than the other datasets (this is not true in general: our generator produces comprehensive datasets that do not necessarily satisfy this property).</p><p>Nevertheless, rules that are not fully supported can still be recognized partially. We have seen this by analyzing the rules learned by Neural-LP, which only supports chain rules of form equation 2.</p><p>NTP also performs better on Chain datasets, but the discrepancy with the other types of datasets is not substantial. This can be explained by the fact that we provided all necessary templates (for more details about the system requirements see (Rocktäschel and Riedel 2017)).</p><p>We cannot draw significant conclusions for FOIL given its unstable behaviour regarding the dataset type.</p><p>In conclusion, our experiments demonstrate the importance of considering datasets with different kinds of rules sets and of considering different measures of performance to be able to fully understand the weaknesses and strengths of a rule learning system.   In this experiment, we analyzed the impact of the dataset size considering four different size-depth combinations: the results for XS-2, XS-3, S-2, and S-3 datasets are reported in <ref type="table" target="#tab_8">Table 5</ref>. We can observe that FOIL is not scalable, since there is a 20% performance gap from the XS-dataset to the S-dataset. Although it does not seem to be influenced by the rules dependency tree depth, showing support to nested rules. AMIE+ seemingly shows constant performance and thus scalability. We can observe that there is a noticeable decrease of performance if we increase the depth of the rule dependency graphs. Neural-LP and NTP are robust to noise and incomplete data but NTP is not scalable yielding good accuracy only on the very small and simple instances (XS-2), while Neural-LP seems to be more scalable (we cannot see a decrease of performance, augmenting the size of the dataset) but does not support nested rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAIN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we have presented RuDaS, a system for generating datasets for rule learning and for evaluating rule learning systems. Our experiments on new, generated datasets have shown that it is very important to have diverse datasets that consider several rule types separately, different sizes, different amount and type of noise and to perform the evaluation using different measures of performance. With our datasets and evaluation tool we provide these capabilities allowing to fully understand the weaknesses and strengths of a rule learning system.</p><p>There are various directions for future work. The dataset generation can be extended to more expressive logics (with negation, existential quantification in rule heads, functions, etc.) including probabilistic inference that would allow to evaluate methods that learn probabilistic rules. Another possibility is to increase the probability to generate special predicate types: transitive predicates, predicates that admit only disjoint combinations of constants (e.g., the relation between a person and their SSN), or functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the sets of true positive examples (TP) as the cardinality |I(R, F) ∩ I(R , F)|, the set of false positive examples (FP) as the cardinality |I(R, F) \ I(R , F)|; the set of false negative examples (FN) as the cardinality |I(R , F) \ I(R, F)|; and the set of true negative examples (TN) as the cardinality of the difference between the Herbrand universe and the union I(R, F) ∪ I(R , F). Given these four definitions, accuracy, precision, recall, F1-score etc. can be defined as usual (Russell and Norvig 2002).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7Figure 2 :</head><label>2</label><figDesc>If a system learns very many (usually wrong) rules, the computation of measures based on a closure may become unfeasible.Table 2 results visualized showing similarity of metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Overview of our generated datasets, altogether 196; column # is the count of datasets described in the corresponding row. All other numbers are averages. For Chain, we have n OW = 0.3, n Noise-= 0.2, and n Noise+ = 0.1. For RDG and DRDG: n OW ∈ {0.2, 0.3, 0.4}, n Noise-∈ {0.15, 0.2, 0.3}, and n Noise+ ∈ {0.1, 0.2, 0.3}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Impact of different metrics, each one averaged on 120 datasets with uniformly distributed categories ∈ {CHAIN, RDG, DRDG}, sizes ∈ {XS,S}, and graph depths ∈ {2,3}; n OW = 0.3, n Noise-= 0.2, n Noise+ = 0.1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>2, 0.3, 0.4}, n Noise-∈ {0.15, 0.3}, and n Noise+ ∈ {0.2, 0.3}. Moreover, in order to give an impression of some</figDesc><table><row><cell cols="5">of the datasets considered in existing evaluations, we in-</cell></row><row><cell cols="5">cluded one manually created dataset, EVEN, inspired by</cell></row><row><cell cols="5">the corresponding dataset used in (Evans and Grefenstette</cell></row><row><cell cols="5">2018) 9 , which contains complete information. We notice</cell></row><row><cell cols="5">that FOIL shows a good performance if the information is</cell></row><row><cell cols="5">exact and complete while showing decreasing performance</cell></row><row><cell cols="5">in more noisy scenarios. This is a result of the assumptions</cell></row><row><cell cols="5">FOIL is based upon: it assumes negative examples to be</cell></row><row><cell cols="5">given in addition in order to guide rule learning and, in par-</cell></row><row><cell cols="5">ticular, missing facts to be false (see Section 4.1 in (Quin-</cell></row><row><cell cols="5">lan 1990)). AMIE+ seems to perform constant on average,</cell></row><row><cell cols="5">showing robustness to noise and incomplete data in all the</cell></row><row><cell cols="5">datasets. Neural-LP and NTP seem to be robust to noise and</cell></row><row><cell cols="5">incomplete data, not showing changes in performance while</cell></row><row><cell cols="3">adding more noise and uncertainty.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">EVEN Complete Incomplete</cell><cell>Incomplete + Noise</cell></row><row><cell>FOIL</cell><cell>1.0</cell><cell>0.4053</cell><cell>0.1919</cell><cell>0.0849</cell></row><row><cell>AMIE+</cell><cell>-</cell><cell>0.2021</cell><cell>0.2098</cell><cell>0.2075</cell></row><row><cell>Neural-LP</cell><cell>-</cell><cell>0.0633</cell><cell>0.0692</cell><cell>0.0649</cell></row><row><cell>NTP</cell><cell>1.0</cell><cell>0.0482</cell><cell>0.0617</cell><cell>0.0574</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Effect of missing consequences and noise on 144 datasets. Each H-score value is averaged on 48 datasets, with uniformly distributed categories ∈ {RDG, DRDG}, sizes ∈ {XS,S}, and graph depths ∈ {2,3}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Impact of dataset category. H-score averaged on 40 datasets. Datasets as in Section 6.1.</figDesc><table><row><cell cols="4">6.4 Scalability: Impact of Dataset Size</cell></row><row><cell></cell><cell>XS-2</cell><cell>XS-3</cell><cell>S-2</cell><cell>S-3</cell></row><row><cell>FOIL</cell><cell cols="2">0.2815 0.2074</cell><cell cols="2">0.0356 0.0934</cell></row><row><cell>AMIE+</cell><cell cols="2">0.1449 0.1319</cell><cell cols="2">0.4392 0.2124</cell></row><row><cell>Neural-LP</cell><cell cols="2">0.1155 0.0673</cell><cell cols="2">0.1281 0.0992</cell></row><row><cell>NTP</cell><cell cols="2">0.1512 0.0432</cell><cell cols="2">0.0652 0.0374</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Impact of dataset size and rule graph depth. H-score averaged on 30 datasets. Datasets as in Section 6.1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/yago-naga/yago3 6 http://rtw.ml.cmu.edu/rtw/overview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">the set memberships are intended to mean "uniformly distributed over". 9 In our version, even(X) :-even(Z), succ(Z, Y ), succ(Y, X) is the only rule, and the input facts are such that we also have an accuracy of 1 if the symmetric rule even(Z) :-even(X), succ(Z, Y ), succ(Y, X) is learned (using the original fact set it would be 0). AMIE+ and Neural-LP do not support unary predicates, present in EVEN.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A System Configurations</head><p>All the systems have the same computational restrictions (i.e. CPU, memory, time limit, etc.). The reader can find all the details <ref type="bibr">(scripts etc.)</ref>  • Parameter for accepting the rules: learned using gridsearch = 0.0 -all the rules are accepted { "data": { "kb": "$DATAPATH/$TRAIN.nl", "templates": "$DATAPATH/rules.nlt" }, "meta": { "parent": "$SYSTEMSPATH/conf/default.conf", "test_graph_creation": False, "experiment_prefix": "$NAME", "test_set": "$TEST", "result_file": "$OUTPUTPATH/results.tsv", "debug": False }, "training": { "num_epochs": 100, "report_interval": 10, "pos_per_batch": 10, "neg_per_pos": 1, "optimizer": "Adam", "learning_rate": 0.001, "sampling_scheme": "all", "init": None, # xavier initialization "clip": (-1.0, 1.0) }, "model": { "input_size": 100, "k_max": 10, "name": "???", "neural_link_predictor": "ComplEx", "l2": 0.01, # 0.01 # 0.0001 "keep_prob": 0.7 } }</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stbenchmark: Towards a benchmark for mapping systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Velegrakis ; Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Velegrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ibench integration metadata generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arocena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking the chase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benedikt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsamoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PODS</title>
		<meeting>of PODS</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Logical rule induction and theory learning using neural theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Campero</surname></persName>
		</author>
		<idno>abs/1809.02193</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural theorem provers do not learn rules without exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha ; De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1906.06805</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proc. of KDD</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning explanatory rules from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FLOPS</title>
		<meeting>of FLOPS</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="64" />
		</imprint>
	</monogr>
	<note>An integrated distance for atoms</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast rule mining in ontological knowledge bases with AMIE+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamberger</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<ptr target="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/amie/" />
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="730" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Foundations of Rule Learning. Cognitive Technologies</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rule learning from knowledge graphs guided by embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISWC, Part I</title>
		<meeting>of ISWC, Part I</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="72" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="https://www.doc.ic.ac.uk/∼shm/applications.html" />
	</analytic>
	<monogr>
		<title level="j">ILP. year n.a. ILP Applications and Datasets</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
	<note>ILP year na</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distance between herbrand interpretations: A measure for approximations to a target concept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bonjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nienhuys-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Nienhuys-Cheng</surname></persName>
		</author>
		<idno>2019-09-03. [Minervini et al. 2018</idno>
		<ptr target="https://blog.aboutamazon.com/innovation/making-search-easier" />
	</analytic>
	<monogr>
		<title level="m">Inductive Logic Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="245" to="286" />
		</imprint>
	</monogr>
	<note>Proc. of NAMPI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable rule learning via learning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Wang ; Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metrics for sets of atoms and logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Preda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the University of Craiova</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Logical and relational learning. Cognitive Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<ptr target="https://github.com/uclmr/ntp" />
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
		<imprint>
			<publisher>Prentice Hall Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3791" to="3803" />
		</imprint>
	</monogr>
	<note>Proc. of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On continuous models of computation: Towards computing the distance between (logic) programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanca ; S</forename><surname>Ceri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Ceri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tanca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Seda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reasoning Web. International Summer School, Tutorial Lectures</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="142" to="172" />
		</imprint>
	</monogr>
	<note>Proc. of IWFM</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rdf2rules: Learning rules from RDF knowledge bases by mining frequent predicate cycles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Svtek ; Vaclav Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Svtek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1512.07734</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Rdfrules: Making rdf rule mining easier and even more efficient. semantic-web-journal</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cohen ; Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://github.com/fanyangxyz/Neural-LP" />
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
