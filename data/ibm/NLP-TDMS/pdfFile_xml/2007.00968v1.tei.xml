<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Project PIAF: Building a Native French Question-Answering Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Keraron</surname></persName>
							<email>rachel@recital.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lancrenon</surname></persName>
							<email>guillaume.lancrenon@data.gouv.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Bras</surname></persName>
							<email>mathilde.bras@data.gouv.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Allary</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Moyse</surname></persName>
							<email>gilles@recital.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
							<email>thomas@recital.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmundo-Pavel</forename><surname>Soriano-Morales</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
							<email>jacopo@recital.ai</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">reciTAL</orgName>
								<orgName type="department" key="dep2">DINUM, Prime Minister&apos;s Office</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>Paris (France) ‡Etalab</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Project PIAF: Building a Native French Question-Answering Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Annotation</term>
					<term>Crowdsourcing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the lack of data for non-English languages, in particular for the evaluation of downstream tasks such as Question Answering, we present a participatory effort to collect a native French Question Answering Dataset. Furthermore, we describe and publicly release the annotation tool developed for our collection effort, along with the data obtained and preliminary baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Along with the availability of massive amounts of data, the increase in computational power has in recent years allowed the development of Deep Learning techniques, leading to significant advancements in the fields of Computer Vision (CV), and Natural Language Processing (NLP), among others. Visual information can, to some extent, be considered to generalize across cultures in many real world applications; in contrast, having to deal with languages, NLP applications are naturally bound to language specificities. Over the years, the NLP community has produced several resources to tackle tasks we call, for simplicity, upstream (such as Part-Of-Speech tagging, Dependency Parsing, etc.), targeting multiple languages and enabling the construction of effective automated systems. Still, for tasks we refer to as downstream, i.e. those which enable the development of value-added end products such as Question Answering (QA) or Conversational Agents, the current state-of-the-art approaches require massive amounts of annotated data which are almost exclusively available in English. Notable exceptions include Machine Translation, for which abundant parallel corpora have been built from resources such as the European parliamentary proceedings, or Language Modeling -which can be tackled in a selfsupervised manner, hence only requiring massive amounts of text in the target language(s). Such asymmetry has recently been acknowledged by a resolution of the EU Parliament. <ref type="bibr">1</ref> In this work, we focus on collecting Question Answering data in French in a participatory setup: we describe the data collection protocol adopted, report descriptive statistics and * : equal contribution 1 European Parliament resolution of 11 September 2018 on language equality in the digital age. http://www.europarl. europa.eu/sides/getDoc.do?pubRef=-//EP/ /TEXT+TA+P8-TA-2018-0332+0+DOC+XML+V0//EN baselines, and provide details on the implementation of the open source annotation tool we developed. Such tool allows volunteers to participate in crowdsourced QA dataset collection campaigns. In summary, we make the following contributions:</p><p>1. we develop and release a novel annotation tool to collect large-scale QA data in a participatory scenario;</p><p>2. we release a native French QA dataset;</p><p>3. we provide baselines using state-of-the-art methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several datasets for QA have been recently produced. The Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> consists of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles. The answer to each question is a segment of text from the corresponding article. Its latest version includes circa 50k additional unanswerable questions <ref type="bibr" target="#b18">(Rajpurkar et al., 2018)</ref>. Targeting open-domain QA, WikiQA <ref type="bibr" target="#b23">(Yang et al., 2015)</ref> consists of Bing queries and corresponding answer sentences taken from Wikipedia articles. CoQA <ref type="bibr" target="#b19">(Reddy et al., 2018)</ref> focuses on conversational aspects, compounding to 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. Based on CNN news articles, NewsQA <ref type="bibr" target="#b21">(Trischler et al., 2017)</ref> consists of 120k Q&amp;A pairs, including unanswerable questions, collected using a 3-stage, siloed process; crowdworkers are split into 3 groups: Questioners, who see only an article's headline and highlights to produce a question; Answerers, who see the question and the full article, then select an answer passage; and Validators, who see the article, the question, and a set of answers that they rank.</p><p>Recognizing that several pieces of information often jointly imply another fact, the two datasets provided by QAngaroo <ref type="bibr" target="#b22">(Welbl et al., 2018)</ref>   <ref type="bibr" target="#b3">(Feng et al., 2015)</ref> is a domain-specific QA dataset targeting the insurance domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do</head><p>We Need Native Non-English Data?</p><p>In short, yes. From the above mentioned data collection efforts, we see that with the exception of Yahoo! Answers, which does not provide supporting evidence for the answers, no large-scale QA dataset is available for languages other than English. Current approaches based on transfer learning and multilingual model pretraining allow to build models able to deal with non-English. In other words, one can effectively finetune a pretrained multilingual language model (e.g. <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>) on English QA data. Nonetheless, as observed by <ref type="bibr" target="#b10">(Lewis et al., 2019)</ref>, native evaluation data for the targeted language is a must-have in order to measure progress on the task for a given language. <ref type="bibr" target="#b10">(Lewis et al., 2019)</ref>  have focused on Chinese <ref type="bibr" target="#b0">(Cui et al., 2018)</ref>, Korean <ref type="bibr" target="#b11">Lim et al., 2018)</ref>. Hence, project PIAF 4 (For a French-language AI or Pour une IA Francophone in French) focuses on filling this gap for French, starting from the Question-Answering use case. In a first stage of the collection effort, we prioritize quality over quantity: we conducted several in-place annotathons wherein volunteers are accompanied by the PIAF team. As showing up to the meetings was usually motivated by curiosity towards AI topics, the presence of the PIAF team served to increase engagement and allowed to provide participants with basic knowledge on how AI models can be built. We provide more details on annotathons in Section 5. Concurrently with our public effort, another team was collecting native French QA data, unbeknownst to us: we provide a comparison with FQUAD (d' <ref type="bibr">Hoffschmidt et al., 2020)</ref> in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Protocol</head><p>Consistently with the recent works targeting QA data collections in non-English languages, we focus on extractive Question Answering and adapt the protocol used for SQuAD: we collect samples in the form of triplets {P, Q, A}, in which the answer A to a question Q is contained in the paragraph P . <ref type="bibr">5</ref> The great majority (∼ 95%) of the Wikipedia articles used to build the SQuAD dataset have a version on the French Wikipedia, making an article-by-article replication of the collection process feasible. Still, such approach suffers from several downsides: for instance, cultural differences can cause significant divergence between the English and French versions of the same Wikipedia article, both in terms of data availability and reliability. Taking the Wikipedia article on the 2016 edition of the Super Bowl (present in the SQuAD dataset) as an example, one can easily notice how the contents significantly differ between its English and French versions. Thus, we select relevant articles the French version of Wikipedia, using the same relevance metric as used in SQuAD (i.e. PageRank); we segment those articles in smaller paragraphs, and collect sets of question/answer pairs corresponding to those paragraphs. Two collection settings are envisioned: the first, certified, restricted to volunteers participating in live annotathons (see Section 5); the second, open, wherein we will keep instances of the annotation platform open to the public web and engage the community to contribute to the collection, using a similar approach as that of Mozilla Common Voice. <ref type="bibr">6</ref> The collected data is publicly released under CC-BY-SA license. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Articles Selection</head><p>For SQuAD, the authors used the highest-ranking, in terms of PageRank, 10k articles from the English Wikipedia. Nonetheless, when applying the same threshold on the French version, we noticed significant differences in terms of structural properties between French and English Wikipedia. For instance, we observed the massive presence of pages referring to years on the French version, a characteristic that the top-10k subsample of the English Wikipedia does not exhibit. After manual inspection, the editing practices seem to differ between the French and English communities: for instance, while the latter do not link all year mentions to the actual page dedicated to that year, the French Wikipedia editors seem to systematically do so -a fact that boosts the PageRank score of such articles and therefore explains their abundant presence in the French top-10k subsample.</p><p>To account for such practices, an extensive manual inspection step on random samples from the French Wikipedia allowed us to identify the following filtering criteria. At section level, we discard those with the following titles: Voir aussi (See also), Articles connexe (Related Articles), Liens externes (External links), Notes et références (References). Those sections most often contain only bullet lists with structured rather than textual information. At article level, we discard those containing a section titledÉvénements (Events): those articles are about years, as in the aforementioned example, their structure is not apt to extractive QA since they mostly consist of lists of events (a date, followed by a very short text). Further, for concerns on data quality we discard articles falling in two Wikipedia categories: Catégorie:Wikipédia:ébauche (Draft) and Catégorie:Homonymie (Disambiguation).</p><p>To summarize, we operate as follows:</p><p>• gather the top-25k articles in terms of PageRank; 8</p><p>• discard the articles matching the above criteria;</p><p>• set a min-max char limit on the paragraph length (min = 500; max = 1000);</p><p>• filter out articles with less than 5 paragraphs.</p><p>Compared to the English SQuAD, we hence obtain annotated QA data on more articles, with less paragraphs per article, and a comparable length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Distribution</head><p>We associated each article to the main category it belongs to. To do so, we first analysed the category/project trees associated to each Wikipedia article. We deemed such information as not directly exploitable, given the presence of several ambiguities and inconsistencies. Nonetheless, such analysis allowed us to shortlist a subset of categories which were most represented. We then proceeded to a manual annotation step to associate each article to the most relevant <ref type="bibr">8</ref> We adapted the code from Project Nayuki https://www.nayuki.io/page/ computing-wikipedias-internal-pageranks category. The category information would be instrumental as a factor for engagement, in our participatory scenario: the volunteers have control over the general category they want to contribute to. In <ref type="table" target="#tab_2">Table 1</ref> we show the distribution of articles per category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Evaluation</head><p>It is of utmost importance to drive the users to produce challenging questions. In the SQuAD collection interface, the user was reminded to avoid using the same words/phrases as the paragraph while writing a question, via a text message displayed on screen. In our collection interface, we provide annotators with examples of good and bad questions. In order to check the quality of the collected data, we execute both manual and automatic evaluations throughout the data collection process, on a rolling basis (i.e. as the collected dataset grows), as in the original SQuAD paper. For manual evaluation, we analyzed 191 questions from the collected certified data (i.e. one randomly sampled triplet per article), and assign scores according to the dimensions defined in the SQuAD paper -see <ref type="table">Table 3</ref>. Conversely, we compute automatically, on the ensemble of the data and on a rolling basis, scores for syntactic divergence and lexical variation. Manual and automatic evaluation results for PI-AFv1.0 are discussed in Section 6. Further, we incrementally measure the performances obtained by state-of-the-art multilingual QA systems, under several experimental setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Annotation Platform</head><p>Effective annotation frameworks are essential for building language-and/or domain-specific NLP corpora. While multiple QA datasets have recently been produced(see Section 2), there is still a lack of annotation frameworks which are open and accessible to the community. The main reasons lie in the usage of proprietary software, the deep link with crowd-sourcing marketplaces (e.g. Mechanical Turk), and to the lack of minimal features enabling QA data collection. We thus present PIAFAnno, a browser-based, mobile-friendly, QA annotation tool. PIAFAnno was created to meet the following constraints:</p><p>1. Web-based crowd-sourcing platform: allowing for a distributed, large-scale contribution.</p><p>2. User and contribution management: supporting different roles among the annotators, as well as keeping control of the progress and the quality of the contributions.</p><p>3. Modern interface: our collection protocol is centered on voluntary participation. Therefore, the workflow should be pleasing and engaging for the annotators.</p><p>4. SQuAD compatible: to make the data quickly actionable by the community, the input and output formats follow the SQuAD format. The SQuAD annotation flow is respected (i.e. creating a number of questions for each paragraph of a single document). <ref type="table" target="#tab_2">Certified  52  52  57  16  52  36  26  291  PIAFv1.0  30  32  38  12  36  25  18  191  Open  228  155  233  85  190  167  97  1155  Total  280  207  290  101  242  203</ref> 123 1446   <ref type="table" target="#tab_3">Table 2</ref>). We aim to fill this gap with PIAFAnno. The crowd-sourcing platform used in the original SQuAD paper <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>, Daemo <ref type="bibr" target="#b5">(Gaikwad et al., 2015)</ref>, is publicly available and open-source. 10 Nonetheless, its development seems to have stalled and the project appears not actively maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arts Geography History Religion Sciences Society/Misc. Sport Total</head><p>Turning to the specific QA annotation platforms, we surveyed two tools: cdQA-annotator <ref type="bibr" target="#b14">(Mikaelian et al., 2019)</ref> and QA-Turk <ref type="bibr" target="#b4">(Fisch, 2018)</ref>. The former is a part of a larger web-based QA suite. The annotation workflow in cdQA-annotator allows for direct selection of the answer span, which facilitates the task. Furthermore, it uses the SQuADv1.1 file format for input and output files. Nonetheless, it includes neither contribution nor user management capabilities. Conversely, QA-Turk, based on ieturk <ref type="bibr" target="#b16">(Quach, 2018)</ref>, is an add-on allowing to create question-answers pairs. By default, it uses Amazon Mechanical Turk as a crowd-sourcing back-end, but can be used with a localbased alternative. While also tailored for crowd-sourcing, it does not support different roles for the contributors. We note that, while these two platforms do not satisfy our requirements, they include valuable features such as Mechanical Turk integration for QA-Turk or the QA tools included in the cdQA suite <ref type="bibr">(model training, visualization, explo-9</ref> We actually began working on implementing a QA module for Doccano but due to a lack of time for pull-requests reviews and other technical details we finally decided to roll out our own solution from scratch. 10 https://github.com/crowdresearch/daemo ration). Finally, those tools do not seem to allow collection of additional answers for a subset of the data (as mentioned above, this allows to both make the evaluation data more robust, and to compute human performance). PIAFAnno aims to overcome these limitations, specifically in two areas: dealing with multiple, role-diverse contributors and making it as easy as possible for them to create annotated samples. As said before, due to the constraints of our protocol, having two types of users (certified and open, see Section 3) and keeping volunteering contributors engaged is essential for the success of our approach. These two features can be easily be extended to other annotation efforts, making PIAFAnno a valuable resource for the creation of custom QA datasets. Source code and installation/deployment instructions for PIAFAnno can be found at https://github.com/etalab/piaf. In the following section, we provide a general description of the inner workings of our annotation platform. The architecture of PIAFAnno is presented in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PIAFAnno System Architecture</head><p>The system consists of seven modules attached to either the back-end or the front-end. All the functionalities described in this section are accessible through a web interface. Backend administration is handled via the Python Django framework. <ref type="bibr">11</ref> The front-end annotation modules were developed using the Vuetify 12 framework on top of VueJS 2.0 13 . PIAFAnno is built in a modular fashion, with a focus on simplicity and ease-of-use, and allows for ad-hoc modifications and extensions. Furthermore, the platform is dockerized via docker-compose allowing for quick and straight-forward installation. Below we describe the characteristics of each of the seven modules. We cover fist the back-end and then move to the front-end modules. Below, we move to describe the interfaces the contributors directly interact with, during the annotation.</p><p>Onboarding The onboarding module aims to facilitate the comprehension of the platform. During preliminary user tests, we found that contributors need a set of guidelines to begin the annotation process according to our quality requirements (e.g., question complexity, answer span length, and so on). It also include a short assessment to evaluate the level of understanding of the annotation guidelines for the onboarding user. The procedure thus consists of two steps: i) the explanation of the guidelines, and ii) the user assessment via simple evaluation questions. This allows us to limit the number of bots, trolls, and contributors who misunderstand the guidelines of the annotation process. This module is still under active development.</p><p>Category Selection As described in Section 3, we manually categorized the source Wikipedia articles into seven topics. This interface allows users to select the category they would like to contribute samples on.</p><p>Annotation Users are required to come up with five question-answer pairs per paragraph. The paragraph is shown according to the original order in their respective Wikipedia article (see <ref type="figure" target="#fig_1">Figure 2)</ref>. The article shown, and thus its paragraphs, belong to the category previously chosen by the contributor. The annotation procedure consists of three steps:</p><p>1. Paragraph reading: the main area of the page is dedicated to the paragraph from where contributors will create their annotations. The text formatting (e.g. font, spacing, line height, color contrast) has been reviewed by a design specialist. A progress bar allows contributors to know where they are in the current annotation task.</p><p>2. Question writing: the question input field is placed below the reading area. The input is limited to 200 characters.</p><p>3. Answer selection: within the paragraph reading area, the user can select the answer to question they previously wrote. We found that web browsers' traditional text span selection lacks speediness, usability, and regularly shows buggy mobile interaction. Specifically, during our preliminary tests, we found several cases of highlighting of incomplete words, and thus incomplete answers, which is a situation we needed to avoid. To address this problem we developed a custom highlighting component. In this component, the highlighting is done at word-level, rather than at characterlevel. This ensures complete answers and increases the annotation efficiency. The answer selection works as follows: with the first click, the user selects the initial word of the answer, while with the second click they will select the last word of the answer span. Finally, the complete answer text span is automatically inferred.</p><p>The question-answer pairs can always be edited by the contributor. Once five question-answer pairs are generated for a given paragraph, they are sent to the back-end and stored in the database. The user is then redirected to the annotation feedback page described below.</p><p>Annotation Feedback The annotation feedback page is shown after a paragraph is completed by the annotator. It congratulates the user and shows them general statistics as a contributor, such as the running number of data samples  contributed. This is part of our effort increase and sustain engagement for our contributors.</p><p>Additional answers The certified users have access to an additional annotation mode. It is similar to the original annotation procedure, but in this case the question is already written and the task is to select the corresponding answer. This procedure is composed of two steps (as in Annotation mode):</p><p>1. Paragraph and Question reading,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Answer selection.</head><p>The question comes from the pool of question-answer pairs already created by other contributors. We consider a sample as complete when two additional answers are selected. In the end, every question will have three answers from three different contributors. Additionally, the user can if needed flag questions (e.g. as unanswerable, ambiguous or offensive questions) using a dedicated button. A screenshot of this mode can be seen in <ref type="figure" target="#fig_2">Figure 3</ref>. Additional answers are, at the time of this writing, under collection, and will be included in the upcoming 1.1 release of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Participatory Approach</head><p>We base our approach on the protocol described previously (see Section 3). We introduce necessary modifications due to the constraints we set to work with: notably, not using Mechanical Turk or similar platforms, thus exploring a method to source voluntary contributions while using a dedicated annotation platform. The reasons for not using Mechanical Turk (or similar platforms) derive from the participatory approach we adopted. Given its experimental nature, we want to have full control on the entire process and to keep it as fair as possible to the volunteering contributors. Full ownership of the process and the associated software tools allows us to quickly and easily modify the annotation effort methodology "on-the-fly": the required changes can be implemented and deployed as transparently as possible for the contributors, safeguarding a pleasant annotation experience.</p><p>The creation of quality question-answer pairs is a complex task that, as noticed in our first annotation events, requires a considerable amount of concentration and engagement. Hence, the annotation procedure needs to keep the users motivated and engaged to discover and continue to use the platform. At the same time, we need to maximize the number of contributions by rallying volunteer contributors. As discussed in the previous sections, the developed software includes features aiming at facilitating the annotation task and at engaging participation. On the other hand, to entice the participation of contributors, as a first approach we explore a voluntary crowdsourced acquisition method, inspired by the Mozilla CommonVoice project. Specifically, we organize weekly annotation events, or annotathons, where contributors are invited to come and participate in the creation of the French QA dataset. These sessions are guided by the PIAF team in order to prepare the annotators to use the annotation tool and to generate highquality annotations. Since participants receive in-house training into the flow and quality expectations of the annotation process, we consider them as certified annotators (as explained before). The first contributors for these sessions were mainly drawn from the public sector. <ref type="bibr">16</ref> The layers of contributions we are currently testing are the following:</p><p>1. Neighboring: we promote the weekly annotation event across our neighboring ecosystem network, ries, institutions, and other communities beyond the French borders.</p><p>Our voluntary contribution approach is currently being tested. Other means of contribution may be explored. We also note that during all the annotation events we carry out, we put emphasis on enhancing artificial intelligence literacy through short, educational presentations. Furthermore, since these crowdsourcing events are innovative within the French public-sector context, we are working with sociology specialists in order to observe and provide feedback on the dynamics and development of the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Dataset Analysis</head><p>PIAFv1.0 compounds to 3835 question/answer pairs collected on 761 paragraphs under a certified setup: volunteers gathered for annotathons animated by the PIAF team. 285 volunteers contributed annotated samples to PIAFv1.0.</p><p>Consistently with SQuAD, we report in <ref type="table">Table 3</ref> the human assessment scores for 191 triplets (one per article, randomly sampled). Conversely, <ref type="figure" target="#fig_3">Figure 4</ref> shows the distributions for lexical variation and syntactic divergence among all triplets.</p><p>The PIAF project went public on October 3rd, 2019. As mentioned in Section 2, a similar effort (d <ref type="bibr">'Hoffschmidt et al., 2020)</ref> was concurrently being conducted, unbeknownst to us, producing a quantitatively larger French QA dataset named FQUAD. Therefore, we include a comparative analysis of PIAFv1.0 and FQUAD. For some analyses below, we use the automatic French translation of the English SQuAD (v1.1) obtained via Google Translate APIs, 17 referred to as SQuAD-Fr. It compounds to 74308 and 9455 samples for training and development sets, respectively, after filtering out bad samples (e.g. those for which the translated answer could not be recovered from the input).</p><p>In <ref type="bibr" target="#b2">(d'Hoffschmidt et al., 2020)</ref>, the authors rely on Camem-BERT <ref type="bibr" target="#b13">(Martin et al., 2019)</ref> for their evaluations, but do not report the hyper-parameters used. For all our experiments, we use batch size = 8, learning rate = 3e −5 , n epochs = 2, max seq len = 384, doc stride = 128.</p><p>In <ref type="table" target="#tab_7">Table 4</ref>  <ref type="bibr">19</ref> This can be explained by a number of factors, including: hyperparameters' setup (not reported in the FQUAD paper); the use of additional answers for computing evaluation scores on the hidden test set (although, for reference, this factor only justifies 3-4 points of difference on the SQuAD dev set). As shown by <ref type="bibr" target="#b6">(Geva et al., 2019)</ref>, employing a small number of annotators (for FQUAD, N = 18) can result in annotator bias: if unaccounted for, e.g. by creating training/evaluation splits based also on annotator identifiers, models might fail to generalize to samples produced by annotators that did not contribute to the training set. In PI-AFv1.0 this risk is mitigated by the large number of volunteering contributors (N = 258). Finally, having a larger pool of annotators, we also ensure higher diversity in the textual samples collected: this seems to have a direct impact for training <ref type="bibr" target="#b13">(Martin et al., 2019)</ref> and for evaluation robustness. The results shown in <ref type="table" target="#tab_7">Table 4</ref> indicate that our participatory approach allows to obtain relatively more challenging evaluation samples. Furthermore, taking as reference a model trained on SQuAD-Fr only, we observe that the addition of samples from PIAFv1.0 during training obtains a slightly larger improvement on FQUAD (dev) than adding a comparable subsample (FQUAD sub ) from the FQUAD training set. Since in the former case there is no risk of annotator bias (no overlap between FQUAD and PIAF annotators), this shows that PIAFv1.0 samples can also effectively be used for training data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Motivated by the scarcity of non-English data, we described our ongoing effort towards gathering native QA samples for the French language, using a participatory approach. Rather than a transactional approach to data collection, as usually adopted in crowd-sourcing efforts, we experiment with a comparatively slower and more engaging process, focusing on quality over quantity. Amongst desirable sideeffects of our approach, we highlight the educational aspects, e.g. introducing a wider audience to AI concepts and methodologies during our annothathons.  <ref type="table">Table 3</ref>: N = 191 randomly sampled triplets were manually assigned into one or more of the above categories. Words relevant to the corresponding reasoning type are in bold, and the annotated answer is underlined.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>The PIAF project is supported by Etalab and receives funding from the Investing for the Future Program, led by the General Secretariat for Investment and the Deposits and Consignments Fund.</p><p>Our gratitude goes to all participants and previous/current/future contributors to the PIAF campaigns; to Djamé Seddah, Miriam Redi, and to all fellow researchers who provided valuable inputs. We thank Project Nayuki for granting us permission to adapt and re-use their code, and the Mozilla Common Voice team for their precious feedback regarding the annotation tool and methodology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>PIAFAnno system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>PIAFAnno's annotation page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>PIAFAnno's additional answer page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Distributions of syntactic divergence (left) and lexical variation (right) on question/sentence pairs for PIAFv1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>target Multi-Hop (or Multi-Step) QA, i.e. the goal is to answer text understanding queries by combining multiple facts that are spread across different documents. The provided datasets, WikiHop and Med-</figDesc><table><row><cell>Hop target, respectively, open-domain and domain-specific</cell></row><row><cell>QA, with the latter focusing on interactions between pairs</cell></row><row><cell>of drugs in the medical domain. Also targeting Multi-Hop</cell></row><row><cell>reasoning, HotpotQA (Yang et al., 2018) includes 113k</cell></row><row><cell>Wikipedia-based question-answer pairs where the ques-</cell></row><row><cell>tions require finding and reasoning over multiple support-</cell></row><row><cell>ing documents to answer.</cell></row><row><cell>TriviaQA (Joshi et al., 2017) includes over 650k question-</cell></row><row><cell>answer-evidence triples, with questions originating from</cell></row><row><cell>trivia enthusiasts independent of the evidence documents,</cell></row><row><cell>automatically gathered and hence noisy. In addition, it pro-</cell></row><row><cell>vides a clean, human-annotated subset of 1975 question-</cell></row><row><cell>document-answer triples whose documents are certified to</cell></row><row><cell>contain all facts required to answer the questions.</cell></row></table><note>The Yahoo! Answers datasets, available for English and French under restrictive non-commercial licenses 2 , include question-answer pairs obtained through the Yahoo! An- swers service. No supporting passage for an answer is pro- vided, although links to relevant web pages might be in- cluded in the more elaborate answers. Available also under a non-commercial license, InsuranceQA</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Category distribution of source articles in Certified (a subset of which is released as PIAFv1.0) and Open splits.</figDesc><table><row><cell>Features</cell><cell>Daemo cdQA QA-Turk PIAFAnno</cell></row><row><cell>Multiple users</cell><cell></cell></row><row><cell>Modern interface</cell><cell></cell></row><row><cell>Open source</cell><cell></cell></row><row><cell>SQuAD compatible</cell><cell></cell></row><row><cell>Actively developed</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>QA annotation tools and their supported features.</figDesc><table /><note>5. Permissive and open-source license: making our tool reusable by the community as a part of our commit- ment to open source and data.4.1. Existing Platforms Before launching our development effort, we reviewed ex- isting open-source platforms that could fit for our scenario. Unfortunately, while there are several sequence-annotation tools, such as brat (Stenetorp et al., 2012), WebAnno (Yi- mam et al., 2013), or Doccano 9 (Nakayama et al., 2018), we identified, to the best of our efforts, only three candi- dates as open-source, web-based, QA annotation platforms (shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>lower (10 absolute F1 points) than that reported by<ref type="bibr" target="#b2">(d'Hoffschmidt et al., 2020)</ref> on the FQUAD hidden test set.</figDesc><table><row><cell>, we report the results obtained by fine-tuning</cell></row><row><cell>CamemBERT 18 on different combinations of the datasets</cell></row><row><cell>available. First, when evaluated on PIAFv1.0 data, the per-</cell></row><row><cell>formances of all models are significantly lower than on</cell></row><row><cell>the FQUAD development set. Second, a model trained on</cell></row><row><cell>the automatic French translation of SQuAD obtains, on the</cell></row><row><cell>FQUAD development set, the same performance as using</cell></row><row><cell>the FQUAD training set.</cell></row><row><cell>Turning to using PIAFv1.0 samples for training, rather than</cell></row><row><cell>for evaluation, we observe that a model trained on SQuAD-</cell></row><row><cell>Fr, and augmented with the PIAFv1.0 samples, obtains</cell></row><row><cell>comparable results (about half a F1 point improvement)</cell></row><row><cell>w.r.t. using the FQUAD training set. Note that our Camem-</cell></row><row><cell>BERT finetuned on FQUAD training samples, and evalu-</cell></row><row><cell>ated on FQUAD development data, obtains a performance</cell></row></table><note>17 translate.google.com 18 We use the implementation provided at https:// github.com/huggingface/transformers.significantly</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>: Combien de Polonais vivent sur Terre? Sentence: Avec une population de 38 millions d'habitants, la Pologne [...] : Quelle maladie a terrassé le prédécesseur du général de Rochambeau? Sen.: Le général Leclerc meurt de la fièvre jaune. La Grèce, d'une superficie de [...] et partage des frontières maritimes avec [...]. La mer Ionienneà l'ouest et [...], encadrent le pays dont le cinquième du territoire est constitué de plus de 9 000îles etîlots dont près de 200 sont habités. Quel aété le résultat en demi-finale? Sen.: En demi-finaleà Wembley, Arsenal du se débarrasser des tenant du titre, Wigan Athletic. Le match se termine sur un score d'1-1. [...] Arsenal se qualifia aux tirs au but, 4-2, avec [...] 9.95 %</figDesc><table><row><cell>Reasoning</cell><cell>Description</cell><cell>Example</cell><cell>Percentage</cell></row><row><cell>Synonymy</cell><cell>Major correspondences between</cell><cell></cell><cell>41.36 %</cell></row><row><cell></cell><cell>the question and the answer sen-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>tence are synonyms.</cell><cell></cell><cell></cell></row><row><cell>World Knowl-</cell><cell>Major correspondences between</cell><cell></cell><cell>26.18 %</cell></row><row><cell>edge</cell><cell>the question and the answer sen-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>tence require world knowledge</cell><cell></cell><cell></cell></row><row><cell></cell><cell>to resolve.</cell><cell></cell><cell></cell></row><row><cell>Syntactic vari-</cell><cell>After the question is paraphrased</cell><cell>Q: Quelle est la profession d'Alain Testart?</cell><cell>67.54 %</cell></row><row><cell>ation</cell><cell>into declarative form, its syn-</cell><cell>Sen.: Mais les différents anthropologues qui</cell><cell></cell></row><row><cell></cell><cell>tactic dependency structure does</cell><cell>se qualifient d'évolutionnistes de nos jours, tel</cell><cell></cell></row><row><cell></cell><cell>not match that of the answer sen-</cell><cell>qu'Alain Testart et Christophe Darmangeat, pro-</cell><cell></cell></row><row><cell></cell><cell>tence even after local modifica-</cell><cell>posent [...]</cell><cell></cell></row><row><cell></cell><cell>tions.</cell><cell></cell><cell></cell></row><row><cell>Multi sentence reasoning</cell><cell>There is anaphora, or higher-level fusion of multiple sen-</cell><cell cols="2">Q: Les iles greques sont elles toutes habitées? Sen.: 12.04 %</cell></row><row><cell></cell><cell>tences is required.</cell><cell></cell><cell></cell></row><row><cell>Ambiguous</cell><cell>We don't agree with the anno-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>tator's answer, or the question</cell><cell></cell><cell></cell></row><row><cell></cell><cell>does not have a unique answer.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">The analyses reported indicate that the samples collected in</cell></row><row><cell></cell><cell></cell><cell cols="2">PIAFv1.0 can effectively be exploited for robust evaluation</cell></row><row><cell></cell><cell></cell><cell>or training.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Furthermore, we presented a novel, open-sourced, and</cell></row><row><cell></cell><cell></cell><cell cols="2">language-agnostic data collection platform for Question</cell></row><row><cell></cell><cell></cell><cell cols="2">Answering tasks, developed within the context of Project</cell></row><row><cell></cell><cell></cell><cell>PIAF.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Both the PIAFAnno platform and the data collected are re-</cell></row><row><cell></cell><cell></cell><cell cols="2">leased under permissive and open-source licenses, with the</cell></row><row><cell></cell><cell></cell><cell cols="2">goal of engaging a diverse and wide community of prac-</cell></row><row><cell></cell><cell></cell><cell cols="2">titioners. We leave the web platform open to the public,</cell></row><row><cell></cell><cell></cell><cell cols="2">in order to collect additional samples from online contribu-</cell></row><row><cell></cell><cell></cell><cell cols="2">tors, which will be included in future releases of the dataset.</cell></row></table><note>QQQ:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>F1 scores obtained after fine-tuning CamemBERT for QA on different training datasets. Models are evaluated on FQUAD (dev) and PIAFv1.0 (when applicable). all refers to the union (shuffled) of FQUAD (train), SQuAD-Fr (train), and PIAFv1.0.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://piaf.etalab.studio/ 5 The presence of unanswerable question marks the difference between SQuADv1.1. and SQuADv2.0. 6 https://voice.mozilla.org 7 PIAF data is also available through the HuggingFace nlp library.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://www.djangoproject.com/ 12 https://vuetifyjs.com/ 13 https://vuejs.org/ 14 The difference between an admin and a super-admin is that the admin is restricted on most of the deletion, modification, and insertion database operations.15  In this study, these are Wikipedia articles. Other types of text sources may be used as input.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Traveling: we go and present our annotation project to local natural language processing, artificial intelligence or open data related events, 3. Peripheral: we build links with and among the external scientific and/or activist communities to get feedback and new avenues of collaboration in order to increase the project's relevance, 4. International: as this project is a French-language effort, and not only French (from France), we seek contributions with other governments, research laborato-16  We leverage Etalab's network and expertise on the organization of annotathons. Etalab is the French government Open Data task force.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">The same CamemBERT model, evaluated on the SQuAD-Fr dev set, obtains better results (F1: 73.28, EM: 59.18) than those reported by (d'Hoffschmidt et al., 2020) (F1: 70.7, EM: 56.9).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A span-extraction dataset for chinese machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07366</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Belblidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brendlé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06071</idno>
		<title level="m">Fquad: French question answering dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="813" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">QA-Turk:intuitive annotation tool for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<ptr target="https://github.com/ajfisch/QA-Turk" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Daemo: A self-governed crowdsourcing marketplace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nistala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cossette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Regino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ginzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Ziulkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cossette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richmond-Fuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herrejón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Flores-Saviaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thilakarathne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rajapakshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abolhassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jaramillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ángel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toxtli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sethia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Setyadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wajirasena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Batagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Damon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nekkanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gongora-Svartzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toledo Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Compton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aariff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palacios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhrmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nistala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Diemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology, UIST &apos;15 Adjunct</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="101" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised training data generation for multilingual question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
	<note>European Languages Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07475</idno>
		<title level="m">Mlqa: Evaluating cross-lingual extractive question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Korquad: Korean qa dataset for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Xcmrc: Evaluating cross-lingual machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05416</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Villemonte De La Clergerie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
		<title level="m">CamemBERT: a Tasty French Language Model. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">cdQA: An end-to-end closed domain question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mikaelian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amrouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nazon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sans</surname></persName>
		</author>
		<ptr target="https://github.com/cdqa-suite/cdQA-annotator" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">doccano: Text annotation tool for human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://github.com/chakki-works/doccano" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ieturk: Intuitive annotation tool for information extraction / named entity recognition using localturk / amazon mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quach</surname></persName>
		</author>
		<ptr target="https://github.com/Varal7/ieturk" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07042</idno>
		<title level="m">Coqa: A conversational question answering challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Brat: A web-based tool for nlpassisted text annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Topić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12</title>
		<meeting>the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Webanno: A flexible, web-based and visually supported system for distributed annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>ACL (Conference System Demonstrations)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
