<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-30">30 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-30">30 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Segmentation Transformer</term>
					<term>Semantic Segmentation</term>
					<term>Con- text Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the context aggregation problem in semantic segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. We empirically demonstrate our method achieves competitive performance on various benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission "HRNet + OCR + SegFix" achieves the 1 st place on the Cityscapes leaderboard by the ECCV 2020 submission deadline. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR.</p><p>We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The first two steps, object region learning and object region representation computation, are integrated as the cross-attention module in the decoder: the linear projections used to classify the pixels, i.e., generate the object regions, are category queries, and the object region representations are the crossattention outputs. The last step is the cross-attention module we add to the encoder, where the keys and values are the decoder output and the queries are the representations at each position. The details are presented in Section 3.3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a problem of assigning a class label to each pixel for an image. It is a fundamental topic in computer vision and is critical for various practical tasks such as autonomous driving. Deep convolutional networks since FCN <ref type="bibr" target="#b46">[47]</ref> have been the dominant solutions. Various studies have been conducted, including high-resolution representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54]</ref>, contextual aggregation <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b5">6]</ref> that is the interest of this paper, and so on.</p><p>The context of one position typically refers to a set of positions, e.g., the surrounding pixels. The early study is mainly about the spatial scale of contexts, i.e., the spatial scope. Representative works, such as ASPP <ref type="bibr" target="#b5">[6]</ref> and PPM <ref type="bibr" target="#b79">[80]</ref>, exploit multi-scale contexts. Recently, several works, such as DANet <ref type="bibr" target="#b17">[18]</ref>, CFNet <ref type="bibr" target="#b76">[77]</ref> and OCNet <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b70">71]</ref>, consider the relations between a position and its contextual positions, and aggregate the representations of the contextual positions with higher weights for similar representations.  <ref type="figure">Fig. 1</ref>: Illustrating the effectiveness of our OCR scheme. GT-OCR estimates the ideal object-contextual representations through exploiting the ground-truth, which is the upper-bound of our method. OCR reports the performance of our proposed object-contextual representations. The three methods, Baseline, OCR and GT-OCR, use the dilated ResNet-101 with output stride 8 as the backbone. We evaluate their (single-scale) segmentation results on Cityscapes val, ADE20K val, PASCAL-Context test and COCO-Stuff test separately.</p><p>We propose to investigate the contextual representation scheme along the line of exploring the relation between a position and its context. The motivation is that the class label assigned to one pixel is the category of the object 4 that the pixel belongs to. We aim to augment the representation of one pixel by exploiting the representation of the object region of the corresponding class. The empirical study, shown in <ref type="figure">Fig. 1</ref>, verifies that such a representation augmentation scheme, when the ground-truth object region is given, dramatically improves the segmentation quality 5 .</p><p>Our approach consists of three main steps. First, we divide the contextual pixels into a set of soft object regions with each corresponding to a class, i.e., a coarse soft segmentation computed from a deep network (e.g., ResNet <ref type="bibr" target="#b24">[25]</ref> or HRNet <ref type="bibr" target="#b53">[54]</ref>). Such division is learned under the supervision of the ground-truth segmentation. Second, we estimate the representation for each object region by aggregating the representations of the pixels in the corresponding object region. Last, we augment the representation of each pixel with the object-contextual representation (OCR). The OCR is the weighted aggregation of all the object region representations with the weights calculated according to the relations between pixels and object regions.</p><p>The proposed OCR approach differs from the conventional multi-scale context schemes. Our OCR differentiates the same-object-class contextual pixels from the different-object-class contextual pixels, while the multi-scale context schemes, such as ASPP <ref type="bibr" target="#b5">[6]</ref> and PPM <ref type="bibr" target="#b79">[80]</ref>, do not, and only differentiate the pixels with different spatial positions. <ref type="figure" target="#fig_0">Fig. 2</ref> provides an example to illustrate the differences between our OCR context and the multi-scale context. On the other hand, our OCR approach is also different from the previous relational context schemes <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b76">77]</ref>. Our approach structures the contextual pixels into object regions and exploits the relations between pixels and object regions. In contrast, the previous relational context schemes consider the contextual pixels separately and only exploit the relations between pixels and contextual pixels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b76">77]</ref> or predict the relations only from pixels without considering the regions <ref type="bibr" target="#b74">[75]</ref>.</p><p>We evaluate our approach on various challenging semantic segmentation benchmarks. Our approach outperforms the multi-scale context schemes, e.g., PSPNet, DeepLabv3, and the recent relational context schemes, e.g., DANet, and the efficiency is also improved. Our approach achieves competitive performance on five benchmarks: 84.5% on Cityscapes test, 45.66% on ADE20K val, 56.65% on LIP val, 56.2% on PASCAL-Context test and 40.5% on COCO-Stuff test. Besides, we extend our approach to Panoptic-FPN <ref type="bibr" target="#b29">[30]</ref> and verify the effectiveness of our OCR on the COCO panoptic segmentation task, e.g., Panoptic-FPN + OCR achieves 44.2% on COCO val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-scale context. PSPNet <ref type="bibr" target="#b79">[80]</ref> performs regular convolutions on pyramid pooling representations to capture the multi-scale context. The DeepLab series <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> adopt parallel dilated convolutions with different dilation rates (each rate captures the context of a different scale). The recent works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b71">72]</ref> propose various extensions, e.g., DenseASPP <ref type="bibr" target="#b67">[68]</ref> densifies the dilated rates to cover larger scale ranges. Some other studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b18">19]</ref> construct the encoder-decoder structures to exploit the multi-resolution features as the multi-scale context. Relational context. DANet <ref type="bibr" target="#b17">[18]</ref>, CFNet <ref type="bibr" target="#b76">[77]</ref> and OCNet <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b70">71]</ref> augment the representation for each pixel by aggregating the representations of the contextual pixels, where the context consists of all the pixels. Different from the global context <ref type="bibr" target="#b45">[46]</ref>, these works consider the relation (or similarity) between the pixels, which is based on the self-attention scheme <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b60">61]</ref>, and perform a weighted aggregation with the similarities as the weights. Double Attention and its related work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref> and ACFNet <ref type="bibr" target="#b74">[75]</ref> group the pixels into a set of regions, and then augment the pixel representations by aggregating the region representations with the consideration of their context relations predicted by using the pixel representation.</p><p>Our approach is a relational context approach and is related to Double Attention and ACFNet. The differences lie in the region formation and the pixel-region relation computation. Our approach learns the regions with the supervision of the ground-truth segmentation. In contrast, the regions in previous approaches except ACFNet are formed unsupervisedly. On the other hand, the relation between a pixel and a region is computed by considering both the pixel and region representations, while the relation in previous works is only computed from the pixel representation. Coarse-to-fine segmentation. Various coarse-to-fine segmentation schemes have been developed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b84">85]</ref> to gradually refine the segmentation maps from coarse to fine. For example, <ref type="bibr" target="#b33">[34]</ref> regards the coarse segmentation map as an additional representation and combines it with the original image or other representations for computing a fine segmentation map.</p><p>Our approach in some sense can also be regarded as a coarse-to-fine scheme. The difference lies in that we use the coarse segmentation map for generating a contextual representation instead of directly used as an extra representation. We compare our approach with the conventional coarse-to-fine schemes in the supplementary material. Region-wise segmentation. There exist many region-wise segmentation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b59">60]</ref> that organize the pixels into a set of regions (usually super-pixels), and then classify each region to get the image segmentation result. Our approach does not classify each region for segmentation and instead uses the region to learn a better representation for the pixel, which leads to better pixel labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Semantic segmentation is a problem of assigning one label l i to each pixel p i of an image I, where l i is one of K different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Multi-scale context. The ASPP <ref type="bibr" target="#b4">[5]</ref> module captures the multi-scale context information by performing several parallel dilated convolutions with different dilation rates <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b69">70]</ref>:</p><formula xml:id="formula_0">y d i = ps=pi+d∆t K d t x s .<label>(1)</label></formula><p>Here, p s = p i +d∆ t is the sth sampled position for the dilation convolution with the dilation rate d (e.g., d = 12, 24, 36 in DeepLabv3 <ref type="bibr" target="#b5">[6]</ref>) at the position p i . t is the position index for a convolution, e.g., {∆ t = (∆ w , ∆ h )|∆ w = −1, 0, 1, ∆ h = −1, 0, 1} for a 3 × 3 convolution. x s is the representation at p s . y d i is the output representation at p i for the dth dilated convolution. K d t is the kernel parameter at position t for for the dth dilated convolution. The output multi-scale contextual representation is the concatenation of the representations output by the parallel dilated convolutions.</p><p>The multi-scale context scheme based on dilated convolutions captures the contexts of multiple scales without losing the resolution. The pyramid pooling module in PSPNet <ref type="bibr" target="#b79">[80]</ref> performs regular convolutions on representations of different scales, and also captures the contexts of multiple scales but loses the resolution for large scale contexts. Relational context. The relational context scheme <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b76">77]</ref> computes the context for each pixel by considering the relations:</p><formula xml:id="formula_1">y i = ρ( s∈I w is δ(x s )),<label>(2)</label></formula><p>where I refers to the set of pixels in the image, w is is the relation between x i and x s , and may be predicted only from x i or computed from x i and x s . δ(·) and ρ(·) are two different transform functions as done in self-attention <ref type="bibr" target="#b60">[61]</ref>. The global context scheme <ref type="bibr" target="#b45">[46]</ref> is a special case of relational context with w is = 1 |I| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Formulation</head><p>The class label l i for pixel p i is essentially the label of the object that pixel p i lies in. Motivated by this, we present an object-contextual representation approach, characterizing each pixel by exploiting the corresponding object representation. The proposed object-contextual representation scheme (1) structurizes all the pixels in image I into K soft object regions, (2) represents each object region as f k by aggregating the representations of all the pixels in the kth object region, and (3) augments the representation for each pixel by aggregating the K object region representations with consideration of its relations with all the object regions:</p><formula xml:id="formula_2">y i = ρ( K k=1 w ik δ(f k )),<label>(3)</label></formula><p>where f k is the representation of the kth object region, w ik is the relation between the ith pixel and the kth object region. δ(·) and ρ(·) are transformation functions. Soft object regions. We partition the image I into K soft object regions</p><formula xml:id="formula_3">{M 1 , M 2 , . . . , M K }.</formula><p>Each object region M k corresponds to the class k, and is represented by a 2D map (or coarse segmentation map), where each entry indicates the degree that the corresponding pixel belongs to the class k.</p><p>We compute the K object regions from an intermediate representation output from a backbone (e.g., ResNet or HRNet). During training, we learn the object region generator under the supervision from the ground-truth segmentation using the cross-entropy loss. Object region representations. We aggregate the representations of all the pixels weighted by their degrees belonging to the kth object region, forming the kth object region representation:</p><formula xml:id="formula_4">f k = i∈Im ki x i .<label>(4)</label></formula><p>Here, x i is the representation of pixel p i .m ki is the normalized degree for pixel p i belonging to the kth object region. We use spatial softmax to normalize each object region M k . Object contextual representations. We compute the relation between each pixel and each object region as below:</p><formula xml:id="formula_5">w ik = e κ(xi,f k ) K j=1 e κ(xi,fj ) .<label>(5)</label></formula><p>Here, κ(x, f ) = φ(x) ψ(f ) is the unnormalized relation function, φ(·) and ψ(·) are two transformation functions implemented by 1 × 1 conv → BN → ReLU. This is inspired by self-attention <ref type="bibr" target="#b60">[61]</ref> for a better relation estimation.</p><p>The object contextual representation y i for pixel p i is computed according to <ref type="bibr">Equation 3</ref>. In this equation, δ(·) and ρ(·) are both transformation functions implemented by 1 × 1 conv → BN → ReLU, and this follows non-local networks <ref type="bibr" target="#b63">[64]</ref>. Augmented representations. The final representation for pixel p i is updated as the aggregation of two parts, (1) the original representation x i , and (2) the object contextual representation y i :</p><formula xml:id="formula_6">z i = g([x i y i ] ).<label>(6)</label></formula><p>where g(·) is a transform function used to fuse the original representation and the object contextual representation, implemented by 1 × 1 conv → BN → ReLU. The whole pipeline of our approach is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Comments: Some recent studies, e.g., Double Attention <ref type="bibr" target="#b7">[8]</ref> and ACFNet <ref type="bibr" target="#b74">[75]</ref>, can be formulated similarly to <ref type="bibr">Equation 3</ref>, but differ from our approach in some aspects. For example, the region formed in Double Attention do not correspond to an object class, and the relation in ACFNet <ref type="bibr" target="#b74">[75]</ref> is computed only from the pixel representation w/o using the object region representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segmentation Transformer: Rephrasing the OCR Method</head><p>We rephrase the OCR pipeline using the Transformer <ref type="bibr" target="#b61">[62]</ref> language and illustrate the Transformer encoder-decoder architecture in <ref type="figure" target="#fig_2">Figure 4</ref>. The aforementioned OCR pipeline consists of three steps: soft object region extraction, object region representation computation, and object-contextual representation computation for each position, and mainly explores the decoder and encoder cross-attention modules.</p><p>Attention. The attention <ref type="bibr" target="#b61">[62]</ref> is computed using the scaled dot-product. The inputs contain: a set of N q queries Q ∈ R d×Nq , a set of N kv keys K ∈ R d×N kv , and a set of N kv values V ∈ R d×N kv . The attention weight a ij is computed as the softmax normalization of the dot-product between the query q i and the key k j :</p><formula xml:id="formula_7">a ij = e 1 √ d q i kj Z i where Z i = N kv j=1 e 1 √ d q i kj .<label>(7)</label></formula><p>The attention output for each query q i is the aggregation of values weighted by attention weights:</p><formula xml:id="formula_8">Attn(q i , K, V) = N kv j=1 α ij v j .<label>(8)</label></formula><p>Decoder cross-attention. The decoder cross-attention module has two roles: soft object region extraction and object region representation computation.</p><p>The keys and values are image features (x i in <ref type="figure" target="#fig_2">Equation 4</ref>). The queries are K category queries (q 1 , q 2 , . . . , q K ), each of which corresponds to a category. The K category queries essentially are used to generate the soft object regions, Connection to class embedding and class attention <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58]</ref>. The category queries are close to the class embedding in Vision Transformer (ViT) <ref type="bibr" target="#b13">[14]</ref> and in Class-Attention in Image Transformers (CaiT) <ref type="bibr" target="#b57">[58]</ref>. We have an embedding for each class other than an integrated embedding for all the classes. The decoder cross attention in segmentation transformer is similar to class attention in CaiT <ref type="bibr" target="#b57">[58]</ref>. The encoder and decoder architecture is close to self-attention in ViT over both the class embedding and image features. If the two cross-attentions and the two self-attentions are conducted simultaneously (depicted in <ref type="figure" target="#fig_4">Figure 5</ref>), it is equivalent to a single self-attention. It is interesting to learn the attention parameters for category queries at the ImageNet pre-training stage.  Connection to OCNet and interlaced self-attention <ref type="bibr" target="#b70">[71]</ref>. The OCNet <ref type="bibr" target="#b70">[71]</ref> exploits the self-attention (i.e., only the encoder self-attention unit is included in <ref type="figure" target="#fig_2">Figure 4</ref>, and the encoder cross-attention unit and the decoder are not included). The self-attention unit is accelerated by an interlaced self-attention unit, consisting of local self-attention and global self-attention that can be simplified as self-attention over the pooled features over the local windows 6 . As an alternative scheme, the category queries in <ref type="figure" target="#fig_2">Figure 4</ref> could be replaced by regularly-sampled or adaptively-pooled image features other than learned as model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Architecture</head><p>Backbone. We use the dilated ResNet-101 <ref type="bibr" target="#b24">[25]</ref> (with output stride 8) or HRNet-W48 <ref type="bibr" target="#b53">[54]</ref> (with output stride 4) as the backbone. For dilated ResNet-101, there are two representations input to the OCR module. The first representation from Stage 3 is for predicting coarse segmentation (object regions). The other representation from Stage 4 goes through a 3 × 3 convolution (512 output channels), and then is fed into the OCR module. For HRNet-W48, we only use the final representation as the input to the OCR module. OCR module. We implement the above formulation of our approach as the OCR module, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. We use a linear function (a 1 × 1 convolution) to predict the coarse segmentation (soft object region) supervised with a pixel-wise cross-entropy loss. All the transform functions, ψ(·), φ(·), δ(·), ρ(·), and g(·), are implemented as 1 × 1 conv → BN → ReLU, and the first three output 256 channels and the last two output 512 channels. We predict the final segmentation from the final representation using a linear function and we also apply a pixel-wise cross-entropy loss on the final segmentation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Empirical Analysis</head><p>We conduct the empirical analysis experiments using the dilated ResNet-101 as the backbone on Cityscapes val. Object region supervision. We study the influence of the object region supervision. We modify our approach through removing the supervision (i.e., loss) on the soft object regions (within the pink dashed box in <ref type="figure" target="#fig_1">Fig. 3</ref>), and adding another auxiliary loss in the stage-3 of ResNet-101. We keep all the other settings the same and report the results in the left-most 2 columns of <ref type="table" target="#tab_1">Table 1</ref>. We can see that the supervision for forming the object regions is crucial for the performance. Pixel-region relations. We compare our approach with other two mechanisms that do not use the region representation for estimating the pixel-region relations: (i) Double-Attention <ref type="bibr" target="#b7">[8]</ref> uses the pixel representation to predict the relation; (ii) ACFNet <ref type="bibr" target="#b74">[75]</ref> directly uses one intermediate segmentation map to indicate the relations. We use DA scheme and ACF scheme to represent the above two mechanisms. We implement both methods by ourselves and only use the dilated ResNet-101 as the backbone without using multi-scale contexts (the results of ACFNet is improved by using ASPP <ref type="bibr" target="#b74">[75]</ref>) The comparison in <ref type="table" target="#tab_1">Table 1</ref> shows that our approach gets superior performance. The reason is that we exploit the pixel representation as well as the region representation for computing the relations. The region representation is able to characterize the object in the specific image, and thus the relation is more accurate for the specific image than that only using the pixel representation. Ground-truth OCR. We study the segmentation performance using the groundtruth segmentation to form the object regions and the pixel-region relations, called GT-OCR, to justify our motivation. (i) Object region formation using the ground-truth: set the confidence of pixel i belonging to kth object region m ki = 1 if the ground-truth label l i ≡ k and m ki = 0 otherwise. (ii) Pixel-region relation computation using the ground-truth: set the pixel-region relation w ik = 1 if the ground-truth label l i ≡ k and w ik = 0 otherwise. We have illustrated the detailed results of GT-OCR on four different benchmarks in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments: Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b10">[11]</ref> is tasked for urban scene understanding. There are totally 30 classes and only 19 classes are used for parsing evaluation. The dataset contains 5K high quality pixel-level finely annotated images and 20K coarsely annotated images. The finely annotated 5K images are divided into 2, 975/500/1, 525 images for training, validation and testing. ADE20K. The ADE20K dataset <ref type="bibr" target="#b81">[82]</ref> is used in ImageNet scene parsing challenge 2016. There are 150 classes and diverse scenes with 1, 038 image-level labels. The dataset is divided into 20K/2K/3K images for training, validation and testing. LIP. The LIP dataset <ref type="bibr" target="#b20">[21]</ref> is used in the LIP challenge 2016 for single human parsing task. There are about 50K images with 20 classes (19 semantic human part classes and 1 background class). The training, validation, and test sets consist of 30K, 10K, 10K images respectively. PASCAL-Context. The PASCAL-Context dataset <ref type="bibr" target="#b48">[49]</ref> is a challenging scene parsing dataset that contains 59 semantic classes and 1 background class. The training set and test set consist of 4, 998 and 5, 105 images respectively. COCO-Stuff. The COCO-Stuff dataset <ref type="bibr" target="#b2">[3]</ref> is a challenging scene parsing dataset that contains 171 semantic classes. The training set and test set consist of 9K and 1K images respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training setting. We initialize the backbones using the model pre-trained on ImageNet and the OCR module randomly. We perform the polynomial learning rate policy with factor (1 − ( iter itermax ) 0.9 ), the weight on the final loss as 1, the weight on the loss used to supervise the object region estimation (or auxiliary loss) as 0.4. We use InPlace-ABN sync <ref type="bibr" target="#b51">[52]</ref> to synchronize the mean and standard-deviation of BN across multiple GPUs. For the data augmentation, we perform random flipping horizontally, random scaling in the range of [0.5, 2] and random brightness jittering within the range of [−10, 10]. We perform the same training settings for the reproduced approaches, e.g., PPM, ASPP, to ensure the fairness. We follow the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b79">80]</ref> for setting up the training for the benchmark datasets.</p><p>Cityscapes: We set the initial learning rate as 0.01, weight decay as 0.0005, crop size as 769 × 769 and batch size as 8 by default. For the experiments evaluated on val/test set, we set training iterations as 40K/100K on train/train+val set separately. For the experiments augmented with extra data: (i) w/ coarse, we first train our model on train + val for 100K iterations with initial learning rate as 0.01, then we fine-tune the model on coarse set for 50K iterations and continue fine-tune our model on train+val for 20K iterations with the same initial learning rate 0.001. (ii) w/ coarse + Mapillary <ref type="bibr" target="#b49">[50]</ref>, we first pre-train our model on the Mapillary train set for 500K iterations with batch size 16 and initial learning rate 0.01 (achieves 50.8% on Mapillary val), then we fine-tune the model on Cityscapes following the order of train + val (100K iterations) → coarse (50K iterations) → train + val (20K iterations), we set the initial learning rate as 0.001 and the batch size as 8 during the above three fine-tuning stages on Cityscapes.</p><p>ADE20K : We set the initial learning rate as 0.02, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and and training iterations as 150K if not specified.</p><p>LIP : We set the initial learning rate as 0.007, weight decay as 0.0005, crop size as 473×473, batch size as 32 and training iterations as 100K if not specified.</p><p>PASCAL-Context: We set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and training iterations as 30K if not specified.</p><p>COCO-Stuff : We set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and training iterations as 60K if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Existing Context Schemes</head><p>We conduct the experiments using the dilated ResNet-101 as the backbone and use the same training/testing settings to ensure the fairness. Multi-scale contexts. We compare our OCR with the multi-scale context schemes including PPM <ref type="bibr" target="#b79">[80]</ref> and ASPP <ref type="bibr" target="#b5">[6]</ref> on three benchmarks including Cityscapes test, ADE20K val and LIP val in <ref type="table" target="#tab_2">Table 2</ref>. Our reproduced PPM/ASPP outperforms the originally reported numbers in <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b5">6]</ref>. From <ref type="table" target="#tab_2">Table 2</ref>, it can be seen that our OCR outperforms both multi-scale context schemes by a large margin. For example, the absolute gains of OCR over PPM (ASPP) for the four comparisons are 1.5% (0.8%), 0.8% (0.7%), 0.78% (0.68%), 0.84% (0.5%). To the best of our knowledge, these improvements are already significant considering that the baselines (with dilated ResNet-101) are already strong and the complexity of our OCR is much smaller. Relational contexts. We compare our OCR with various relational context schemes including Self-Attention <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b63">64]</ref>, Criss-Cross attention <ref type="bibr" target="#b26">[27]</ref> (CC-Attention), DANet <ref type="bibr" target="#b17">[18]</ref> and Double Attention <ref type="bibr" target="#b7">[8]</ref> on the same three benchmarks including   Cityscapes test, ADE20K val and LIP val. For the reproduced Double Attention, we fine-tune the number of the regions (as it is very sensitive to the hyper-parameter choice) and we choose 64 with the best performance. More detailed analysis and comparisons are illustrated in the supplementary material. According to the results in <ref type="table" target="#tab_3">Table 3</ref>, it can be seen that our OCR outperforms these relational context schemes under the fair comparisons. Notably, the complexity of our OCR is much smaller than most of the other methods. Complexity. We compare the efficiency of our OCR with the efficiencies of the multi-scale context schemes and the relational context schemes. We measure the increased parameters, GPU memory, computation complexity (measured by the number of FLOPs) and inference time that are introduced by the context modules, and do not count the complexity from the backbones. The comparison in <ref type="table" target="#tab_4">Table 4</ref> shows the superiority of the proposed OCR scheme. Parameters: Most relational context schemes require less parameters compared with the multi-scale context schemes. For example, our OCR only requires 1/2 and 2/3 of the parameters of PPM and ASPP separately.</p><p>Memory: Both our OCR and Double Attention require much less GPU memory compared with the other approaches (e.g., DANet, PPM). For example, our GPU memory consumption is 1/4, 1/10, 1/2, 1/10 of the memory consumption of PPM, DANet, CC-Attention and Self-Attention separately.</p><p>FLOPs: Our OCR only requires 1/2, 7/10, 3/10, 2/5 and 1/2 of the FLOPs based on PPM, ASPP, DANet, CC-Attention and Self-Attention separately.</p><p>Running time: The runtime of OCR is very small: only 1/2, 1/2, 1/3, 1/3 and 1/2 of the runtime with PPM, ASPP, DANet, CC-Attention and Self-Attention separately.</p><p>In general, our OCR is a much better choice if we consider the balance between performance, memory complexity, GFLOPs and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the State-of-the-Art</head><p>Considering that different approaches perform improvements on different baselines to achieve the best performance, we categorize the existing works to two groups according to the baselines that they apply: (i) simple baseline: dilated ResNet-101 with stride 8; (ii) advanced baseline: PSPNet, DeepLabv3, multi-grid (MG), encoder-decoder structures that achieve higher resolution outputs with stride 4 or stronger backbones such as WideResNet-38, Xception-71 and HRNet.</p><p>For fair comparison with the two groups fairly, we perform our OCR on a simple baseline (dilated ResNet-101 with stride 8) and an advanced baseline (HRNet-W48 with stride 4). Notably, our improvement with HRNet-W48 (over ResNet-101) is comparable with the gain of the other work based on advanced baseline methods. For example, DGCNet <ref type="bibr" target="#b77">[78]</ref> gains 0.7% with Multi-grid while OCR gains 0.6% with stronger backbone on Cityscapes test. We summarize all the results in <ref type="table" target="#tab_5">Table 5</ref> and illustrate the comparison details on each benchmark separately as follows. Cityscapes. Compared with the methods based on the simple baseline on Cityscape test w/o using the coarse data, our approach achieves the best performance 81.8%, which is already comparable with some methods based on the advanced baselines, e.g, DANet, ACFNet. Our approach achieves better performance 82.4% through exploiting the coarsely annotated images for training.</p><p>For comparison with the approaches based on the advanced baselines, we perform our OCR on the HRNet-W48, and pre-train our model on the Mapillary dataset <ref type="bibr" target="#b49">[50]</ref>. Our approach achieves 84.2% on Cityscapes test. We further apply a novel post-processing scheme SegFix <ref type="bibr" target="#b72">[73]</ref> to refine the boundary quality, which brings 0.3% ↑ improvement. Our final submission "HRNet + OCR + SegFix" achieves 84.5%, which ranks the 1 st place on the Cityscapes leaderboard by the time of our submission. In fact, we perform PPM and ASPP on HRNet-W48 separately and empirically find that directly applying either PPM or ASPP does not improve the performance and even degrades the performance, while our OCR consistently improves the performance.</p><p>Notably, the very recent work <ref type="bibr" target="#b55">[56]</ref> sets a new state-of-the-art performance 85.4% on Cityscapes leaderboard via combining our "HRNet + OCR" and a new hierarchical multi-scale attention mechanism.  proves the performance to 56.65%, which outperforms the previous approaches. The very recent work CNIF <ref type="bibr" target="#b62">[63]</ref> achieves the best performance (56.93%) through injecting the hierarchical structure knowledge of human parts. Our approach potentially benefit from such hierarchical structural knowledge. All the results are based on only flip testing without multi-scale testing 7 . PASCAL-Context. We evaluate the performance over 59 categories following <ref type="bibr" target="#b53">[54]</ref>. It can be seen that our approach outperforms both the previous best methods based on simple baselines and the previous best methods based on advanced baselines. The HRNet-W48 + OCR approach achieves the best performance 56.2%, significantly outperforming the second best, e.g., ACPNet (54.7%) and ACNet (54.1%). COCO-Stuff. It can be seen that our approach achieves the best performance, 39.5% based ResNet-101 and 40.5% based on HRNetV2-48. Qualitative Results. We illustrate the qualitative results in the supplementary material due to the limited pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Panoptic Segmentation</head><p>To verify the generalization ability of our method, we apply OCR scheme on the more challenging panoptic segmentation task <ref type="bibr" target="#b30">[31]</ref>, which unifies both the instance segmentation task and the semantic segmentation task. Dataset. We choose the COCO dataset <ref type="bibr" target="#b42">[43]</ref> to study the effectiveness of our method on panoptic segmentation. We follow the previous work <ref type="bibr" target="#b29">[30]</ref> and uses all 2017 COCO images with 80 thing and 53 stuff classes annotated. Training Details. We follow the default training setup of "COCO Panoptic Segmentation Baselines with Panoptic FPN (3× learning schedule)" 8 in Detec-tron2 <ref type="bibr" target="#b65">[66]</ref>. The reproduced Panoptic FPN reaches higher performance than the original numbers in the paper <ref type="bibr" target="#b29">[30]</ref> (Panoptic FPN w/ ResNet-50, PQ: 39.2% / Panoptic FPN w/ ResNet-101, PQ: 40.3%) and we choose the higher reproduced results as our baseline.</p><p>In our implementation, we use the original prediction from the semantic segmentation head (within Panoptic-FPN) to compute the soft object regions and then we use a OCR head to predict a refined semantic segmentation map. We set the loss weights on both the original semantic segmentation head and the OCR head as 0.25. All the other training settings are kept the same for fair comparison. We directly use the same OCR implementation (for the semantic segmentation task) without any tuning. Results. In <ref type="table" target="#tab_6">Table 6</ref>, we can see that OCR improves the PQ performance of Panoptic-FPN (ResNet-101) from 43.0% to 44.2%, where the main improvements come from better segmentation quality on the stuff region measured by mIoU and PQ St . Specifically, our OCR improves the mIoU and PQ St of Panoptic-FPN (ResNet-101) by 1.0% and 2.3% separately. In general, the performance of "Panoptic-FPN + OCR" is very competitive compared to various recent methods <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b68">69]</ref>. We also report the results of Panoptic-FPN with PPM and ASPP to illustrate the advantages of our OCR in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we present an object-contextual representation approach for semantic segmentation. The main reason for the success is that the label of a pixel is the label of the object that the pixel lies in and the pixel representation is strengthened by characterizing each pixel with the corresponding object region representation. We empirically show that our approach brings consistent improvements on various benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustrating the multi-scale context with the ASPP as an example and the OCR context for the pixel marked with I. (a) ASPP: The context is a set of sparsely sampled pixels marked with I, I. The pixels with different colors correspond to different dilation rates. Those pixels are distributed in both the object region and the background region. (b) Our OCR: The context is expected to be a set of pixels lying in the object (marked with color blue). The image is chosen from ADE20K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustrating the pipeline of OCR. (i) form the soft object regions in the pink dashed box. (ii) estimate the object region representations in the purple dashed box ; (iii) compute the object contextual representations and the augmented representations in the orange dashed box. See Section 3.2 and 3.3 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Segmentation transformer. Rephrasing the OCR pipeline showing in Figure 3 using the Transformer encoder-decoder architecture. The encoder self-attention unit in the gray box could be a local version and is optional and is useful for boosting the image features. The decoder self-attention unit serves as the role of interacting the category queries and can be discarded for a single decoder layer or moved after the decoder cross-attention unit.M 1 , M 2 , . . . , M K , which are later spatially softmax-normalized as the weightsm in Equation 4. Computingm is the same as the manner of computing attention the weight α ij in Equation 7. The object region representation computation manner in Equation 4 is the same as Equation 8. Encoder cross-attention. The encoder cross-attention module (with the subsequent FFN) serves as the role of aggregating the object region representations as shown in Equation 3. The queries are image features at each position, and the keys and values are the decoder outputs. Equation 5 computes the weights in a way the same as the attention computation manner Equation 7, and the contextual aggregation Equation 3 is the same as Equation 8 and ρ(·) corresponds to the FFN operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>An alternative of segmentation transformer (shown inFigure 4). The module in the dashed box is repeated several times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Influence of object region supervision and pixel-region relation estimation scheme. We can find both the object region supervision and our pixelregion relation scheme are important for the performance.</figDesc><table><row><cell cols="2">Object region supervision</cell><cell></cell><cell>Pixel-region relations</cell><cell></cell></row><row><cell cols="2">w/o supervision w/ supervision</cell><cell>DA scheme</cell><cell>ACF scheme</cell><cell>Ours</cell></row><row><cell>77.31%</cell><cell>79.58%</cell><cell>79.01%</cell><cell>78.02%</cell><cell>79.58%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with multi-scale context scheme. We use to mark the result w/o using Cityscapes val for training. We can find OCR consistently outperforms both PPM and ASPP across different benchmarks under the fair comparisons.</figDesc><table><row><cell>Method</cell><cell cols="3">Cityscapes (w/o coarse) Cityscapes (w/ coarse) ADE20K</cell><cell>LIP</cell></row><row><cell>PPM [80]</cell><cell>78.4%</cell><cell>81.2%</cell><cell>43.29%</cell><cell>−</cell></row><row><cell>ASPP [6]</cell><cell>−</cell><cell>81.3%</cell><cell>−</cell><cell>−</cell></row><row><cell>PPM (Our impl.)</cell><cell>80.3%</cell><cell>81.6%</cell><cell>44.50%</cell><cell>54.76%</cell></row><row><cell>ASPP (Our impl.)</cell><cell>81.0%</cell><cell>81.7%</cell><cell>44.60%</cell><cell>55.01%</cell></row><row><cell>OCR</cell><cell>81.8%</cell><cell>82.4%</cell><cell>45.28%</cell><cell>55.60%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with relational context scheme. Our method consistently performs better across different benchmarks. Notably, Double Attention is sensitive to the region number choice and we have fine-tuned this hyper-parameter as 64 to report its best performance.</figDesc><table><row><cell>Method</cell><cell cols="3">Cityscapes (w/o coarse) Cityscapes (w/ coarse) ADE20K</cell><cell>LIP</cell></row><row><cell>CC-Attention [27]</cell><cell>81.4%</cell><cell>-</cell><cell>45.22%</cell><cell>-</cell></row><row><cell>DANet [18]</cell><cell>81.5%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Self Attention (Our impl.)</cell><cell>81.1%</cell><cell>82.0%</cell><cell>44.75%</cell><cell>55.15%</cell></row><row><cell>Double Attention (Our impl.)</cell><cell>81.2%</cell><cell>82.0%</cell><cell>44.81%</cell><cell>55.12%</cell></row><row><cell>OCR</cell><cell>81.8%</cell><cell>82.4%</cell><cell>45.28%</cell><cell>55.60%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Complexity comparison. We use input feature map of size [1 × 2048 × 128 × 128] to evaluate their complexity during inference. The numbers are obtained on a single P40 GPU with CUDA 10.0. All the numbers are the smaller the better. Our OCR requires the least GPU memory and the least runtime.</figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>Memory</cell><cell>FLOPs</cell><cell>Time</cell></row><row><cell>PPM (Our impl.)</cell><cell>23.1M</cell><cell>792M</cell><cell>619G</cell><cell>99ms</cell></row><row><cell>ASPP (Our impl.)</cell><cell>15.5M</cell><cell>284M</cell><cell>492G</cell><cell>97ms</cell></row><row><cell>DANet (Our impl.)</cell><cell>10.6M</cell><cell>2339M</cell><cell>1110G</cell><cell>121ms</cell></row><row><cell>CC-Attention (Our impl.)</cell><cell>10.6M</cell><cell>427M</cell><cell>804G</cell><cell>131ms</cell></row><row><cell>Self-Attention (Our impl.)</cell><cell>10.5M</cell><cell>2168M</cell><cell>619G</cell><cell>96ms</cell></row><row><cell>Double Attention (Our impl.)</cell><cell>10.2M</cell><cell>209M</cell><cell>338G</cell><cell>46ms</cell></row><row><cell>OCR</cell><cell>10.5M</cell><cell>202M</cell><cell>340G</cell><cell>45ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art. We use M to represent multiscale context and R to represent relational context. Red, Green, Blue represent the top-3 results. We use , † and ‡ to mark the result w/o using Cityscapes val, the method using Mapillary dataset and the method using the Cityscapes video dataset separately FromTable 5, it can be seen that our OCR achieves competitive performance (45.28% and 45.66%) compared with most of the previous approaches based on both simple baselines and advanced baselines. For example, the ACFNet<ref type="bibr" target="#b23">[24]</ref> exploits both the multi-scale context and relational context to achieve higher performance. The very recent ACNet<ref type="bibr" target="#b18">[19]</ref> achieves the best performance through combining richer local and global contexts. LIP. Our approach achieves the best performance 55.60% on LIP val based on the simple baselines. Applying the stronger backbone HRNetV2-W48 further im-</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell>Stride</cell><cell>Context schemes</cell><cell cols="3">Cityscapes (w/o coarse) Cityscapes (w/ coarse) ADE20K</cell><cell>LIP</cell><cell>PASCAL Context</cell><cell>COCO-Stuff</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Simple baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSPNet [80]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M</cell><cell>78.4</cell><cell>81.2</cell><cell>43.29</cell><cell>-</cell><cell>47.8</cell><cell>-</cell></row><row><cell>DeepLabv3 [6]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M</cell><cell>-</cell><cell>81.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSANet [81]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>80.1</cell><cell>81.4</cell><cell>43.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAC [79]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M</cell><cell>78.1</cell><cell>-</cell><cell>44.30</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AAF [29]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>79.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSSPN [41]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>-</cell><cell>77.8</cell><cell>-</cell><cell>43.68</cell><cell>-</cell><cell>-</cell><cell>38.9</cell></row><row><cell>DepthSeg [32]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>-</cell><cell>78.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMAN [48]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.81</cell><cell>-</cell><cell>-</cell></row><row><cell>JPPNet [39]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.37</cell><cell>-</cell><cell>-</cell></row><row><cell>EncNet [76]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.65</cell><cell>-</cell><cell>51.7</cell><cell>-</cell></row><row><cell>GCU [38]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>-</cell><cell>-</cell><cell>44.81</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>APCNet [24]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M,R</cell><cell>-</cell><cell>-</cell><cell>45.38</cell><cell>-</cell><cell>54.7</cell><cell>-</cell></row><row><cell>CFNet [77]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>79.6</cell><cell>-</cell><cell>44.89</cell><cell>-</cell><cell>54.0</cell><cell>-</cell></row><row><cell>BFP [12]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>81.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.6</cell><cell>-</cell></row><row><cell>CCNet [27]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>81.4</cell><cell>-</cell><cell>45.22</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ANNet [84]</cell><cell>ResNet-101</cell><cell>8×</cell><cell>M,R</cell><cell>81.3</cell><cell>-</cell><cell>45.24</cell><cell>-</cell><cell>52.8</cell><cell>-</cell></row><row><cell>OCR (Seg. transformer)</cell><cell>ResNet-101</cell><cell>8×</cell><cell>R</cell><cell>81.8</cell><cell>82.4</cell><cell>45.28</cell><cell>55.60</cell><cell>54.8</cell><cell>39.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Advanced baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DenseASPP [68]</cell><cell>DenseNet-161</cell><cell>8×</cell><cell>M</cell><cell>80.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DANet [18]</cell><cell>ResNet-101 + MG</cell><cell>8×</cell><cell>R</cell><cell>81.5</cell><cell>-</cell><cell>45.22</cell><cell>-</cell><cell>52.6</cell><cell>39.7</cell></row><row><cell>DGCNet [78]</cell><cell>ResNet-101 + MG</cell><cell>8×</cell><cell>R</cell><cell>82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.7</cell><cell>-</cell></row><row><cell>EMANet [36]</cell><cell>ResNet-101 + MG</cell><cell>8×</cell><cell>R</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.1</cell><cell>39.9</cell></row><row><cell>SeENet [51]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>M</cell><cell>81.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SGR [40]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>R</cell><cell>-</cell><cell>-</cell><cell>44.32</cell><cell>-</cell><cell>52.5</cell><cell>39.1</cell></row><row><cell>OCNet [72]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>M,R</cell><cell>81.7</cell><cell>-</cell><cell>45.45</cell><cell>54.72</cell><cell>-</cell><cell>-</cell></row><row><cell>ACFNet [75]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>M,R</cell><cell>81.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CNIF [63]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.93</cell><cell>-</cell><cell></cell></row><row><cell>GALD [37]</cell><cell>ResNet-101 + ASPP</cell><cell>8×</cell><cell>M,R</cell><cell>81.8</cell><cell>82.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">GALD  † [37] ResNet-101 + CGNL + MG 8×</cell><cell>M,R</cell><cell>-</cell><cell>83.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Mapillary [52] WideResNet-38 + ASPP</cell><cell>8×</cell><cell>M</cell><cell>-</cell><cell>82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">GSCNN  † [55] WideResNet-38 + ASPP</cell><cell>8×</cell><cell>M</cell><cell>82.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>SPGNet [10]</cell><cell>2× ResNet-50</cell><cell>4×</cell><cell>-</cell><cell>81.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ZigZagNet [42]</cell><cell>ResNet-101</cell><cell>4×</cell><cell>M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.1</cell><cell>-</cell></row><row><cell>SVCNet [13]</cell><cell>ResNet-101</cell><cell>4×</cell><cell>R</cell><cell>81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.2</cell><cell>39.6</cell></row><row><cell>ACNet [19]</cell><cell>ResNet-101 + MG</cell><cell>4×</cell><cell>M,R</cell><cell>82.3</cell><cell>-</cell><cell>45.90</cell><cell>-</cell><cell>54.1</cell><cell>40.1</cell></row><row><cell>CE2P [45]</cell><cell>ResNet-101 + PPM</cell><cell>4×</cell><cell>M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.10</cell><cell>-</cell><cell></cell></row><row><cell cols="2">VPLR  † ‡ [83] WideResNet-38 + ASPP</cell><cell>4×</cell><cell>M</cell><cell>-</cell><cell>83.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>DeepLabv3+ [7]</cell><cell>Xception-71</cell><cell>4×</cell><cell>M</cell><cell>-</cell><cell>82.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPC [4]</cell><cell>Xception-71</cell><cell>4×</cell><cell>M</cell><cell>82.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>DUpsampling [57]</cell><cell>Xception-71</cell><cell>4×</cell><cell>M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.5</cell><cell>-</cell></row><row><cell>HRNet [54]</cell><cell>HRNetV2-W48</cell><cell>4×</cell><cell>-</cell><cell>81.6</cell><cell>-</cell><cell>-</cell><cell>55.90</cell><cell>54.0</cell><cell></cell></row><row><cell>OCR (Seg. transformer)</cell><cell>HRNetV2-W48</cell><cell>4×</cell><cell>R</cell><cell>82.4</cell><cell>83.0</cell><cell>45.66</cell><cell>56.65</cell><cell>56.2</cell><cell>40.5</cell></row><row><cell>OCR  † (Seg. transformer)</cell><cell>HRNetV2-W48</cell><cell>4×</cell><cell>R</cell><cell>83.6</cell><cell>84.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ADE20K.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Panoptic segmentation results on COCO val 2017. The performance of Panoptic-FPN [30] is reproduced based on the official open-source Detectron2 [66] and we use the 3× learning rate schedule by default. Our OCR consistently improves the PQ performance with both backbones.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell>AP</cell><cell>PQ Th</cell><cell>mIoU</cell><cell>PQ St</cell><cell>PQ</cell></row><row><cell>ResNet-50</cell><cell cols="6">Panoptic-FPN Panoptic-FPN + OCR 40.4 (+0.4 ) 48.6 (+0.3 ) 44.3 (+1.4 ) 33.9 (+2.7 ) 42.7 (+1.2 ) 40.0 48.3 42.9 31.2 41.5</cell></row><row><cell>ResNet-101</cell><cell cols="6">Panoptic-FPN Panoptic-FPN + OCR 42.7 (+0.3 ) 50.2 (+0.5 ) 45.5 (+1.0 ) 35.2 (+2.3 ) 44.2 (+1.2 ) 42.4 49.7 44.5 32.9 43.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use "object" to represent both "things" and "stuff" following<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b52">53]</ref>. 5 See Section 3.4 for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The local and/or global self-attention units in interlaced self-attention<ref type="bibr" target="#b70">[71]</ref> could be applied to Vision Transformer<ref type="bibr" target="#b14">[15]</ref> for acceleration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Only few methods adopt multi-scale testing. For example, CNIF [63] gets the improved performance from 56.93% to 57.74%. 8 https://github.com/facebookresearch/detectron2/blob/master/MODEL ZOO.md</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Region-based semantic segmentation with endto-end training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12814</idno>
		<title level="m">Graph-based global reasoning networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>abs/2010.11929</idno>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>abs/2010.11929</idno>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adaptive context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detect, replace, refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recognition using regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Label refinement network for coarse-to-fine semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Shapemask: Learning to segment novel objects by refining shape priors</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Global aggregation then local distribution in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Zigzagnet: Fusing top-down and bottom-up context for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05996</idno>
		<title level="m">Devil in the details: Towards accurate single and multiple human parsing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Macro-micro adversarial network for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Junqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Towards bridging semantic gap to improve semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Not using the car to see the sidewalk-quantifying and controlling the effects of context in classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>abs/2103.17239</idno>
		<ptr target="https://arxiv.org/abs/2103.17239" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Auto-context and its application to high-level vision tasks and 3d brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Selective search for object recognition. IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning compositional neural information fusion for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Sognet: Scene overlap graph network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07527</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1809.00916</idno>
		<ptr target="http://arxiv.org/abs/1809.00916" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dahua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">A 3d coarse-to-fine framework for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
