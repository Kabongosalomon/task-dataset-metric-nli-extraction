<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointRend: Image Segmentation as Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PointRend: Image Segmentation as Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over-and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-ofthe-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are oversmoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https:// github.com/facebookresearch/detectron2/ tree/master/projects/PointRend.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image segmentation tasks involve mapping pixels sampled on a regular grid to a label map, or a set of label maps, on the same grid. For semantic segmentation, the label map indicates the predicted category at each pixel. In the case of instance segmentation, a binary foreground vs. background map is predicted for each detected object. The modern tools of choice for these tasks are built on convolutional neural networks (CNNs) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>CNNs for image segmentation typically operate on regular grids: the input image is a regular grid of pixels, their hidden representations are feature vectors on a regular grid, and their outputs are label maps on a regular grid. Regular grids are convenient, but not necessarily computation- <ref type="bibr">56×56</ref> 112×112 224×224</p><p>Mask R-CNN + PointRend 28×28 28×28 224×224 <ref type="figure">Figure 1</ref>: Instance segmentation with PointRend. We introduce the PointRend (Point-based Rendering) module that makes predictions at adaptively sampled points on the image using a new pointbased feature representation (see <ref type="figure">Fig. 3</ref>). PointRend is general and can be flexibly integrated into existing semantic and instance segmentation systems. When used to replace Mask R-CNN's default mask head <ref type="bibr" target="#b18">[19]</ref> (top-left), PointRend yields significantly more detailed results (top-right). (bottom) During inference, PointRend iterative computes its prediction. Each step applies bilinear upsampling in smooth regions and makes higher resolution predictions at a small number of adaptively selected points that are likely to lie on object boundaries (black points). All figures in the paper are best viewed digitally with zoom. Image source: <ref type="bibr" target="#b40">[41]</ref>.</p><p>ally ideal for image segmentation. The label maps predicted by these networks should be mostly smooth, i.e., neighboring pixels often take the same label, because highfrequency regions are restricted to the sparse boundaries between objects. A regular grid will unnecessarily oversample the smooth areas while simultaneously undersampling object boundaries. The result is excess computation in smooth regions and blurry contours ( <ref type="figure">Fig. 1</ref>, upper-left). Image segmentation methods often predict labels on a low-resolution regular grid, e.g., 1/8-th of the input <ref type="bibr" target="#b34">[35]</ref> for semantic segmentation, or 28×28 <ref type="bibr" target="#b18">[19]</ref> for instance segmentation, as a compromise between undersampling and oversampling. Analogous sampling issues have been studied for decades in computer graphics. For example, a renderer maps a model (e.g., a 3D mesh) to a rasterized image, i.e. a  <ref type="bibr" target="#b18">[19]</ref> with its standard mask head (left image) vs. with PointRend (right image), using ResNet-50 <ref type="bibr" target="#b19">[20]</ref> with FPN <ref type="bibr" target="#b27">[28]</ref>. Note how PointRend predicts masks with substantially finer detail around object boundaries. regular grid of pixels. While the output is on a regular grid, computation is not allocated uniformly over the grid. Instead, a common graphics strategy is to compute pixel values at an irregular subset of adaptively selected points in the image plane. The classical subdivision technique of <ref type="bibr" target="#b47">[48]</ref>, as an example, yields a quadtree-like sampling pattern that efficiently renders an anti-aliased, high-resolution image.</p><p>The central idea of this paper is to view image segmentation as a rendering problem and to adapt classical ideas from computer graphics to efficiently "render" highquality label maps (see <ref type="figure">Fig. 1</ref>, bottom-left). We encapsulate this computational idea in a new neural network module, called PointRend, that uses a subdivision strategy to adaptively select a non-uniform set of points at which to compute labels. PointRend can be incorporated into popular meta-architectures for both instance segmentation (e.g., Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>) and semantic segmentation (e.g., FCN <ref type="bibr" target="#b34">[35]</ref>). Its subdivision strategy efficiently computes high-resolution segmentation maps using an order of magnitude fewer floating-point operations than direct, dense computation.</p><p>PointRend is a general module that admits many possible implementations. Viewed abstractly, a PointRend module accepts one or more typical CNN feature maps f (x i , y i ) that are defined over regular grids, and outputs high-resolution predictions p(x i , y i ) over a finer grid. Instead of making excessive predictions over all points on the output grid, PointRend makes predictions only on carefully selected points. To make these predictions, it extracts a point-wise feature representation for the selected points by interpolating f , and uses a small point head subnetwork to predict output labels from the point-wise features. We will present a simple and effective PointRend implementation.</p><p>We evaluate PointRend on instance and semantic segmentation tasks using the COCO <ref type="bibr" target="#b28">[29]</ref> and Cityscapes <ref type="bibr" target="#b8">[9]</ref> benchmarks. Qualitatively, PointRend efficiently computes sharp boundaries between objects, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="figure">Fig. 8</ref>. We also observe quantitative improvements even though the standard intersection-over-union based metrics for these tasks (mask AP and mIoU) are biased towards object-interior pixels and are relatively insensitive to boundary improvements. PointRend improves strong Mask R-CNN and DeepLabV3 <ref type="bibr" target="#b4">[5]</ref> models by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Rendering algorithms in computer graphics output a regular grid of pixels. However, they usually compute these pixel values over a non-uniform set of points. Efficient procedures like subdivision <ref type="bibr" target="#b47">[48]</ref> and adaptive sampling <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref> refine a coarse rasterization in areas where pixel values have larger variance. Ray-tracing renderers often use oversampling <ref type="bibr" target="#b49">[50]</ref>, a technique that samples some points more densely than the output grid to avoid aliasing effects. Here, we apply classical subdivision to image segmentation.</p><p>Non-uniform grid representations. Computation on regular grids is the dominant paradigm for 2D image analysis, but this is not the case for other vision tasks. In 3D shape recognition, large 3D grids are infeasible due to cubic scaling. Most CNN-based approaches do not go beyond coarse 64×64×64 grids <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref>. Instead, recent works consider more efficient non-uniform representations such as meshes <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14]</ref>, signed distance functions <ref type="bibr" target="#b36">[37]</ref>, and octrees <ref type="bibr" target="#b45">[46]</ref>. Similar to a signed distance function, PointRend can compute segmentation values at any point.</p><p>Recently, Marin et al. <ref type="bibr" target="#b35">[36]</ref> propose an efficient semantic segmentation network based on non-uniform subsampling of the input image prior to processing with a standard semantic segmentation network. PointRend, in contrast, focuses on non-uniform sampling at the output. It may be possible to combine the two approaches, though <ref type="bibr" target="#b35">[36]</ref> is currently unproven for instance segmentation.</p><p>Instance segmentation methods based on the Mask R-CNN meta-architecture <ref type="bibr" target="#b18">[19]</ref> occupy top ranks in recent challenges <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3]</ref>. These region-based architectures typically predict masks on a 28×28 grid irrespective of object size. This is sufficient for small objects, but for large objects it produces undesirable "blobby" output that oversmooths the fine-level details of large objects (see <ref type="figure">Fig. 1</ref>, top-left). Alternative, bottom-up approaches group pixels to form object masks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25]</ref>. These methods can produce more detailed output, however, they lag behind regionbased approaches on most instance segmentation benchmarks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40]</ref>. TensorMask <ref type="bibr" target="#b6">[7]</ref>, an alternative slidingwindow method, uses a sophisticated network design to predict sharp high-resolution masks for large objects, but its accuracy also lags slightly behind. In this paper, we show that a region-based segmentation model equipped with PointRend can produce masks with fine-level details while improving the accuracy of region-based approaches.</p><p>Semantic segmentation. Fully convolutional networks (FCNs) <ref type="bibr" target="#b34">[35]</ref> are the foundation of modern semantic segmentation approaches. They often predict outputs that have lower resolution than the input grid and use bilinear upsampling to recover the remaining 8-16× resolution. Results may be improved with dilated/atrous convolutions that replace some subsampling layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> at the expense of more memory and computation.</p><p>Alternative approaches include encoder-decoder achitectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> that subsample the grid representation in the encoder and then upsample it in the decoder, using skip connections <ref type="bibr" target="#b43">[44]</ref> to recover filtered details. Current approaches combine dilated convolutions with an encoderdecoder structure <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> to produce output on a 4× sparser grid than the input grid before applying bilinear interpolation. In our work, we propose a method that can efficiently predict fine-level details on a grid as dense as the input grid.  <ref type="figure">Figure 3</ref>: PointRend applied to instance segmentation. A standard network for instance segmentation (solid red arrows) takes an input image and yields a coarse (e.g. 7×7) mask prediction for each detected object (red box) using a lightweight segmentation head. To refine the coarse mask, PointRend selects a set of points (red dots) and makes prediction for each point independently with a small MLP. The MLP uses interpolated features computed at these points (dashed red arrows) from (1) a fine-grained feature map of the backbone CNN and (2) from the coarse prediction mask. The coarse mask features enable the MLP to make different predictions at a single point that is contained by two or more boxes. The proposed subdivision mask rendering algorithm (see <ref type="figure">Fig. 4</ref> and §3.1) applies this process iteratively to refine uncertain regions of the predicted mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We analogize image segmentation (of objects and/or scenes) in computer vision to image rendering in computer graphics. Rendering is about displaying a model (e.g., a 3D mesh) as a regular grid of pixels, i.e., an image. While the output representation is a regular grid, the underlying physical entity (e.g., the 3D model) is continuous and its physical occupancy and other attributes can be queried at any real-value point on the image plane using physical and geometric reasoning, such as ray-tracing.</p><p>Analogously, in computer vision, we can think of an image segmentation as the occupancy map of an underlying continuous entity, and the segmentation output, which is a regular grid of predicted labels, is "rendered" from it. The entity is encoded in the network's feature maps and can be accessed at any point by interpolation. A parameterized function, that is trained to predict occupancy from these interpolated point-wise feature representations, is the counterpart to physical and geometric reasoning.</p><p>Based on this analogy, we propose PointRend (Pointbased Rendering) as a methodology for image segmentation using point representations. A PointRend module accepts one or more typical CNN feature maps of C channels f ∈ R C×H×W , each defined over a regular grid (that is typically 4× to 16× coarser than the image grid), and outputs predictions for the K class labels p ∈ R K×H ×W over a regular grid of different (and likely higher) resolution. A PointRend module consists of three main components: (i) A point selection strategy chooses a small number of real-value points to make predictions on, avoiding excessive computation for all pixels in the high-resolution output grid. (ii) For each selected point, a point-wise feature representation is extracted. Features for a real-value point are computed by bilinear interpolation of f , using the point's 4 nearest neighbors that are on the regular grid of f . As a result, it is able to utilize sub-pixel information encoded in the channel dimension of f to predict a segmentation that has higher resolution than f . (iii) A point head: a small neural network trained to predict a label from this point-wise feature representation, independently for each point.</p><p>The PointRend architecture can be applied to instance segmentation (e.g., on Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>) and semantic segmentation (e.g., on FCNs <ref type="bibr" target="#b34">[35]</ref>) tasks. For instance segmentation, PointRend is applied to each region. It computes masks in a coarse-to-fine fashion by making predictions over a set of selected points (see <ref type="figure">Fig. 3</ref>). For semantic segmentation, the whole image can be considered as a single region, and thus without loss of generality we will describe PointRend in the context of instance segmentation. We discuss the three main components in more detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point Selection for Inference and Training</head><p>At the core of our method is the idea of flexibly and adaptively selecting points in the image plane at which to predict segmentation labels. Intuitively, these points should be located more densely near high-frequency areas, such as object boundaries, analogous to the anti-aliasing problem in ray-tracing. We develop this idea for inference and training.</p><p>Inference. Our selection strategy for inference is inspired by the classical technique of adaptive subdivision <ref type="bibr" target="#b47">[48]</ref> in computer graphics. The technique is used to efficiently render high resolutions images (e.g., via ray-tracing) by computing only at locations where there is a high chance that the value is significantly different from its neighbors; for all other locations the values are obtained by interpolating already computed output values (starting from a coarse grid).</p><p>For each region, we iteratively "render" the output mask in a coarse-to-fine fashion. The coarsest level prediction is made on the points on a regular grid (e.g., by using a standard coarse segmentation prediction head). In each iteration, PointRend upsamples its previously predicted segmentation using bilinear interpolation and then selects the N most uncertain points (e.g., those with probabilities closest to 0.5 for a binary mask) on this denser grid. PointRend then computes the point-wise feature representation (described shortly in §3.2) for each of these N points and predicts their labels. This process is repeated until the segmentation is upsampled to a desired resolution. One step of this procedure point prediction <ref type="figure">Figure 4</ref>: Example of one adaptive subdivision step. A prediction on a 4×4 grid is upsampled by 2× using bilinear interpolation. Then, PointRend makes prediction for the N most ambiguous points (black dots) to recover detail on the finer grid. This process is repeated until the desired grid resolution is achieved. We show N =14 2 points sampled using different strategies for the same underlying coarse prediction. To achieve high performance only a small number of points are sampled per region with a mildly biased sampling strategy making the system more efficient during training.</p><formula xml:id="formula_0">8 × 8 8 × 8 4 × 4 2×</formula><p>is illustrated on a toy example in <ref type="figure">Fig. 4</ref>.</p><p>With a desired output resolution of M ×M pixels and a starting resolution of M 0 ×M 0 , PointRend requires no more than N log 2 M M0 point predictions. This is much smaller than M ×M , allowing PointRend to make high-resolution predictions much more effectively. For example, if M 0 is 7 and the desired resolutions is M =224, then 5 subdivision steps are preformed. If we select N =28 2 points at each step, PointRend makes predictions for only 28 2 ·4.25 points, which is 15 times smaller than 224 2 . Note that fewer than N log 2 M M0 points are selected overall because in the first subdivision step only 14 2 points are available.</p><p>Training. During training, PointRend also needs to select points at which to construct point-wise features for training the point head. In principle, the point selection strategy can be similar to the subdivision strategy used in inference. However, subdivision introduces sequential steps that are less friendly to training neural networks with backpropagation. Instead, for training we use a non-iterative strategy based on random sampling.</p><p>The sampling strategy selects N points on a feature map to train on. <ref type="bibr" target="#b0">1</ref> It is designed to bias selection towards uncertain regions, while also retaining some degree of uniform coverage, using three principles. (i) Over generation: we over-generate candidate points by randomly sampling kN points (k&gt;1) from a uniform distribution. (ii) Importance sampling: we focus on points with uncertain coarse predictions by interpolating the coarse prediction values at all kN points and computing a taskspecific uncertainty estimate (defined in §4 and §5). The most uncertain βN points (β ∈ [0, 1]) are selected from the kN candidates. (iii) Coverage: the remaining (1 − β)N points are sampled from a uniform distribution. We illustrate this procedure with different settings, and compare it to regular grid selection, in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>At training time, predictions and loss functions are only computed on the N sampled points (in addition to the coarse segmentation), which is simpler and more efficient than backpropagation through subdivision steps. This design is similar to the parallel training of RPN + Fast R-CNN in a Faster R-CNN system <ref type="bibr" target="#b12">[13]</ref>, whose inference is sequential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Point-wise Representation and Point Head</head><p>PointRend constructs point-wise features at selected points by combining (e.g., concatenating) two feature types, fine-grained and coarse prediction features, described next.</p><p>Fine-grained features. To allow PointRend to render fine segmentation details we extract a feature vector at each sampled point from CNN feature maps. Because a point is a real-value 2D coordinate, we perform bilinear interpolation on the feature maps to compute the feature vector, following standard practice <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10]</ref>. Features can be extracted from a single feature map (e.g., res 2 in a ResNet); they can also be extracted from multiple feature maps (e.g., res 2 to res 5 , or their feature pyramid <ref type="bibr" target="#b27">[28]</ref> counterparts) and concatenated, following the Hypercolumn method <ref type="bibr" target="#b16">[17]</ref>.</p><p>Coarse prediction features. The fine-grained features enable resolving detail, but are also deficient in two regards. First, they do not contain region-specific information and thus the same point overlapped by two instances' bounding boxes will have the same fine-grained features. Yet, the point can only be in the foreground of one instance. Therefore, for the task of instance segmentation, where different regions may predict different labels for the same point, additional region-specific information is needed.</p><p>Second, depending on which feature maps are used for the fine-grained features, the features may contain only relatively low-level information (e.g., we will use res 2 with DeepLabV3). In this case, a feature source with more contextual and semantic information can be helpful. This issue affects both instance and semantic segmentation.</p><p>Based on these considerations, the second feature type is a coarse segmentation prediction from the network, i.e., a K-dimensional vector at each point in the region (box) representing a K-class prediction. The coarse resolution, by design, provides more globalized context, while the channels convey the semantic classes. These coarse predictions are similar to the outputs made by the existing architectures, and are supervised during training in the same way as existing models. For instance segmentation, the coarse prediction can be, for example, the output of a lightweight 7×7 resolution mask head in Mask R-CNN. For semantic segmentation, it can be, for example, predictions from a stride 16 feature map.</p><p>Point head. Given the point-wise feature representation at each selected point, PointRend makes point-wise segmentation predictions using a simple multi-layer perceptron (MLP). This MLP shares weights across all points (and all regions), analogous to a graph convolution <ref type="bibr" target="#b22">[23]</ref> or a PointNet <ref type="bibr" target="#b42">[43]</ref>. Since the MLP predicts a segmentation label for each point, it can be trained by standard task-specific segmentation losses (described in §4 and §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments: Instance Segmentation</head><p>Datasets. We use two standard instance segmentation datasets: COCO <ref type="bibr" target="#b28">[29]</ref> and Cityscapes <ref type="bibr" target="#b8">[9]</ref>. We report the standard mask AP metric <ref type="bibr" target="#b28">[29]</ref> using the median of 3 runs for COCO and 5 for Cityscapes (it has higher variance).</p><p>COCO has 80 categories with instance-level annotation. We train on train2017 (∼118k images) and report results on val2017 (5k images). As noted in <ref type="bibr" target="#b15">[16]</ref>, the COCO ground-truth is often coarse and AP for the dataset may not fully reflect improvements in mask quality. Therefore we supplement COCO results with AP measured using the 80 COCO category subset of LVIS <ref type="bibr" target="#b15">[16]</ref>, denoted by AP . The LVIS annotations have significantly higher quality. Note that for AP we use the same models trained on COCO and simply re-evaluate their predictions against the higherquality LVIS annotations using the LVIS evaluation API.</p><p>Cityscapes is an ego-centric street-scene dataset with 8 categories, 2975 train images, and 500 validation images. The images are higher resolution compared to COCO (1024×2048 pixels) and have finer, more pixel-accurate ground-truth instance segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture. Our experiments use Mask R-CNN with a</head><p>ResNet-50 <ref type="bibr" target="#b19">[20]</ref> + FPN <ref type="bibr" target="#b27">[28]</ref> backbone. The default mask head in Mask R-CNN is a region-wise FCN, which we denote by "4× conv". <ref type="bibr" target="#b1">2</ref> We use this as our baseline for comparison. For PointRend, we make appropriate modifications to this baseline, as described next.</p><p>Lightweight, coarse mask prediction head. To compute the coarse prediction, we replace the 4× conv mask head with a lighter weight design that resembles Mask R-CNN's box head and produces a 7×7 mask prediction. Specifically, for each bounding box, we extract a 14×14 feature map from the P 2 level of the FPN using bilinear interpolation. The features are computed on a regular grid inside the bounding box (this operation can seen as a simple version of RoIAlign). Next, we use a stride-two 2×2 convolution layer with 256 output channels followed by ReLU <ref type="bibr" target="#b38">[39]</ref>, which reduces the spatial size to 7×7. Finally, similar to Mask R-CNN's box head, an MLP with two 1024-wide hidden layers is applied to yield a 7×7 mask prediction for each of the K classes. ReLU is used on the MLP's hidden layers and the sigmoid activation function is applied to its outputs.</p><p>PointRend. At each selected point, a K-dimensional feature vector is extracted from the coarse prediction head's output using bilinear interpolation. PointRend also interpolates a 256-dimensional feature vector from the P 2 level of the FPN. This level has a stride of 4 w.r.t. the input image. These coarse prediction and fine-grained feature vectors are concatenated. We make a K-class prediction at selected points using an MLP with 3 hidden layers with 256 channels. In each layer of the MLP, we supplement the 256 output channels with the K coarse prediction features to make the input vector for the next layer. We use ReLU inside the MLP and apply sigmoid to its output.</p><p>Training. We use the standard 1× training schedule and data augmentation from Detectron2 <ref type="bibr" target="#b48">[49]</ref> by default (full details are in the appendix). For PointRend, we sample 14 2 points using the biased sampling strategy described in the §3.1 with k=3 and β=0.75. We use the distance between 0.5 and the probability of the ground truth class interpolated from the coarse prediction as the point-wise uncertainty measure. For a predicted box with ground-truth class c, we sum the binary cross-entropy loss for the c-th MLP output over the 14 2 points. The lightweight coarse prediction head uses the average cross-entropy loss for the mask predicted for class c, i.e., the same loss as the baseline 4× conv head. We sum all losses without any re-weighting. During training, Mask R-CNN applies the box and mask heads in parallel, while during inference they run as a cascade. We found that training as a cascade does not improve the baseline Mask R-CNN, but PointRend can benefit from it by sampling points inside more accurate boxes, slightly improving overall performance (∼0.2% AP, absolute).</p><p>Inference. For inference on a box with predicted class c, unless otherwise specified, we use the adaptive subdivision technique to refine the coarse 7×7 prediction for class c to the 224×224 in 5 steps. At each step, we select and update (at most) the N =28 2 most uncertain points based on the absolute difference between the predictions and 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>We compare PointRend to the default 4× conv head in Mask R-CNN in   <ref type="bibr" target="#b18">[19]</ref>. Mask AP is reported. AP is COCO mask AP evaluated against the higher-quality LVIS annotations <ref type="bibr" target="#b15">[16]</ref> (see text for details). A ResNet-50-FPN backbone is used for both COCO and Cityscapes models. PointRend outperforms the standard 4× conv mask head both quantitively and qualitatively. Higher output resolution leads to more detailed predictions, see <ref type="figure" target="#fig_0">Fig. 2</ref>   ing the COCO categories using the LVIS annotations (AP ) and for Cityscapes, which we attribute to the superior annotation quality in these datasets. Even with the same output resolution PointRend outperforms the baseline. The difference between 28×28 and 224×224 is relatively small because AP uses intersection-over-union <ref type="bibr" target="#b10">[11]</ref> and, therefore, is heavily biased towards object-interior pixels and less sensitive to the boundary quality. Visually, however, the difference in boundary quality is obvious, see <ref type="figure">Fig. 6</ref>.</p><p>Subdivision inference allows PointRend to yield a high resolution 224×224 prediction using more than 30 times less compute (FLOPs) and memory than the default 4× conv head needs to output the same resolution (based on taking a 112×112 RoIAlign input), see  <ref type="table">Table 3</ref>: Subdivision inference parameters. Higher output resolution improves AP. Although improvements saturate quickly (at underlined values) with the number of points sampled at each subdivision step, qualitative results may continue to improve for complex objects. AP is COCO mask AP evaluated against the higherquality LVIS annotations <ref type="bibr" target="#b15">[16]</ref> (see text for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>56×56</head><p>112×112 224×224 28×28 <ref type="figure">Figure 7</ref>: Anti-aliasing with PointRend. Precise object delineation requires output mask resolution to match or exceed the resolution of the input image region that the object occupies. prediction is sufficient (e.g., in the areas far away from object boundaries). In terms of wall-clock runtime, our unoptimized implementation outputs 224×224 masks at ∼13 fps, which is roughly the same frame-rate as a 4× conv head modified to output 56×56 masks (by doubling the default RoIAlign size), a design that actually has lower COCO AP compared to the 28×28 4× conv head (34.5% vs. 35.2%). <ref type="table">Table 3</ref> shows PointRend subdivision inference with different output resolutions and number of points selected at each subdivision step. Predicting masks at a higher resolution can improve results. Though AP saturates, visual improvements are still apparent when moving from lower (e.g., 56×56) to higher (e.g., 224×224) resolution outputs, see <ref type="figure">Fig. 7</ref>. AP also saturates with the number of points sampled in each subdivision step because points are selected in the most ambiguous areas first. Additional points may make predictions in the areas where a coarse prediction is already sufficient. For objects with complex boundaries, however, using more points may be beneficial.   <ref type="table">Table 5</ref>: Larger models and a longer 3× schedule <ref type="bibr" target="#b17">[18]</ref>. PointRend benefits from more advanced models and the longer training. The gap between PointRend and the default mask head in Mask R-CNN holds. AP is COCO mask AP evaluated against the higher-quality LVIS annotations <ref type="bibr" target="#b15">[16]</ref> (see text for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We conduct a number of ablations to analyze PointRend. In general we note that it is robust to the exact design of the point head MLP. Changes of its depth or width do not show any significant difference in our experiments.</p><p>Point selection during training. During training we select 14 2 points per object following the biased sampling strategy ( §3.1). Sampling only 14 2 points makes training computationally and memory efficient and we found that using more points does not improve results. Surprisingly, sampling only 49 points per box still maintains AP, though we observe an increased variance in AP. <ref type="table" target="#tab_5">Table 4</ref> shows PointRend performance with different selection strategies during training. Regular grid selection achieves similar results to uniform sampling. Whereas biasing sampling toward ambiguous areas improves AP. However, a sampling strategy that is biased too heavily towards boundaries of the coarse prediction (k&gt;10 and β close to 1.0) decreases AP. Overall, we find a wide range of parameters 2&lt;k&lt;5 and 0.75&lt;β&lt;1.0 delivers similar results.</p><p>Larger models, longer training. Training ResNet-50 + FPN (denoted R50-FPN) with the 1× schedule under-fits on COCO. In <ref type="table">Table 5</ref> we show that the PointRend improvements over the baseline hold with both longer training schedule and larger models (see the appendix for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask R-CNN + 4 conv</head><p>Mask R-CNN + PointRend DeeplabV3 DeeplabV3 + PointRend × <ref type="figure">Figure 8</ref>: Cityscapes example results for instance and semantic segmentation. In instance segmentation larger objects benefit more from PointRend ability to yield high resolution output. Whereas for semantic segmentation PointRend recovers small objects and details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: Semantic Segmentation</head><p>PointRend is not limited to instance segmentation and can be extended to other pixel-level recognition tasks. Here, we demonstrate that PointRend can benefit two semantic segmentation models: DeeplabV3 <ref type="bibr" target="#b4">[5]</ref>, which uses dilated convolutions to make prediction on a denser grid, and Se-manticFPN <ref type="bibr" target="#b23">[24]</ref>, a simple encoder-decoder architecture.</p><p>Dataset. We use the Cityscapes <ref type="bibr" target="#b8">[9]</ref> semantic segmentation set with 19 categories, 2975 training images, and 500 validation images. We report the median mIoU of 5 trials.</p><p>Implementation details. We reimplemented DeeplabV3 and SemanticFPN following their respective papers. Se-manticFPN uses a standard ResNet-101 <ref type="bibr" target="#b19">[20]</ref>, whereas DeeplabV3 uses the ResNet-103 proposed in <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b2">3</ref> We follow the original papers' training schedules and data augmentation (details are in the appendix).</p><p>We use the same PointRend architecture as for instance segmentation. Coarse prediction features come from the (already coarse) output of the semantic segmentation model. Fine-grained features are interpolated from res 2 for DeeplabV3 and from P 2 for SemanticFPN. During training we sample as many points as there are on a stride 16 feature map of the input (2304 for deeplabV3 and 2048 for Se-manticFPN). We use the same k=3, β=0.75 point selection strategy. During inference, subdivision uses N =8096 (i.e., the number of points in the stride 16 map of a 1024×2048 image) until reaching the input image resolution. To measure prediction uncertainty we use the same strategy during training and inference: the difference between the most confident and second most confident class probabilities.</p><p>DeeplabV3. In <ref type="table" target="#tab_7">Table 6</ref> we compare DeepLabV3 to DeeplabV3 with PointRend. The output resolution can also be increased by 2× at inference by using dilated convolutions in res 4 stage, as described in <ref type="bibr" target="#b4">[5]</ref>. Compared to both,   PointRend has higher mIoU. Qualitative improvements are also evident, see <ref type="figure">Fig. 8</ref>. By sampling points adaptively, PointRend reaches 1024×2048 resolution (i.e. 2M points) by making predictions for only 32k points, see <ref type="figure">Fig. 9</ref>.</p><p>SemanticFPN. <ref type="table" target="#tab_8">Table 7</ref> shows that SemanticFPN with PointRend improves over both 8× and 4× output stride variants without PointRend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Instance Segmentation Details</head><p>We use SGD with 0.9 momentum; a linear learning rate warmup <ref type="bibr" target="#b14">[15]</ref> over 1000 updates starting from a learning rate of 0.001 is applied; weight decay 0.0001 is applied; horizontal flipping and scale train-time data augmentation; the batch normalization (BN) <ref type="bibr" target="#b20">[21]</ref> layers from the ImageNet pre-trained models are frozen (i.e., BN is not used); no testtime augmentation is used.</p><p>COCO <ref type="bibr" target="#b28">[29]</ref>: 16 images per mini-batch; the training schedule is 60k / 20k / 10k updates at learning rates of 0.02 / 0.002 / 0.0002 respectively; training images are resized randomly to a shorter edge from 640 to 800 pixels with a step of 32 pixels and inference images are resized to a shorter edge size of 800 pixels.</p><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref>: 8 images per mini-batch the training schedule is 18k / 6k updates at learning rates of 0.01 / 0.001 respectively; training images are resized randomly to a shorter edge from 800 to 1024 pixels with a step of 32 pixels and inference images are resized to a shorter edge size of 1024 pixels.</p><p>Longer schedule: The 3× schedule for COCO is 210k / 40k / 20k updates at learning rates of 0.02 / 0.002 / 0.0002, respectively; all other details are the same as the setting described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Semantic Segmentation Details</head><p>DeeplabV3 <ref type="bibr" target="#b4">[5]</ref>: We use SGD with 0.9 momentum with 16 images per mini-batch cropped to a fixed 768×768 size; the training schedule is 90k updates with a poly learning rate <ref type="bibr" target="#b33">[34]</ref> update strategy, starting from 0.01; a linear learning rate warmup <ref type="bibr" target="#b14">[15]</ref> over 1000 updates starting from a learning rate of 0.001 is applied; the learning rate for ASPP and the prediction convolution are multiplied by 10; weight decay of 0.0001 is applied; random horizontal flipping and scaling of 0.5× to 2.0× with a 32 pixel step is used as training data augmentation; BN is applied to 16 images minibatches; no test-time augmentation is used;</p><p>SemanticFPN <ref type="bibr" target="#b23">[24]</ref>: We use SGD with 0.9 momentum with 32 images per mini-batch cropped to a fixed 512×1024 size; the training schedule is 40k / 15k / 10k updates at learning rates of 0.01 / 0.001 / 0.0001 respectively; a linear learning rate warmup <ref type="bibr" target="#b14">[15]</ref> over 1000 updates starting from a learning rate of 0.001 is applied; weight decay 0.0001 is applied; horizontal flipping, color augmentation <ref type="bibr" target="#b32">[33]</ref>, and crop bootstrapping <ref type="bibr" target="#b1">[2]</ref> are used during training; scale traintime data augmentation resizes an input image from 0.5× to 2.0× with a 32 pixel step; BN layers are frozen (i.e., BN is not used); no test-time augmentation is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. AP Computation</head><p>The first version (v1) of this paper on arXiv has an error in COCO mask AP evaluated against the LVIS annotations <ref type="bibr" target="#b15">[16]</ref> (AP ). The old version used an incorrect list of the categories not present in each evaluation image, which resulted in lower AP values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example result pairs from Mask R-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>a) regular grid b) uniform c) mildly biased k = 3, β = 0.75 d) heavily biased k = 10, β = 0.75 k = 1, β = 0.0 Point sampling during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>and Fig. 6 .224×224Figure 6 :</head><label>66</label><figDesc>28×28 PointRend inference with different output resolutions. High resolution masks align better with object boundaries. mask head output resolution FLOPs # activations 4×</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>PointRend outperforms the default head on both datasets. The gap is larger when evaluat-+0.9) 39.2 (+1.6) 35.5 (+2.5) PointRend 224×224 36.3 (+1.1) 39.7 (+2.1) 35.8 (+2.8)</figDesc><table><row><cell></cell><cell>output</cell><cell></cell><cell>COCO</cell><cell>Cityscapes</cell></row><row><cell cols="2">mask head resolution</cell><cell>AP</cell><cell>AP</cell><cell>AP</cell></row><row><cell>4× conv</cell><cell>28×28</cell><cell>35.2</cell><cell>37.6</cell><cell>33.0</cell></row><row><cell>PointRend</cell><cell>28×28</cell><cell>36.1 (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PointRend vs. the default 4× conv mask head for Mask R-CNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FLOPs (multiply-adds) and activation counts for a 224×224 output resolution mask. PointRend's efficient subdivision makes 224×224 output feasible in contrast to the standard 4× conv mask head modified to use an RoIAlign size of 112×112.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. PointRend</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Training-time point selection strategies with 14 2 points per box. Mildly biasing sampling towards uncertain regions performs the best. Heavily biased sampling performs even worse than uniform or regular grid sampling indicating the importance of coverage. AP is COCO mask AP evaluated against the higher-quality LVIS annotations<ref type="bibr" target="#b15">[16]</ref> (see text for details).</figDesc><table><row><cell>COCO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>DeeplabV3 with PointRend for Cityscapes semantic segmentation outperforms baseline DeepLabV3. Dilating the res4 stage during inference yields a larger, more accurate prediction, but at much higher computational and memory costs; it is still inferior to using PointRend.Figure 9: PointRend inference for semantic segmentation.PointRend refines prediction scores for areas where a coarser prediction is not sufficient. To visualize the scores at each step we take arg max at given resolution without bilinear interpolation.</figDesc><table><row><cell>method</cell><cell cols="2">output resolution mIoU</cell></row><row><cell>SemanticFPN P2-P5</cell><cell>256×512</cell><cell>77.7</cell></row><row><cell>SemanticFPN P2-P5 + PointRend</cell><cell>1024×2048</cell><cell>78.6 (+0.9)</cell></row><row><cell>SemanticFPN P3-P5</cell><cell>128×256</cell><cell>77.4</cell></row><row><cell>SemanticFPN P3-P5 + PointRend</cell><cell>1024×2048</cell><cell>78.5 (+1.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>SemanticFPN with PointRend for Cityscapes semantic segmentation outperform the baseline SemanticFPN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The value of N can be different for training and inference selection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Four layers of 3×3 convolutions with 256 output channels are applied to a 14×14 input feature map. Deconvolution with a 2×2 kernel transforms this to 28×28. Finally, a 1×1 convolution predicts mask logits.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It replaces the ResNet-101 res 1 7×7 convolution with three 3×3 convolutions (hence "ResNet-103").</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TensorMask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mesh R-CNN. In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">InstanceCut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Im-ageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SGN: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient segmentation: Learning downsampling near semantic boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyam</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating antialiased images at low sampling densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Don</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo-Wilfried</forename><surname>Paphio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsonga</surname></persName>
		</author>
		<idno>BY-NC-SA 2.0</idno>
		<ptr target="https://www.flickr.com/photos/paphio/2855627782/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Physically based rendering: From theory to implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Pharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Humphreys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An improved illumination model for shaded display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turner</forename><surname>Whitted</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Realtime kd-tree construction on graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
