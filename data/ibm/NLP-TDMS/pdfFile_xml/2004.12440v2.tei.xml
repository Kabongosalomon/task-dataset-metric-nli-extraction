<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
							<email>wuqianhui@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<email>zijlin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BÃ¶rje</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
							<email>borje.karlsson@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data. Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer. However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language. In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language. The proposed method works for both single-source and multi-source crosslingual NER. For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models. Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-theart methods for both single-source and multisource cross-lingual NER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is the task of identifying text spans that belong to pre-defined categories, like locations, person names, etc. It's a fundamental component in many downstream tasks, and has been greatly advanced by deep neural networks <ref type="bibr" target="#b8">(Lample et al., 2016;</ref><ref type="bibr" target="#b2">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b15">Peters et al., 2017)</ref>. However, these approaches generally require massive manually labeled data, which prohibits their adaptation to lowresource languages due to high annotation costs.</p><p>One solution to tackle that is to transfer knowledge from a source language with rich labeled data to a target language with little or even no labeled data, which is referred to as cross-lingual NER <ref type="bibr" target="#b28">(Wu and Dredze, 2019;</ref><ref type="bibr" target="#b27">Wu et al., 2020)</ref>. In this paper, following <ref type="bibr" target="#b28">Wu and Dredze (2019)</ref> and <ref type="bibr" target="#b27">Wu et al. (2020)</ref>, we focus on the extreme scenario of crosslingual NER where no labeled data is available in the target language, which is challenging in itself and has attracted considerable attention from the research community in recent years. Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer. Label-projection based methods focus on using labeled data in a source language to generate pseudo-labelled data in the target language for training an NER model. For example, <ref type="bibr" target="#b13">Ni et al. (2017)</ref> creates automatically labeled NER data for the target language via label projection on comparable corpora and develops a heuristic scheme to select good-quality projection-labeled data. <ref type="bibr" target="#b10">Mayhew et al. (2017)</ref> and <ref type="bibr" target="#b30">Xie et al. (2018)</ref> translate the source language labeled data at the phrase/word level to generate pairwise labeled data for the target language. Differently, model-transfer based methods <ref type="bibr" target="#b28">(Wu and Dredze, 2019;</ref><ref type="bibr" target="#b27">Wu et al., 2020)</ref> focus on training a shared NER model on the labeled data in the source language with languageindependent features, such as cross-lingual word representations <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, and then directly testing the model on the target language.</p><p>However, there are limitations in both labelprojection based methods and model-transfer based methods. The former relies on labeled data in the source language for label projection, and thus is not applicable in cases where the required labeled data is inaccessible (e.g., due to privacy/sensitivity issues). Meanwhile, the later does not leverage unlabeled data in the target language, which can be much cheaper to obtain and probably contains very useful language information.</p><p>In this paper, we propose a teacher-student learning method for cross-lingual NER to address the mentioned limitations. Specifically, we leverage multilingual BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> as the base model to produce language-independent features. A previously trained NER model for the source language is then used as a teacher model to predict the probability distribution of entity labels (i.e., soft labels) for each token in the non-pairwise unlabeled data in the target language. Finally, we train a student NER model for the target language using the pseudo-labeled data with such soft labels. The proposed method does not rely on labelled data in the source language, and it also leverages the available information from unlabeled data in the target language, thus avoiding the mentioned limitations of previous works. Note that we use the teacher model to predict soft labels rather than hard labels (i.e., one-hot labelling vector), as soft labels can provide much more information <ref type="bibr" target="#b6">(Hinton et al., 2015)</ref> for the student model. <ref type="figure" target="#fig_0">Figure 1</ref> shows the differences between the proposed teacher-student learning method and the typical label-projection or model-transfer based methods.</p><p>We further extend our teacher-student learning method to multi-source cross-lingual NER, considering that there are usually multiple source languages available in practice and we would prefer transferring knowledge from all source languages rather than a single one. In this case, our method still enjoys the same advantages in terms of data availability and inference efficiency, compared with existing works <ref type="bibr" target="#b19">(TÃ¤ckstrÃ¶m, 2012;</ref><ref type="bibr" target="#b4">Enghoff et al., 2018;</ref><ref type="bibr" target="#b17">Rahimi et al., 2019)</ref>. Moreover, we propose a method to measure the similarity between each source language and the target language, and use this similarity to better weight the supervision from the corresponding teacher model.</p><p>We evaluate our proposed method for 3 target languages on benchmark datasets, using different source language settings. Experimental results show that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER. We also conduct case studies and statistical analyses to discuss why teacher-student learning reaches better results.</p><p>The main contributions of this work are:</p><p>â¢ We propose a teacher-student learning method for single-source cross-lingual NER, which addresses limitations of previous works w.r.t data availability and usage of unlabeled data.</p><p>â¢ We extend the proposed method to multisource cross-lingual NER, using a measure of the similarities between source/target languages to better weight teacher models.</p><p>â¢ We conduct extensive experiments validating the effectiveness and reasonableness of the proposed methods, and further analyse why they attain superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Single-Source Cross-Lingual NER: Such approaches consider one single source language for knowledge transfer. Previous works can be divided into two categories: label-projection and modeltransfer based methods. Label-projection based methods aim to build pseudo-labeled data for the target language to train an NER model. Some early works proposed to use bilingual parallel corpora and project model expectations <ref type="bibr" target="#b25">(Wang and Manning, 2014)</ref> or labels <ref type="bibr" target="#b13">(Ni et al., 2017)</ref> from the source language to the target language with external word alignment information. But obtaining parallel corpora is expensive or even infeasible. To tackle that, recent methods proposed to firstly translate source-language labeled data at the phrase level <ref type="bibr" target="#b10">(Mayhew et al., 2017)</ref> or word level <ref type="bibr" target="#b30">(Xie et al., 2018)</ref>, and then directly copy labels across languages. But translation introduces extra noise due to sense ambiguity and word order differences between languages, thus hurting the trained model.</p><p>Model-transfer based methods generally rely on language-independent features (e.g., crosslingual word embeddings <ref type="bibr" target="#b13">(Ni et al., 2017;</ref><ref type="bibr" target="#b28">Wu and Dredze, 2019;</ref><ref type="bibr" target="#b12">Moon et al., 2019)</ref>, word clusters , gazetteers <ref type="bibr" target="#b33">(Zirikly and Hagiwara, 2015)</ref>, and wikifier features <ref type="bibr" target="#b24">(Tsai et al., 2016)</ref>), so that a model trained with such features can be directly applied to the target language. For further improvement, <ref type="bibr" target="#b27">Wu et al. (2020)</ref> proposed constructing a pseudotraining set for each test case and fine-tuning the model before inference. However, these methods do not leverage any unlabeled data in the target language, though such data can be easy to obtain and benefit the language/domain adaptation.</p><p>Multi-Source Cross-Lingual NER: Multisource cross-lingual NER considers multiple source languages for knowledge transfer. <ref type="bibr" target="#b19">TÃ¤ckstrÃ¶m (2012)</ref> and <ref type="bibr" target="#b12">Moon et al. (2019)</ref> concatenated the labeled data of all source languages to train a unified model, and performed cross-lingual NER in a direct model transfer manner.  leveraged adversarial networks to learn language-independent features, and learns a mixture-of-experts model <ref type="bibr" target="#b18">(Shazeer et al., 2017)</ref> to weight source models at the token level. However, both methods straightly rely on the availability of labeled data in the source languages.</p><p>Differently, Enghoff et al. <ref type="formula" target="#formula_1">(2018)</ref> implemented multi-source label projection and studied how source data quality influence performance. <ref type="bibr" target="#b17">Rahimi et al. (2019)</ref> applied truth inference to model the transfer annotation bias from multiple sourcelanguage models. However, both methods make predictions via an ensemble of source-language models, which is cumbersome and computationally expensive, especially when a source-language model has massive parameter space.</p><p>Teacher-Student Learning: Early applications of teacher-student learning targeted model compression <ref type="bibr" target="#b0">(Bucilu et al., 2006)</ref>, where a small student model is trained to mimic a pre-trained, larger teacher model or ensemble of models. It was soon applied to various tasks like image classification <ref type="bibr" target="#b6">(Hinton et al., 2015;</ref><ref type="bibr" target="#b32">You et al., 2017)</ref>, dialogue generation <ref type="bibr" target="#b14">(Peng et al., 2019)</ref>, and neural machine translation <ref type="bibr" target="#b21">(Tan et al., 2019)</ref>, which demonstrated the usefulness of the knowledge transfer approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Classification Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Gradient Back-Propagation Encoder Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Classification Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference Training</head><p>Unlabeled Target-Language Data <ref type="figure">Figure 2</ref>: Framework of the proposed teacher-student learning method for single-source cross-lingual NER.</p><p>In this paper, we investigate teacher-student learning for the task of cross-lingual NER, in both single-source and multi-source scenarios. Different from previous works, our proposed method does not rely on the availability of labelled data in source languages or any pairwise texts, while it can also leverage extra information in unlabeled data in the target language to enhance the cross-lingual transfer. Moreover, compared with using an ensemble of source-language models, our method uses a single student model for inference, which can enjoy higher efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Named entity recognition can be formulated as a sequence labeling problem, i.e., given a sentence</p><formula xml:id="formula_0">x = {x i } L i=1</formula><p>with L tokens, an NER model is supposed to infer the entity label y i for each token x i and output a label sequence y = {y i } L i=1 . Under the paradigm of cross-lingual NER, we assume there are K source-language models previously trained with language-independent features. Our proposed teacher-student learning method then uses those K source-language models as teachers to train an effective student NER model for the target language on its unlabeled data D tgt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-Source Cross-Lingual NER</head><p>Here we firstly consider the case of only one source language (K = 1) for cross-lingual NER. The overall framework of the proposed teacher-student learning method for single-source cross-lingual NER is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">NER Model Structure</head><p>As shown in <ref type="figure">Figure 2</ref>, for simplicity, we employ the same neural network structure for both teacher (source-language) and student (target-language) NER models. Note that the student model is flexible and its structure can be determined according to the trade-off between performance and training/inference efficiency.</p><p>Here the adopted NER model consists of an encoder layer and a linear classification layer. Specifically, given an input sequence</p><formula xml:id="formula_1">x = {x i } L i=1 with L tokens, the encoder layer f Î¸ maps it into a sequence of hidden vectors h = {h i } L i=1 : h = f Î¸ (x)<label>(1)</label></formula><p>Here f Î¸ (Â·) can be any encoder model that produces cross-lingual token representations, and h i is the hidden vector corresponding to the i-th token x i . With each h i derived, the linear classification layer computes the probability distribution of entity labels for the corresponding token x i , using a softmax function:</p><formula xml:id="formula_2">p(x i , Î) = softmax(W h i + b)<label>(2)</label></formula><p>where p(x i , Î) â R |C| with C being the entity label set, and Î = {f Î¸ , W, b} denotes the to-belearned model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Teacher-Student Learning</head><p>Training:</p><p>We train the student model to mimic the output probability distribution of entity labels by the teacher model, on the unlabeled data in the target language D tgt . Knowledge from the teacher model is expected to transfer to the student model, while the student model can also leverage helpful language-specific information available in the unlabeled target-language data. Given an unlabeled sentence x â D tgt in the target language, the teacher-student learning loss w.r.t x is formulated as the mean squared error (MSE) between the output probability distributions of entity labels by the student model and those by the teacher model, averaged over tokens. Note that here we follow <ref type="bibr" target="#b31">Yang et al. (2019)</ref> and use the MSE loss, because it is symmetric and mimics all probabilities equally. Suppose that for the i-token in x , i.e., x i , the probability distribution of entity labels output by the student model is denoted aÅ p(x i , Î S ), and that output by the teacher model as p(x i , Î T ). Here Î S and Î T , respectively, denote the parameters of the student and the teacher models. The teacher-student learning loss w.r.t x is then defined as:</p><formula xml:id="formula_3">L(x , Î S ) = 1 L L i=1 MSE p(x i , Î S ),p(x i , Î T )</formula><p>(3) And the whole training loss is the summation of losses w.r.t all sentences in D tgt , as defined below.</p><formula xml:id="formula_4">L(Î S ) = x âDtgt L(x , Î S )<label>(4)</label></formula><p>Minimizing L(Î S ) will derive the student model.</p><p>Inference: For inference in the target language, we only utilize the learned student model to predict the probability distribution of entity labels for each token x i in a test sentence x. Then we take the entity label c â C with the highest probability as the predicted label y i for x i :</p><formula xml:id="formula_5">y i = arg max cp (x i , Î S ) c<label>(5)</label></formula><p>where p(x i , Î S ) c denotes the predicted probability corresponding to the entity label c in p(x i , Î S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Source Cross-Lingual NER</head><p>The framework of the proposed teacher-student learning method for multi-source (K &gt; 1) crosslingual NER is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Extension to Multiple Teacher Models</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, we extend the singleteacher framework in <ref type="figure">Figure 2</ref> into a multi-teacher one, while keeping the student model unchanged.</p><p>Note that, for simplicity, all teacher models and the student model use the same model structure as 3.1.1. Take the k-th teacher model for example, and denote its parameters as Î (k) T . Given a sentence x = {x i } L i=1 with L tokens from the unlabeled data D tgt in the target language, the output probability distribution of entity labels w.r.t the i-th token x i can be derived as Eq. 1 and 2, which is denoted asp(</p><formula xml:id="formula_6">x i , Î (k) T ).</formula><p>To combine all teacher models, we add up their output probability distributions with a group of weights {Î± k } K k=1 as follows.</p><formula xml:id="formula_7">p(x i , Î T ) = K k=1 Î± k Â·p(x i , Î (k) T )<label>(6)</label></formula><p>wherep(x i , Î T ) is the combined probability distribution of entity labels,</p><formula xml:id="formula_8">Î T = {Î (k) T } K k=1</formula><p>is the set of parameters of all teacher models, and Î± k is the weight corresponding to the k-th teacher model, with K k=1 Î± k = 1 and Î± k â¥ 0, âk â {1, . . . , K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Weighting Teacher Models</head><p>Here we elaborate on how to derive the weights {Î± k } K k=1 in cases w/ or w/o unlabeled data in the source languages. Source languages more similar to the target language should generally be assigned higher weights to transfer more knowledge.</p><p>Without Any Source-Language Data: It is straightforward to average over all teacher models:</p><formula xml:id="formula_9">Î± k = 1 K , âk â {1, 2, . . . , K}<label>(7)</label></formula><p>With Unlabeled Source-Language Data: As no labeled data is available, existing supervised language/domain similarity learning methods for a target task (i.e., NER) <ref type="bibr" target="#b11">(McClosky et al., 2010)</ref> are not applicable here. Inspired by Pinheiro <ref type="formula" target="#formula_1">(2018)</ref>, we propose to introduce a language identification auxiliary task for calculating similarities between source and target languages, and then weight teacher models based on this metric. In the language identification task, for the kth source language, each unlabeled sentence u <ref type="bibr">(k)</ref> in it is associated with the language index k to build its training dataset, denoted as D (k) src = {(u (k) , k)}. We also assume that in the mdimensional language-independent feature space, sentences from each source language should be clustered around the corresponding language embedding vector. We thus introduce a learnable language embedding vector Âµ (k) â R m for the k-th source language, and then utilize a bilinear operator to measure similarity between a given sentence u and the k-th source language:</p><formula xml:id="formula_10">s(u, Âµ (k) ) = g T (u)M Âµ (k)<label>(8)</label></formula><p>where g(Â·) can be any language-independent model that outputs sentence embeddings, and M â R mÃm denotes the parameters of the bilinear operator. By building a language embedding matrix P â R mÃK with each Âµ (k) column by column, and applying a softmax function over the bilinear operator, we can derive language-specific probability distributions w.r.t u as below.</p><p>q(u, M, P ) = softmax g T (u)M P</p><p>Then the parameters M and P are trained to identify the language of each sentence in {D (k) src } K k=1 , via minimizing the cross-entropy (CE) loss:</p><formula xml:id="formula_12">L(P, M ) = â 1 Z (u (k) ,k)âDsrc CE q(u (k) , M, P ), k + Î³ P P T â I 2 F (10) where D src is the union set of {D (k) src } K k=1 , Z = |D src |, Â· 2</formula><p>F denotes the squared Frobenius norm, and I is an identity matrix. The regularizer in L(P, M ) is to encourage different dimensions of the language embedding vectors to focus on different aspects, with Î³ â¥ 0 being its weighting factor.</p><p>With learned M and P = [Âµ (1) , Âµ (2) , . . . , Âµ (K) ], we compute the weights {Î± k } K i=1 using the unlabeled data in the target language D tgt :</p><formula xml:id="formula_13">Î± k = 1 |D tgt | x âDtgt exp s(x , Âµ (k) )/Ï K i=1 exp s(x , Âµ (i) )/Ï<label>(11)</label></formula><p>where Ï is a temperature factor to smooth the output probability distribution. In our experiments, we set it as the variance of all values in {s(x , Âµ (k) )}, âx â D tgt , âk â {1, ..., K}, so that Î± k would not be too biased to either 0 or 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Teacher-Student Learning</head><p>Training: With the combined probability distribution of entity labels from multiple teacher models, i.e.,p(x i , Î T ) in Eq. 6, the training loss for the student model is identical to Eq. 3 and 4.</p><p>Inference: For inference on the target language, we only use the learned student model and make predictions as in the single-source scenario (Eq. 5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments for 3 target languages (i.e., Spanish, Dutch, and German) on standard benchmark datasets, to validate the effectiveness and reasonableness of our proposed method for single-and multi-source cross lingual NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Datasets We use two NER benchmark datasets: CoNLL-2002 (Spanish and Dutch) <ref type="bibr" target="#b22">(Tjong Kim Sang, 2002)</ref>; CoNLL-2003 (English and German) <ref type="bibr" target="#b23">(Tjong Kim Sang and De Meulder, 2003)</ref>. Both are annotated with 4 entity types: PER, LOC, ORG, and MISC. Each language-specific dataset is split into training, development, and test sets. <ref type="table">Table 1</ref> reports the dataset statistics. All sentences are tokenized into sequences of subwords with WordPiece <ref type="bibr" target="#b29">(Wu et al., 2016)</ref>. Following <ref type="bibr" target="#b28">Wu and Dredze (2019)</ref>, we also use the BIO entity labelling scheme.</p><p>In our experiments, for each source language, an NER model is trained previously with its corresponding labeled training set. As for the target language, we discard the entity labels from its training set, and use it as unlabeled target-language data D tgt . Similarly, unlabeled source-language data for learning language similarities (Eq. 10) is simulated via discarding the entity labels of each training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Configurations</head><p>We leverage the cased multilingual BERT BASE <ref type="bibr" target="#b28">(Wu and Dredze, 2019</ref>) for both f (Â·) in Eq. 1 and g(Â·) in Eq. 8, with 12 Transformer blocks, 768 hidden units, 12 self-attention head, GELU activations <ref type="bibr" target="#b5">(Hendrycks and Gimpel, 2016)</ref>, and learned positional embeddings. We use the final hidden vector of the first [CLS] token as the sentence embedding for g(Â·), and use the mean value of sentence embeddings w.r.t the k-th source language to initialize Âµ (k) in Eq. 8. es nl de <ref type="bibr">) 59.30 58.40 40.40 Tsai et al. (2016</ref><ref type="bibr">) 60.55 61.56 48.12 Ni et al. (2017</ref> 65.10 65.40 58.50 <ref type="bibr" target="#b10">Mayhew et al. (2017)</ref>    <ref type="formula" target="#formula_1">(2019)</ref>, we freeze the parameters of the embedding layer and the bottom three layers of BERT BASE . For the optimizers, we use AdamW <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2017)</ref> with learning rate of 5e â 5 for teacher models <ref type="bibr" target="#b26">(Wolf et al., 2019)</ref>, and 1e â 4 for the student model <ref type="bibr" target="#b31">(Yang et al., 2019)</ref> to converge faster. As for language similarity measuring (i.e., Eq. 10), we set Î³ = 0.01 following Pinheiro (2018). Besides, we use a low-rank approximation for the bilinear operator M , i.e., M = U T V where U, V â R dÃm with d m, and we empirically set d = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Metric</head><p>We use phrase level F1score as the evaluation metric, following <ref type="bibr" target="#b22">Tjong Kim Sang (2002)</ref>. For each experiment, we conduct 5 runs and report the average F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>Single-Source Cross-Lingual NER <ref type="table" target="#tab_2">Table 2</ref> reports the results of different single-source crosslingual NER methods. All results are obtained with English as the source language and others as target languages.</p><p>It can be seen that our proposed method outperforms the previous state-of-the-art methods. Particularly, compared with the remarkable <ref type="bibr" target="#b28">Wu and Dredze (2019)</ref> and <ref type="bibr" target="#b12">Moon et al. (2019)</ref>, which use nearly the same NER model as our method but is based on direct model transfer, our method obtains significant and consistent improvements in es nl de <ref type="bibr" target="#b19">TÃ¤ckstrÃ¶m (2012)</ref> 61.90 59.90 36.40 <ref type="bibr" target="#b17">Rahimi et al. (2019)</ref>   F1-scores, ranging from 0.51 for Dutch to 1.80 for German. That well demonstrates the benefits of teacher-student learning over unlabeled targetlanguage data, compared to direct model transfer.</p><p>Moreover, compared with the latest meta-learning based method <ref type="bibr" target="#b27">(Wu et al., 2020)</ref>, our method requires much lower computational costs for both training and inference, meanwhile reaching superior performance.</p><p>Multi-Source Cross-Lingual NER Here we select source languages in a leave-one-out manner, i.e., all languages except the target one are regarded as source languages. For fair comparisons, we take Spanish, Dutch, and German as target languages, respectively. <ref type="table" target="#tab_4">Table 3</ref> reports the results of different methods for multi-source cross-lingual NER. Both our teacher-student learning methods, i.e., Ours-avg (averaging teacher models, Eq. 7) and Ours-sim (weighting teacher models with learned language similarities, Eq. 11), outperform previous state-ofthe-art methods on Spanish and German by a large margin, which well demonstrates their effectiveness. We attribute the large performance gain to the teacher-student learning process to further leverage helpful information from unlabeled data in the target language. Though <ref type="bibr" target="#b12">Moon et al. (2019)</ref> achieves superior performance on Dutch, it is not applicable in cases where the labeled source-language data is inaccessible, and thus it still suffers from the aforementioned limitation w.r.t. data availability.</p><p>Moreover, compared with Ours-avg, Ours-sim brings consistent performance improvements. That means, if unlabeled data in source languages is available, using our proposed language similarity measuring method for weighting different teacher  <ref type="table">Table 4</ref>: Ablation study of the proposed teacher-student learning method for cross-lingual NER. HL: Hard Label; MT: Direct Model Transfer; *-avg: averaging source-language models; *-sim: weighting sourcelanguage models with learned language similarities. models can be superior to simply averaging them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Analyses on Teacher-Student Learning To validate the reasonableness of our proposed teacherstudent learning method for cross-lingual NER, we introduce the following baselines. 1) Hard Label (HL), which rounds the probability distribution of entity labels (i.e., soft labels output by teacher models) into a one-hot labelling vector (i.e., hard labels) to guide the learning of the student model. Note that in multi-source cases, we use the combined probability distribution of multiple teacher models (Eq. 6) to derive the hard labels. To be consistent with Eq. 3, we still adopt the MSE loss here. In fact, both MSE loss and cross-entropy loss lead to the same observation described in this subsection. 2) Direct Model Transfer (MT), where NO unlabeled target-language data is available to perform teacher-student learning, and thus it degenerates into: a) directly applying the source-language model in single-source cases, or b) directly applying a weighted ensemble of source-language models in multi-source cases, with weights derived via Eq. 6 and Eq. 11. <ref type="table">Table 4</ref> reports the ablation study results. It can be seen that using hard labels (i.e., HL-*) would result in consistent performance drops in all crosslingual NER settings, which validates using soft labels in our proposed teacher-student learning method can convey more information for knowledge transfer than hard labels. Moreover, we can also observe that, using direct model transfer (i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#1 Spanish</head><p>Source-Language Model: ...Etchart <ref type="bibr">1</ref>.00] Sydney <ref type="bibr">0.98]</ref>  <ref type="figure">( Australia</ref>  <ref type="bibr">1</ref>.00] ) , 23 may ( EFE <ref type="bibr">[O, 0.53]</ref> ) . Ours: Por Mario <ref type="bibr">[B-PER]</ref> Etchart <ref type="bibr">[I-PER]</ref> Sydney <ref type="bibr">[B-LOC]</ref> ( Australia <ref type="bibr">[B-LOC]</ref> ) , 23 may ( EFE <ref type="bibr">[B-ORG]</ref>    <ref type="table">Table 5</ref>: Comparison between the proposed language similarity measuring method and the commonly used cosine/ 2 metrics for multi-source cross-lingual NER.</p><p>MT-*) would lead to even more significant performance drops in all cross-lingual NER settings (up to 1.46 F1-score). Both demonstrate that leveraging unlabeled data in the target language can be helpful, and that the proposed teacher-student learning method is capable of leveraging such information effectively for cross-lingual NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analyses on Language Similarity Measuring</head><p>We further compare the proposed language similarity measuring method with other commonly used unsupervised metrics, i.e., cosine similarity and 2 distance. Specifically, s(x , Âµ (k) ) in Eq. 11 is replaced by cosine similarity or negative 2 distance between x and the mean value of sentence embeddings w.r.t the k-th source language.</p><p>As shown in <ref type="table">Table 5</ref>, replacing the proposed language similarity measuring method with either cosine / 2 metrics leads to consistent performance drops across all target languages. This further demonstrates the benefits of our language identification based similarity measuring method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Why Teacher-Student Learning Works?</head><p>By analyzing which failed cases of directly applying the source-language model are corrected by the proposed teacher-student learning method, we try to bring up insights on why teacher-student learning works, in the case of single-source cross-lingual NER. Firstly, teacher-student learning can probably help to learn label preferences for some specific words in the target language. Specifically, if a word appears in the unlabeled target-language data and the teacher model consistently predicts it to be associated with an identical label with high probabilities, the student model would learn the preferred label w.r.t that word, and predict it in cases where the sentence context may not provide enough information. Such label preference can help the predictions for tokens that are less ambiguous and generally associated with an identical entity label. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, in example #1, the source-language (teacher) model, fails to identify "EFE" as an ORG in the test sentences, while the student model (i.e., Ours) can correctly label it, because it has seen "EFE" labeled as ORG by the teacher model with high probabilities in the unlabeled target-language data D tgt . Similar results can also be observed in example #2 and #3.</p><p>Moreover, teacher-student learning may help to find a better classifying hyperplane for the student NER model with unlabelled target-language data. Actually, we notice that the source-language model generally makes correct label predictions with higher probabilities, and makes mispredictions with relatively lower probabilities. By calcu-lating the proportion of its mispredictions that are corrected by our teacher-student learning method in different probability intervals, we find that our method tends to correct the low-confidence mispredictions, as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. We conjecture that, with the help of unlabeled target-language data, our method can probably find a better classifying hyperplane for the student model, so that the low-confidence mispredictions, which are closer to the classifying hyperplane of the source-language model, can be clarified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a teacher-student learning method for single-/multi-source cross-lingual NER, via using source-language models as teachers to train a student model on unlabeled data in the target language. The proposed method does not rely on labelled data in the source languages and is capable of leveraging extra information in the unlabelled target-language data, which addresses the limitations of previous label-projection based and model-transfer based methods. We also propose a language similarity measuring method based on language identification, to better weight different teacher models. Extensive experiments on benchmark datasets show that our method outperforms the existing state-of-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison between previous cross-lingual NER methods (a/b) and the proposed method (c). (a): direct model transfer; (b): label projection with pairwise texts; (c): proposed teacher-student learning method. M src/tgt : learned NER model for source/target language; {X, Y } src : labeled data in source language; {X } tgt : unlabeled data in target language; {X , Y } tgt /{X , P } tgt : pseudo-labeled data in target language with hard labels / soft labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Framework of the proposed teacher-student learning method for multi-source cross-lingual NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Case study on why teacher-student learning works. The GREEN ( RED ) highlight indicates a correct (incorrect) label. The real-valued numbers indicate the predicted probability corresponding to the entity label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Percentage of corrected mispredictions, in different probability intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>65.95 66.50 59.11  Xie et al. (2018)   72.37 71.25 57.76Wu and Dredze (2019)  â  74.50 79.50 71.10   </figDesc><table><row><cell>Moon et al. (2019)  â </cell><cell>75.67 80.38 71.42</cell></row><row><cell>Wu et al. (2020)</cell><cell>76.75 80.44 73.16</cell></row><row><cell>Ours</cell><cell>76.94 80.89 73.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons of single-source cross-lingual NER. â  denotes the reported results w.r.t. freezing the bottom three layers of BERT BASE as in this paper.</figDesc><table><row><cell>Network Training We implement our proposed</cell></row><row><cell>method based on huggingface Transformers 1 . Fol-</cell></row><row><cell>lowing Wolf et al. (2019), we use a batch size of 32,</cell></row><row><cell>and 3 training epochs to ensure convergence of op-</cell></row><row><cell>timization. Following Wu and Dredze</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>71.80 67.60 59.10 Chen et al. (2019) 73.50 72.40 56.00 Moon et al. (2019) â  76.53 83.35 72.44</figDesc><table><row><cell>Ours-avg</cell><cell>77.75 80.70 74.97</cell></row><row><cell>Ours-sim</cell><cell>78.00 81.33 75.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons of multi-source cross-lingual NER. Ours-avg: averaging teacher models (Eq. 7) . Ours-sim: weighting teacher models with learned language similarities (Eq. 11). â  denotes the reported results w.r.t. freezing the bottom three layers of BERT BASE .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>) . Examples in Dtgt: Asi lo anunciÃ³ a EFE 1.00] Hans Gaasbek, el abogado de Murillo, argumentando que ... Vanderpoorten[O, 0.87]  : ' Dit is een eerste stap in de herwaardering van het beroepsonderwijs " Ours: Vanderpoorten[B-PER]  : ' Dit is een eerste stap in de herwaardering van het beroepsonderwijs " Examples in Dtgt: Vanderpoorten [B-PER, 0.99] stond op het punt die reputatie te bezwadderen. ... dabei berÃ¼cksichtigt werden mÃ¼sse , forderte Hof 0.85]  eine " Transparenz " â¦ Ours: Weil die Altersstruktur dabei berÃ¼cksichtigt werden mÃ¼sse , forderte Hof[B-PER]  eine " Transparenz " â¦ Examples in Dtgt: â¦ meint Hof 0.99]  , den der " erstaunliche Pragmatismus der Jugendlichen " beeindruckt .</figDesc><table><row><cell>#2 Dutch Source-Language Model: #3 Source-Language Model:</cell></row><row><cell>German</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multisource cross-lingual model transfer: Learning what to share</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3098" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-resource named entity recognition via multi-source projection: Not quite there yet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vium Enghoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">SÃ¸ren</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AgiÄ</forename><surname>AndÅ¾eljko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</title>
		<meeting>the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual multi-level adversarial transfer to enhance low-resource name tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="3823" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cheap translation for cross-lingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parul</forename><surname>Awasthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01389</idno>
		<title level="m">Towards lingua franca named entity recognition with bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Teacherstudent framework enhanced multi-domain dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuke</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07137</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Massively multilingual transfer for NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nudging the envelope of direct transfer methods for multilingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>the NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-lingual named entity recognition via wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-lingual projected expectation regularization for weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">Transformers: State-of-theart natural language processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhanced meta-learning for cross-lingual named entity recognition with minimal resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>BÃ¶rje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural crosslingual named entity recognition with minimal resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiateng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Model compression with two-stage multi-teacher knowledge distillation for web question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wutao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08381</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from multiple teacher networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1285" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crosslingual transfer of named entity recognizers without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayah</forename><surname>Zirikly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="390" to="396" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
