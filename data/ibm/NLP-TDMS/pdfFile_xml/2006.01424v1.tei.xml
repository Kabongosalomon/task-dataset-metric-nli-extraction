<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>be available at: https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range featurewise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new stateof-the-arts on multiple SISR benchmarks. Our code will be available at: https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super resolution (SISR) aims at recovering a high-resolution (HR) image from its low-resolution (LR) counterpart. SISR has numerous applications in the areas of satellite imaging, medical imaging, surveillance monitoring and high-definition display and imaging etc <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. The mapping between LR and HR image is not bijective, which yields more possibilities for a faithful and high-quality HR recovery. Due to this ill-posed nature, SISR remains challenging in the past decades.</p><p>Early efforts in traditional methods provide good practices for resolving SISR. By fully using the intrinsic property of the LR images, they mostly focus on local prior and non-local prior for patch matching and reconstruction. Specifically, local prior based methods, like bilinear or bicubic interpolation, reconstruct pixels merely by the weighted sum of neighbour ones. To go beyond the local limitation, methods based on non-local mean filtering <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> start to globally search similar patches over the whole LR image.</p><p>The non-local search for self-similarity can be further extended to cross-scale cues. It has been verified that cross-scale patch similarity widely exists in natural images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42]</ref>. Intuitively, in addition to non-local pixel-to-pixel matching, pixels can also be matched with larger image patches. The natural cross-scale feature correspondence makes us search high-frequency details directly from LR images, leading to more faithful, accurate and high-quality reconstructions.</p><p>Since the first deep learning-based method <ref type="bibr" target="#b3">[4]</ref> was proposed, discriminative learning based methods make it possible to use large-scale external image priors for SISR. Compared with traditional methods, they tend to have better feature representation ability, faster inference speed, endto-end trainable paradigm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, and significant performance improvement. To further take the advantages of deep SISR, for several years, efforts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref> have been made on increasing the depth or width of the networks to increase the receptive field or improve the feature representation. However, the essence of the solutions was not changed, but locally finding external similar patches. It yields great limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref> were proposed. They explored non-local selfsimilarity property and embedded the non-local modules into the deep network.</p><p>What should be the next progress for deep SISR? One intuitive idea is following the traditional methods to explore the non-local cross-scale self-similarity in deep networks. Recently, Shocher et al. <ref type="bibr" target="#b24">[25]</ref> proposed a zero-shot superresolution (ZSSR) network to learn the high-frequency details from a pair of down-sampled LR and LR itself using one single test LR image. The essence of ZSSR is an implicit cross-scale patch matching approach using a lightweight network. However, inferring with ZSSR requires additional training time for each new LR image, which is not elegant and efficient enough for practical applications.</p><p>Following the successful path of non-local attention modules, in this paper, we are seeking ways of incorporating cross-scale non-local attention scheme into the deep SR network. Specifically, we propose a novel Cross-Scale Non-Local (CS-NL) attention module, learning to mine long-range dependencies between LR features to largerscale HR patches within the same feature map, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. After that, we integrate the previous local prior, In-Scale Non-Local (IS-NL) prior and the proposed Cross-Scale Non-Local prior into a Self-Exemplars Mining (SEM) module, and fuse them with multi-branch mutualprojection. Finally, we embed the SEM module into a recurrent framework for image super-resolution task.</p><p>In summary, the main contributions of this paper are three-fold:</p><p>â€¢ The core contribution of the paper is to propose the first Cross-Scale Non-Local (CS-NL) attention module in deep networks for SISR task. We explicitly formulate the pixel-to-patch and patch-to-patch similarities inside the image, and demonstrate that additionally mining cross-scale self-similarities greatly improves the SISR performance.</p><p>â€¢ We then propose a powerful Self-Exemplar Mining (SEM) cell to fuse information recurrently. Inside the cell, we exhaustively mine all the possible intrinsic priors by combining local, in-scale non-local, and the proposed cross-scale non-local feature correlations, and embrace rich external statistics learned by the network.</p><p>â€¢ The newly proposed recurrent SR network achieves the state-of-the-art performance on multiple image benchmarks. Extensive ablation experiments further verify the effectiveness of the novel network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Self-Similarity in Image SR The fact that small patches tend to recur within and across scale of a same image has been verified for most natural images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42]</ref>. Since then, a category of self-similarity based approaches has been extensively developed and achieves promising results. Such algorithms utilize the cross-scale information redundancy of a given image as a unique source for reconstruction without relying on any external examples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. In the pioneering work, Glasner et al. <ref type="bibr" target="#b8">[9]</ref> proposed to jointly exploit repeating patches within and across image scales by integrating the idea of multiple image SR and examplebased SR into a unified framework. Furthermore, Freedman et al. <ref type="bibr" target="#b6">[7]</ref> effectively assumed that similar patches exist in an extremely localized region and thus can greatly reduce computation time. Following this fashion, Yang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a very fast regression model that focused on only in-place cross-scale similarity. To handle appearance variations in the scene, Huang et al. <ref type="bibr" target="#b12">[13]</ref> enlarged the internal dictionary by modeling geometric transformations. The idea of internal data repetition has also been applied to solve SR with blur and noisy images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Deep CNNs for Image SR The first work that introduced CNN to solve image SR was proposed by <ref type="bibr" target="#b3">[4]</ref>, where they interpret the three consecutive convolution layers as corresponding extraction, non-linear mapping and reconstruction step in sparse coding. Kim et al. <ref type="bibr" target="#b13">[14]</ref> proposed a very deep model VDSR with more than 16 convolution layers benefiting from effective residual learning. To further unleash the power of deep CNNs, Lim et al. <ref type="bibr" target="#b18">[19]</ref> integrated residual blocks into the SR framework to form a very wide model (EDSR) and a very deep model (MDSR). As the network goes as deep as hundreds of layers, Zhang et al. <ref type="bibr" target="#b38">[39]</ref> utilized densely connected blocks with global feature fusion to effectively exploit hierarchical features from all intermediate layers. Besides extensive efforts spent on designing wider and deeper structures, algorithms with attention modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> were proposed to further enhance representation power of deep CNNs by exploring feature correlations along either spatial or channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Local Attention in Deep Networks</head><p>In recent years, there is an emerging trend of applying non-local attention mechanism to solve various computer vision problems. In general, non-local attention in deep CNNs allows the network to concentrate more on informative areas. Wang et al. <ref type="bibr" target="#b28">[29]</ref> initially proposed non-local neural network to seek semantic relationships for high-level tasks, such as image classification and object detection. On the contrary, nonlocal attention for image restoration is based on non-local similarities prior. Methods, such as NRLN <ref type="bibr" target="#b19">[20]</ref>, RNAN <ref type="bibr" target="#b36">[37]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>, incorporate non-local operation into their networks in order to make better use of image structural cues, by considering long-range feature correlations. As such, they achieved considerable performance gain. However, existing non-local approaches for image restoration only explored feature similarities at the same scale, while ignoring abundant internal LR-HR exemplars across scales, leading to relatively low performance. It is known that the internal HR correspondences contain more relevant high-frequency information and stronger predictive power. To this end, we propose Cross-Scale Non-Local (CS-NL) attention by exploring cross-scale feature correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross-Scale Non-Local (CS-NL) Attention</head><p>In this section, we formulate the proposed cross-scale non-local attention, and compare it with the existing inscale non-local attention.</p><p>In-Scale Non-Local (IS-NL) Attention Non-local attention can explore self-exemplars by summarizing related features from the whole images. Formally, given image feature map X, the non-local attention is defined as</p><formula xml:id="formula_0">Z i,j = g,h exp(Ï†(X i,j , X g,h )) u,v exp(Ï†(X i,j , X u,v )) Ïˆ(X g,h ), (1)</formula><p>where (i, j), (g, h) and (u, v) are pairs of coordinates of X. Ïˆ(Â·) is feature transformation function, and Ï†(Â·, Â·) is correlation function to measure similarity that is defined as</p><formula xml:id="formula_1">Ï†(X i,j , X g,h ) = Î¸(X i,j ) T Î´(X g,h ),<label>(2)</label></formula><p>where Î¸(Â·) and Î´(Â·) are feature transformations. Note that the pixel-wise correlation is measured in the same scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Scale Non-Local (CS-NL) Attention</head><p>The above formulation can be easily extended to a cross-scale version referring to <ref type="figure" target="#fig_1">Figure 2</ref>. Instead of measuring the pixel-wise mutual correlation as the in-scale non-local module, the proposed cross-scale attention is designed to measure the correlation between low-resolution pixels and larger-scale patches in the LR image. To super-resolve the LR image, the Cross-Scale Non-Local (CS-NL) attention directly utilizes the patches matched to each pixel within this LR image.</p><p>Hence, for super-resolution purposes, cross-scale nonlocal attention is built upon in-scale attention by finding candidates in features Y = X â†“ s downsampled by scaling factor s. The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity. Downsampling operation in this paper is bilinear interpolation.</p><p>Suppose the input feature map is X (W Ã—H), to compute pixel-patch similarity, we need to first downsample X to Y ( W s Ã— H s ) and find pixel-wise similarity between X and Y , and finally use corresponding s Ã— s patches in X to superresolve pixels in X, thus the output Z will be sW Ã— sH. Cross-scale attention can be adapted from Eq.1 as</p><formula xml:id="formula_2">Z sÃ—s si,sj = g,h exp(Ï†(X i,j , Y g,h )) u,v exp (Ï†(X i,j , Y u,v )) Ïˆ(X sÃ—s sg,sh ),<label>(3)</label></formula><p>where Z sÃ—s si,sj now is the feature patch of size s Ã— s located at (si, sj). We obtain the weighted-averaged features Z sÃ—s si,sj directly from the feature patches X sÃ—s sg,sh extracted from the input feature maps. Intuitively, with the cross-scale attention, we can mine more faithful and richer high-frequency details from the original intrinsic image resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Based</head><p>Cross-Scale Non-Local Attention Feature-wise affinity measurement can be problematic. First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features. Take the average pooling as an example, an original region representing a HR window and its flipped version have exactly the same high-level features. Therefore, it is likely that many erroneous matches will be synthesized to HR tensors. Besides, adjacent target regions (e.g. Z sÃ—s si,sj and Z sÃ—s s(i+1),s(j) ) are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Scale NL Attention</head><p>In-Scale NL Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual-Projected Fusion</head><p>Local Branch Based on the above analysis, we generalize to empirically implement our Cross-Scale Non-Local (CS-NL) attention using another patch-wise matching. Therefore, Eq.3 is generalized to,</p><formula xml:id="formula_3">! " ! "#$ % " Conv ReLU Stride Conv Deconv Self-Exemplars Mining Cell % " % &amp; % $ Concat SEM ! $ SEM ! " SEM</formula><formula xml:id="formula_4">Z spÃ—sp si,sj = g,h exp Ï†(X pÃ—p i,j , Y pÃ—p g,h ) u,v exp Ï†(X pÃ—p i,j , Y pÃ—p u,v ) Ïˆ(X spÃ—sp sg,sh ),<label>(4)</label></formula><p>and Eq.4 will be identical to Eq.3 if p = 1. The measured correlations are efficiently extended to patch-level, and regions in the output feature map Z are now densely overlapped due to patch-based matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>The proposed network architecture is shown in <ref type="figure" target="#fig_2">Figure  3</ref>. It is basically a recurrent neural network, with each recurrent cell called Self-Exemplars Mining (SEM) fully integrating local, in-scale non-local, and a newly proposed Cross-Scale Non-Local (CS-NL) priors. In this section, we introduce them in a bottom-up manner. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the newly-proposed Cross-Scale Non-Local (CS-NL) attention module embedded into the deep networks. As formulated in section 3, we apply a patch-level cross-scale similarity-matching in the CS-NL attention module. Specifically, suppose we are conducting an s-scale super-resolution with the module, given a feature map X of spatial size (W, H), we first bilinearly downsample it to Y with scale s, and match the p Ã— p patches in X with the downsampled p Ã— p candidates in Y to obtain the softmax matching score. Finally, we conduct deconvolution on the score by weighted adding the patches of size (sp, sp) extracted from X. The obtained Z of size (sW, sH), will be s times super-resolved than X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CS-NL Attention Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Exemplars Mining (SEM) Cell</head><p>Multi-Branch Exemplars Inside the Self-Exemplars Mining (SEM) cell, we exhaustively mine all the possible intrinsic priors, and embrace rich external image priors. Specifically, we mine the image self-similarities and learn the new information using a multi-branch structure, including the conventional Local (L) and In-Scale Non-Local (IS-NL) branches, and also the newly proposed CS-NL branch.</p><p>The local branch, in <ref type="figure" target="#fig_2">Figure 3</ref>, is a simple identical pathway connecting the convolutional features to the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type="bibr" target="#b1">[2]</ref> and a deconvolution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type="bibr" target="#b1">[2]</ref>, we divide the feature maps into region grids, where the inter-dependencies are captured independently in each grid. This reduces the computation burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual-Projected Fusion</head><p>While three-branch structure in SEM generates three feature maps by independently exploiting each information sources from LR images, how to fuse these separate tensors into a comprehensive feature map remains unclear. One possible solution is simply adding or concatenating them together, as widely used in previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. In this paper, we proposed a mutual-projected fusion to progressively combine features together. The algorithm procedure is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>To allow the network to concentrate on more informative features, we first compute the residual R IC between two features from IS-NL F I and CS-NL F C branch, and then after a single convolution layer conv on R IC , the features are added back to F I to obtain F IC .</p><formula xml:id="formula_5">R IC = F I âˆ’ F C ,<label>(5)</label></formula><formula xml:id="formula_6">F IC = conv(R IC ) + F I .<label>(6)</label></formula><p>Intuitively, the residual feature R IC represents the details existing in one source while missing in the other. Such inter-residual projection allows the network to focus on only the distinct information between sources while bypassing the common knowledge, thus improves the discriminative ability of the network.</p><p>Motivated by the traditional Image SR and recent DBPN <ref type="bibr" target="#b10">[11]</ref>, we adopt the back-projection approach to incorporate local information to regularize the feature and correct reconstruction errors. Following <ref type="bibr" target="#b10">[11]</ref>, the final fused feature H is computed by,</p><formula xml:id="formula_7">e = F L âˆ’ downsample(F IC ),<label>(7)</label></formula><formula xml:id="formula_8">H = upsample(e) + F IC ,<label>(8)</label></formula><p>where F L is the feature maps of the Local branch, downsample is a stride convolution to down-sample F IC , and upsample is a stride deconvolution to upscale the feature maps. The mutual-projected operation guarantees a residual learning while fusing different feature sources, enabling a more discriminative feature learning compared with trivial adding or concatenating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recurrent Framework</head><p>The repeated SEM cells are embedded into a recurrent framework, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. At each iteration i, the hidden unit H i of SEM is directly the fused feature map H, and the output unit L i is the computed by H i going through a two-layer CNN. Note that the initial features L 0 are directly computed by the LR image I LR through only two convolutional layers.</p><p>Later on, the extracted deep SR features H i from each iteration i are concatenated together into a wide tensor and mapped to the SR image I SR via one single convolution operation. The network is trained solely with L 1 reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, we use 800 images from DIV2K <ref type="bibr" target="#b27">[28]</ref> dataset to train our models. For testing, we report the performance on five standard benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b33">[34]</ref>, B100 <ref type="bibr" target="#b20">[21]</ref>, Urban100 <ref type="bibr" target="#b12">[13]</ref> and Manga109 <ref type="bibr" target="#b21">[22]</ref>. For evaluation, all the SR results are first transformed into YCbCr space and evaluated by PSNR and SSIM <ref type="bibr" target="#b29">[30]</ref> metrics on Y channel only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We set the recurrence number of SEM as 12 following <ref type="bibr" target="#b19">[20]</ref>. For the Cross-Scale Non-Local (CS-NL) attention in SEM, we set patch size p = 3 and stride s = 1 for dense sampling. We use 3 Ã— 3 as filter size for all convolution layers except for those in attention blocks where the kernel size is 1 Ã— 1. The filter size for stride convolution and deconvolution in SEM are set to be equal at each scale, e.g., 6 Ã— 6, 9 Ã— 9 and 8 Ã— 8 for scale factor 2, 3, 4, respectively. All intermediate features have channel C = 128 except for those embedded features in attention module, where C = 64. The last convolution layer in SEM has 3 convolution filters that transfer a deep SR feature to an RGB image.</p><p>During training, we crop 16 images with patch size 48 Ã— 48 to form a input batch. The training examples are augmented by random rotating 90 â€¢ , 180 â€¢ , 270 â€¢ and horizontal flipping. To optimize our model, we use ADAM optimizer <ref type="bibr" target="#b14">[15]</ref> with Î² 1 = 0.9, Î² 2 = 0.999, and =1e-8. The initial learning rate is set to 1e-4 and reduced to half every 150 epochs. The training stops at 500 epochs. We implement the model using PyTorch, and train it on Nvidia V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons with State-of-the-arts</head><p>To verify the effectiveness of the proposed model, we compare our approach with 11 state-of-the-art methods, which are LapSRN <ref type="bibr" target="#b16">[17]</ref>, SRMDNF <ref type="bibr" target="#b35">[36]</ref>, MemNet <ref type="bibr" target="#b26">[27]</ref>, EDSR <ref type="bibr" target="#b18">[19]</ref>, DBPN <ref type="bibr" target="#b10">[11]</ref>, RDN <ref type="bibr" target="#b38">[39]</ref>, RCAN <ref type="bibr" target="#b36">[37]</ref>, NLRN <ref type="bibr" target="#b19">[20]</ref>, SRFBN <ref type="bibr" target="#b17">[18]</ref>, OISR <ref type="bibr" target="#b11">[12]</ref> and SAN <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_0">Table 1</ref>, We report the quantitative comparisons for scale factor Ã—2, Ã—3 and Ã—4. Compared with other methods, our CS-NL-embedded recurrent model achieved the best performance on multiple benchmarks for almost all scaling factors. It worth noting that our model significantly outperforms NLRN, which is the first proposed in-scale non-local network for image restoration. Our method has better performance when the scaling factor is larger. For Ã—4 settings, our CS-NL embedded model achieves the state-of-the-art PSNR for all the testing benchmarks. In particular, for Urban100 and Manga109 dataset, our model outperforms previous state-of-the-art approaches by 0.4 dB and 0.2 dB, respectively. These datasets contains abundant repeated patterns, such as edges and small corners. Therefore, the superior performance demonstrates the effectiveness of our attention in exploiting internal HR hints. We claim that cross-scale intrinsic priors are indeed effective for a more faithful reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluations In</head><p>Qualitative Evaluations The qualitative evaluations on Urban100 dataset are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. The proposed model is proven to be more effective for images with repeated high-frequency features like windows, lines, squares, etc. For example, in the figure of building, LR image contains plenty of window features covering long-range of spatial-frequency. Directly utilizing those cross-scale self-exemplars from the images will be significantly better than searching for in-scale features or external patches in the training set. For all the shown examples, our method perceptually out-performs other state-of-the-arts by a large margin.  which only needs 20% parameters of RCAN and SAN, but achieves the second best result. Therefore, our CSNLN obtains better parameter efficiency in comparison with other methods, by effectively mining internal HR hints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>Cross-Scale v.s. In-Scale Attention The key difference between our cross-scale non-local attention and the in-scale one is to allow network to benefit from abundant internal HR hints with different scales. To verify it, we visualize its correlation maps on 6 images from Urban100 <ref type="bibr" target="#b12">[13]</ref>, and compare it with in-scale non-local attention. As shown in <ref type="figure">Figure 6</ref>, these images contain extensive recurrences of small patterns both within scale and across scale. It is interesting to point out that once the image contains repeated edges, such redundant recurrences are not limited to where high scale patterns appear, but also can be found in-place or even in the area that pattern tends to slightly shrink. For example, the HR appearance of a small corner can be simply found by properly zooming out. All these recurrences contain valuable high frequency information for reconstruction. As shown in <ref type="figure">Figure 6</ref>, the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeating structures in the images, demonstrating its effectiveness for exploiting internal HR information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Exemplars Mining Module</head><p>To demonstrate the effectiveness of our proposed Self-Exemplars Mining (SEM) module, we construct a baseline model by removing all branches, resulting in a fully convolutional recurrent network (RNN). To keep the total parameters same as other variants, we set 10 convolution layers in the recurrent block. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the basic RNN achieves 33.32 dB on Set14 (Ã—2). Results in first 4 columns demonstrate the effectiveness of individual branch, as each of them brings improvement over the baseline. Furthermore, from last 4 columns, we find that combining these branches achieves the best performance. For example, when cross-scale nonlocal branch is added, the performance is improved from <ref type="figure">Figure 6</ref>. Comparisons of correlation maps of CS-NL attention and IS-NL attention. For each group of three columns, the left one is the input image, the middle one shows the in-scale attention, and the right one depicts the cross-scale attention. one can see that the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeated structures in the images, demonstrating its effectiveness for exploiting internal HR information.   <ref type="bibr" target="#b32">33</ref>.47 dB to 33.64 dB. When both local branch and nonlocal branch are added to the network, the best performance is achieved by further adding cross-scale non-local branch, resulting in a improvement from 33.62 dB to 33.74 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local (L) branch</head><p>These facts indicate that the cross-scale correlations learned by our attention can not be captured by either simple convolution or previous in-scale attention module, demonstrating that our CS-NL attention is of crucial importance for fully exploiting information from LR images.</p><p>Patch-Based Matching v.s. Pixel-Based Matching In practical implementation, we compute patch-wise correlation rather than pixel-wise correlation. Here we investigate the influence of patch size p in CS-NL attention. We compare the patch size of 1 Ã— 1, 3 Ã— 3 and 5 Ã— 5, where 1 Ã— 1 is equivalent to pixel-wise matching. As shown in <ref type="table">Table 4</ref>, the performance peak is at 3 Ã— 3, which is higher than pixel based matching, indicating that a small patch can serve as a better region descriptor. However, when using a larger patch size, the performance is worse than the pixel-based matching. This is mainly because larger patches mean additional constraint on the content when evaluating similarity, and therefore it becomes harder to find well-matched cor-respondences. All these results show that it is necessary to choose a proper patch size for effectively computing correlations in CS-NL attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual-Projected Fusion</head><p>We show the effectiveness of our mutual-projected fusion by comparing it with other commonly used fusion strategies, e.g., feature addition and concatenation. As shown in <ref type="table">Table 5</ref>, it can be found that our projection based fusion obtains the best result. By replacing the addition and concatenation with mutual projection, the performance improves about 0.05 dB and 0.12 dB. These results demonstrate the effectiveness of our fusion module in progressively aggregating information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed the first Cross-Scale Non-Local (CS-NL) attention module for image super-resolution deep networks. With the novel module, we are able to sufficiently discover the widely existing cross-scale feature similarities in natural images. Further integrating it with local and the previous in-scale non-local priors, while embracing the abundant external information learned by the network, our recurrent model achieved state-of-the-art performance for multiple benchmarks. Our experiments suggest that exploring cross-scale long-range dependencies will greatly benefit single image super-resolution (SISR) task, and possibly is also promising for general image restoration task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of most engaged patches captured by our Cross-Scale Non-Local (CS-NL) attention. Cross-scale similarities widely exist in natural images. Multiple high-resolution (HR) patches from the low-resolution (LR) image itself significantly improve target patch super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed Cross-Scale Non-Local (CS-NL) attention module. The bottom green box is for patch-level cross-scale similarity-matching. The upper branch shows extracting the original HR patches in LR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The recurrent architecture with the proposed Self-Exemplars Mining (SEM) cell. Inside SEM, it fuses features learned from a proposed Cross-Scale Non-Local (CS-NL) attention, with others from the In-Scale Non-Local (IS-NL) and the local paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Mutual-projected fusion. Downsample and upsample operations are implemented using stride convolution and stride deconvolution, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison for 4Ã— SR on Urban100 dataset. For all the shown examples, especially the images with repeated edges or structures, our method perceptually out-performs other state-of-the-arts by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on benchmark datasets. Best and second best results are colored with red and blue.</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>PSNR</cell><cell>Set5 SSIM</cell><cell cols="2">Set14 PSNR SSIM</cell><cell cols="2">B100 PSNR SSIM</cell><cell cols="2">Urban100 PSNR SSIM</cell><cell cols="2">Manga109 PSNR SSIM</cell></row><row><cell>LapSRN [17]</cell><cell>Ã—2</cell><cell>37.52</cell><cell>0.9591</cell><cell>33.08</cell><cell>0.9130</cell><cell>31.08</cell><cell>0.8950</cell><cell>30.41</cell><cell>0.9101</cell><cell>37.27</cell><cell>0.9740</cell></row><row><cell>MemNet [27]</cell><cell>Ã—2</cell><cell>37.78</cell><cell>0.9597</cell><cell>33.28</cell><cell>0.9142</cell><cell>32.08</cell><cell>0.8978</cell><cell>31.31</cell><cell>0.9195</cell><cell>37.72</cell><cell>0.9740</cell></row><row><cell>EDSR [19]</cell><cell>Ã—2</cell><cell>38.11</cell><cell>0.9602</cell><cell>33.92</cell><cell>0.9195</cell><cell>32.32</cell><cell>0.9013</cell><cell>32.93</cell><cell>0.9351</cell><cell>39.10</cell><cell>0.9773</cell></row><row><cell>SRMDNF [36]</cell><cell>Ã—2</cell><cell>37.79</cell><cell>0.9601</cell><cell>33.32</cell><cell>0.9159</cell><cell>32.05</cell><cell>0.8985</cell><cell>31.33</cell><cell>0.9204</cell><cell>38.07</cell><cell>0.9761</cell></row><row><cell>DBPN [11]</cell><cell>Ã—2</cell><cell>38.09</cell><cell>0.9600</cell><cell>33.85</cell><cell>0.9190</cell><cell>32.27</cell><cell>0.9000</cell><cell>32.55</cell><cell>0.9324</cell><cell>38.89</cell><cell>0.9775</cell></row><row><cell>RDN [39]</cell><cell>Ã—2</cell><cell>38.24</cell><cell>0.9614</cell><cell>34.01</cell><cell>0.9212</cell><cell>32.34</cell><cell>0.9017</cell><cell>32.89</cell><cell>0.9353</cell><cell>39.18</cell><cell>0.9780</cell></row><row><cell>RCAN [37]</cell><cell>Ã—2</cell><cell>38.27</cell><cell>0.9614</cell><cell>34.12</cell><cell>0.9216</cell><cell>32.41</cell><cell>0.9027</cell><cell>33.34</cell><cell>0.9384</cell><cell>39.44</cell><cell>0.9786</cell></row><row><cell>NLRN [20]</cell><cell>Ã—2</cell><cell>38.00</cell><cell>0.9603</cell><cell>33.46</cell><cell>0.9159</cell><cell>32.19</cell><cell>0.8992</cell><cell>31.81</cell><cell>0.9249</cell><cell>-</cell><cell>-</cell></row><row><cell>SRFBN [18]</cell><cell>Ã—2</cell><cell>38.11</cell><cell>0.9609</cell><cell>33.82</cell><cell>0.9196</cell><cell>32.29</cell><cell>0.9010</cell><cell>32.62</cell><cell>0.9328</cell><cell>39.08</cell><cell>0.9779</cell></row><row><cell>OISR [12]</cell><cell>Ã—2</cell><cell>38.21</cell><cell>0.9612</cell><cell>33.94</cell><cell>0.9206</cell><cell>32.36</cell><cell>0.9019</cell><cell>33.03</cell><cell>0.9365</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [2]</cell><cell>Ã—2</cell><cell>38.31</cell><cell>0.9620</cell><cell>34.07</cell><cell>0.9213</cell><cell>32.42</cell><cell>0.9028</cell><cell>33.10</cell><cell>0.9370</cell><cell>39.32</cell><cell>0.9792</cell></row><row><cell>CSNLN (ours)</cell><cell>Ã—2</cell><cell>38.28</cell><cell>0.9616</cell><cell>34.12</cell><cell>0.9223</cell><cell>32.40</cell><cell>0.9024</cell><cell>33.25</cell><cell>0.9386</cell><cell>39.37</cell><cell>0.9785</cell></row><row><cell>LapSRN [17]</cell><cell>Ã—3</cell><cell>33.82</cell><cell>0.9227</cell><cell>29.87</cell><cell>0.8320</cell><cell>28.82</cell><cell>0.7980</cell><cell>27.07</cell><cell>0.8280</cell><cell>32.21</cell><cell>0.9350</cell></row><row><cell>MemNet [27]</cell><cell>Ã—3</cell><cell>34.09</cell><cell>0.9248</cell><cell>30.00</cell><cell>0.8350</cell><cell>28.96</cell><cell>0.8001</cell><cell>27.56</cell><cell>0.8376</cell><cell>32.51</cell><cell>0.9369</cell></row><row><cell>EDSR [19]</cell><cell>Ã—3</cell><cell>34.65</cell><cell>0.9280</cell><cell>30.52</cell><cell>0.8462</cell><cell>29.25</cell><cell>0.8093</cell><cell>28.80</cell><cell>0.8653</cell><cell>34.17</cell><cell>0.9476</cell></row><row><cell>SRMDNF [36]</cell><cell>Ã—3</cell><cell>34.12</cell><cell>0.9254</cell><cell>30.04</cell><cell>0.8382</cell><cell>28.97</cell><cell>0.8025</cell><cell>27.57</cell><cell>0.8398</cell><cell>33.00</cell><cell>0.9403</cell></row><row><cell>RDN [39]</cell><cell>Ã—3</cell><cell>34.71</cell><cell>0.9296</cell><cell>30.57</cell><cell>0.8468</cell><cell>29.26</cell><cell>0.8093</cell><cell>28.80</cell><cell>0.8653</cell><cell>34.13</cell><cell>0.9484</cell></row><row><cell>RCAN [37]</cell><cell>Ã—3</cell><cell>34.74</cell><cell>0.9299</cell><cell>30.65</cell><cell>0.8482</cell><cell>29.32</cell><cell>0.8111</cell><cell>29.09</cell><cell>0.8702</cell><cell>34.44</cell><cell>0.9499</cell></row><row><cell>NLRN [20]</cell><cell>Ã—3</cell><cell>34.27</cell><cell>0.9266</cell><cell>30.16</cell><cell>0.8374</cell><cell>29.06</cell><cell>0.8026</cell><cell>27.93</cell><cell>0.8453</cell><cell>-</cell><cell>-</cell></row><row><cell>SRFBN [18]</cell><cell>Ã—3</cell><cell>34.70</cell><cell>0.9292</cell><cell>30.51</cell><cell>0.8461</cell><cell>29.24</cell><cell>0.8084</cell><cell>28.73</cell><cell>0.8641</cell><cell>34.18</cell><cell>0.9481</cell></row><row><cell>OISR [12]</cell><cell>Ã—3</cell><cell>34.72</cell><cell>0.9297</cell><cell>30.57</cell><cell>0.8470</cell><cell>29.29</cell><cell>0.8103</cell><cell>28.95</cell><cell>0.8680</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [2]</cell><cell>Ã—3</cell><cell>34.75</cell><cell>0.9300</cell><cell>30.59</cell><cell>0.8476</cell><cell>29.33</cell><cell>0.8112</cell><cell>28.93</cell><cell>0.8671</cell><cell>34.30</cell><cell>0.9494</cell></row><row><cell>CSNLN (ours)</cell><cell>Ã—3</cell><cell>34.74</cell><cell>0.9300</cell><cell>30.66</cell><cell>0.8482</cell><cell>29.33</cell><cell>0.8105</cell><cell>29.13</cell><cell>0.8712</cell><cell>34.45</cell><cell>0.9502</cell></row><row><cell>LapSRN [17]</cell><cell>Ã—4</cell><cell>31.54</cell><cell>0.8850</cell><cell>28.19</cell><cell>0.7720</cell><cell>27.32</cell><cell>0.7270</cell><cell>25.21</cell><cell>0.7560</cell><cell>29.09</cell><cell>0.8900</cell></row><row><cell>MemNet [27]</cell><cell>Ã—4</cell><cell>31.74</cell><cell>0.8893</cell><cell>28.26</cell><cell>0.7723</cell><cell>27.40</cell><cell>0.7281</cell><cell>25.50</cell><cell>0.7630</cell><cell>29.42</cell><cell>0.8942</cell></row><row><cell>EDSR [19]</cell><cell>Ã—4</cell><cell>32.46</cell><cell>0.8968</cell><cell>28.80</cell><cell>0.7876</cell><cell>27.71</cell><cell>0.7420</cell><cell>26.64</cell><cell>0.8033</cell><cell>31.02</cell><cell>0.9148</cell></row><row><cell>SRMDNF [36]</cell><cell>Ã—4</cell><cell>31.96</cell><cell>0.8925</cell><cell>28.35</cell><cell>0.7787</cell><cell>27.49</cell><cell>0.7337</cell><cell>25.68</cell><cell>0.7731</cell><cell>30.09</cell><cell>0.9024</cell></row><row><cell>DBPN [11]</cell><cell>Ã—4</cell><cell>32.47</cell><cell>0.8980</cell><cell>28.82</cell><cell>0.7860</cell><cell>27.72</cell><cell>0.7400</cell><cell>26.38</cell><cell>0.7946</cell><cell>30.91</cell><cell>0.9137</cell></row><row><cell>RDN [39]</cell><cell>Ã—4</cell><cell>32.47</cell><cell>0.8990</cell><cell>28.81</cell><cell>0.7871</cell><cell>27.72</cell><cell>0.7419</cell><cell>26.61</cell><cell>0.8028</cell><cell>31.00</cell><cell>0.9151</cell></row><row><cell>RCAN [37]</cell><cell>Ã—4</cell><cell>32.63</cell><cell>0.9002</cell><cell>28.87</cell><cell>0.7889</cell><cell>27.77</cell><cell>0.7436</cell><cell>26.82</cell><cell>0.8087</cell><cell>31.22</cell><cell>0.9173</cell></row><row><cell>NLRN [20]</cell><cell>Ã—4</cell><cell>31.92</cell><cell>0.8916</cell><cell>28.36</cell><cell>0.7745</cell><cell>27.48</cell><cell>0.7306</cell><cell>25.79</cell><cell>0.7729</cell><cell>-</cell><cell>-</cell></row><row><cell>SRFBN [18]</cell><cell>Ã—4</cell><cell>32.47</cell><cell>0.8983</cell><cell>28.81</cell><cell>0.7868</cell><cell>27.72</cell><cell>0.7409</cell><cell>26.60</cell><cell>0.8015</cell><cell>31.15</cell><cell>0.9160</cell></row><row><cell>OISR [12]</cell><cell>Ã—4</cell><cell>32.53</cell><cell>0.8992</cell><cell>28.86</cell><cell>0.7878</cell><cell>27.75</cell><cell>0.7428</cell><cell>26.79</cell><cell>0.8068</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [2]</cell><cell>Ã—4</cell><cell>32.64</cell><cell>0.9003</cell><cell>28.92</cell><cell>0.7888</cell><cell>27.78</cell><cell>0.7436</cell><cell>26.79</cell><cell>0.8068</cell><cell>31.18</cell><cell>0.9169</cell></row><row><cell>CSNLN (ours)</cell><cell>Ã—4</cell><cell>32.68</cell><cell>0.9004</cell><cell>28.95</cell><cell>0.7888</cell><cell>27.80</cell><cell>0.7439</cell><cell>27.22</cell><cell>0.8168</cell><cell>31.43</cell><cell>0.9201</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Model size and performance comparsion on Set5 (2Ã—) .</figDesc><table><row><cell></cell><cell>EDSR</cell><cell>DBPN</cell><cell>RDN</cell><cell>RCAN</cell><cell>SAN</cell><cell>CSNLN</cell></row><row><cell>Para.</cell><cell>43M</cell><cell>10M</cell><cell>22.3M</cell><cell>16M</cell><cell>15.7M</cell><cell>3M</cell></row><row><cell>PSNR</cell><cell>38.11</cell><cell>38.09</cell><cell>38.24</cell><cell>38.27</cell><cell>38.31</cell><cell>38.28</cell></row><row><cell cols="7">Model Size Analysis We report the model size and per-</cell></row><row><cell cols="7">formance for recently competitive SR methods in Table 2.</cell></row><row><cell cols="7">Comparing with others, our model has the least parameters,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the branch features in SEM. We report the PSNR results on Set14 (2Ã—) after 200 epochs. With an additional CS-NL branch, the performance becomes 33.74dB compared with the one without CS-NL, 33.62dB.</figDesc><table><row><cell cols="2">Attention Patch Size</cell><cell>1Ã—1</cell><cell>3Ã—3</cell><cell>5Ã—5</cell></row><row><cell>PSNR</cell><cell></cell><cell>33.67</cell><cell>33.74</cell><cell>33.61</cell></row><row><cell></cell><cell cols="4">Table 4. Effects of patch size for matching.</cell></row><row><cell>Fusion</cell><cell>addition</cell><cell cols="2">concatenation</cell><cell>Mutual</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Projection</cell></row><row><cell>PSNR</cell><cell>33.69</cell><cell>33.62</cell><cell></cell><cell>33.74</cell></row></table><note>Table 5. Comparison of fusion operators.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is in part supported by IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discrete wavelet transform-based satellite image resolution enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Anbarjafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale-wise convolution for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thouis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egon C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superresolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 12th International Conference on Computer Vision</title>
		<meeting>the IEEE 12th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ode-inspired network design for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feedback network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to superresolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super-resolution using sub-band self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast image superresolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computed tomography super-resolution using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bramler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3944" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide activation for efficient image and video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10082</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Survey of face detection on low-quality images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Image restoration for under-display camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Emerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehoon</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Large</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04857</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Wilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
