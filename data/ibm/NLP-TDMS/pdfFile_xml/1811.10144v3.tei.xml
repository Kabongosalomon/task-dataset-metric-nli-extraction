<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ReLER</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC→Market1501) and 4.4% (Market1501→DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the oneshot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-ID) aims at matching images of a person in one camera with the images of this person from other different cameras. Because of its essential applications in security and surveillance, person re-ID has been drawing lots of attention from both academia and industry. Despite the dramatic performance improvement obtained by the convolutional neural network <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38]</ref>, it is reported that deep re-ID models trained on the source domain </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-similarity Grouping</head><p>Model Updating with Assigned Labels Self-spaces <ref type="figure">Figure 1</ref>. Illustration of proposed Self-similarity Grouping (SSG). We group target images by three cues, whole bodies, upper parts and bottom parts, independently and assign labels according to the corresponding group. The global information of the whole body and the local information of body parts can help us learn a better representation of individuals. may have a significant performance drop on the target domain <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> due to the data-bias existing between source and target datasets. Since it is costly and unfeasible to annotate all images in target dataset, one of the most popular solutions for such issue is unsupervised domain adaptation (UDA).</p><p>Currently, the common UDA has been studied extensively in image classification, object detection, face recognition and semantic segmentation. However, the traditional UDA approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">27]</ref> always have an assumption that the source and target domain share the same set of classes, which does not hold for the person re-ID problem. Notably, in the person re-ID, different datasets have different identities (i.e. classes). Recently, several unsupervised domain adaptation approaches for person re-ID have been proposed and achieve some promising improvements. Some works aim to translate the appearance of images from the source domain to the target domain based on the generative adversarial networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">40]</ref> by preserving the annotation information of the source domain. Besides, the disparities of cameras is another critical factor influencing re-ID performance, and HHL <ref type="bibr" target="#b49">[49]</ref> is proposed to address intra-domain image variations caused by different camera configurations. However, the performances of these UDA approaches are still far behind their fully-supervised counterparts. The main reason is that most previous works focus on increasing the training samples or comparing the similarity or dissimilarity between the source dataset and the target dataset but ignoring the similar natural characteristics existing in the training samples from the target domain.</p><p>In order to address the problem above and discover the similarities among person images in target dataset, we proposed unsupervised Self-similarity Grouping (SSG) to mine the potential similarities from global to local manner. The critical idea of SSG arises from some recent re-ID works based on part matching, where different parts contain different discriminative information of a person. <ref type="figure">Fig 1 illustrates</ref> our proposed SSG approach. In particular, we extract features of all persons in target dataset and group them by three different cues, whole bodies (A), upper parts (B) and lower parts (C) independently. Then, we can obtain three sets of groups: C A , C B , C C . By assigning a pseudo label to each group, we can pair every person with different pseudo labels. For instance, given a person x i , it should be assigned by three pseudo labels, C i A , C i B and C i C . As a result, we can establish a new dataset with pseudo labels, which can be treated as the normally labeled dataset. Since individuals with the same pseudo label should share lots of similarities, we iteratively mine these by finetuning the pre-trained model with the established dataset.</p><p>Upon our SSG, we further present a semi-supervised solution based clustering-guided annotation to approach the performance of the fully-supervised counterpart and efficiently achieve the adaption from the source domain to the target one. It is no more natural for us to think of the simplest semi-supervised solution, i.e. one shot learning. In particular, the traditional one shot learning is based on the setting that only one sample from each category is labeled. However, the traditional one shot setting is not suitable for person re-ID case. Unlike image recognition problem, which is usually based on a closed set assumption, person re-ID problem is actually an open set problem. In other words, we cannot know in advance how many identities are included in a given unlabeled target dataset. Thus, the superior characteristics from traditional one shot setting cannot be directly applied to the re-ID case. To tackle the above-mentioned issue, we innovatively provide a clustering-guided semi-supervised solution. The proposed semi-supervised training strategy is based on the clusteringguided annotations, which samples a single image from each clustering. By doing this, we can significantly avoid choosing the same identity as two different ones. Hence, it allows us to leverage traditional one shot learning method and achieve similar performance.</p><p>We summarize our contributions as follows:</p><p>• We propose the Self-similarity Grouping (SSG) method, which is a simple yet effective unsupervised domain adaptation (UDA) framework for person re-ID, in order to recover the performance of its fully supervised counterpart.</p><p>• We introduce a similarity-guided semi-supervised training strategy for person re-ID and integrate it into the UDA framework, so that we can train unsupervised branch and semi-supervised branch jointly and effectively boost the process of domain adaption.</p><p>• We conduct extensive experiments and ablation study on several popular benchmarks including Market1501 <ref type="bibr" target="#b45">[45]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b46">46]</ref> and MSMT <ref type="bibr" target="#b40">[40]</ref>, to demonstrate the effectiveness of proposed SSG and semi-supervised solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised domain adaptation. Our work is closely related to unsupervised domain adaptation (UDA) where no data in target domain are labeled during training. Some works in this community try to address this problem by reducing the discrepancy between source domain and target domain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b42">42]</ref>. For example, CORAL <ref type="bibr" target="#b34">[34]</ref> learns a linear transformation that aligns the mean and covariance of feature distribution between two domains. And Sun <ref type="bibr" target="#b35">[35]</ref> proposes deep CORAL to extend original approach to deep neural networks with a nonlinear transformation. Some other methods aim to learn a transformation to generate samples that are similar to target domains by adversarial learning approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b21">21]</ref>. Recently, some works solve this problem by mapping the source data and target data to the same feature space for the domain-invariant representations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b37">37]</ref>. For instance, Ganin et al. <ref type="bibr" target="#b15">[16]</ref> propose a gradient reversal layer (GRL) and integrate it into a standard deep neural network for minimizing the classification loss while maximizing domain confusion loss. However, most of existing unsupervised domain adaptation methods are based on an assumption that class labels are the same across domains, while the person identities of different re-ID datasets are entirely different. Hence, the approaches mentioned above cannot be utilized directly for person re-ID task. Self-similarity Grouping</p><formula xml:id="formula_0">Self-space A Self-space B Self-space C Assigned Labels (Self-space B)</formula><p>Assigned Labels (Self-space A)</p><p>Assigned Labels Unsupervised re-ID. Some methods based on handcraft features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24]</ref> can be directly applied for unsupervised person re-ID. However, these methods always have a poor performance on large-scale dataset because they ignore the distribution of samples in the dataset. Benefit from the success of deep learning, some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref> attempt to address unsupervised domain adaptation based on deep learning framework. Deng et al. <ref type="bibr" target="#b7">[8]</ref> aim to translate images from the source domain to the target domain by proposed similarity preserving generative adversarial network(SPGAN). And the translated images are utilized for training re-ID model in a supervised way. In <ref type="bibr" target="#b39">[39]</ref>, a Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) is proposed to learn an attribute-semantic and identity discriminative feature representation space for target domain without using additional labeled data in the target domain. In <ref type="bibr" target="#b49">[49]</ref>, Zhong et al. introduce a Hetero-Homogeneous Learning (HHL) method, which aims to improve the generalization ability of re-ID models on the target set by achieving camera invariance and domain connectedness simultaneously. Although these unsupervised domain adaptation approaches achieve promising progress, the performance is still unsatisfactory compared with the fully supervised approaches.</p><p>Semi-supervised re-ID. Semi-supervised learning aims at learning a task from one or very few training examples <ref type="bibr" target="#b10">[11]</ref>, and there are some works of one shot person re-ID <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b41">41]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, Bak et al. utilize a metric learning approach for a pair of cameras which can be split into texture and color components for one shot image-based re-ID. Wu et al. <ref type="bibr" target="#b41">[41]</ref> propose a progressive sampling method to gradually predict reliable pseudo labels and update deep model for one shot video-based re-ID. However, the previous one shot re-ID works actually does not make sense. As described in the previous section, re-ID problem is an open set problem which means we cannot know how many identities in that dataset, so we cannot achieve the one/few shot setting under this situation. Based on the above analysis, in this paper, we aim to address person re-ID domain adaptation by Self-similarity grouping approach and further improve the performance by clustering-guided semisupervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Problem Definition For unsupervised domain adaptation in person re-ID, we have a labeled source dataset {X S , Y S }, which contains N s person images and each image x i s has a corresponding label y i s , where y i s ∈ {1, 2, ..., P s }. P s is the number of identities in the source dataset. Also, we have another target dataset {X t }, which consists of N t person images. Note that the identity of each image x t in target dataset {X t } is unknown. The goal of UDA person re-ID is to learn great discriminative embeddings of target dataset by only using the supervised information of source dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully Supervised Pre-training</head><p>Many existing UDA approaches are based on a model pre-trained on source dataset, and we follow the similar set-ting in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51</ref>] to obtain the pre-trained model. In particular, we first utilize ResNet50 <ref type="bibr" target="#b19">[19]</ref> pre-trained on Im-ageNet <ref type="bibr" target="#b6">[7]</ref> as backbone network. The last fully connected (FC) layer is discarded and two additional FC layers are added. The first one has 2048 dimensions named as "FC-2048". The output of second FC layer is P s dimensional, where P s is the number of identity in source dataset, named as "FC-#ID". Given each labeled image x s in the source dataset and its ground truth identify y s , we train the baseline model with cross-entropy loss and hard-batch triplet loss <ref type="bibr" target="#b20">[20]</ref>. Specifically, the cross-entropy loss is employed with "FC-#ID" by casting the training process as a classication problem and hard-batch triplet loss is employed with "FC-2048" by treating the training process as a verification problem. We name this model as baseline in this paper. The baseline model achieves good performance with fully labeled data, but always fails when adopted to a new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Self-similarity Grouping</head><p>Although the re-ID performance drops dramatically when directly adopted to another dataset, it is still much better than the performance of directly applying the ResNet50 pre-trained on ImageNet, which is almost zero. From this observation, we believe that the model trained on source dataset still learns some useful representations of a person for the re-ID task. The reason why it performs so severely on target dataset is that the similarities among different person images cannot be discovered correctly. In order to mine these similarities and make use of them for the re-ID task, we propose Self-similarity Grouping(SSG) approach. The overview of proposed SSG approach is shown in <ref type="figure" target="#fig_2">Fig 2.</ref> The motivation of SSG is that we aim to encourage the model to discover the similarities existing in target dataset by selfsimilarity grouping. Then each unlabeled person is assigned with a pseudo label based on grouping results, which can be further used to reconstruct the target dataset and fine-tune the baseline model. Inspired by recent re-ID work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">38]</ref>, we compare the similarities between two persons not only by global information obtaining from the whole body but also by more fine and local information getting from upper and lower parts of a person. By combining the global and local information, we can obtain a more robust and discriminative representation of a person, which is more informative for self-similarity grouping.</p><p>To formulate the porposed SSG algorithm, we first feed each unlabeled images x i t in target dataset into the baseline model trained by configurations described in Sec 3.1 for feature extraction, denote as F i t ∈ IR H×W ×C (blue one in <ref type="figure" target="#fig_2">Fig 2)</ref>. Then, we split F i t into two parts horizontally and each part contains the information of upper body or lower body and denoted as F i t up ∈ IR H 2 × W 2 ×C (green one) and</p><formula xml:id="formula_1">F i t low ∈ IR H 2 × W</formula><p>2 ×C (orange one). Next, we employ the Global Average Pooling (GAP) operation on whole feature map and two sliced feature maps, i.e. F i t , F i t up and F i t low , to obtain three feature vectors f i t , f i t up and f i t low . We repeat above steps on every unlabeled images to generate three sets of feature vectors.</p><formula xml:id="formula_2">       f t = {f 1 t , ..., f k</formula><p>In addition to the feature set in equation <ref type="formula">(1)</ref>, following the setting of the baseline model, we also employ one FC layer after the f i t to get a global embedding vector f i t e , which is 2048-dims and shares the same self label with f i t . Note that this FC layer will also be updated during training.</p><p>Finally, we employ the self-labels as the supervised information to fine-tune the pre-trained model for cross dataset adaptation using triplet loss L triplet , which will be elaborated in Section 3.4. Specifically, given an image, each feature vector and its corresponding self-label are used as two inputs for L triplet . The full objective function of SSG is formulated as following,</p><formula xml:id="formula_3">L ssg =L triple (f t , y t ) + L triple (f t up , y t up ) + L triple (f t low , y t low ) + L triple (f te , y t )<label>(3)</label></formula><p>During training, we follow the above steps to fine-tine the baseline model and employ the SSG for each iteration. During testing, we concatenate f i t , f i t up , f i t low together as the final representation of each image x i t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Clustering-guided Semi-Supervised Training</head><p>Although unsupervised domain adaptation for person re-ID has been studied extensively <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b49">49]</ref>, there is still more than 25% and 15% performance drop in mAP and Rank-1 accuracy comparing to their fully supervised counterparts. In order to narrow the enormous performance gap, we further extend our SGG to a semi-supervised setting.</p><p>Previous semi-supervised person re-ID works are mostly under an one shot setting, which is actually not practical in  And from these labeled images, we employ the step-wise learning approach to exploit the dataset gradually and obtain a more robust model.</p><p>the re-ID case since it is hard to know the number of independent identities in advance for a new dataset (as discussed in Section 2). Hence, we introduce a new semisupervised training strategy based on clustering-guided annotation, which is more practical and useful for real-world applications. As shown in <ref type="figure" target="#fig_3">Fig 3,</ref> we employ the unsupervised clustering algorithm on f t to generate N g groups. Then, we randomly sample a single image from each group to form a very small sub-dataset X g with N g images. Next, we label this small sub-dataset manually and perform labels assignment based on this annotation. Specifically, we extract features of all images in sub-dataset X g and follow the same operation described in Section 3.2 to obtain three feature vector sets f g , f g up and f g low and treat each of them as an identity dictionary. Given an unlabeled image x i t , we find the most similar images from X g by different cues, whole bodies, upper parts and lower parts, and assign x i t with corresponding labels y i tg , y i tg up and y i tg low , which can be formulated as following.</p><formula xml:id="formula_4">             y i tg = arg min k=1:Ng dist{f i t , f k g } y i tg up = arg min k=1:Ng dist{f i t up , f k g up } y i tg low = arg min k=1:Ng dist{f i t low , f k g low }<label>(4)</label></formula><p>Note that we employ the k-reciprocal encoding <ref type="bibr" target="#b47">[47]</ref>, a variation of Jaccard distance between nearest neighbor sets, as the distance metric for similarity measurement.Since each image in X g is from different groups, it's less possible that two different images share the same identity, which allows us to adopt some one shot learning approaches and further improve the performance. In particular, we follow the step-wise learning approach proposed in <ref type="bibr" target="#b41">[41]</ref> to exploit the whole dataset during training stage progressively. Furthermore, since SSG and clustering-guided semisupervised training strategy share the same feature space, we can design a simple yet effective way to train the whole framework jointly and end-to-end, as shown in <ref type="figure" target="#fig_3">Fig 3.</ref> And the superiority of joint training strategy will be illustrated in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>Fully Supervised Training. As describe in Sec 3.1, we utilize both the batch-hard triplet loss proposed in <ref type="bibr" target="#b20">[20]</ref> and the softmax loss jointly. The triplet loss with hard mining is first proposed in <ref type="bibr" target="#b20">[20]</ref> as an improved version of the original semi-hard triplet loss <ref type="bibr" target="#b32">[32]</ref>. We randomly sample P identities and K instances for each mini-batch to meet the requirement of the batch-hard triplet loss. Typically, the loss function is formulated as follows:</p><formula xml:id="formula_5">L triplet = P i=1 K a=1 [α+ hardest positive max p=1...K ||x (i) a − x (i) p || 2 − min n=1...K j=1...P j =i ||x (i) a − x (i) p || 2 hardest negative ] + (5) where x (i) a , x (i) p , x (i)</formula><p>n are features extracted from the anchor, positive and negative samples respectively, and α is the margin hyperparameter. Besides batch-hard triplet loss, we employ softmax cross entropy loss for discriminative learning as well, which can be formulated as follows:</p><formula xml:id="formula_6">L sof tmax = − P i=1 K a=1 log e W T y a,i xa,i C k=1 e W T k xa,i<label>(6)</label></formula><p>where y i,a is the ground truth identity of the sample {a, i}, and C is number of identity. Our loss function for optimization is the combination of softmax loss and batch-hard triplet loss as follows:</p><formula xml:id="formula_7">L baseline = L sof tmax + L triplet<label>(7)</label></formula><p>Unsupervised and Semi-Supervised Training Unsupervised and semi-supervised training share the same loss function, and we just leverage the hard-batch triplet loss for metric learning. Also, each loss function has four components, whole body, upper body, lower body and global embedding, which can be formulated as follows:</p><formula xml:id="formula_8">L ssg =L triple (f t , y t ) + L triple (f t up , y t up ) + L triple (f t low , y t low ) + L triple (f te , y t ) L semi =L triple (f t , y tg ) + L triple (f t up , y tg up ) + L triple (f t low , y tg low ) + L triple (f te , y tg )<label>(8)</label></formula><p>Hence, the objective function used in jointly training strategy is L jointly = L ssg + L semi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed method on three large-scale re-ID datasets, i.e. Market1501 <ref type="bibr" target="#b45">[45]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b46">46]</ref> and MSMT17 <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocol</head><p>Market1501 <ref type="bibr" target="#b45">[45]</ref> contains 32,668 images of 1,501 labeled persons from six camera views. Specifically, 12,936 person images of 751 identities detected by DPM <ref type="bibr" target="#b11">[12]</ref> are used for training. For testing, in total 19,732 person images of 750 identities plus some distractors form the gallery set, and 3,368 manually cropped person regions from 750 identities form the query set.</p><p>DukeMTMC-ReID <ref type="bibr" target="#b46">[46]</ref> is a subset of the DukeMTMC dataset <ref type="bibr" target="#b31">[31]</ref>. It contains 1,812 identities captured by 8 cameras. There are 16,522 training images, 2,228 query images, and 17,661 gallery images, with 1,404 identities appearing in more than two cameras. Also, similar to the Market1501, the rest 408 identities are considered as distractors.</p><p>MSMT17 <ref type="bibr" target="#b40">[40]</ref> is the largest re-ID dataset, which contains 126,441 bounding boxes of 4,101 identities taken by 15 cameras during 4 days. These 15 cameras include 12 outdoor and 3 indoor ones. Faster RCNN <ref type="bibr" target="#b30">[30]</ref> is utilized for pedestrian bounding box detection. To the authors' best knowledge, the MSMT17 is the most challenging re-ID dataset with large-scale images and multiple cameras.</p><p>Evaluation Protocol In our experiment, we use Cumulative Matching Characteristic (CMC) curve and the mean average precision (mAP) to evaluate the performance of re-ID. For Market-1501 and DukeMTMC-ReID, we use the evaluation packages provided by <ref type="bibr" target="#b45">[45]</ref> and <ref type="bibr" target="#b46">[46]</ref>, respectively. Moreover, for simplicity, all results reported in this paper are under the single-query setting, and no post-processing like re-ranking <ref type="bibr" target="#b47">[47]</ref> is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Baseline training As described in Section 3.1, we first train a baseline model on the source dataset by following the training strategy described in <ref type="bibr" target="#b51">[51]</ref>. Specifically, we keep the size of input images and resize them to 256 × 128. For data augmentation, we employ random cropping, flipping and random erasing <ref type="bibr" target="#b48">[48]</ref>. To meet the requirement of hardbatch triplet loss, each mini-batch is sampled with randomly selected P = 16 identities and randomly sampled K = 8 images for each identity from the training set, so that the mini-batch size is 128. And in our experiment, we set the margin parameter to 0.5. During training, we use the Adam <ref type="bibr" target="#b22">[22]</ref> with weight decay 0.0005 to optimize the parameters for 150 epochs. The initial learning rate is set to 3 × 10 −4 and decays to 3 × 10 −5 after 100 epochs.</p><p>Unsupervised and Semi-supervised training. For unsupervised branch and semi-supervised branch, we follow the same data augmentation strategy and triplet loss setting. And we decrease the initial learning rate from 3 × 10 −4 to 6 × 10 −5 and change training epoch from 150 to 70. For fairness, we randomly select a single image from each clustering to annotate and preserve them for all ablation study. Besides, the whole framework is trained for several iterations until the model is stable.</p><p>Our model is implemented on Pytorch <ref type="bibr" target="#b28">[28]</ref> platform and trained with two NVIDIA TITAN X GPUs. All our experiments on different datasets follow the same settings as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Comparison between supervised learning, direct transfer and state-of-the-art unsupervised method. The performance of supervised baseline method and the direct transfer method are specified in <ref type="table">Table 1</ref>. When comparing two methods, we can clearly find that there is a large performance drop when directly adopting source-trained model on target dataset. For instance, the baseline model trained on Market1501 tested on Market1501 achieves 92.5% in rank-1 accuracy and 80.8% in mAP, but it drops to 30.5% and 16.1% when tested on DukeMTMC-reID, where the performance gap is more than 55%. Also, a similar drop can be observed when DukeMTMC-reID is used as the training set and tested on Market1501. Nevertheless, many recently proposed unsupervised domain adaptation methods cannot well address this great performance gap. For example, when transferring from DukeMTMC-reID to Market1501, to our best knowledge, the best UDA approach <ref type="bibr" target="#b7">[8]</ref> only achieved 53.7% and 75.8% on mAP and rank-1 accuracy, which is lower than the fully supervised method by about 20%.</p><p>Effectiveness of Self-similarity Grouping. We perform several ablation studies to prove the effectiveness of proposed SSG as listed in <ref type="table">Table 1</ref>. Specifically, with SSG, we improve the performance by 21.2% and 27.1% in mAP and rank-1 accuracy when the model is transferred from DukeMTMC-reID to Market1501. Similarly, when the model is trained on Market1501 and tested on DukeMTMCreID, the performance gain becomes +37.9% and +42.9% in rank-1 accuracy and mAP, respectively. Moreover, compared with state-of-the-art UDA method, SSG can improve the performance by more than 5% on Market1501 and more than 3% on DukeMTMC-reID. This is because proposed SSG mines the potential similarity from global to fine manner, which can learn a more robust and discriminative model compared with <ref type="bibr" target="#b7">[8]</ref>. Thus, SSG is a simple yet effective unsupervised method for domain adaptation in person re-ID.</p><p>In addition, we compare the performance of SSG with different number of sliced horizontal feature spatial parts. <ref type="table" target="#tab_3">Table 2</ref> suggests that SSG achieves the best results when we only split the feature maps into two parts: the up-  <ref type="table">Table 1</ref>. Comparison of various methods on the target domains. When tested on DukeMTMC-reID, Market-1501 is used as the source and vice versa. "Baseline" denotes using the full identity labels on the corresponding target dataset(See Section 3.1). "Direct Transfer" means directly applying the source-trained model on the target domain."UDA" stands for the state-of-art unsupervised domain adaptation approach. "Baseline+xxx" means using "xxx" domain adaptation method upon baseline model. "SSG" means Self-similarity Grouping in Section 3.2. "SSG + " and "SSG ++ " stand for proposed SSG enhanced by clustering-guided semi-supervised training w/o and w/ joint training strategy described in Section 3.3.   <ref type="table">Table 3</ref>. Comparison of proposed cluster-guided annotation with random sampling annotation on Market1501 dataset and DukeMTMC-reID dataset. SSG * is semi-supervised training with random sampling annotation per and lower body. It infers that the upper and lower body contain the most discriminative information for re-ID. More fragments of features may break this information and yield worse similarity mining and matching. Effectiveness of clustering-guided semi-supervised training. <ref type="table">Table 1</ref> shows the performance of the proposed cluster-guided semi-supervised method (SSG + ). Compared to the unsupervised one (SSG), SSG + outperforms SSG by 4.2% and 1.4% in terms of mAP and rank-1 accuracy when tested on Market1501. In addition, it outperforms the direct transfer method by 35.9% in mAP and 26.8% in rank-1 accuracy. These demonstrate the effectiveness of the clustering-guided semi-supervised training.</p><p>Effectiveness of joint training strategy Instead of finetuning the model obtained by SSG for SSG + , we conduct experiments to jointly train the model parameters using both SSG and SSG + losses, and we denote this jointly trained model as SSG ++ . In <ref type="table">Table 1</ref>, when transferred from DukeMTMC-reID to Market1501, SSG ++ outperforms SSG + by 6.2% and 4.8% in terms of mAP and rank-1 accuracy. The performance of testing on DukeMTMC-reID also boosts by 3.6% and 1.8% in mAP and rank-1 accuracy. It indicates that joint training strategy has its superiority on both two datasets, and training also becomes more efficient.</p><p>Effectiveness of clustering-guided annotation Intuitively, compared to random sampling from the target domain, clustering-guided annotation (i.e. sampling from unsupervised clustered groups and annotating) will increase the identity diversity in the sample set, and enhance the learned feature representation ability with limited supervised information. To validate this intuition, we compare clustering-guided and random sampling annotation on the jointly learned SSG ++ . For fairness, we randomly sampled the same number of images from the whole dataset for the latter. In <ref type="table">Table 3</ref>, SSG ++ surpasses the random sampling one by 2.7% and 1.6% in mAP and rank-1 accuracy when testing on DukeMTMC-reID and the similar improvement on performance can be found when testing on Market1501 as well. It verifies that the proposed clustering-guided annotation is better than the random one.</p><p>In conclusion, SSG ++ with clustering-guided annotation yields the best performance on both Market1501 and DukeMTMC-reID dataset. For instance, we achieve 42.1% and 31.6% improvements in mAP and rank-1 accuracy when testing on Market1501 compared with direct adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparision with State-of-arts</head><p>In this section, we compare the proposed method with state-of-the-art unsupervised learning methods on Mar-ket1501, DukeMTMC-reID and MSMT17 in <ref type="table" target="#tab_5">Table 4</ref>, <ref type="table" target="#tab_7">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref> respectively. SSG outperforms existing approaches with dominantly advantage. In particular, our model outperforms the best published method ARN <ref type="bibr" target="#b23">[23]</ref> by almost 20% on mAP when testing on Market1501 and DukeMTMC-reID dataset. Moreover, it also surpasses the unpublished UDAP <ref type="bibr" target="#b7">[8]</ref> and MAR(CVPR2019) <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Market1501</head><p>On Market-1501, we compare our results with two hand-crafted features, i.e. Bagof-Words (BoW) <ref type="bibr" target="#b45">[45]</ref> and local maximal occurrence (LOMO) <ref type="bibr" target="#b24">[24]</ref>, three unsupervised methods, including UMDL <ref type="bibr" target="#b29">[29]</ref>,PUL <ref type="bibr" target="#b9">[10]</ref> and CAMEL <ref type="bibr" target="#b43">[43]</ref>, and six unsupervised domain adaptation methods, including PTGAN <ref type="bibr" target="#b40">[40]</ref>, SPGAN <ref type="bibr" target="#b7">[8]</ref>, TJ-AIDL <ref type="bibr" target="#b39">[39]</ref>, ARN <ref type="bibr" target="#b23">[23]</ref>, UDAP <ref type="bibr" target="#b33">[33]</ref> and MAR <ref type="bibr" target="#b44">[44]</ref>. The two hand-crafted features are directly applied to the test dataset without any training process, but it is obvious that both features fail to obtain competitive results. While training on target set, unsupervised methods always obtain higher results than hand-crafted features. Comparing with unsupervised domain adaptation methods, our proposed SSG is superior. Only in unsupervised setting, we achieve rank-1 accuracy = 80.0% and mAP = 58.3%, which outperforms the best unsupervised method <ref type="bibr" target="#b33">[33]</ref> by 4.6% and 4.2%. Furthermore, with clustering-guided semisupervised training strategy, we further improve the performance by 10% on mAP and 6% on rank-1 accuracy.</p><p>Results on DukeMTMC-reID The similar improvement can also be observed when we test it on DukeMTMC-reID dataset. Specifically, we achieve mAP = 53.4% and rank-1 accuracy = 73.0% by unsupervised SSG and obtain mAP = 60.3% and rank-1 accuracy= 76.0% under semi-supervised setting. Compared with best unsupervised method, our result is 4.4%/11.3% and 4.6%/7.6% higher on mAP and rank-1 accuracy. Therefore, the superiority of the proposed SSG adaptation approach for person re-ID can be concluded. In addition, we improve the performance by a large margin and recover about 90% performance of the fully supervised method by clustering-guided semi-supervised training strategy.</p><p>Results on MSMT17 In addition, we further evaluate the proposed SSG approach on MSMT17 dataset, which is the largest and most challenging re-ID dataset. We achieve mAP= 13.3% and rank-1 accuracy= 32.2% when   trained DukeMTMC-reID, which improves state-of-the-arts by 10.0% and 20.4%. Also, similar improvement can be observed while trained on Market1501 as well, which further verifies the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed Self-similarity Grouping (SSG), which can mine the potential similarity existing in target dataset automatically by different appearance cues (from global to local) in an unsupervised manner, to tackle the challenging domain adaption in person re-ID. Furthermore, we introduce a clustering-guided semi-supervised approach upon proposed SSG to adopt traditional one shot learning method to person re-ID, which is an open set problem. Extensive experimental results demonstrate that the performance of our approach outperforms the state-of-thearts by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed SSG approach. The CNN model is ResNet50 and pre-trained on source dataset. For each iteration, after feature extraction, we (1) split the feature maps into an upper part and a lower part and employ GAP on the whole, upper and lower feature maps. (2) Then, we group person images with different feature representations(blue, green and orange) and assign each different self-pseudo labels by grouping results(A, B, C). (3) Next, we update the CNN model by minimizing the triplet loss with each pseudo label. (4) During testing, we concatenate three feature representations together as the final representation of query person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Overview of proposed clustering-guided semisupervised training strategy combined with SSG. Given a target dataset, we annotate a few images based on the clustering results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The performance of proposed SSG with different number of splitted parts when trained DukeMTMC-reID dataset and tested on Market1501 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">DukeMTMC-reID→ Market1501 mAP R1 R10</cell></row><row><cell>SSG  *</cell><cell>66.8</cell><cell>84.5</cell><cell>95.3</cell></row><row><cell>SSG ++</cell><cell>68.7</cell><cell>86.2</cell><cell>96.5</cell></row><row><cell>Methods</cell><cell cols="3">Market1501→ DukeMTMC-reID mAP R1 R10</cell></row><row><cell>SSG  *</cell><cell>57.6</cell><cell>74.4</cell><cell>88.1</cell></row><row><cell>SSG ++</cell><cell>60.3</cell><cell>76.0</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of proposed SSG approach with state-ofarts unsupervised domain adaptive person re-ID methods on Mar-ket1501 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparison of proposed SSG approach with stateof-arts unsupervised domain adaptive person re-ID methods on DukeMTMC dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">DukeMTMC-reID→ MSMT17 mAP R1 R10</cell></row><row><cell>PTGAN [40]</cell><cell>3.3</cell><cell>11.8</cell><cell>27.4</cell></row><row><cell>SSG</cell><cell>13.3</cell><cell>32.2</cell><cell>51.2</cell></row><row><cell>SSG ++</cell><cell>18.3</cell><cell>41.6</cell><cell>62.2</cell></row><row><cell>Methods</cell><cell cols="3">Market1501→ MSMT17 mAP R1 R10</cell></row><row><cell>PTGAN [40]</cell><cell>2.9</cell><cell>10.2</cell><cell>24.4</cell></row><row><cell>SSG</cell><cell>13.2</cell><cell>31.6</cell><cell>49.6</cell></row><row><cell>SGG ++</cell><cell>16.6</cell><cell>37.6</cell><cell>57.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison of proposed SSG approach with state-of-arts unsupervised domain adaptive person re-ID methods on MSMT17 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , ..., f Nt t } f t up = {f 1 t up , ..., f k t up , ..., f Nt t up } f t low = {f 1 t low , ..., f k t low , ..., f Nt t low }(1)Based on these feature vectors sets, we utilize unsupervised clustering algorithm<ref type="bibr" target="#b8">[9]</ref> on each set to obtain a series of groups, leading to every person image can be assigned with a pseudo label according to the group it belongs to, named as self-label. As shown inFig 2,we group images according to the three kinds of feature vectors, thus we can get three self-labels for each image x i t , denoted as y i t , y i t up and y i t low . As a result, we can establish a new target dataset where each image has three self labels based on grouping result of three feature vectors, described as following.X T = {x i t : (y i t , y i t up , y i t low ); 1 ≤ i ≤ N t }(2)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work is part supported by IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) and ARC DECRA DE190101315.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Symmetry-driven accumulation of local features for human characterization and re-identification. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Best practices for fine-tuning visual classifiers to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<editor>Kdd</editor>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><forename type="middle">Quang</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sta: Spatial-temporal attention for large-scale videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<imprint>
			<pubPlace>Hugo Larochelle, François Laviolette, Mario</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptation and reidentification network: An unsupervised deep transfer learning approach to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPRW</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3550" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1306" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11334</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09349</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07111</idno>
		<title level="m">Adapting deep visuomotor representations with weak pairwise constraints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crossview asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07717</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
