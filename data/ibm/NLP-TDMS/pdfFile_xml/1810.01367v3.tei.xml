<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
						</author>
						<title level="a" type="main">FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling. * Equal contribution. Order determined by coin toss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>p(z(t 1 )) 0 1 t z p(z(t 0 )) Figure 1: FFJORD transforms a simple base distribution at t 0 into the target distribution at t 1 by integrating over learned continuous dynamics.</p><p>Reversible generative models use cheaply invertible neural networks to transform samples from a fixed base distribution. Examples include NICE <ref type="bibr" target="#b3">(Dinh et al., 2014)</ref>, Real NVP <ref type="bibr" target="#b4">(Dinh et al., 2017)</ref>, and Glow <ref type="bibr" target="#b11">(Kingma &amp; Dhariwal, 2018)</ref>. These models are easy to sample from, and can be trained by maximum likelihood using the change of variables formula. However, this requires placing awkward restrictions on their architectures, such as partitioning dimensions or using rank one weight matrices, in order to avoid an O(D 3 ) cost determinant computation. <ref type="bibr">Recently, Chen et al. (2018)</ref> introduced a continuous-time analog of normalizing flows, defining the mapping from latent variables to data using ordinary differential equations (ODEs). In their model, the likelihood can be computed using relatively cheap trace operations. A more flexible, but still restricted, family of network architectures can be used to avoid this O(D 2 ) time cost.</p><p>Extending this work, we introduce an unbiased stochastic estimator of the likelihood that has O(D) time cost, allowing completely unrestricted architectures. Furthermore, we have implemented GPU-based adaptive ODE solvers to train and evaluate these models on modern hardware. We call our approach Free-form Jacobian of Reversible Dynamics <ref type="bibr">(FFJORD)</ref>.</p><p>In contrast to directly parameterizing a normalized distribution <ref type="bibr" target="#b16">(Oord et al., 2016;</ref><ref type="bibr" target="#b5">Germain et al., 2015)</ref>, the change of variables formula allows one to specify a complex normalized distribution implicitly by warping a normalized base distribution p z (z) through an invertible function f : R D → R D . Given a random variable z ∼ p z (z) the log density of x = f (z) follows</p><formula xml:id="formula_0">log p x (x) = log p z (z) − log det ∂f (z) ∂z<label>(1)</label></formula><p>where ∂f (z) /∂z is the Jacobian of f . In general, computation of the log determinant has a time cost of O(D 3 ). Much work have gone into using restricted neural network architectures to make computing the Jacobian determinant more tractable. These approaches broadly fall into three categories:</p><p>Normalizing flows. By restricting the functional form of f , various determinant identities can be exploited <ref type="bibr" target="#b19">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr">Berg et al., 2018)</ref>. These models cannot be trained directly on data and be able to sample because they do not have a tractable analytic inverse f −1 but have been shown to be useful in representing the approximate posterior for variational inference <ref type="bibr" target="#b12">(Kingma &amp; Welling, 2014)</ref>. Autoregressive transformations. By using an autoregressive model and specifying an ordering in the dimensions, the Jacobian of f is enforced to be lower triangular <ref type="bibr" target="#b13">(Kingma et al., 2016;</ref><ref type="bibr" target="#b15">Oliva et al., 2018)</ref>. These models excel at density estimation for tabular datasets <ref type="bibr" target="#b17">(Papamakarios et al., 2017)</ref>, but require D sequential evaluations of f to invert, which is prohibitive when D is large. Partitioned transformations. Partitioning the dimensions and using affine transformations makes the determinant of the Jacobian cheap to compute, and the inverse f −1 computable with the same cost as f <ref type="bibr" target="#b3">(Dinh et al., 2014;</ref>. This method allows the use of convolutional architectures, excelling at density estimation for image data <ref type="bibr" target="#b4">(Dinh et al., 2017;</ref><ref type="bibr" target="#b11">Kingma &amp; Dhariwal, 2018)</ref>.</p><p>Throughout this work, we refer to reversible generative models those that use the change of variables to transform a base distribution to the model distribution while maintaining both efficient density estimation and efficient sampling capabilities using a single pass of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OTHER GENERATIVE MODELS</head><p>There exist several approaches to generative modeling approaches which don't use the change of variables equation for training. Generative adversarial networks (GANs) <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref> use large, unrestricted neural networks to transform samples from a fixed base distribution. Lacking a closed-form likelihood, an auxiliary discriminator model must be trained to estimate various divergences in order to provide a training signal. Autoregressive models <ref type="bibr" target="#b5">(Germain et al., 2015;</ref><ref type="bibr" target="#b16">Oord et al., 2016)</ref> directly specify the joint distribution p(x) as a sequence of explicit conditional distributions using the product rule. These models require at least O(D) evaluations to sample from.Variational autoencoders (VAEs) Kingma &amp; Welling (2014) use an unrestricted architecture to explicitly specify the conditional likelihood p(x|z), but can only efficiently provide a lower bound on the marginal likelihood p(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTINUOUS NORMALIZING FLOWS</head><p>Chen et al. (2018) define a generative model for data x ∈ R D similar to those based on (1) which replaces the warping function with an integral of continuous-time dynamics. The generative process works by first sampling from a base distribution z 0 ∼ p z0 (z 0 ). Then, given an ODE defined by the parametric function f (z(t), t; θ), we solve the initial value problem z(t 0 ) = z 0 , ∂z(t) /∂t = f (z(t), t; θ) to obtain z(t 1 ) which constitutes our observable data. These models are called Continous Normalizing Flows (CNF). The change in log-density under this model follows a second differential equation, called the instantaneous change of variables formula: (Chen et al., 2018), ∂ log p(z(t)) ∂t = − Tr ∂f ∂z(t)</p><p>.</p><p>(2)</p><p>We can compute total change in log-density by integrating across time:</p><formula xml:id="formula_1">log p(z(t 1 )) = log p(z(t 0 )) − t1 t0</formula><p>Tr ∂f ∂z(t) dt.</p><p>Under review as a conference paper at ICLR 2019  Given a datapoint x, we can compute both the point z 0 which generates x, as well as log p(x) under the model by solving the initial value problem:</p><formula xml:id="formula_3">z 0 log p(x) − log p z0 (z 0 ) solutions = t0 t1 f (z(t), t; θ) − Tr ∂f ∂z(t) dt dynamics , z(t 1 ) log p(x) − log p(z(t 1 )) = x 0 initial values<label>(4)</label></formula><p>which integrates the combined dynamics of z(t) and the log-density of the sample backwards in time from t 1 to t 0 . We can then compute log p(x) using the solution of (4) and adding log p z0 (z 0 ). The existence and uniqueness of (4) require that f and its first derivatives be Lipschitz continuous <ref type="bibr" target="#b9">(Khalil, 2002)</ref>, which can be satisfied in practice using neural networks with smooth Lipschitz activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">BACKPROPAGATING THROUGH ODE SOLUTIONS WITH THE ADJOINT METHOD</head><p>CNFs are trained to maximize (3). This objective involves the solution to an initial value problem with dynamics parameterized by θ. For any scalar loss function which operates on the solution to an initial value problem</p><formula xml:id="formula_4">L(z(t 1 )) = L t1 t0 f (z(t), t; θ)dt<label>(5)</label></formula><p>then <ref type="bibr" target="#b18">Pontryagin (1962)</ref> shows that its derivative takes the form of another initial value problem</p><formula xml:id="formula_5">dL dθ = − t0 t1 ∂L ∂z(t) T ∂f (z(t), t; θ) ∂θ dt.<label>(6)</label></formula><p>The quantity − ∂L /∂z(t) is known as the adjoint state of the ODE. Chen et al. (2018) use a black-box ODE solver to compute z(t 1 ), and then another call to a solver to compute (6) with the initial value ∂L /∂z(t1). This approach is a continuous-time analog to the backpropgation algorithm <ref type="bibr" target="#b20">(Rumelhart et al., 1986;</ref><ref type="bibr" target="#b1">Andersson, 2013)</ref> and can be combined with gradient-based optimization methods to fit the parameters θ.  <ref type="formula" target="#formula_0">(2015)</ref> is a one-layer neural network with only a single hidden unit. In contrast, the instantaneous transformation used in planar continuous normalizing flows (Chen et al., 2018) is a one-layer neural network with many hidden units. In this section, we construct an unbiased estimate of the log-density with O(D) cost, allowing completely unrestricted neural network architectures to be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UNBIASED LINEAR-TIME LOG-DENSITY ESTIMATION</head><p>In general, computing Tr ( ∂f /∂z(t)) exactly costs O(D 2 ), or approximately the same cost as D evaluations of f , since each entry of the diagonal of the Jacobian requires computing a separate derivative of f . However, there are two tricks that can help. First, vector-Jacobian products v T ∂f ∂z can be computed for approximately the same cost as evaluating f , using reverse-mode automatic differentiation. Second, we can get an unbiased estimate of the trace of a matrix by taking a double product of that matrix with a noise vector:</p><formula xml:id="formula_6">Tr(A) = E p( ) [ T A ].<label>(7)</label></formula><p>The above equation holds for any D-by-D matrix A and distribution p( ) over D-dimensional vectors such that E[ ] = 0 and Cov( ) = I. The Monte Carlo estimator derived from <ref type="formula" target="#formula_6">(7)</ref> is known as the Hutchinson's trace estimator <ref type="bibr" target="#b8">(Hutchinson, 1989;</ref><ref type="bibr" target="#b0">Adams et al., 2018)</ref>.</p><p>To keep the dynamics deterministic within each call to the ODE solver, we can use a fixed noise vector for the duration of each solve without introducing bias:</p><formula xml:id="formula_7">log p(z(t 1 )) = log p(z(t 0 )) − t1 t0 Tr ∂f ∂z(t) dt = log p(z(t 0 )) − t1 t0 E p( ) T ∂f ∂z(t) dt = log p(z(t 0 )) − E p( ) t1 t0 T ∂f ∂z(t) dt (8)</formula><p>Typical choices of p( ) are a standard Gaussian or Rademacher distribution <ref type="bibr" target="#b8">(Hutchinson, 1989</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">REDUCING VARIANCE WITH BOTTLENECK CAPACITY</head><p>Often, there exist bottlenecks in the architecture of the dynamics network, i.e. hidden layers whose width H is smaller than the dimensions of the input D. In such cases, we can reduce the variance of Hutchinson's estimator by using the cyclic property of trace. Since the variance of the estimator for Tr(A) grows asymptotic to ||A|| 2 F <ref type="bibr" target="#b8">(Hutchinson, 1989)</ref>, we suspect that having fewer dimensions should help reduce variance. If we view view the dynamics as a composition of two functions f = g • h(z) then we observe </p><p>When f has multiple hidden layers, we choose H to be the smallest dimension. This bottleneck trick can reduce the norm of the matrix which may also help reduce the variance of the trace estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FFJORD: A CONTINUOUS-TIME REVERSIBLE GENERATIVE MODEL</head><p>Our complete method uses the dynamics defined in (2) and the efficient log-likelihood estimator of (8) to produce the first scalable and reversible generative model with an unconstrained Jacobian, leading to the name Free-Form Jacobian of Reversible Dyanamics (FFJORD). Pseudo-code of our method is given in Algorithm 1, and <ref type="table" target="#tab_1">Table 1</ref> summarizes the capabilities of our model compared to previous work.</p><p>Assuming the cost of evaluating f is on the order of O(DH) where D is the dimensionality of the data and H is the size of the largest hidden dimension in f , then the cost of computing the likelihood in models which stack transformations that exploit <ref type="formula" target="#formula_0">(1)</ref>  Algorithm 1 Unbiased stochastic log-density estimation using the FFJORD model</p><formula xml:id="formula_9">Require: dynamics f θ , start time t 0 , stop time t 1 , minibatch of samples x. ← sample unit variance(x.shape) Sample outside of the integral function f aug ([z t , log p t ], t): Augment f with log-density dynamics. f t ← f θ (z(t), t) Evaluate dynamics g ← T ∂f ∂z z(t)</formula><p>Compute vector-Jacobian product with automatic differentiation</p><formula xml:id="formula_10">Tr = matrix multiply(g, ) Unbiased estimate of Tr( ∂f ∂z ) with T ∂f ∂z return [f t , − Tr] Concatenate dynamics of state and log-density end function [z, ∆ logp ] ← odeint(f aug , [x, 0], t 0 , t 1 ) Solve the ODE, ie. t1 t0 f aug ([z(t), log p(z(t))], t) dt logp(x) ← log p z0 (z) -∆ logp Add change in log-density return logp(x) 4 EXPERIMENTS Data</formula><p>Glow FFJORD <ref type="figure">Figure 2</ref>: Comparison of trained FFJORD and Glow models on 2-dimensional distributions including multi-modal and discontinuous densities.</p><p>We demonstrate the power of FFJORD on a variety of density estimation tasks as well as approximate inference within variational autoencoders <ref type="bibr" target="#b12">(Kingma &amp; Welling, 2014)</ref>. Experiments were conducted using a suite of GPUbased ODE-solvers and an implementation of the adjoint method for backpropagation 1 . In all experiments the Runge-Kutta 4(5) algorithm with the tableau from Shampine (1986) was used to solve the ODEs. We ensure tolerance is set low enough so numerical error is negligible; see Appendix C.</p><p>We used Hutchinson's trace estimator <ref type="formula" target="#formula_6">(7)</ref> during training and the exact trace when reporting test results. This was done in all experiments except for our density estimation models trained on MNIST and CIFAR10 where computing the exact Jacobian trace was not computationally feasible. There, we observed that the variance of the log-likelihood over the validation set induced by the trace estimator is less than 10 −4 .</p><p>The dynamics of FFJORD are defined by a neural network f which takes as input the current state z(t) ∈ R D and the current time t ∈ R. We experimented with several ways to incorporate t as an input to f , such as hyper-networks, but found that simply concatenating t on to z(t) at the input to every layer worked well and was used in all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DENSITY ESTIMATION ON TOY 2D DATA</head><p>We first train on 2 dimensional data to visualize the model and the learned dynamics. 2 In <ref type="figure">Figure 2</ref>, we show that by warping a simple isotropic Gaussian, FFJORD can fit both multi-modal and even discontinuous distributions. The number of evaluations of the ODE solver is roughly 70-100 on all datasets, so we compare against a Glow model with 100 discrete layers.</p><p>The learned distributions of both FFJORD and Glow can be seen in <ref type="figure">Figure 2</ref>. Interestingly, we find that Glow learns to stretch the single mode base distribution into multiple modes but has trouble modeling the areas of low probability between disconnected regions. In contrast, FFJORD is capable Samples Data  of modeling disconnected modes and can also learn convincing approximations of discontinuous density functions (middle row in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DENSITY ESTIMATION ON REAL DATA</head><p>We perform density estimation on five tabular datasets preprocessed as in <ref type="bibr" target="#b17">Papamakarios et al. (2017)</ref> and two image datasets; MNIST and CIFAR10. On the tabular datasets, FFJORD performs the best out of reversible models by a wide margin but is outperformed by recent autoregressive models. Of those, FFJORD outperforms MAF <ref type="formula">(</ref> On MNIST we find that FFJORD can model the data as well as Glow and Real NVP by integrating a single flow defined by one neural network. This is in contrast to Glow and Real NVP which must compose many flows together to achieve similar performance. When we use multiple flows in a multiscale architecture (like those used by Glow and Real NVP) we obtain better performance on MNIST and comparable performance to Glow on CIFAR10. Notably, FFJORD is able to achieve this performance while using less than 2% as many parameters as Glow. We also note that Glow uses a learned base distribution whereas FFJORD and Real NVP use a fixed Gaussian. A summary of our results on density estimation can be found in <ref type="table" target="#tab_3">Table 2</ref> and samples can be seen in <ref type="figure">Figure 3</ref>. Full details on architectures used, our experimental procedure, and additional samples can be found in Appendix B.1.</p><p>In general, our approach is slower than competing methods, but we find the memory-efficiency of the adjoint method allows us to use much larger batch sizes than those methods. On the tabular datasets we used a batch sizes up to 10,000 and on the image datasets we used a batch size of 900.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VARIATIONAL AUTOENCODER</head><p>We compare FFJORD to other normalizing flows for use in variational inference.  In VAEs it is common for the encoder network to also output the parameters of the flow as a function of the input x. With FFJORD, we found this led to differential equations which were too difficult to integrate numerically. Instead, the encoder network outputs a low-rank update to a global weight matrix and an input-dependent bias vector. Neural network layers inside of FFJORD take the form</p><formula xml:id="formula_11">layer(h; x, W, b) = σ     W Dout×Din +Û (x) Dout×kV (x) Din×k T   h + b Dout×1 +b(x) Dout×1   (10)</formula><p>where h is the input to the layer, σ is an element-wise activation function, D in and D out are the input and output dimensionality of this layer, andÛ (x),V (x),b(x) are data-dependent parameters returned from the encoder networks. A full description of the model architectures used and our experimental setup can be found in Appendix B.2.</p><p>On every dataset tested, FFJORD outperforms all other competing normalizing flows. A summary of our variational inference results can be found in <ref type="table" target="#tab_5">Table 3</ref>.  We perform a series of ablation experiments to gain a better understanding of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FASTER TRAINING WITH BOTTLENECK TRICK</head><p>We plot the training losses on MNIST using an encoderdecoder architecture (see Appendix B.1 for details). Loss during training is plotted in <ref type="figure" target="#fig_4">Figure 4</ref>, where we use the trace estimator directly on the D × D Jacobian or we use the bottleneck trick to reduce the dimension to H × H. Interestingly, we find that while the bottleneck trick (9) can lead to faster convergence when the trace is estimated using a Gaussian-distributed , we did not observe faster convergence when using a Rademacher-distributed . The full computational cost of integrating the instantaneous change of variables (2) is O(DH L) where D is dimensionality of the data, H is the size of the hidden state, and L is the number of function evaluations (NFE) that the adaptive solver uses to integrate the ODE. In general, each evaluation of the model is O(DH) and in practice, H is typically chosen to be close to D. Since the general form of the discrete change of variables equation <ref type="formula" target="#formula_0">(1)</ref> requires O(D 3 )-cost, one may wonder whether the number of evaluations L depends on D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NUMBER</head><p>We train VAEs using FFJORD flows with increasing latent dimension D. The NFE throughout training is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. In all models, we find that the NFE increases throughout training, but converges to the same value, independent of D. This phenomenon can be verified with a simple thought experiment. Take an R D isotropic Gaussian distribution as the data distribution and set the base distribution of our model to be an isotropic Gaussian. Then the optimal differential equation is zero for any D, and the number evaluations is zero. We can conclude that the number of evaluations is not dependent on the dimensionality of the data but the complexity of its distribution, or more specifically, how difficult it is to transform its density into the base distribution.  <ref type="figure">Figure 6</ref>: For image data, multiscale architectures require the ODE solver to use a greater number of function evaluations (NFE), but these models achieve better performance.</p><p>Crucial to the scalability of Real NVP and Glow is the multiscale architecture originally proposed in <ref type="bibr" target="#b4">Dinh et al. (2017)</ref>. We compare an single-scale encoder-decoder style FFJORD with a multiscale FFJORD on the MNIST dataset where both models have a comparable number of parameters and plot the total NFE-in both forward and backward passes-against the loss achieved in <ref type="figure">Figure 6</ref>. We find that while the single-scale model uses approximately one half as many function evaluations as the multiscale model, it is not able to achieve the same performance as the multiscale model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SCOPE AND LIMITATIONS</head><p>Number of function evaluations can be prohibitive. The number of function evaluations required to integrate the dynamics is not fixed ahead of time and is a function of the data, model architecture, and model parameters. We find that this tends to grow as the models trains and can become prohibitively large, even when memory stays constant due to the adjoint method. Various forms of regularization such as weight decay and spectral normalization <ref type="bibr" target="#b14">(Miyato et al., 2018)</ref> can be used to reduce the this quantity but their use tends to hurt performance slightly.</p><p>Limitations of general-purpose ODE solvers. In theory, we can model any differential equation (given mild assumptions based on existence and uniqueness of the solution), but in practice our reliance on general-purpose ODE solvers restricts us to non-stiff differential equations that can be efficiently solved. ODE solvers for stiff dynamics exist, but they evaluate f many more times to achieve the same error. We find that using a small amount of weight decay sufficiently constrains the ODE to be non-stiff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have presented FFJORD, a reversible generative model for high dimensional data which can compute exact log-likelihoods and can be sampled from efficiently. Our model uses continuous-time dynamics to produce a generative model which is parameterized by an unrestricted neural network. All required quantities for training and sampling can be computed using automatic-differentiation, Hutchinson's trace estimator, and black-box ODE solvers. Our model stands in contrast to other methods with similar properties which rely on restricted, hand-engineered neural network architectures. We have demonstrated that this additional flexibility allows our approach to achieve improved performance on density estimation and variational inference. We also demonstrate FFJORD's ability to model distributions which comparable methods such as Glow and Real NVP cannot model.</p><p>We believe there is much room for further work exploring and improving this method. We are interested specifically in ways to reduce the number of function evaluations used by the ODE-solver without hurting predictive performance. Advancements like these will be crucial in scaling this method to even higher-dimensional datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>We thank Roger Grosse and Yulia Rubanova for helpful discussions. On the tabular datasets we performed a grid-search over network architectures. We searched over models with 1, 2, 5, or 10 flows with 1, 2, 3, or 4 hidden layers per flow. Since each dataset has a different number of dimensions, we searched over hidden dimensions equal to 5, 10, or 20 times the data dimension (hidden dimension multiplier in <ref type="table" target="#tab_9">Table 4</ref>). We tried both the tanh and softplus nonlinearities. The best performing models can be found in the <ref type="table" target="#tab_9">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Samples</head><p>On the image datasets we experimented with two different model architectures; a single flow with an encoder-decoder style architecture and a multiscale architecture composed of multiple flows.</p><p>While they were able to fit MNIST and obtain competitive performance, the encoder-decoder architectures were unable to fit more complicated image datasets such as CIFAR10 and Street View House Numbers. The architecture for MNIST which obtained the results in <ref type="table" target="#tab_3">Table 2</ref> was composed of four convolutional layers with 64 → 64 → 128 → 128 filters and down-sampling with strided convolutions by two every other layer. There are then four transpose-convolutional layers who's filters mirror the first four layers and up-sample by two every other layer. The softplus activation function is used in every layer.</p><p>The multiscale architectures were inspired by those presented in <ref type="bibr" target="#b4">Dinh et al. (2017)</ref>. We compose multiple flows together interspersed with "squeeze" operations which down-sample the spatial resolution of the images and increase the number of channels. These operations are stacked into a "scale block" which contains N flows, a squeeze, then N flows. For MNIST we use 3 scale blocks and for CIFAR10 we use 4 scale blocks and let N = 2 for both datasets. Each flow is defined by 3 convolutional layers with 64 filters and a kernel size of 3. The softplus nonlinearity is used in all layers.</p><p>Both models were trained with the Adam optimizer <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref>. We trained for 500 epochs with a learning rate of .001 which was decayed to .0001 after 250 epochs. Training took place on six GPUs and completed after approximately five days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 VARIATIONAL AUTOENCODER</head><p>Our experimental procedure exactly mirrors that of <ref type="bibr">Berg et al. (2018)</ref>. We use the same 7-layer encoder and decoder, learning rate (.001), optimizer (Adam Kingma &amp; Ba <ref type="formula" target="#formula_0">(2014)</ref>), batch size <ref type="formula" target="#formula_0">(100)</ref>, and early stopping procedure (stop after 100 epochs of no validaiton improvment). The only difference was in the nomralizing flow used in the approximate posterior.</p><p>We performed a grid-search over neural network architectures for the dynamics of FFJORD. We searched over networks with 1 and 2 hidden layers and hidden dimension 512, 1024, and 2048. We used flows with 1, 2, or 5 steps and wight matrix updates of rank 1, 20, and 64. We use the softplus activation function for all datasets except for Caltech Silhouettes where we used tanh. The best performing models can be found in the    APPENDIX C NUMERICAL ERROR FROM THE ODE SOLVER ODE solvers are numerical integration methods so there is error inherent in their outputs. Adaptive solvers (like those used in all of our experiments) attempt to predict the errors that they accrue and modify their step-size to reduce their error below a user set tolerance. It is important to be aware of this error when we use these solvers for density estimation as the solver outputs the density that we report and compare with other methods. When tolerance is too low, we run into machine precision errors. Similarly when tolerance is too high, errors are large, our training objective becomes biased and we can run into divergent training dynamics.</p><p>Since a valid probability density function integrates to one, we take a model trained on <ref type="figure">Figure 1</ref> and numerically find the area under the curve using Riemann sum and a very fine grid. We do this for a range of tolerance values and show the resulting error in <ref type="figure" target="#fig_7">Figure 8</ref>. We set both atol and rtol to the same tolerance. The numerical error follows the same order as the tolerance, as expected. During training, we find that the error becomes non-negligible when using tolerance values higher than 10 −5 . For most of our experiments, we set tolerance to 10 −5 as that gives reasonable performance while requiring few number of evaluations. For the tabular experiments, we use atol=10 −8 and rtol=10 −6 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>SCALABLE DENSITY EVALUATION WITH UNRESTRICTED ARCHITECTURES Switching from discrete-time dynamics to continuous-time dynamics reduces the primary computational bottleneck of normalizing flows from O(D 3 ) to O(D 2 ), at the cost of introducing a numerical ODE solver. This allows the use of more expressive architectures. For example, each layer of the original normalizing flows model of Rezende &amp; Mohamed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is O((DH +D 3 )L) where L is the number of transformations used. For CNF, this reduces to O((DH + D 2 )L) for CNFs, whereL is the number of evaluations of f used by the ODE solver. With FFJORD, this reduces further to O((DH + D)L).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b17">Papamakarios et al., 2017)</ref> on all but one dataset and manages to outperform TAN Oliva et al. (2018) on the MINIBOONE dataset. These models require O(D) sequential computations to sample from while the best performing method, MAF-DDSF<ref type="bibr" target="#b7">(Huang et al., 2018)</ref>, cannot be sampled from analytically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The variance of our model's log-density estimator can be reduced using neural network architectures with a bottleneck layer, speeding up training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Number of function evaluates used by the adaptive ODE solver (NFE) is approximately independent of datadimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>DataFigure 7 :</head><label>7</label><figDesc>Samples and data from our image models. MNIST on left, CIFAR10 on right. APPENDIX A QUALITATIVE SAMPLES Samples from our FFJORD models trained on MNIST and CIFAR10 can be found in Figure 7. APPENDIX B EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS B.1 DENSITY ESTIMATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Numerical integration shows that the density under the model does integrate to one given sufficiently low tolerance. Both log and non-log plots are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A comparison of the abilities of generative modeling approaches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 3: Samples and data from our image models. MNIST on left, CIFAR10 on right.</figDesc><table><row><cell></cell><cell cols="5">POWER GAS HEPMASS MINIBOONE BSDS300</cell><cell>MNIST</cell><cell>CIFAR10</cell></row><row><cell>Real NVP</cell><cell>-0.17</cell><cell>-8.33</cell><cell>18.71</cell><cell>13.55</cell><cell>-153.28</cell><cell>1.06*</cell><cell>3.49*</cell></row><row><cell>Glow</cell><cell>-0.17</cell><cell>-8.15</cell><cell>18.92</cell><cell>11.35</cell><cell>-155.07</cell><cell>1.05*</cell><cell>3.35*</cell></row><row><cell>FFJORD</cell><cell>-0.46</cell><cell>-8.59</cell><cell>14.92</cell><cell>10.43</cell><cell cols="2">-157.40 0.99* (1.05  † )</cell><cell>3.40*</cell></row><row><cell>MADE</cell><cell>3.08</cell><cell>-3.56</cell><cell>20.98</cell><cell>15.59</cell><cell>-148.85</cell><cell>2.04</cell><cell>5.67</cell></row><row><cell>MAF</cell><cell cols="2">-0.24 -10.08</cell><cell>17.70</cell><cell>11.75</cell><cell>-155.69</cell><cell>1.89</cell><cell>4.31</cell></row><row><cell>TAN</cell><cell cols="2">-0.48 -11.19</cell><cell>15.12</cell><cell>11.01</cell><cell>-157.03</cell><cell>-</cell><cell>-</cell></row><row><cell>MAF-DDSF</cell><cell cols="2">-0.62 -11.96</cell><cell>15.09</cell><cell>8.86</cell><cell>-157.73</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Negative log-likehood on test data for density estimation models; lower is better. In nats for tabular data and bits/dim for MNIST and CIFAR10. *Results use multi-scale convolutional architectures.† Results use a single flow with a convolutional encoder-decoder architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>± .06 104.28 ± .39 4.53 ± .02 110.80 ± .46 Planar 86.06 ± .31 102.65 ± .42 4.40 ± .06 109.66 ± .42 IAF 84.20 ± .17 102.41 ± .04 4.47 ± .05 111.58 ± .38 Sylvester 83.32 ± .06 99.00 ± .04 4.45 ± .04 104.62 ± .29 FFJORD 82.82 ± .01 98.33 ± .09 4.39 ± .01 104.03 ± .43</figDesc><table><row><cell></cell><cell>MNIST</cell><cell>Omniglot</cell><cell>Frey Faces Caltech Silhouettes</cell></row><row><cell>No Flow</cell><cell>86.55</cell><cell></cell></row></table><note>We train a VAE (Kingma &amp; Welling, 2014) on four datasets using a FFJORD flow and compare to VAEs with no flow, Planar Flows (Rezende &amp; Mohamed, 2015), Inverse Autoregressive Flow (IAF) (Kingma et al., 2016), and Sylvester normalizing flows (Berg et al., 2018). To provide a fair comparison, our encoder/decoder architectures and learning setup exactly mirror those of Berg et al. (2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Negative ELBO on test data for VAE models; lower is better. In nats for all datasets except Frey Faces which is presented in bits per dimension. Mean/stdev are estimated over 3 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Models were trained on a single GPU and training took between four hours and three days depending on the dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="5">nonlinearity # layers hidden dim multiplier # flow steps batchsize</cell></row><row><cell>POWER</cell><cell>tanh</cell><cell>3</cell><cell>10</cell><cell>5</cell><cell>10000</cell></row><row><cell>GAS</cell><cell>tanh</cell><cell>3</cell><cell>20</cell><cell>5</cell><cell>1000</cell></row><row><cell>HEPMASS</cell><cell>softplus</cell><cell>2</cell><cell>10</cell><cell>10</cell><cell>10000</cell></row><row><cell>MINIBOONE</cell><cell>softplus</cell><cell>2</cell><cell>20</cell><cell>1</cell><cell>1000</cell></row><row><cell>BSDS300</cell><cell>softplus</cell><cell>3</cell><cell>20</cell><cell>2</cell><cell>10000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Best performing model architectures for density estimation on tabular data with FFJORD.</figDesc><table><row><cell>Dataset</cell><cell cols="5">nonlinearity # layers hidden dimension # flow steps rank</cell></row><row><cell>MNIST</cell><cell>softplus</cell><cell>2</cell><cell>1024</cell><cell>2</cell><cell>64</cell></row><row><cell>Omniglot</cell><cell>softplus</cell><cell>2</cell><cell>512</cell><cell>5</cell><cell>20</cell></row><row><cell>Frey Faces</cell><cell>softplus</cell><cell>2</cell><cell>512</cell><cell>2</cell><cell>20</cell></row><row><cell>Caltech</cell><cell>tanh</cell><cell>1</cell><cell>2048</cell><cell>1</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Best performing model architectures for VAEs with FFJORD. B.3 STANDARD DEVIATIONS FOR TABULAR DENSITY ESTIMATION .17 ± 0.01 -8.33 ± 0.14 18.71 ± 0.02 13.55 ± 0.49 -153.28 ± 1.78 Glow -0.17 ± 0.01 -8.15 ± 0.40 18.92 ± 0.08 11.35 ± 0.07 -155.07 ± 0.03 FFJORD -0.46 ± 0.01 -8.59 ± 0.12 14.92 ± 0.08 10.43 ± 0.04 -157.40 ± 0.19 MADE 3.08 ± 0.03 -3.56 ± 0.04 20.98 ± 0.02 15.59 ± 0.50 -148.85 ± 0.28 MAF -0.24 ± 0.01 -10.08 ± 0.02 17.70 ± 0.02 11.75 ± 0.44 -155.69 ± 0.28 TAN -0.48 ± 0.01 -11.19 ± 0.02 15.12 ± 0.02 11.01 ± 0.48 -157.03 ± 0.07 MAF-DDSF -0.62 ± 0.01 -11.96 ± 0.33 15.09 ± 0.40 8.86 ± 0.15 -157.73 ± 0.04</figDesc><table><row><cell></cell><cell>POWER</cell><cell>GAS</cell><cell>HEPMASS MINIBOONE</cell><cell>BSDS300</cell></row><row><cell>Real NVP</cell><cell>-0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Negative log-likehood on test data for density estimation models. Means/stdev over 3 runs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We plan on releasing the full code, including our GPU-based implementation of ODE solvers and the adjoint method, upon publication.2 Videos of the learned dynamics can be found at https://imgur.com/a/Rtr3Mbq.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating the Spectral Density of Large Implicit Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saunderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general-purpose software framework for dynamic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Andersson ; Leonard Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
	</analytic>
	<monogr>
		<title level="m">Sylvester normalizing flows for variational inference</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rianne van den Berg</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Neural autoregressive flows. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Hutchinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1059" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nonlinear Systems. Pearson Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Khalil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral normalization for generative adversarial networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<title level="m">Transformation autoregressive networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mathematical theory of optimal processes. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Semenovich Pontryagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some practical Runge-Kutta formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shampine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">173</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
