<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RepPoints v2: Verification Meets Regression for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RepPoints v2: Verification Meets Regression for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Verification and regression are two general methodologies for prediction in neural networks. Each has its own strengths: verification can be easier to infer accurately, and regression is more efficient and applicable to continuous target variables. Hence, it is often beneficial to carefully combine them to take advantage of their benefits. In this paper, we take this philosophy to improve state-of-the-art object detection, specifically by RepPoints. Though RepPoints provides high performance, we find that its heavy reliance on regression for object localization leaves room for improvement. We introduce verification tasks into the localization prediction of RepPoints, producing RepPoints v2, which provides consistent improvements of about 2.0 mAP over the original RepPoints on the COCO object detection benchmark using different backbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO test-dev by a single model. Moreover, we show that the proposed approach can more generally elevate other object detection frameworks as well as applications such as instance segmentation. The code is available at https://github.com/Scalsol/RepPointsV2. * This work is done when Yihong Chen is an intern at Microsoft Research Asia.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Two common methodologies for neural network prediction are verification and regression. While either can drive network features to fit the final task targets, they each have different strengths. For the object localization problem, verification can be easier to infer because each feature is spatially aligned with the target to be verified. On the other hand, regression is often more efficient and it can also predict continuous target variables that enable subtle localization refinement.</p><p>To take advantage of all these benefits, earlier object localization methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref> combined verification and regression by first performing coarse localization through verifying several anchor box hypotheses, and then refining the localization by regressing box offsets. This combination approach was shown to be effective and led to state-of-the-art performance at the time. However, recent methods based purely on regression, which directly regress the object extent from each feature map position <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, could perform competitively or even better, when comparing a representative regression method, RepPoints, to RetinaNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>In this work, we examine whether pure regression based methods can be enhanced by the inclusion of verification methodology. We observe that verification has proven to be advantageous when used in certain ways. In CornerNet <ref type="bibr" target="#b12">[13]</ref>, feature map points are verified as a bounding box corner or not, in contrast to verifying anchor boxes for coarse hypothesis localization in RetinaNet <ref type="bibr" target="#b15">[16]</ref>. This use of verification leads to significantly better localization performance as shown in <ref type="table" target="#tab_0">Table 1</ref>. The difference may be attributed to corner points representing the exact spatial extent of a ground-truth object box, while an anchor box gives only a coarse hypothesis. In addition, each feature in corner point verification is well aligned to the corresponding point, while in anchor verification, the center feature used for verification lies away from the boundary area.</p><p>To elevate the performance of regression-based methods, specifically RepPoints <ref type="bibr" target="#b29">[30]</ref>, we thus seek to incorporate effective and compatible forms of verification. However, the different granularity of object representations processed by the two methods, i.e., whole objects in RepPoints and object parts (corners) in corner verification, presents an obstacle. To address this issue, we propose to model verification tasks by auxiliary side-branches that are added to the major regression branch at only the feature level and result level, without affecting intermediate representations. Through these auxiliary side-branches, verification can be fused with regression to provide the following benefits: better features by multi-task learning, feature enhancement through inclusion of verification cues, and joint inference by both methodologies. The fusion is simple, intuitive, general enough to utilize any kind of verification cue, and does not disrupt the flow of the RepPoints algorithm.</p><p>Through different techniques for harnessing verification, the localization and classification ability of RepPoints is substantially improved. The resulting detector, called RepPoints v2, shows consistent improvements of about 2.0 mAP over the original RepPoints on the COCO benchmark with different backbones. It also achieves 52.1 mAP on the COCO object detection test-dev set with a single ResNeXt-101-DCN model.</p><p>The proposed approach of choosing proper verification tasks and introducing them into a regression framework as auxiliary branches is flexible and general. It can be applied to object detection frameworks other than RepPoints, such as FCOS <ref type="bibr" target="#b26">[27]</ref>. The additional corner and within-box verification tasks are shown to improve a vanilla FCOS detector by 1.3 mAP on COCO test-dev using a ResNet-50 model. This approach can be also applied beyond object detection, such as to instance segmentation by Dense RepPoints <ref type="bibr" target="#b30">[31]</ref>, where additional contour and mask verification tasks improve performance by 1.3 mAP using a ResNet-50 model on the COCO instance segmentation test-dev set, reaching 38.9 mask mAP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Verification based object detection Early deep learning based object detection approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref> adopt a multi-scale sliding window mechanism to verify whether each window is an object or not. Corner/extreme point based verification is also proposed <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4]</ref> where the verification of a 4-d hypothesis is factorized into sub-problems of verifying 2-d corners, such that the hypothesis space is more completely covered. A sub-pixel offset branch is typically included in these methods to predict continuous corner coordinates through regression. However, since this mainly deals with quantization error due to the lower resolution of the feature map compared to the input image, we treat these methods as purely verification based in our taxonomy.</p><p>Regression based object detection Achieving object detection by pure regression dates back to YOLO <ref type="bibr" target="#b19">[20]</ref> and DenseBox <ref type="bibr" target="#b9">[10]</ref>, where four box borders are regressed at each feature map position. Though attractive for their simplicity, their accuracy is often limited due to the large displacements of regression targets, the issue of multiple objects located within a feature map bin, and extremely imbalanced positive and negative samples. Recently, after alleviating these issues by a feature pyramid network (FPN) <ref type="bibr" target="#b14">[15]</ref> structure along with a focal loss <ref type="bibr" target="#b15">[16]</ref>, regression-based object detection has regained attention <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>, with performance on par or even better than other verification or hybrid methods. Our work advances in this direction, by leveraging verification methodology into regression based detectors without disrupting its flow and largely maintaining the convenience of the original detectors. We mainly base our study on the RepPoints detector, but the method can be generally applied to other regression based detectors.</p><p>Hybrid approaches Most detectors are hybrid, for example, those built on anchors or proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. The verification and regression methodologies are employed in succession, where the anchors and proposals which provide coarse box localization are verified first, and then are refined by regression to produce the detection output. The regression target is usually at a relatively small displacement that can be easily inferred. Our work demonstrates a different hybrid approach, where the verification and regression steps are not run in serial but instead mostly in parallel to better combine their strengths. Moreover, this paper utilizes the more accurate corner verification tasks to complement regression based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Verification Meets Regression for Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Brief Review of a Pure Regression Method: RepPoints</head><p>RepPoints <ref type="bibr" target="#b29">[30]</ref> adopts pure regression to achieve object localization. Starting from a feature map position p = (x, y), it directly regresses a set of points</p><formula xml:id="formula_0">R = {p i = (x i , y i )} n i=1</formula><p>to represent the spatial extent of an object using two successive steps:</p><formula xml:id="formula_1">p i = p + ∆p i = p + g i (F p ), p i = p i + ∆p i = p i + g i (concat({F pi } n i=1 )) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">R = {p i = (x i , y i )} n i=1</formula><p>is the intermediate point set representation; F p denotes the feature vector at position p; g i and g i are 2-d regression functions implemented by a linear layer. The bounding boxes of an object are obtained by applying a conversion function T on the point sets R and R , where T is modeled as the min-max, partial min-max or moment function.</p><p>The direct regression in RepPoints <ref type="bibr" target="#b29">[30]</ref> makes it a simple framework without anchoring. Though no anchor verification step is employed, it performs no worse than anchor-based detectors, i.e. RetinaNet <ref type="bibr" target="#b15">[16]</ref>, in localization accuracy as shown in <ref type="table" target="#tab_0">Table 1</ref>. Nevertheless, we are motivated by the potential synergy between regression and verification to consider the following questions: What kind of verification tasks can benefit the regression-based RepPoints <ref type="bibr" target="#b29">[30]</ref>? Can various verification tasks be conveniently fused into the RepPoints framework without impairing the original detector?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Verification Tasks</head><p>We first discuss a pair of verification tasks that may help regression-based localization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Corner Point Verification</head><p>Two corner points, e.g. the top-left corner and bottom-right corner, can determine the spatial extent of a bounding box, providing an alternative to the usual 4-d descriptor consisting of the box's center point and size. This has been used in several bottom-up object detection methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28]</ref>, which in general perform worse than other kinds of detectors in classification, but is significantly better in object localization, as seen in <ref type="table" target="#tab_0">Table 1</ref>. In later sections, we show that this verification task can complement regression based methods to obtain more accurate object localization.</p><p>Corner point verification operates by associating a score to each point in the feature map, indicating its probability of being a corner point. An additional offset is predicted to produce continuous coordinates for corner points, which are initially quantized due to the lower resolution of the feature map compared to the input image, e.g. 8× downsampling. Following the original implementation <ref type="bibr" target="#b12">[13]</ref>, corner pooling is computed in the head, with a focal loss <ref type="bibr" target="#b15">[16]</ref> to train the corner score prediction and a smooth L1 loss for the sub-pixel offset prediction. In label assignment, each feature map point is labeled positive if a ground truth corner point is located within its feature bin, and others are labeled negative. In computing the loss, the negative samples around each ground truth corner are assigned lower weights by an inverse Gaussian function with respect to its distance to the ground-truth corner point. A more detailed description is given in Appendix A.</p><p>Different from CornerNet <ref type="bibr" target="#b12">[13]</ref>, which employs a special backbone architecture with an Hourglass structure and a single-level high resolution feature map (4× downsampled from the original image), most other recent object detectors adopt an FPN backbone with multi-level feature maps. We adapt the corner verification to utilize multi-level feature maps, e.g. the C3-C7 settings in RepPoints <ref type="bibr" target="#b29">[30]</ref>. Specifically, all ground truth corner points are assigned to every feature map level, contrary to the usual practice in FPN-based object detection of assignment according to object size. We find that assignment in this manner performs slightly better although it disregards the scale normalization issue, probably due to more positive samples at each level in training. It also performs substantially better than training on a single feature map level of highest resolution, i.e. C3, and then copying/resizing the predicted score/offset map to other levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Within-box foreground verification</head><p>Another verification task with potential to benefit regression based object detectors is to verify whether a feature map point is located within an object box or not. This within-box foreground verification task provides localization information evenly inside an object box, in contrast to corner points which focus on the box extremes. It is thus not as precise as corner points in describing object bounds, but may benefit object detectors given a coarse localization criterion.</p><p>We also differentiate among different object categories by using a non-binary category-aware foreground heatmap. Concretely, for C object categories, there is a C-channel output, with each channel indicating the probability of a feature point being in the corresponding object category. The same as for corner point verification, each ground truth object is assigned to every level of an FPN backbone.</p><p>Normalized focal loss. In training, a vanilla focal loss lets larger objects contribute significantly more than smaller objects, resulting in poorly learnt foreground scores for small objects. To address this issue, a normalized focal loss is proposed, which normalizes every positive feature map point by the total number of positive points within the same object box in the feature map. For negative points, the normalized loss uses the number of positive points as the denominator. A more detailed description is given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A General Fusion Method</head><p>In this section, we incorporate these forms of verification to elevate the performance of regressionbased methods. In general, regression-based methods detect objects in a top-down manner where all intermediate representations model the whole object. Since the two verification tasks process object parts, such as a corner or a foreground point, the different granularity of their object representations complicates fusion of the two methodologies.</p><p>To address this issue, we propose to model verification tasks by auxiliary side-branches that are fused with the major regression branch in a manner that does not affect its intermediate representations, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Fusion occurs only at the feature level and result level. With these auxiliary side-branches, the detector can gain several benefits:</p><p>Better features by multi-task learning The auxiliary verification tasks provide richer supervision in learning, yielding stronger features that increase detection accuracy, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Note that this multi-task learning is different from that of Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>. In Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>, the bounding box object detection benefits from the object mask prediction task, but it requires extra annotation of the object mask. In contrast, our additional auxiliary tasks are automatically generated from only the object bounding box annotation, allowing them to be applied in scenarios where just bounding box annotations are available. Feature enhancement for better regression The verification output includes strong cues regarding corner locations and the foreground area, which should benefit the regression task. Since the prediction output of these verification tasks has the same resolution as the feature map used for regression at each FPN level, we directly fuse them by applying a plus operator on the original feature map and an embedded feature map produced from the verification output by one 1 × 1 conv layer. The embedding aims to project any verification output to the same dimension as the original feature map, and is shared across feature map levels. Note that for the verification output, a copy detached from back-propagation is fed into the embedding convolution layer to avoid affecting the learning of that verification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corner</head><p>Joint inference by both methodologies Feature-level fusion implicitly aids object localization. We also explicitly utilize the verification output from corner prediction together with regression-based localization in a joint inference approach that makes use of both of their strengths. Specifically, by corner verification, the sub-pixel corner localization in a small neighborhood is usually more accurate than that by the main regression branch, but is worse at judging whether it is a real corner point since it lacks a whole picture of the object. On the contrary, the main regression branch is better for the latter while worse in accurate sub-pixel localization. To combine their strengths, we refine a corner point p t of the bounding box predicted from the main regression branch by</p><formula xml:id="formula_3">refine(p t ) = arg max q t q t −p t ≤r s(q t ),<label>(2)</label></formula><p>where t indicates the corner type (top-left or bottom-right); q t is a sub-pixel corner point produced by a corner prediction branch at a feature map position; s(q t ) is the verification score; r is the neighborhood threshold, set to 1 by default. Note that this result-level fusion is designed for the corner verification task only.</p><p>This fusion method is flexible and general, utilizing any kind of verification cue, as it avoids interaction with the intermediate representations in the main branch, and thus has few requirements on the types of verification target. It also does not disrupt the overall flow of the main branch and largely maintains the convenience of the original detector built on the main branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RepPoints v2: Fusing Verification into RepPoints</head><p>RepPoints is a pure regression based object detector. We now complement it with verification tasks of different forms, specifically for corners and within-box foreground. To increase the compatibility of RepPoints with the auxiliary verification tasks, we first make a small modification to it, such that the first two points explicitly represent the top-left and bottom-right corner points. We refer to this as the explicit-corners variant. These corner points replace the conversion function used in the original RepPoints, so that the bounding box is defined by these corner points instead of by a min-max or momentum operation on the point set. With the corner points, the conversion function becomes</p><formula xml:id="formula_4">T (R) = x 1 + x 2 2 , y 1 + y 2 2 , x 2 − x 1 , y 2 − y 1 .<label>(3)</label></formula><p>where the four numbers denote x-center, y-center, width and height, respectively. To this explicitcorners variant of RepPoints, we add the auxiliary side-branches for verification. Specifically, we take the feature map right after the 3 rd conv layer of the localization head as input, to reuse the existing head for computational savings. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, a 3 × 3 convolutional layer is applied on this feature map, followed by two small sub-networks for the two verification tasks. The corner sub-network consists of a corner pooling layer <ref type="bibr" target="#b12">[13]</ref> followed by a 1 × 1 conv layer to predict heatmap scores and sub-pixel offsets. The foreground sub-network is a single 1 × 1 conv layer to predict the foreground score heatmap. In training, we adopt a multi-task loss:</p><formula xml:id="formula_5">L = L RepPoints + λ 1 L corner + λ 2 L foreground ,<label>(4)</label></formula><p>with loss weights λ 1 = 0.25 and λ 2 = 0.1. More details are given in Appendix A.</p><p>Customizing the general fusion method of Section 3.3 to RepPoints, we use corner verification for multi-task learning, feature enhancement and joint inference. Foreground verification is instead used only for multi-task learning and feature enhancement. The resulting detector is named RepPoints v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Extension to Other Detectors and Problems</head><p>The fusion method used for RepPoints can also improve other detectors such as FCOS <ref type="bibr" target="#b26">[27]</ref>. As FCOS shares similar classification and localization heads as in RepPoints, the fusion of RepPoints v2 can directly be applied to it. Concretely, the corner and foreground verification heads are applied on the feature map after the 3 rd conv layer. The verification output maps are fused into the main branch, and the final regression results are obtained by the joint inference described in Section 3.3.</p><p>The fusion method can also be extended to other tasks such as instance segmentation by Dense RepPoints <ref type="bibr" target="#b30">[31]</ref>, a regression-based method. Since there is an additional object mask annotation, more fine-grained verification formats can be used, such as object contour verification and category-aware semantic segmentation. As shown in <ref type="table" target="#tab_7">Table 8</ref>, the additional verification methodology brings 1.3 mask AP gains to Dense RepPoints on the COCO test-dev set. More details are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the challenging MS COCO 2017 benchmark <ref type="bibr" target="#b13">[14]</ref>, which is split into train, val and test-dev sets with 115K, 5K and 20K images, respectively. We train all the models using the train set and conduct an ablation study on the val set. A system-level comparison to other methods is reported on the test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We use the mmdetection codebase <ref type="bibr" target="#b1">[2]</ref> for experiments. All experiments perform training with an SGD optimizer on 8 GPUs with 2 images per GPU, using an initial learning rate of 0.01, a weight decay of 0.0001 and momentum of 0.9. In ablations, most experiments follow the 1× settings where 12 epochs with single-scale training of [800, 1333] are used, with learning rate decayed by 10× after epoch 8 and 11. Most of the ablations use a ResNet-50 <ref type="bibr" target="#b8">[9]</ref> backbone pretrained on ImageNet <ref type="bibr" target="#b22">[23]</ref>. We also test our approach using multi-scale ([480, 960]) and longer training (2× settings with 24 epochs in total and the learning rate decayed at epoch 16 and 22) on stronger backbones to see whether the gains by the proposed approaches hold on these stronger baselines.</p><p>In inference, unless otherwise specified, we adopt a single-scale test approach with the image size the same as in single-scale training. We also conduct multi-scale testing on the strongest backbone for comparison with the previous state-of-the-art approaches. An IoU threshold of 0.6 is applied for Non-Maximum Suppression (NMS) to remove duplicate boxes.</p><p>For RepPoints <ref type="bibr" target="#b29">[30]</ref>, we use an improved implementation by replacing the IoU assigner with an ATSS assigner <ref type="bibr" target="#b31">[32]</ref>, yielding 39.1 mAP on COCO val using a ResNet-50 model and the 1× settings, 0.9 mAP higher than that reported in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Explicit-corners variant. We first validate the effectiveness of the explicit-corners variant of RepPoints described in Section 3.4, as shown in <ref type="table" target="#tab_1">Table 2</ref>. This variant performs on par with the three variants used in the original RepPoints, but performs 0.2-0.3 mAP better than other variants when the verification module is added. This could be contributed to more effective interaction between verification and regression tasks in this explicit-corners variant.   <ref type="table" target="#tab_2">Table 3</ref> ablates the two forms of verification. The corner verification task alone brings 1.4 mAP gains over the RepPoints baseline. The benefits are mainly for higher IoU criteria, e.g. AP 90 is improved by 4.0 mAP while AP 50 increases by only 0.2 mAP. The additional foreground verification task brings another 0.5 mAP in gains, but mainly on lower IoU criteria, for example, AP 50 is improved by 0.9 AP while AP 90 remains about the same.  <ref type="table" target="#tab_3">Table 4</ref> ablates the types of fusion, specifically multi-task learning, feature enhancement for regression, and joint inference. Multi-task learning brings a 0.4 mAP gain over the RepPoints baseline. Note that this multi-task learning does not rely on annotations beyond bounding boxes, in contrast to that in Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>. The additional feature enhancement operation brings another 0.7 gain. The explicit fusion by joint inference brings increases mAP by 0.8, such that the full approach surpasses its counterpart without verification modules by 1.9 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forms of verification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Types of fusion</head><p>Complexity analysis. Our approach involves slightly more parameters (38.3M vs 37.0M) and marginally more computation (244.2G vs 211.0G) than the original RepPoints. This overhead mainly occurs at the additional heads to produce verification score/offset maps. We also conduct RepPoints with heavier computation, by adding one more convolutional layers on the heads, resulting in a baseline with similar parameters and computations as our approach (38M/235.8G v.s. 38.3M/244.2G). The enhanced baseline model performs 0.2 mAP better than the vanilla RepPoints, indicating that the improvements by our approach are mostly not due to more parameters and computation.</p><p>Stronger baselines. We further validate our method on stronger RepPoints baselines, using longer/multi-scale training (2× settings) and stronger backbones, as shown in <ref type="table" target="#tab_4">Table 5</ref>. It can be seen that the gains are well maintained on these stronger RepPoints baselines, at about 2.0 mAP. This indicates that the proposed approach is largely complementary to improved baseline architecture, in contrast to many techniques that have exhibited decreasing gains with respect to stronger baselines.</p><p>Visualization. The visualization results are given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to State-of-the-art Methods</head><p>We compare the proposed method to other state-of-the-art object detectors on the COCO2017 test-dev set, as shown in <ref type="table" target="#tab_5">Table 6</ref>. We use GIoU <ref type="bibr" target="#b21">[22]</ref> loss instead of smooth-l1 loss in the regression branch here. With ResNet-101 as the backbone, our method achieves 46.0 mAP without bells and whistles. By using stronger ResNeXt-101 <ref type="bibr" target="#b28">[29]</ref> and DCN <ref type="bibr" target="#b2">[3]</ref> models, the accuracy rises to 49.4 mAP.</p><p>With additional multi-scale tests as in <ref type="bibr" target="#b31">[32]</ref>, the proposed method achieves 52.1 mAP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Extension to Other Detectors and Applications</head><p>Direct application to FCOS FCOS <ref type="bibr" target="#b26">[27]</ref> is another popular regression based object detector. We directly apply our approach without modification to this detector, and 1.3 mAP improvements are obtained as shown in <ref type="table" target="#tab_6">Table 7</ref>, indicating generality of the proposed approach.</p><p>Extension to instance segmentation <ref type="table" target="#tab_7">Table 8</ref> shows the effect of additional verification modules in the instance segmentation method of Dense RepPoints <ref type="bibr" target="#b30">[31]</ref>. The additional contour and foreground modules improve accuracy by 1.3 mAP, demonstrating the broad applicability of the fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose RepPoints v2, which enhances the original regression-based RepPoints by fusing verification tasks in various ways. A new variant of RepPoints is proposed to increase the compatibility with the auxiliary verification tasks. The resulting object detector shows consistent improvements over the original RepPoints under different backbones and training approaches. It also achieves 52.1 mAP on the COCO test-dev. Moreover, this approach could be easily transferred to other detectors and the instance segmentation domain, boosting the performance of the base detector/segmenter by a considerable margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Since this work is about designing better object detectors, researchers and engineers engaged in object detection and instance segmentation on natural images, medical images and even video data may benefit from this paper. If there is any failure in this system, the model may not detect objects correctly. Similar to most object detectors, the detection results may not be interpretable, thus it is hard to predict failure scenarios. This object detector also leverages biases in the dataset used for training, and may incur a performance drop on datasets which have a domain gap with the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Verification Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Corner Point Verification</head><p>Ground-truth assignment. We follow CornerNet <ref type="bibr" target="#b12">[13]</ref> to assign ground-truth corners. For each corner, only the corner itself is positive location, and all other locations are negative. Moreover, the penalty given to negative locations within a radius of the positive location is reduced. Specifically, for a given corner point p = (p x , p y ) on original image, the size of the ground-truth heatmap Y with s× downsampled rate is H s × W s , and the corresponding location of p on Y isp = p s . The penalty weight of negative locations is defined as a inverse Gaussian function:</p><formula xml:id="formula_6">Y xy = exp − (x −p x ) 2 + (y −p y ) 2 2σ 2 p<label>(5)</label></formula><p>where σ p is an object size-adaptive standard deviation, x and y indicate the location of a negative point. Note that for different positive points, the penalty weight of a negative point may be different. Therefore, the largest one as the penalty weight of the negative point.</p><p>For additional offset prediction, we follow <ref type="bibr" target="#b12">[13]</ref> that only supervises the positive locations. For a given corner point p and its corresponding downsampled locationp, the training target is:</p><formula xml:id="formula_7">o(p) = p x s − p x s , p y s − p y s<label>(6)</label></formula><p>Loss. Follow CornerNet <ref type="bibr" target="#b12">[13]</ref>, we use a modified focal loss <ref type="bibr" target="#b15">[16]</ref> to learn the corner heatmap. The loss is defined as</p><formula xml:id="formula_8">L heatmap = −1 N H i=1 W j=1 (1 − p ij ) α log (p ij ) if y ij = 1 (1 − y ij ) β (p ij ) α log (1 − p ij ) otherwise<label>(7)</label></formula><p>where N is the number of objects in an image, p ij and y ij are the score and label at location (i, j) in the predicted heatmap. We set α = 2 and β = 4, following <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition, the loss to learn offset are defined as:</p><formula xml:id="formula_9">L offset = 1 N N k=1 SmoothL1Loss (o(p k ),ô(p k ))<label>(8)</label></formula><p>where o is the groundtruth offset,ô is the predicted offset,p k is the k-th corner point. Finally, the overall loss L corner of corner branch is simply defined as the summation of L heatmap and L offset .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Within-box Foreground Verification</head><p>Normalized focal loss. The normalized focal loss is defined as:</p><formula xml:id="formula_10">L fg = C c=1 H i=1 W j=1    −1 N W w cij · α (1 − p cij ) γ log (p cij ) if y cij = 1 −1 N (1 − α) (p cij ) γ log (1 − p cij ) otherwise<label>(9)</label></formula><p>where y cij is the value on the ground-truth foreground heatmap, p cij is the c-th category score at location (i, j) of the predicted heatmap, w cij is the normalizing factor, which is defined as:</p><formula xml:id="formula_11">w cij =    1 Scij if y cij = 1 0 otherwise<label>(10)</label></formula><p>where S cij is the area of the object that (i, j) lies in. If multiple objects of the same category collide at the same location, we would take the smallest size. N W is defined as</p><formula xml:id="formula_12">C c=1 H i=1 W j=1</formula><p>w cij , the sum of normalizing factor at all locations. N is the number of positive points. α and γ is set as 0.25, 2, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Overall Loss</head><p>The overall loss is defined as:</p><formula xml:id="formula_13">L = L RepPoints + λ 1 L corner + λ 2 L fg ,<label>(11)</label></formula><p>and λ 1 = 0.25 and λ 2 = 0.1. L corner and L fg are defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extension to Instance Segmentation</head><p>Training settings. We based on Dense RepPoints <ref type="bibr" target="#b30">[31]</ref> to validate the effectiveness of our method, due to the Dense RepPoints is the state-of-the-arts regression-based instance segmentation approach. Because the contour points has no type, only one heatmap is used for predicting all contour points. Other parameters, network architectures and training details are same as object detection.</p><p>Joint inference. With only a few modifications, joint inference can also be used for instance segmentation. For a predicted representative point, if it is close to the contour point, then we refine the predicted representative point set by adding the adjacent contour point into the set. More specifically, if the score of representative point in the contour heatmap is greater than 0.5, then the point with the highest contour score among all the points with a distance less than 1 are added to the set.</p><p>Experimental results. The results is given in <ref type="table" target="#tab_8">Table 9</ref>. ResNet-50 backbone and 3x scheduler are adopted. By adding verification module, the performance are elevated by 1.0 mAP, further applying the joint inference, additional 0.3 mAP is improved. This demonstrates the flexibility of our proposed method. <ref type="figure" target="#fig_2">Figure 3</ref> shows some object detection results comparison on COCO 2017 <ref type="bibr" target="#b16">[17]</ref> between RepPoints v1 <ref type="bibr" target="#b29">[30]</ref> and RepPoints v2. Both methods adopt ResNet-50 backbone and 1x scheduler. As can be seen, compared to RepPoints v1, RepPoints v2 could provide us more precise localization results. <ref type="figure" target="#fig_3">Figure 4</ref> gives the visualization of main component of RepPoints v2. From left to right are set of representative points predicted, foreground prediction, top-left corner prediction and bottom-right corner prediction. As can be seen, all components could provide informative cues, leading to better performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the general fusion method. The outputs of verification modules (corner and foreground) are incorporated with the input feature to elevate the performance of regression-based object localization, and then joint inference is further employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the corner module and foreground module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization results of RepPoints v1 and RepPoints v2. image on the top row is the detection of RepPoints v1 and the bottom row is for RepPoints v2. The red boxes are generated without joint inference while green boxes adopts joint inference. As can be seen, our full version of RepPoints v2 could achieve better localization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of main component of RepPoints v2. From left to right are set of representative points predicted, foreground prediction, top-left corner prediction and bottom-right corner prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Analysis of the performance on COCO val set among different methods. "RepPoints*" indicates our improved re-implementation of RepPoints.</figDesc><table><row><cell>Method</cell><cell>methodology</cell><cell>backbone</cell><cell>AP</cell><cell cols="5">AP50 AP60 AP70 AP80 AP90</cell></row><row><cell>RetinaNet [16]</cell><cell>ver.+reg.</cell><cell cols="3">ResNeXt-101 40.0 60.9</cell><cell>56.4</cell><cell>48.7</cell><cell>35.8</cell><cell>14.6</cell></row><row><cell>CornerNet [13]</cell><cell>verification</cell><cell>HG-104</cell><cell cols="2">40.6 56.1</cell><cell>52.0</cell><cell>46.8</cell><cell>38.8</cell><cell>23.4</cell></row><row><cell>RepPoints* [30]</cell><cell>regression</cell><cell>ResNet-50</cell><cell cols="2">39.1 58.8</cell><cell>54.8</cell><cell>48.0</cell><cell>35.5</cell><cell>14.4</cell></row><row><cell>RepPoints v2</cell><cell>ver.+reg.</cell><cell>ResNet-50</cell><cell cols="2">41.0 59.9</cell><cell>55.9</cell><cell>49.1</cell><cell>37.2</cell><cell>18.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of the explicit-corners variant of RepPoints.</figDesc><table><row><cell>variant</cell><cell>+verification AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>min-max</cell><cell>39.1 40.7</cell><cell>58.8 59.8</cell><cell>42.4 43.7</cell><cell>22.4 23.3</cell><cell>42.8 44.4</cell><cell>50.5 54.0</cell></row><row><cell>partial min-max</cell><cell>39.0 40.7</cell><cell>58.7 59.7</cell><cell>42.4 43.6</cell><cell>21.8 23.1</cell><cell>42.5 44.4</cell><cell>50.7 54.0</cell></row><row><cell>momentum</cell><cell>39.1 40.8</cell><cell>58.9 59.7</cell><cell>42.2 43.7</cell><cell>22.3 23.5</cell><cell>42.6 44.7</cell><cell>50.8 53.9</cell></row><row><cell>explicit-corners</cell><cell>39.1 41.0</cell><cell>58.8 59.9</cell><cell>42.5 43.9</cell><cell>22.4 23.8</cell><cell>42.6 44.8</cell><cell>50.6 54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablations on two forms of verification.</figDesc><table><row><cell>corner foreground AP</cell><cell cols="6">AP50 AP75 AP90 APS APM APL</cell></row><row><cell>39.1</cell><cell>58.8</cell><cell>42.5</cell><cell>14.4</cell><cell>22.4</cell><cell>42.6</cell><cell>50.6</cell></row><row><cell>40.5</cell><cell>59.0</cell><cell>43.5</cell><cell>18.4</cell><cell>23.4</cell><cell>44.1</cell><cell>53.5</cell></row><row><cell>41.0</cell><cell>59.9</cell><cell>43.9</cell><cell>18.5</cell><cell>23.8</cell><cell>44.8</cell><cell>54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablations on three types of fusion.</figDesc><table><row><cell>multi-task enhance feature joint inference AP</cell><cell cols="6">AP50 AP75 AP90 APS APM APL</cell></row><row><cell>39.1</cell><cell>58.8</cell><cell>42.5</cell><cell>14.4</cell><cell>22.4</cell><cell>42.6</cell><cell>50.6</cell></row><row><cell>39.5</cell><cell>58.9</cell><cell>42.7</cell><cell>14.6</cell><cell>22.5</cell><cell>43.1</cell><cell>51.0</cell></row><row><cell>40.2</cell><cell>60.0</cell><cell>43.5</cell><cell>15.7</cell><cell>24.1</cell><cell>43.8</cell><cell>52.5</cell></row><row><cell>41.0</cell><cell>59.9</cell><cell>43.9</cell><cell>18.5</cell><cell>23.8</cell><cell>44.8</cell><cell>54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experiments on RepPoints baselines with stronger backbones using 2× settings (24 epochs) and multi-scale training ([480, 960]) on COCO val set.</figDesc><table><row><cell>backbone</cell><cell>+verification AP</cell><cell cols="5">AP50 AP60 AP70 AP80 AP90</cell></row><row><cell>ResNet-50</cell><cell>41.8 43.9</cell><cell>61.8 63.1</cell><cell>58.1 59.3</cell><cell>51.1 52.5</cell><cell>38.6 40.1</cell><cell>15.9 20.6</cell></row><row><cell>ResNet-101</cell><cell>43.4 45.5</cell><cell>63.3 64.5</cell><cell>59.4 60.6</cell><cell>53.0 54.1</cell><cell>40.4 42.3</cell><cell>18.0 22.2</cell></row><row><cell>ResNeXt-101</cell><cell>45.5 47.3</cell><cell>65.9 66.9</cell><cell>62.1 62.9</cell><cell>55.2 56.1</cell><cell>42.4 44.0</cell><cell>19.7 23.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of RepPoints v2 to state-of-the-art detectors on COCO test-dev. * denotes that the number is obtained by multi-scale testing.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell cols="2">epoch AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>RetinaNet [16]</cell><cell>ResNet-101</cell><cell>18</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>FCOS [27]</cell><cell>ResNeXt-101</cell><cell>24</cell><cell>43.2</cell><cell>62.8</cell><cell>46.6</cell><cell>26.5</cell><cell>46.2</cell><cell>53.3</cell></row><row><cell>DCN V2* [36]</cell><cell>ResNet-101+DCN</cell><cell>18</cell><cell>46.0</cell><cell>67.9</cell><cell>50.8</cell><cell>27.8</cell><cell>49.1</cell><cell>59.5</cell></row><row><cell>RepPoints* [30]</cell><cell>ResNet-101+DCN</cell><cell>24</cell><cell>46.5</cell><cell>67.4</cell><cell>50.9</cell><cell>30.3</cell><cell>49.7</cell><cell>57.1</cell></row><row><cell>MAL* [11]</cell><cell>ResNeXt-101</cell><cell>24</cell><cell>47.0</cell><cell>66.1</cell><cell>51.2</cell><cell>30.2</cell><cell>50.1</cell><cell>58.9</cell></row><row><cell>FreeAnchor* [33]</cell><cell>ResNeXt-101</cell><cell>24</cell><cell>47.3</cell><cell>66.3</cell><cell>51.5</cell><cell>30.6</cell><cell>50.4</cell><cell>59.0</cell></row><row><cell>ATSS* [32]</cell><cell>ResNeXt-101+DCN</cell><cell>24</cell><cell>50.7</cell><cell>68.9</cell><cell>56.3</cell><cell>33.2</cell><cell>52.9</cell><cell>62.4</cell></row><row><cell>TSD* [25]</cell><cell>SENet154+DCN</cell><cell>24</cell><cell>51.2</cell><cell>71.9</cell><cell>56.0</cell><cell>33.8</cell><cell>54.8</cell><cell>64.2</cell></row><row><cell>CornerNet [13]</cell><cell>HG-104</cell><cell>100</cell><cell>40.5</cell><cell>56.5</cell><cell>43.1</cell><cell>19.4</cell><cell>42.7</cell><cell>53.9</cell></row><row><cell>ExtremeNet [35]</cell><cell>HG-104</cell><cell>100</cell><cell>40.2</cell><cell>55.5</cell><cell>43.2</cell><cell>20.4</cell><cell>43.2</cell><cell>53.1</cell></row><row><cell>CenterNet [4]</cell><cell>HG-104</cell><cell>100</cell><cell>44.9</cell><cell>62.4</cell><cell>48.1</cell><cell>25.6</cell><cell>47.4</cell><cell>57.4</cell></row><row><cell>RepPoints v2</cell><cell>ResNet-50</cell><cell>24</cell><cell>44.4</cell><cell>63.5</cell><cell>47.7</cell><cell>26.6</cell><cell>47</cell><cell>54.6</cell></row><row><cell>RepPoints v2</cell><cell>ResNet-101</cell><cell>24</cell><cell>46.0</cell><cell>65.3</cell><cell>49.5</cell><cell>27.4</cell><cell>48.9</cell><cell>57.3</cell></row><row><cell>RepPoints v2</cell><cell>ResNeXt-101</cell><cell>24</cell><cell>47.8</cell><cell>67.3</cell><cell>51.7</cell><cell>29.3</cell><cell>50.7</cell><cell>59.5</cell></row><row><cell>RepPoints v2</cell><cell>ResNet-101+DCN</cell><cell>24</cell><cell>48.1</cell><cell>67.5</cell><cell>51.8</cell><cell>28.7</cell><cell>50.9</cell><cell>60.8</cell></row><row><cell>RepPoints v2</cell><cell>ResNeXt-101+DCN</cell><cell>24</cell><cell>49.4</cell><cell>68.9</cell><cell>53.4</cell><cell>30.3</cell><cell>52.1</cell><cell>62.3</cell></row><row><cell>RepPoints v2*</cell><cell>ResNeXt-101+DCN</cell><cell>24</cell><cell>52.1</cell><cell>70.1</cell><cell>57.5</cell><cell>34.5</cell><cell>54.6</cell><cell>63.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Applying the verification module to FCOS, which is implemented in mmdetection.</figDesc><table><row><cell></cell><cell>backbone</cell><cell>AP</cell><cell cols="6">AP50 AP75 AP90 APS APM APL</cell></row><row><cell>FCOS</cell><cell cols="2">ResNet-50 38.2</cell><cell>57.1</cell><cell>41.2</cell><cell>15.3</cell><cell>22.2</cell><cell>42.3</cell><cell>49.5</cell></row><row><cell cols="3">+verification ResNet-50 39.5</cell><cell>57.7</cell><cell>41.9</cell><cell>18.4</cell><cell>22.3</cell><cell>43.2</cell><cell>52.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Adding the verification module to the instance segmentation algorithm Dense RepPoints on COCO test-dev.</figDesc><table><row><cell></cell><cell>backbone</cell><cell cols="6">APmask AP50 AP75 APS APM APL</cell></row><row><cell cols="2">Dense RepPoints ResNet-50</cell><cell>37.6</cell><cell>60.4</cell><cell>40.2</cell><cell>20.9</cell><cell>40.5</cell><cell>48.6</cell></row><row><cell>+verification</cell><cell>ResNet-50</cell><cell>38.9</cell><cell>61.5</cell><cell>41.9</cell><cell>21.2</cell><cell>42.0</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Adding the verification module to the instance segmentation algorithm Dense RepPoints on COCO test-dev.</figDesc><table><row><cell></cell><cell>backbone</cell><cell cols="6">APmask AP50 AP75 APS APM APL</cell></row><row><cell cols="2">Dense RepPoints ResNet-50</cell><cell>37.6</cell><cell>60.4</cell><cell>40.2</cell><cell>20.9</cell><cell>40.5</cell><cell>48.6</cell></row><row><cell>+contour&amp;fg</cell><cell>ResNet-50</cell><cell>38.6</cell><cell>61.4</cell><cell>41.7</cell><cell>21.3</cell><cell>41.8</cell><cell>50.8</cell></row><row><cell>+joint inference</cell><cell>ResNet-50</cell><cell>38.9</cell><cell>61.5</cell><cell>41.9</cell><cell>21.2</cell><cell>42.0</cell><cell>51.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector. arxiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grid R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9656" to="9665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dense reppoints: Representing visual objects with dense point sets. arxiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02424</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Objects as points. arxiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable convnets V2: more deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
