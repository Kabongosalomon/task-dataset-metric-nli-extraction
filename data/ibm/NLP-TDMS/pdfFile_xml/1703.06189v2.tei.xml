<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<email>nevatia@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the previous state-of-theart methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms stateof-the-art performance on THUMOS-14 and ActivityNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We address the problem of generating Temporal Action Proposals (TAP) in long untrimmed videos, akin to generation of object proposals in images for rapid object detection. As in the case for objects, the goal is to make action proposals have high precision and recall, while maintaining computational efficiency.</p><p>There has been considerable work in action classification task where a "trimmed" video is classified into one of specified categories <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>. There has also been work on localizing the actions in a longer, "untrimmed" video <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>, i.e. temporal action localization. A straightforward way to use action classification techniques for localization is to use temporal sliding windows, however there is a trade-off between density of the sliding windows and computation time. Taking cues from the success of pro- * indicates equal contributions. ground truth sliding window prediction location refinement Timeline <ref type="figure">Figure 1</ref>. Temporal action proposal generation from a long untrimmed video. We propose a Temporal Unit Regression Network (TURN) to jointly predict action proposals and refine the location by temporal coordinate regression.</p><p>posal frameworks in object detection tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>, there has been recent work for generating temporal action proposals in videos <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref> to improve the precision and accelerate the speed of temporal localization. State-of-the-art methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref> formulate TAP generation as a binary classification problem (i.e. action vs. background) and apply sliding window approach as well. Denser sliding windows usually would lead to higher recall rates at the cost of computation time. Instead of basing on sliding windows, Deep Action Proposals (DAPs) <ref type="bibr" target="#b3">[4]</ref> uses a Long Short-term Memory (LSTM) network to encode video streams and infer multiple action proposals inside the streams. However, the performance of average recall (AR), which is computed by the average of recall at temporal intersection over union (tIoU) between 0.5 and 1, suffers at small number of predicted proposals compared with the sliding window based method <ref type="bibr" target="#b22">[23]</ref>  <ref type="bibr" target="#b0">1</ref> .</p><p>To achieve high temporal localization accuracy and efficient computation cost, we propose to use temporal boundary regression. Boundary regression has been a successful practice for object localization, as in <ref type="bibr" target="#b20">[21]</ref>. However, temporal boundary regression for actions has not been attempted in the past work.</p><p>We present a novel method for fast TAP generation: Temporal Unit Regression Network (TURN). A long untrimmed video is first decomposed into short (e.g. 16 or 32 frames) video units, which serve as basic processing blocks. For each unit, we extract unit-level visual features using off-the-shelf models (C3D and two-stream CNN model are evaluated) to represent video units. Features from a set of contiguous units, called a clip, are pooled to create clip features. Multiple temporal scales are used to create a clip pyramid. To provide temporal context, clip-level features from the internal and surrounding units are concatenated. Each clip is then treated as a proposal candidate and TURN outputs a confidence score, indicating whether it is an action instance or not. In order to better estimate the action boundary, TURN outputs two regression offsets for the starting time and ending time of an action in the clip. Non-maximum suppression (NMS) is then applied to remove redundant proposals. The source code is available at https://github.com/jiyanggao/TURN-TAP. DAPs <ref type="bibr" target="#b3">[4]</ref> and Sparse-prop <ref type="bibr" target="#b1">[2]</ref> use Average Recall vs. Average Number of retrieved proposals (AR-AN) to evaluate the TAP performance. There are two issues with AR-AN metric: (1) the correlation between AR-AN of TAP and mean Average Precision (mAP) of action localization was not explored ; (2) the average number of retrieved proposals is related to average video length of the test dataset, which makes AR-AN less reliable when evaluating across different datasets. Spatio-temporal action detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref> used Recall vs. Proposal Number (R-N), however this metric does not take video lengths into consideration.</p><p>There are two criteria for a good metric: (1) it should be capable of evaluating the performance of different methods on the same dataset effectively; (2) it should be capable of evaluating the performance of the same method across different datasets (generalization capability). We should expect better TAP would lead to better localization performance, using the same localizer. We propose a new metric, Average Recall vs. Frequency of retrieved proposals (AR-F), for TAP evaluation. In Section 4.2, we validate that the proposed method satisfies the two criteria by quantitative correlation analysis between TAP performance and action localization performance.</p><p>We test TURN on THUMOS-14 and ActivityNet for TAP generation. Experimental results show that TURN outperforms the previous state-of-the-art methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> by a large margin under AR-F and AR-AN. For run-time performance, TURN runs at over 880 frames per second (FPS) with C3D features and 260 FPS with flow CNN features on a single TITAN X GPU. We further plug TURN as a proposal generation step in existing temporal action localization pipelines, and observe an improvement of mAP from state-of-the-art 19% to 25.6% (at tIoU=0.5) on THUMOS-14 by changing only the proposals. State-of-the-art local-ization performance is also achieved on ActivityNet. We show state-of-the-art performance on generalization capability by training TURN on THUMOS-14 and transfer it to ActivityNet without fine-tuning, strong generalization capability is also shown by test TURN across different subsets in ActivityNet without fine-tuning.</p><p>In summary, our contributions are four-fold:</p><p>(1) We propose a novel architecture for temporal action proposal generation using temporal coordinate regression.</p><p>(2) Our proposed method achieves high efficiency (&gt;800 fps) and outperforms previous state-of-the-art methods by a large margin.</p><p>(3) We show state-of-the-art generalization performance of TURN across different action datasets without dataset specific fine-tuning.</p><p>(4) We propose a new metric, AR-F, to evaluate the performance of TAP and compare AR-F with AR-AN and AR-N by quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal Action Proposal. Sparse-prop <ref type="bibr" target="#b1">[2]</ref> proposes the use of STIPs <ref type="bibr" target="#b14">[15]</ref> and dictionary learning for classindependent proposal generation. S-CNN [23] presents a two-stage action localization system, in which the first stage is temporal proposal generation, and shows the effectiveness of temporal proposals for action localization. S-CNN's proposal network is based on fine-tuning 3D convolutional networks (C3D) <ref type="bibr" target="#b26">[27]</ref> to binary classification task. DAPs <ref type="bibr" target="#b3">[4]</ref> adopts LSTM networks to encode a video stream and produce proposals inside the video stream.</p><p>Temporal Action Localization. Based on the progress of action classification, temporal action localization has been received much attentions recently. Ma et al. <ref type="bibr" target="#b16">[17]</ref> address the problem of early action detection. They propose to train a LSTM network with ranking loss and merge the detection spans based on the frame-wise prediction scores generated by the LSTM. Singh et al. <ref type="bibr" target="#b24">[25]</ref> extend two-stream <ref type="bibr" target="#b23">[24]</ref> framework to multi-stream bi-directional LSTM networks and achieved state-of-the-art performance on MPII-Cooking dataset <ref type="bibr" target="#b21">[22]</ref>. Sun et al. <ref type="bibr" target="#b25">[26]</ref> transfer knowledge from web images to address temporal localization in untrimmed web videos. S-CNN [23] presents a twostage action localization framework: first using proposal networks to generate temporal proposals and then score the proposals with localization networks, which is trained with classification and localization loss.</p><p>Spatio-temporal Action Localization. A handful of efforts have been seen in spatio-temporal action localization. Gkioxari et al. <ref type="bibr" target="#b8">[9]</ref> extract proposals from RGB images with SelectiveSearch <ref type="bibr" target="#b27">[28]</ref> and then apply R-CNN <ref type="bibr" target="#b7">[8]</ref> on both RGB and optical flow images for action detection. Weinzaepfel et al. <ref type="bibr" target="#b30">[31]</ref> replace SelectiveSearch <ref type="bibr" target="#b27">[28]</ref> with Edge-Boxes <ref type="bibr" target="#b35">[36]</ref>. Mettes et al. <ref type="bibr" target="#b17">[18]</ref> propose to use sparse points Multiple temporal scales are used to create a clip pyramid at an anchor unit. TURN takes a clip as input, and outputs a confidence score, indicating whether it is an action instance or not, and two regression offsets of start and end times to refine the temporal action boundaries.</p><p>as supervision for action detection to save tedious annotation work.</p><p>Object Proposals and Detection. Object proposal generation methods can be classified into two types based the features they use. One relies on hand-crafted low-level visual features, such as SelectiveSearch <ref type="bibr" target="#b27">[28]</ref> and Edgebox <ref type="bibr" target="#b35">[36]</ref>. R-CNN <ref type="bibr" target="#b7">[8]</ref> and Fast R-CNN <ref type="bibr" target="#b6">[7]</ref> are built on this type of proposals. The other type is based on deep ConvNet feature maps, such as RPNs <ref type="bibr" target="#b20">[21]</ref>, which introduces the use of anchor boxes and spatial regression for object proposal generation. YOLO <ref type="bibr" target="#b19">[20]</ref> and SSD <ref type="bibr" target="#b15">[16]</ref> divide images into grids and regress object bounding boxes based on the grid cells. Bounding box coordinate regression is a common design shared in second type of object proposal frameworks. Inspired by object proposals, we adopt temporal regression in action proposal generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we will describe the Temporal Unit Regression Network (TURN) and the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Unit Processing</head><p>As we discussed before, the large-scale nature of video proposal generation requires the solution to be computationally efficient. Thus, extracting visual feature for the same window or overlapped windows repeatedly should be avoided. To accomplish this, we use video units as the basic processing units in our framework. A video V contains T frames, V = {t i } T 1 , and is divided into T /n u consecutive video units , where n u is the frame number of a unit. A unit is represented as</p><formula xml:id="formula_0">u = {t i } s f +nu s f</formula><p>, where s f is the starting frame, s f + n u is the ending frame. Units are not overlapped with each other.</p><p>Each unit is processed by a visual encoder E v to get a unit-level representation f u = E v (u). In our experiments, C3D <ref type="bibr" target="#b26">[27]</ref>, optical flow based CNN model and RGB image CNN model <ref type="bibr" target="#b23">[24]</ref> are investigated. Details are given in Section 4.2. are the context before and after c respectively, n ctx is the number of units we consider for context. Internal feature and context feature are pooled from unit features separately by a function P . The final feature f c for a clip is the concatenation of context features and the internal features; f c is given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Clip Pyramid Modeling</head><formula xml:id="formula_1">f c = P ({u j } su su−nctx ) P ({u j } eu su ) P ({u j } eu+nctx eu )</formula><p>where represents vector concatenation and mean pooling is used for P . We scan an untrimmed video by building window pyramids at each unit position, i.e. an anchor unit. A clip pyramid p consists of temporal windows with different temporal resolution, p = {c nc }, n c ∈ {n c,1 , n c,2 , ...}. Note that, although multi-resolution clips would have temporal overlaps, the clip-level features are computed from unit-level features, which are only calculated once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unit-level Temporal Coordinate Regression</head><p>The intuition behind temporal coordinate regression is that human can infer the approximate start and end time of an action instance (e.g. shooting basketball, swing golf) without watching the entire instance, similarly, neural networks might also be able to infer the temporal boundaries. Specifically, we design a unit regression model that takes a clip-level representation f c as input, and have two sibling output layers. The first one outputs a confidence score indicating whether the input clip is an action instance. The second one outputs temporal coordinate regression offsets. The regression offsets are</p><formula xml:id="formula_2">o s = s clip − s gt , o e = e clip − e gt<label>(1)</label></formula><p>where s clip , e clip is the index of starting unit and ending unit of the input clip; s gt , e gt is the index of starting unit and ending unit of the matched ground truth.</p><p>There are two salient aspects in our coordinate regression model. First, instead of regressing the temporal coordinates at frame-level, we adopt unit-level coordinate regression. As the basic unit-level features are extracted to encode n u frames, the feature may not be discriminative enough to regress the coordinates at frame-level. Comparing with frame-level regression, unit-level coordinate regression is easier to learn and more effective. Second, in contrast to spatial bounding box regression, we don't use coordinate parametrization. We directly regress the offsets of the starting unit coordinates and the ending unit coordinates. The reason is that objects can be re-scaled in images due to camera projection, so the bounding box coordinates should be first normalized to some standard scale. However, actions' time spans can not be easily rescaled in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>For training TURN, we assign a binary class label (of being an action or not) to each clip (generated at each anchor unit). A positive label is assigned to a clip if: (1) the window clip with the highest temporal Intersection over Union (tIoU) overlaps with a ground truth clip; or (2) the window clip has tIoU larger than 0.5 with any of the ground truth clips. Note that, a single ground truth clip may assign positive labels to multiple window clips. Negative labels are assigned to non-positive clips whose tIoU is equal to 0.0 (i.e. no overlap) for all ground truth clips. We design a multi-task loss L to jointly train classification and coordinates regression.</p><formula xml:id="formula_3">L = L cls + λL reg<label>(2)</label></formula><p>where L cls is the loss for action/background classification, which is a standard Softmax loss. L reg is for temporal coordinate regression and λ is a hyper-parameter. The regression loss is</p><formula xml:id="formula_4">L reg = 1 N pos N i=1 l * i |(o s,i − o * s,i ) + (o e,i − o * e,i )| (3)</formula><p>L1 distance is adopted. l * i is the label, 1 for positive samples and 0 for background samples. N pos is the number of positive samples. The regression loss is calculated only for positive samples.</p><p>During training, the background to positive samples ratio is set to be 10 in a mini-batch. The learning rate and batch size are set as 0.005 and 128 respectively. We use the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer to train TURN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section, we introduce the evaluation metrics, experimental setup and discuss the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics</head><p>We consider three different metrics to assess the quality of TAP, the major difference is in the way to consider the retrieve number of proposals: Average Recall vs. Number of retrieved proposals (AR-N) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11]</ref>, Average Recall vs. Average Number of retrieved proposals (AR-AN) <ref type="bibr" target="#b3">[4]</ref>, Average Recall vs. Frequency of retreived proposals (AR-F). Average Recall (AR) is calculated as a mean value of recall rate at tIoU between 0.5 and 1.</p><p>AR-N curve. In this metric, the numbers of retrieved proposals (N) for all test videos are the same. This curve plots AR versus number of retrieved proposals.</p><p>AR-AN curve. In this metric, AR is calculated as a function of average number of retrieved proposals (AN). AN is calculated as: Θ = ρΦ, ρ ∈ (0, 1]. In which, Φ = 1 n n i=1 Φ i is the average number of all proposals of test videos. ρ is the ratio of picked proposals to evaluate. n is the number of test videos and Φ i is the number of all proposals for each video. By scanning the ratio ρ from 0 to 1, the number of retrieved proposals in each video varies from 0 to number of all proposals and thus the average number of retrieved proposals also varies.</p><p>AR-F curve. This is the new metric that we propose. We measure average recall as a function of proposal frequency (F), which denotes the number of retrieved proposals per second for a video. For a video of length l i and proposal frequency of F , the retrieved proposal number of this video is R i = F l i .</p><p>We also report Recall@X-tIoU curve: recall rate at X with regard to different tIoU. X could be number of retrieved proposals (N), average number of retrieved proposals (AN) and proposal frequency (F).</p><p>For the evaluation of temporal action localization, we follow the traditional mean Average Precision (mAP) metric used in THUMOS-14 and ActivityNet. A prediction is regarded as positive only when it has correct category prediction and tIoU with ground truth higher than a threshold. We use the official evaluation toolkit provided by THUMOS-14 and ActivityNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on THUMOS-14</head><p>Datasets. The temporal action localization part of THUMOS-14 contains over 20 hours of videos from 20 sports classes. This part consists of 200 videos in validation set and 213 videos in test set. TURN model is trained on the validation set, as the training set of THUMOS-14 contains only trimmed videos.</p><p>Experimental setup. We perform the following experiments: (1) different temporal proposal evaluation metrics are compared; (2) the performance of TURN and other TAP generation methods are compared under evaluation metrics (i.e AR-F and AR-AN) mentioned above; (3) different TAP generation methods are compared on the temporal action localization task with the same localizer/classifier. Specifically, we feed the proposals into a localizer/classifier, which outputs the confidence scores of 21 classes (20 classes of action plus background). Two localizer/classifiers are adopted: (a) SVM classifiers: one-vs-all linear SVM classifiers are trained for all 21 classes using C3D fc6 features; (b) S-CNN localizer: the pre-trained localization network of S-CNN [23] is adopted.</p><p>For TURN model, the context unit number n ctx is 4, λ is 2.0, the dimension of middle layer f m is 1000, temporal window pyramids is built with {1, 2, 4, 8, 16, 32} units. We test TURN with different unit sizes n u ∈ {16, 32}, and different unit features, including C3D <ref type="bibr" target="#b26">[27]</ref>, optical flow based CNN feature and RGB CNN feature <ref type="bibr" target="#b23">[24]</ref>. The NMS threshold is set to be 0.1 smaller than tIoU in evaluation. We implement TURN model in Tensorflow <ref type="bibr" target="#b0">[1]</ref>.</p><p>Comparison of different evaluation metrics. To validate the effectiveness of different evaluation metrics, we compare AR-F, AR-N, AR-AN by a correlation analysis with localization performance (mAP). We generate seven different sets of proposals, including random proposals, slidinig windows and variants of S-CNN <ref type="bibr" target="#b22">[23]</ref> proposals (details are given in the supplementary material). We then test the localization performance using the proposals, as shown in <ref type="figure" target="#fig_2">Figure 3</ref> (a)-(c). SVM classifiers are used for localization.</p><p>A detailed analysis of correlation and video length is given in <ref type="figure" target="#fig_2">Figure 3 (d)</ref>. The test videos are sorted by video lengths and then divided evenly into four groups. The average video length of the group is the x-axis, and y-axis represents the correlation coefficient between action localization performance and TAP performance of the group. Each point in 3 (d) represents the correlation of TAP and localization performance of one group under different evaluation metrics. As can be observed in <ref type="figure" target="#fig_2">Figure 3</ref>, the correlation coefficient between mAP and AR-F is consistently higher than 0.9 at all video lengths. In contrast, correlation of AR-N and mAP is affected by video length distribution. Note that, AR-AN also shows a stable correlation with mAP, this is partially because the TAP generation methods we use gen- erate proportional numbers of proposals to video length. To assess generalization, assume that we have two different datasets, S 0 and S 1 , whose average number of all proposals are Φ 0 and Φ 1 respectively. As introduced before, average number of retrieved proposals Θ = ρΦ, ρ ∈ (0, 1] is dependent on Φ. When we compare AR at some certain AN = Θ x between S 0 and S 1 , as Φ 0 and Φ 1 are different, we need to set different ρ 0 and ρ 1 . It means that the ratios between retrieved proposals and all generated proposals are different for S 0 and S 1 , which make the AR calculated for S 0 and S 1 at the same AN = Θ x can not be compared directly. For AR-F, the number of proposals retrieved is based on "frequency", which is independent with the average number of all generated proposals.</p><p>In summary, AR-N cannot evaluate TAP performance effectively on the same dataset, as number of retrieved proposals should vary with video lengths. AR-AN cannot be used to compare TAP performance among different datasets, as the retrieval ratio depends on dataset's video length distribution, which makes the comparison unreasonable. AR-F satisfies both requirements.</p><p>Comparison of visual features. We test TURN with three unit-level features to assess the effect of visual features on AR performance: C3D <ref type="bibr" target="#b26">[27]</ref> features, RGB CNN features with temporal mean pooling and dense flow CNN <ref type="bibr" target="#b31">[32]</ref> features. The C3D model is pre-trained on Sports1m <ref type="bibr" target="#b12">[13]</ref>, all 16 frames in a unit are input into C3D and the output of f c6 layer is used as unit-level feature. For RGB CNN features, we uniformly sample 8 frames from a unit, extract "Flatten 673" features using a ResNet [10] model (pre-trained on training set of ActivityNet v1.3 dataset <ref type="bibr" target="#b31">[32]</ref>) and compute the mean of these 8 features as the unit-level feature. For dense flow CNN features, we sample 6 consecutive frames at the center of a unit and calculate optical flow <ref type="bibr" target="#b4">[5]</ref> between them. The flows are then fed into a BN-Inception model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref> that is pre-trained on training set of ActivityNet v1.3 dataset <ref type="bibr" target="#b31">[32]</ref>. The output of "global pool" layer of BN-Inception is used as the unit-level feature.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, dense flow CNN feature (TURN-FL) gives the best results, indicating optical flow can capture temporal action information effectively. In contrast, RGB CNN features (TURN-RGB) show inferior performance and C3D (TURN-C3D) gives competitive performance.</p><p>Temporal context and unit-level coordinate regression. We compare four variants of TURN to show the effectiveness of temporal context and unit regression: (1) binary cls w/o ctx: binary classification (no regression) without the use of temporal context, (2) binary cls w/ ctx: binary classification (no regression) with the use of context, (3) frame reg w/ ctx: frame-level coordinate regression with the use of context and (4) unit reg w/ ctx: unit-level coordinate regression with the use of context (i.e. our full model). The four variants are compared with AR-F curves. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, temporal context helps to classify action and background by providing additional information. As shown in AR-F curve, unit reg w/ ctx has higher AR than the other variants at all frequencies, indicating that unit-level regression can effectively refine the proposal location. Some TURN proposal results are shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>Comparison with state-of-the-art. We compare TURN with the state-of-the-art methods under AR-AN, AR-F, Recall@AN-tIoU, Recall@F-tIoU metrics. The TAP generation methods include DAPs <ref type="bibr" target="#b3">[4]</ref>, SCNN-prop <ref type="bibr" target="#b22">[23]</ref>, Sparseprop <ref type="bibr" target="#b1">[2]</ref>, sliding window, and random proposals. For DAPs, Sparse-prop and SCNN-prop, we plot the curves using the proposal results provided by the authors. "Sliding window proposals" include all sliding windows of length from 16 to 512 overlapped by 75%, each window is assigned with a random score. "Random proposals" are generated by assigning random starting and ending temporal coordinates (ending temporal coordinate is larger than starting temporal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AR-F AR-AN</head><p>Recall@F-tIoU Recall@AN-tIoU <ref type="figure">Figure 5</ref>. Proposal performance on THUMOS-14 dataset under 4 metrics: AR-F, AR-AN, Recall@F-tIoU, Recall@AN-tIoU. For AR-AN and Recall@AN-tIoU, we use the codes provided by <ref type="bibr" target="#b3">[4]</ref> coordinate), each random window is assigned with a random score. As shown in <ref type="figure">Figure 5</ref>, TURN outperforms the state-of-the-art consistently by a large margin under all four metrics.</p><p>How unit size affects AR and run-time performance? The impact of unit size on AR and computation speed is evaluated with n u ∈ {16, 32}. We keep other hyperparameters the same as in Section 4.2. <ref type="table" target="#tab_0">Table 1</ref> shows comparison of the three TURN variants (TURN-FL-16, TURN-FL-32, TURN-C3D-16) and three state-of-the-art TAP methods, in terms of recall (AR@F=1.0) and run-time (FPS) performance. We randomly select 100 videos from THUMOS-14 validation set and run TURN-FL-16, TURN-FL-32 and TURN-C3D-16 on a single Nvidia TITAN X GPU. The run-time of DAPs <ref type="bibr" target="#b3">[4]</ref> and SCNN-prop <ref type="bibr" target="#b22">[23]</ref> are provided in <ref type="bibr" target="#b3">[4]</ref>, which were tested on a TITAN X GPU and a GTX 980 GPU respectively. The hardware used in <ref type="bibr" target="#b1">[2]</ref> is not specified in the paper. As can be seen, there is a trade-off between AR and FPS: smaller unit size leads to higher recall rate, and also higher computational complexity. We consider unit size as tempo-ral coordinate precision, for example, unit size of 16 and 32 frames represent approximately half second and one second respectively. The major part of computation time comes from unit-level feature extraction. Smaller unit size leads to more number of units, which increases computation time; on the other hand, smaller unit size also increases temporal coordinate precision, which improves the precision of temporal regression. C3D feature is faster than flow CNN feature, but with a lower performance. Compared with stateof-the-art methods, we can see that TURN-C3D-16 outperforms current state-of-the-art AR performance, but accelerates computation speed by more than 6 times. TURN-FL-16 achieves the highest AR performance with competitive run-time performance.</p><p>TURN for temporal action localization. We feed proposal results of different TAP generation methods into the same temporal action localizers/classifiers to compare the quality of proposals. The value of mAP@tIoU=0.5 is reported in <ref type="table" target="#tab_1">Table 2</ref>. TURN outperforms all other methods in both the SVM classifier and S-CNN localizer. Sparse-prop, SCNN-prop and DAPs all use C3D to extract features. It is worth noting that the localization results of four different proposals suit well with their proposal performance under AR-F metric in <ref type="figure">Figure 5</ref>: the methods that have better performance under AR-F achieve higher mAP in temporal action localization. A more detailed comparison of state-of-the-art localization methods is given in <ref type="table" target="#tab_2">Table 3</ref>. It can be seen that, by applying TURN with linear SVM classifiers for action localization, we achieve comparable performance with the state-of-the-art methods. By further incorporating S-CNN localizer, we outperform all other methods by a large margin at all tIoU thresholds. The experimental results prove the high-quality of TURN proposals. TURN helps action localization on two aspects: (1) TURN serves as the first stage of a localization pipeline (e.g. S-CNN, SVM) to generate high-quality TAP, and thus increases the localization performance; (2) TURN accelerates localization pipelines by filtering out many background segments, thus reducing the unnecessary computation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on ActivityNet</head><p>Datasets. ActivityNet datasets provide rich and diverse action categories. There are three releases of ActivityNet dataset: v1.1, v1.2 and v1.3. All three versions define a 5-level hierarchy of action classes. Nodes on higher level represent more abstract action categories. For example, the node "Housework" on level-3 has child nodes "Interior cleaning", "Sewing, repairing, &amp; maintaining textiles" and "Laundry" on level-4. From the hierarchical action categories definition, a subset can be formed by including all action categories that belong to a certain node.</p><p>Experiment setup. To compare with previous work, we do experiments on v1.1 (on subsets of "Works" and "Sports") for temporal action localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>, v1.2 for proposal generalization capability following the same evaluation protocol as in <ref type="bibr" target="#b3">[4]</ref>. On v1.3, we design a different experimental setup to test TURN's cross-domain generalization capability: four subsets having distinct semantic meanings are selected, including "Participating in Sports, Exercise, or Recreation", "Vehicles", "Housework" and "Arts and Entertainment". We also check that the action categories in different subsets are not semantically related: for example, "archery", "dodge ball" in "Sports" subset, "changing car wheels", "fixing bicycles" in "Vehicles" subset, "vacuuming floor", "cleaning shoes" in "Housework" subset, "ballet", "playing saxophone" in "Arts" subset.</p><p>The evaluation metrics include AR@AN curve for temporal action proposal and mAP for action localization. AR@F=1.0 is reported for comparing proposal performance on different subsets. The validation set is used for testing as the test set is not publicly available.</p><p>To train TURN, we set the number of frames in a unit n u to be 16, the context unit number n ctx to be 4, L to be 6 and λ to be 2.0. We build the temporal window pyramid with {2, 4, 8, 16, 32, 64, 128} number of units. The NMS threshold is set to be 0.1 smaller than tIoU in evaluation. For the temporal action localizer, SVM classifiers are trained with two-stream CNN features in "Sports" and "Works" subsets.</p><p>Generalization capability of TURN. One important property of TAP is the expectation to generalize beyond the categories it is trained on.</p><p>On ActivityNet v1.2, we follow the same evaluation protocol from <ref type="bibr" target="#b3">[4]</ref>: model trained on THUMOS-14 validation set and tested in three different sets of ActivityNet v1.2: the whole set of ActivityNet v1.2 (all 100 categories), Activi-tyNet v1.2 ∩ THUMOS-14 (on 9 categories shared between the two) and ActivityNet v1.2 1024 frames (videos with unseen categories with annotations up to 1024 frames). To avoid any possible dataset overlap and enable direct comparison, we use C3D (pre-trained on Sports1M) as feature extractor, the same as DAPs did. As shown in <ref type="figure">Figure 7</ref>, TURN has better generalization capability in all three sets.</p><p>On ActivityNet v1.3, we implement a different setup for evaluating generalization capability on subsets that contain semantically distinct actions: (1) we train TURN on one subset and test on the other three subsets, (2) we train on the ensemble of all 4 subsets and test on each subset. TURN is trained with C3D unit features, to avoid any overlap of training data. We also report performance of sliding win- As can be seen in <ref type="table" target="#tab_3">Table 4</ref>, the overall generalization capability is strong. Specifically, the generalization capability when training on "Sports" subset is the best compared with other subsets, which may indicate that more training data would lead to better generalization performance. The "Ensemble" row shows that using training data from other subsets would not harm the performance of each subset.</p><p>TURN for temporal action localization. Temporal action localization performance is evaluated and compared on "Works" and "Sports" subsets of ActivityNet v1.1. TURN trained with dense flow CNN features is used for comparison. On v1.1, TURN-FL-16 proposal is fed into one-vs-all SVM classifiers which trained with two-stream CNN features. From the results shown in <ref type="table" target="#tab_4">Table 5</ref>, we can see that TURN proposals improve localization performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel and effective Temporal Unit Regression Network (TURN) for fast TAP generation. We proposed a new metric for TAP: Average Recall-Proposal Frequency (AR-F). AR-F is robustly correlated with temporal action localization performance and it allows performance comparison among different datasets. TURN can runs at over 880 FPS with the state-of-the-art AR performance. TURN is robust on different visual features, including C3D and dense flow CNN features. We showed the effectiveness of TURN as a proposal generation stage in localization pipelines on THUMOS-14 and ActivityNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of Temporal Unit Regression Network (TURN). A long video is decomposed into short video units, and CNN features are calculated for each unit. Features from a set of contiguous units, called a clip, are pooled to create clip features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>clip (i.e. window) c is composed of units, c = {u j } su+nc su , where s u is the index of starting unit and n c is the number of units inside c. e u = s u + n c is the index of ending unit, and {u j } eu su is called internal units of c. Besides the internal units, context units for c are also modeled. {u j } su su−nctx and {u j } eu+nctx eu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a)-(c) show the correlation between temporal action localization performance and TAP performance under different metrics. (d) shows correlation coefficient between temporal action localization and TAP performance versus video length on THUMOS-14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of TURN variants on THUMOS-14 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative examples of retrieved proposals by TURN on THUMOS-14 dataset. GT indicates ground truth. TP and FP indicate true positive and false positive respectively. "reg prop" and "cls prop" indicate regression proposal and classification proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>14 Figure 7 .</head><label>147</label><figDesc>DAPs ActivityNet ∩ THUMOS-14 TURN-C3D-16 ActivityNet TURN-C3D-16 ActivityNet 1024 frames TURN-C3D-16 ActivityNet ∩ THUMOS-Comparison of generalizability on ActivityNet v1.2 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Run-time and AR Comparison on THUMOS-14.</figDesc><table><row><cell>method</cell><cell cols="2">AR@F=1.0 (%) FPS</cell></row><row><cell>DAPs [4]</cell><cell>35.7</cell><cell>134.3</cell></row><row><cell>SCNN-prop [23]</cell><cell>38.3</cell><cell>60.0</cell></row><row><cell>Sparse-prop [2]</cell><cell>33.3</cell><cell>10.2</cell></row><row><cell>TURN-FL-16</cell><cell>43.5</cell><cell>129.4</cell></row><row><cell>TURN-FL-32</cell><cell>42.4</cell><cell>260.6</cell></row><row><cell>TURN-C3D-16</cell><cell>39.3</cell><cell>880.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Temporal action localization performance (mAP %</cell></row><row><cell cols="4">@tIoU=0.5) evaluated on different proposals on THUMOS-14.</cell></row><row><cell></cell><cell cols="3">DAPs SVM[4] Our SVM S-CNN</cell></row><row><cell>Sparse-prop[2]</cell><cell>7.8</cell><cell>8.1</cell><cell>15.3</cell></row><row><cell>DAPs[4]</cell><cell>13.9</cell><cell>9.5</cell><cell>16.3</cell></row><row><cell>SCNN-prop[23]</cell><cell>7.6 2</cell><cell>14.0</cell><cell>19.0</cell></row><row><cell>TURN-C3D-16</cell><cell>-</cell><cell>16.4</cell><cell>22.5</cell></row><row><cell>TURN-FL-16</cell><cell>-</cell><cell>17.8</cell><cell>25.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Temporal action localization performance (mAP %) comparison at different tIoU thresholds on THUMOS-14. Oneata et al.[19] 36.6 33.6 27.0 20.8 14.4 Yeung et al.[33] 48.9 44.0 36.0 26.4 17.1 Yuan et al.</figDesc><table><row><cell>tIoU</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell>[35]</cell><cell></cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell cols="3">S-CNN [23]</cell><cell></cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell cols="4">TURN-C3D-16 + SVM</cell><cell cols="5">46.4 41.5 34.3 24.9 16.4</cell></row><row><cell cols="3">TURN-FL-16 + SVM</cell><cell></cell><cell cols="5">48.3 43.2 35.1 26.2 17.8</cell></row><row><cell cols="9">TURN-C3D-16 +S-CNN 48.8 45.5 40.3 31.5 22.5</cell></row><row><cell cols="4">TURN-FL-16 + S-CNN</cell><cell cols="5">54.0 50.9 44.1 34.9 25.6</cell></row><row><cell cols="2">GT</cell><cell>TP reg prop</cell><cell cols="2">TP cls prop</cell><cell cols="2">FP reg prop</cell><cell cols="2">FP cls prop</cell></row><row><cell>57.2</cell><cell>57.6</cell><cell></cell><cell></cell><cell></cell><cell>61.9</cell><cell></cell><cell>63.1</cell><cell>63.9</cell><cell>Time</cell></row><row><cell>8.6</cell><cell>10.4 11.1</cell><cell></cell><cell>12.9</cell><cell>14.2</cell><cell>33.3 34.0 34.9</cell><cell></cell><cell cols="2">37.2 38.4</cell><cell>Time</cell></row><row><cell>29.9</cell><cell>33.1</cell><cell>35.7</cell><cell>37.4</cell><cell cols="2">115.2 116.7</cell><cell></cell><cell cols="2">121.6 122.5</cell><cell>123.9</cell><cell>Time</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Proposal generalization performance (AR@F=1.0 %) of TURN-C3D-16 on different subsets of ActivityNet. AR@F=1.0) are reported inTable 4. The left-most column lists subsets used for training. The numbers of action classes and training videos with each subset are shown in brackets. The top row lists subsets for test. The off-diagonal elements indicate that the training data and test data are from different subsets; the diagonal elements indicate the training data and test data are from the same subsets.</figDesc><table><row><cell></cell><cell>Arts</cell><cell cols="3">Housework Vehicles Sports</cell></row><row><cell>Sliding Windows</cell><cell>24.44</cell><cell>27.63</cell><cell>27.59</cell><cell>25.72</cell></row><row><cell>Arts (23; 685)</cell><cell>44.30</cell><cell>44.38</cell><cell>40.85</cell><cell>38.43</cell></row><row><cell>Housework (10; 373)</cell><cell>40.27</cell><cell>44.30</cell><cell>38.65</cell><cell>36.54</cell></row><row><cell>Vehicles (5; 238)</cell><cell>38.43</cell><cell>40.05</cell><cell>42.22</cell><cell>30.70</cell></row><row><cell>Sports (26; 1294)</cell><cell>43.26</cell><cell>43.58</cell><cell>41.40</cell><cell>46.62</cell></row><row><cell>Ensemble (64; 2590)</cell><cell>45.30</cell><cell>48.12</cell><cell>42.33</cell><cell>46.72</cell></row><row><cell cols="5">dows (lengths of 32, 64, 128, 256, 512, 1024 and 2048,</cell></row><row><cell cols="5">overlap 50% ) in each subset. Average recall at frequency</cell></row><row><cell>1.0 (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Temporal action localization performance (mAP%</cell></row><row><cell cols="3">@tIoU=0.5) on ActivityNet v1.1</cell><cell></cell></row><row><cell>Subsets</cell><cell>[3]</cell><cell cols="3">[33] Sliding Windows TURN-FL-16</cell></row><row><cell>Sports</cell><cell cols="2">33.2 36.7</cell><cell>27.3</cell><cell>37.1</cell></row><row><cell>Work</cell><cell cols="2">31.1 39.9</cell><cell>29.6</cell><cell>41.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Newly released evaluation results from DAPs authors show that SCNN-prop<ref type="bibr" target="#b22">[23]</ref> outperforms DAPs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This number should be higher, as DAPs authors adopted an incorrect frame rate when using S-CNN proposals.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The lear submission at thumos 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
