<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quaternion Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Grassucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Cicero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Comminiello</surname></persName>
						</author>
						<title level="a" type="main">Quaternion Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latest Generative Adversarial Networks (GANs) are gathering outstanding results through a large-scale training, thus employing models composed of millions of parameters requiring extensive computational capabilities. Building such huge models undermines their replicability and increases the training instability. Moreover, multi-channel data, such as images or audio, are usually processed by real-valued convolutional networks that flatten and concatenate the input, losing any intra-channel spatial relation. To address these issues, here we propose a family of quaternion-valued generative adversarial networks (QGANs). QGANs exploit the properties of quaternion algebra, e.g., the Hamilton product for convolutions. This allows to process channels as a single entity and capture internal latent relations, while reducing by a factor of 4 the overall number of parameters. We show how to design QGANs and to extend the proposed approach even to advanced models. We compare the proposed QGANs with real-valued counterparts on multiple image generation benchmarks. Results show that QGANs are able to generate visually pleasing images and to obtain better FID scores with respect to their real-valued GANs. Furthermore, QGANs save up to 75% of the training parameters. We believe these results may pave the way to novel, more accessible, GANs capable of improving performance and saving computational resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative models including generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10]</ref> and variational autoecoders (VAEs) <ref type="bibr" target="#b22">[23]</ref> have been recently spectators of an increasing widespread development due to the massive availability of large datasets covering a large range of applications. The demand to learn such complex data distributions Authors are with the Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, Via Eudossiana 18, 00184 Rome, Italy, e-mail: {eleonora.grassucci, danilo.comminiello}@uniroma1.it leads to define models far from the original approach of a simple GAN, which was characterized by fully connected layers and evaluated on benchmark datasets such as the MNIST <ref type="bibr" target="#b9">[10]</ref>. Multiple pathways have been covered to improve GANs generation ability. A first branch aims at stabilizing the training process which is notoriously unstable, often leading to a lack of convergence. This includes constraining the discriminator network to be 1-Lipschitz by introducing a gradient penalty in the loss function, normalizing the spectral norm of the network weights or adding a consistency regularization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. Other significant improvements are gained by architectural innovations such as self-attention modules, flexible activation functions or style-based generator <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref>. A crucial improvement in the quality of image generation has been brought by broadly scaling up the networks and involving wider batch sizes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Indeed, BigGAN closed the visual quality gap between GANs generated images and real-world samples in ImageNet <ref type="bibr" target="#b1">[2]</ref>. Most of the latest GANs are somehow inspired to it.</p><p>However, these impressive results come at the cost of huge models with hundred of millions of free parameters which require large computational resources. This drastically reduces the accessibility and the diffusion of these kind of models. Moreover, GANs are notorious fragile models, thus the training with this amount of parameters may result in unstable or less handy process. Furthermore, when dealing with multidimensional inputs, such as images, 3D audio, multi-sensor signals or human-pose estimation, among others, real-valued networks break the original structure of the inputs. Channels are processed as independent entities and just concatenated in a tensor without exploiting any correlation or intra-channel information.</p><p>In order to address these limitations, neural networks in hypercomplex domains have been proposed. Among them, quaternion neural networks (QNNs) leverage the properties of the non-commutative quaternion algebra to define lower-complexity models and preserve relations among channels. Indeed, QNNs process channels together as a single entity, thus maintaining the original input design and correlation. Due to this feature, QNNs are able to capture internal relations while saving up to the 75% of free parameters thanks to hypercomplex-valued operations, including the Hamilton product.</p><p>Encouraged by the promising results of other generative models in the quaternion domain <ref type="bibr" target="#b11">[12]</ref> and the need to make deep GANs more accessible, we introduce the family of quaternion generative adversarial networks (QGANs). QGANs are completely defined in the quaternion domain and, among other properties, they exploit the quaternion convolutions derived from the hypercomplex algebra <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref> to improve the generation ability of the model while reducing the overall number of parameters. We present the core-blocks to define a vanilla QGAN in the quaternion adversarial fashion and then explain how to derive more advanced QGANs to prove the improved generation ability of the proposed approach in multiple image generation benchmarks. We show that the quaternion spectral normalized GAN (QSNGAN) is able to earn a better FID and a more pleasant visual quality of the generated images with respect to its real-valued counterpart thanks to the quaternion inner operations. Moreover, the proposed QSNGAN has just 25% the number of free parameters with respect to the real-valued SNGAN.</p><p>We believe that these theoretical statements and empirical results lay the foundations for novel deep GANs in hypercomplex domains capable of grasping internal input relations while scaling down computational requirements, thus saving memory and being more accessible. To the best of our knowledge, this is the first attempt to define GANs in a hypercomplex domain.</p><p>The contribution of this chapter is threefold:</p><p>i) we introduce the family of quaternion generative adversarial networks (QGANs) proving their enhanced generation ability and lower-complexity with respect to its real-valued counterpart on different benchmark datasets; ii) we define the theoretically correct approach to apply the quaternion batch normalization (QBN) and redefine existing approaches as its approximations; iii) we propose and define the spectral normalization in the quaternion domain (QSN) proving its efficacy on two image generation benchmarks.</p><p>The chapter is organized as follows. Section 2 presents the fundamental properties of quaternion algebra, while Section 3 describes the quaternion adversarial framework and the quaternion-valued core blocks used in QGANs. Section 4 lays the foundations for the quaternion generative adversarial networks and presents a simple quaternion vanilla GAN and a more advanced and complex QGAN model. Section 5 proves the effectiveness of the presented QGANs on a thorough empirical evaluation, and, finally, conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quaternion Algebra</head><p>Quaternions are hypercomplex numbers of rank 4, being a direct non-commutative extension of complex-valued numbers. The quaternion domain H lies in a fourdimensional associative normed division algebra over real numbers, belonging to the class of Clifford algebras <ref type="bibr" target="#b35">[36]</ref>. A quaternion is defined as the composition of one scalar element and three imaginary ones:</p><formula xml:id="formula_0">= 0 + 1ˆ+ 2ˆ+ 3ˆ= 0 + q<label>(1)</label></formula><p>with 0 , 1 , 2 , 3 ∈ R and beingˆ= (1, 0, 0),ˆ= (0, 1, 0),ˆ= (0, 0, 1) unit axis vectors representing the orthonormal basis in R 3 . A pure quaternion is a quaternion without its scalar part 0 , resulting in the vector q = 1ˆ+ 2ˆ+ 3ˆ. As for complex numbers, also the quaternion algebra relies upon the relations among the imaginary components:ˆ2</p><formula xml:id="formula_1">=ˆ2 =ˆ2 = −1 (2)ˆ=ˆ×ˆ=ˆ;ˆˆ=ˆ×ˆ=ˆ;ˆˆ=ˆ×ˆ=ˆ( 3)</formula><p>While the scalar product of two quaternions and is simply defined as the element-wise product · = 0 0 + 1 1 + 2 2 + 3 3 , quaternion vector multiplication, denoted with ×, is not commutative, i.e.,ˆˆ≠ˆˆ. In fact:ˆ= −ˆˆ;ˆˆ= −ˆˆ;ˆˆ= −ˆˆ.</p><p>Due to the non-commutative property, we need to introduce the quaternion product, commonly known as Hamilton product. We will see that Hamilton product plays a crucial role in neural networks. It is defined as:</p><formula xml:id="formula_2">= ( 0 + 1ˆ+ 2ˆ+ 3ˆ) ( 0 + 1ˆ+ 2ˆ+ 3ˆ) = ( 0 0 − 1 1 − 2 2 − 4 4 ) + ( 0 1 + 1 0 + 2 3 − 4 3 )+ ( 0 2 − 1 3 + 2 0 + 4 2 )+ ( 0 3 + 1 2 − 2 1 + 4 1 )ˆ.<label>(4)</label></formula><p>The above product can be rewritten in a more concise form as:</p><formula xml:id="formula_3">= 0 0 − q · p + 0 p + 0 q + q × p,<label>(5)</label></formula><p>where 0 0 − q · p is the scalar element of the new quaternion in output and 0 p + 0 q + q × p is instead the vector part of the quaternion. From <ref type="bibr" target="#b4">(5)</ref> it is easy to define a concise form of product for pure quaternions too:</p><formula xml:id="formula_4">qp = −q · p + q × p.<label>(6)</label></formula><p>where the scalar product is the same as before for full quaternions and the vector product is q × p = ( 2 3 − 3 2 )ˆ+ (</p><formula xml:id="formula_5">3 1 − 1 3 )ˆ+ ( 1 2 − 2 1 )ˆ.</formula><p>Similarly to complex numbers, the complex conjugate of a quaternion can be defined as:</p><formula xml:id="formula_6">* = 0 − 1ˆ− 2ˆ− 3ˆ= 0 − q<label>(7)</label></formula><p>Also the norm is defined and it is equal to | | = √ * = √︃ 2 0 + 2 1 + 2 2 + 2 3 that is the euclidean norm in R 4 . Indeed, is said to be a unit quaternion if | | = 1, as well as a pure unit quaternion if 2 = −1. Moreover, a quaternion is endowed with an inverse determined by:</p><formula xml:id="formula_7">−1 = * | | 2 .</formula><p>Note that for unit quaternions, the relation * = −1 holds.</p><p>A quaternion has also a polar form:</p><formula xml:id="formula_8">= | | (cos ( ) + v sin ( )) = | |<label>(8)</label></formula><p>where ∈ R is the argument of the quaternion, cos ( ) = 0 / , sin ( ) = / and v = / is a pure unit quaternion. Following, quaternions show interestingly properties when they can be interpreted as points and hyperplanes in R 4 . Among them, we find involutions, which are generally defined as self-inverse mappings or mappings that are their own inverse. Quaternions have an infinite number of involutions <ref type="bibr" target="#b6">[7]</ref> that can be generalized by the formula:</p><formula xml:id="formula_9">v = −v v<label>(9)</label></formula><p>where is an arbitrary quaternion to be involved and v is any unit vector and the axis of the involution. Among the infinite involutions, the most relevant ones are the three perpendicular involutions defined as:</p><formula xml:id="formula_10">= −ˆˆ= 0 + 1ˆ− 2ˆ− 3= −ˆˆ= 0 − 1ˆ+ 2ˆ− 3= −ˆˆ= 0 − 1ˆ− 2ˆ+ 3ˆ(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10)</head><p>which are the first involutions identified <ref type="bibr" target="#b4">[5]</ref> and they are crucial for the study of the second-order statistics of a quaternion signal, as we will see in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Learning in the Quaternion Domain</head><p>In this section, we introduce the quaternion adversarial approach as well as the fundamental quaternion-valued operations employed to define the family of QGANs in next sections. It is worth noting that in a quaternion neural network each element is a quaternion, including inputs, weights, biases and outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Quaternion Adversarial Framework</head><p>Generative adversarial networks are built upon a minimax game between the generator network ( ) and the discriminator one ( ), as a special case of the concept initially proposed to implement artificial curiosity <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. They are trained in an adversarial fashion through the following objective function introduced in [10]:</p><formula xml:id="formula_11">min max ( , ) = E ∼ data ( ) {log ( )} + E ∼ ( ) {log(1 − ( ( )))} (11)</formula><p>where data is the real data distribution and is the noise distribution. The two terms in the objective are two cross-entropies <ref type="bibr" target="#b13">[14]</ref>. Indeed, the first term is the cross-entropy between [1 0] T and [ ( ) 1 − ( )] T , whereas the second term is the cross-entropy between [0 1] T and [ ( ( )) 1 − ( ( ))] T . In order to introduce the family of QGANs, first we need to delineate this adversarial approach in the quaternion domain. Thus, we redefine the cross-entropy function, as suggested in <ref type="bibr" target="#b26">[27]</ref>, by replacing real numbers with hypercomplex numbers and computing the operations element-wise. Thus, the quaternion cross-entropy (QCE) between the target quaternion and the estimated one˜can be defined as follows:</p><formula xml:id="formula_12">QCE( ,˜) = 1 ∑︁ =1 0 log(˜0) + (1 − 0 ) log(1 −˜0) + 1 log(˜1) + (1 − 1 ) log(1 −˜1) + 2 log(˜2) + (1 − 2 ) log(1 −˜2) + 3 log(˜3) + (1 − 3 ) log(1 −˜3) .<label>(12)</label></formula><p>More in general, several objective functions proposed to train GANs can be redefined in the quaternion domain. Among the most common ones, we find the Wasserstein distance with a gradient penalty that enforces the Lipschitz continuity of the discriminator, which is defined as follows <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>:</p><formula xml:id="formula_13">( , ) = E ∼ data { ( )} − E ∼ ( ) { ( ( ))} − Eˆ∼ˆ (||∇ˆ(ˆ)|| 2 − 1) 2 (13)</formula><p>where the last term is the gradient penalty that is a regularization technique for the discriminator.</p><p>Other works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3]</ref> consider instead the hinge loss, which is given, respectively for the discriminator and the generator, by:</p><formula xml:id="formula_14">( ,ˆ) = E ∼ data ( ) {min(0, −1 + ( ))} + E ∼ ( ) {min(0, −1 − ( ( )))} ,<label>(14)</label></formula><formula xml:id="formula_15">(ˆ, ) = −E ∼ ( ) ˆ( ( ))) .<label>(15)</label></formula><p>Being <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref> the composition of expected values and cross-entropies, both the definitions of the Wasserstein loss and of the hinge loss in the quaternion domain are straightforwardly derived by following the procedure shown for the adversarial loss in (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quaternion Fully Connected Layers</head><p>In real-valued neural networks, fully connected layers are generally defined as:</p><formula xml:id="formula_16">y r = (W r x r + b r )<label>(16)</label></formula><p>where W r x r performs the multiplication between the weight matrix W r and the input x r , b r is the bias and (·) is any activation function. In order to define the same operation in the quaternion domain, we represent the quaternion weight matrix as <ref type="bibr" target="#b15">(16)</ref>, is performed by a vector multiplication between two quaternions, i.e., by the Hamilton product W ⊗ x:</p><formula xml:id="formula_17">W = W 0 + W 1ˆ+ W 2ˆ+ W 3ˆ, the quaternion input as x = x 0 + x 1ˆ+ x 2ˆ+ x 3ˆa nd the quaternion bias as b = b 0 + b 1ˆ+ b 2ˆ+ b 3ˆ. Therefore, Wx in</formula><formula xml:id="formula_18">W ⊗ x = (W 0 x 0 − W 1 x 1 − W 2 x 2 − W 3 x 3 ) + (W 0 x 1 + W 1 x 0 + W 2 x 3 − W 3 x 2 )+ (W 0 x 2 − W 1 x 3 + W 2 x 0 + W 3 x 1 )+ (W 0 x 3 + W 1 x 2 − W 2 x 1 + W 3 x 0 )ˆ.<label>(17)</label></formula><p>Note that W has dimensionality 1 4 |W r | since it is composed of four submatrices W 0 , W 1 , W 2 and W 3 each one with 1/16 the dimension of W r . This is a key feature of QNNs since the results of the quaternion layer with product W ⊗ x has the same output dimension of the real-valued layer built upon W r x r but with 1/4 the number of parameters to train. Note also that the submatrices are shared over each component of the quaternion input. The sharing allows the weights to capture internal relations among quaternion elements since each charcteristic in a component will have an influence in the other components through the common weights. In this way the relations among components are preserved and captured by the weights of the network which is able to process inputs without losing intra-channel information. The bias b is then added with a sum component by component. Finally, in QNNs the activation functions are applied to the input element-wise resulting in the so called split activation functions. That is, suppose to consider a common Rectified Linear Unit (ReLU) activation function and the quaternion z = W ⊗ x + b, the final result y of the layer will be:</p><formula xml:id="formula_19">y = ReLU(z 0 ) + ReLU(z 1 )ˆ+ ReLU(z 2 )ˆ+ ReLU(z 3 )ˆ.<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quaternion Convolutional Layers</head><p>Convolutional layers are generally applied to multichannel inputs, such as images. Supposing to deal with color images, real-valued neural networks break the structure of the input and concatenates the red, green and blue (RGB) channels in a tensor. Quaternion-valued convolutions, instead, preserve the correlations among the channels and encapsulates the image in a quaternion as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37</ref>]:</p><formula xml:id="formula_20">x = 0 +ˆ+ˆ+ˆ(19)</formula><p>The image channels are the real coefficients of the imaginary units while the scalar part is set to 0. Encapsulating channels in a quaternion allows to treat them as a single entity and thus to preserve intra-channels relations. A visual explanation of the quaternion representation of color images is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-valued image</head><p>Quaternion-valued image Similarly to the definition of fully connected layers in the previous section, let us consider now a real-valued convolutional layer delineated by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-valued CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quaternion-valued QCNN</head><formula xml:id="formula_21">y = (W * x + b)<label>(20)</label></formula><p>where * is the convolution operator. Quaternion convolutional layers are built with the same procedure depicted for fully connected layers thus considering the Hamilton product instead of the standard vector multiplication. That is, the convolution operator W * x is replaced for quaternion weights and inputs with</p><formula xml:id="formula_22">W * x = (W 0 * x 0 − W 1 * x 1 − W 2 * x 2 − W 3 * x 3 ) + (W 0 * x 1 + W 1 * x 0 + W 2 * x 3 − W 3 * x 2 )+ (W 0 * x 2 − W 1 * x 3 + W 2 * x 0 + W 3 * x 1 )+ (W 0 * x 3 + W 1 * x 2 − W 2 * x 1 + W 3 * x 0 )ˆ.<label>(21)</label></formula><p>Note that in convolutional networks the sharing weights are crucial to properly process channels. Indeed, the RGB channels of an image interact with each other by resulting in combined colors, such as yellow or violet, through a representation of pixels in the color space. Nonetheless, real-valued networks are not able to catch these interactions since they process input channels separately, while QCNNs not only preserves the input design but also capture these relations through the sharing of weights. Actually, QCNNs perform a double learning: the convolution operator has the task of learning external relations among the pixels of the image, while the Hamilton product accomplishes the learning among the channels. Furthermore, as for linear layers, QCNNs are built with 1/4 the number of parameters with respect to their real-valued counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Quaternion Pooling Layers</head><p>Many neural networks make use of pooling layers, such as max pooling or average pooling, to extract high-level information and reduce input dimensions. As done before for previous layers in the quaternion domain, also this set of operations can be redefined in the quaternion domain.</p><p>The simplest examples of pooling in the hypercomplex domain are average and sum poolings. Indeed, applying these operations to each quaternion component, as done for split activation function, will not affect the final result <ref type="bibr" target="#b36">[37]</ref>. A different approach must be defined, instead, for max pooling. Indeed, the maximum of a single component is not guaranteed by the maximum of all the other components. In order to address this issue, a guidance matrix has to be introduced. As in <ref type="bibr" target="#b36">[37]</ref>, the matrix is built through the quaternion amplitude and keeps trace of the maximum position, which is then mapped back to the original quaternion matrix in order to proceed with the pooling computation. However, max pooling operations are rarely employed in GANs, thus we only make use of average and sum pooling in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Quaternion Batch Normalization</head><p>Introduced in <ref type="bibr" target="#b18">[19]</ref>, batch normalization (BN) has immediately became an everpresent module in neural networks. The idea behind BN is to normalize inputs to have zero mean and unit variance. This normalization helps the generalization ability of the network among different batches of training data and between train and test data distribution. Moreover, reducing the internal covariate shift remarkably improves the training speed, thus leading to a faster convergence of the model. For these reasons, also QNNs are endowed with batch normalization. However, different versions of this method were proposed in literature. An elegant whitening procedure based on the standard covarince matrix is introduced in <ref type="bibr" target="#b7">[8]</ref>. Here, the Cholesky decomposition is used to compute the square root of the inverse of the covariance matrix, which is often intractable. The authors asserts that approach ensures zero mean, unit variance and decorrelation among components. However, the covariance matrix is not able to recover the complete second-order statistics in the quaternion domain <ref type="bibr" target="#b3">[4]</ref> and the decomposition requires heavy matrix calculations and computational time <ref type="bibr" target="#b17">[18]</ref>. Another remarkable approach is introduced in <ref type="bibr" target="#b33">[34]</ref>, where the input is standardized computing the average of the variance of each component. Nevertheless, describing the second-order statistics of a signal in the quaternion domain needs meticulous computations and the approach in <ref type="bibr" target="#b33">[34]</ref> is an approximation of the complete variance. Notwithstanding the approximation, this method allows to notably reduce computational time.</p><p>The proper theoretically procedure to reach a centered, decorrelated and unitvariance quaternion signal would be represented by performing a whitening procedure. Ideally, we should consider the covariance matrix and then decompose it to whiten the input in order to avoid computing the square root of the inverse which is often unfeasible. However, due to the interactions among components, second-order statistics for quaternion random variables are not completely described by the standard covariance matrix <ref type="bibr" target="#b3">[4]</ref>. For this reason, the augmented covariance matrix should be considered instead. Such matrix is augmented with the complementary covariance matrices C qq , C qq , C qq that are the covariance matrices of the quaternion with its three perpendicular involutions q , q , q . Thus, the augmented covariance matrix, which completely characterizes the second-order information of the augmented quaternion vectorq, is defined as:</p><formula xml:id="formula_23">C qq = E qq H =          C qq C qqˆCqqˆCqqĈ H qqˆC qˆqˆCqˆqˆCqˆqĈ H qqˆC qˆqˆCqˆqˆCqˆqĈ H qqˆC qˆqˆCqˆqˆCqˆqˆ        <label>(22)</label></formula><p>where (·) H is the conjugate transpose operator. The formulation in <ref type="bibr" target="#b21">(22)</ref> recovers the complete statistical information of a general quaternion signal. Thus, the theoretically procedure should be delineated as:</p><formula xml:id="formula_24">x =C −1/2 qq (x − E {x})<label>(23)</label></formula><p>or substituting the inverse square rootC −1/2 qq with a decomposition of it. However, the construction of the augmented covariance matrix may be quite difficult and computational expensive due to the computation of each sub-covariance matrix. Moreover, C −1/2 qq includes skew-symmetric sub-matrices <ref type="bibr" target="#b3">[4]</ref>, which make the decomposition more difficult.</p><p>In order to simplify the calculation of (22) and make it more feasible for practical applications, a particular case can be considered by leveraging the Q-properness property <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. The Q-properness entails that the quaternion signal is not correlated with its involutions, implying vanishing complementary covariance matrices, i.e., C qq = C qq = C qq = 0. Also, for Q-proper random variables the following relation holds:</p><formula xml:id="formula_25">var {q } = E q 2 = 2 , = {0, 1, 2, 3}<label>(24)</label></formula><p>Thus, considering a Q-proper quaternion, the covariance in <ref type="formula" target="#formula_23">(22)</ref> becomes:</p><formula xml:id="formula_26">C qq = E qq H =         C qq 0 0 0 0 C qˆqˆ0 0 0 0 C qˆqˆ0 0 0 0 C qˆqˆ        = 4 2 I<label>(25)</label></formula><p>Assuming Q-properness for a random variable saves a lot of calculations and computational costs. Notwithstanding the theoretical correctness of the above defined approach, quaternion batch normalization (QBN) techniques adopted so far in the literature relies in some approximations.</p><p>We assume the input signal is Q-proper, thus we consider the covariance in (25) and build the normalization as follows:</p><formula xml:id="formula_27">x = x − √︁ var {x} + = x − √ 4 2 +<label>(26)</label></formula><p>where is the quaternion input mean value, which is a quaternion itself, and it is defined as:</p><formula xml:id="formula_28">= 1 ∑︁ =1 0, + 1,ˆ+ 2,ˆ+ 3,ˆ=¯0 +¯1ˆ+¯2ˆ+¯3ˆ.<label>(27)</label></formula><p>The final output is computed as follows:</p><formula xml:id="formula_29">QBN(x) = x +<label>(28)</label></formula><p>where is a shifting quaternion parameter and is a scalar parameter.</p><p>In conclusion, the QBN proposed by <ref type="bibr" target="#b7">[8]</ref> is an elegant approximation, nevertheless it is not able to catch the complete second-order statistics information, while requiring heavy computations <ref type="bibr" target="#b17">[18]</ref>. Thus, we believe that considering Q-proper signals, which are indeed very frequent, is a good approximation which also extremely reduces the computational requirements. For our experiments, we adopt the method represented by (28).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Quaternion Spectral Normalization</head><p>Among the wide variety of proposed techniques to stabilize GANs traning, the spectral normalization (SN) <ref type="bibr" target="#b24">[25]</ref> is one of the most widespread method. Previously, the crucial importance of having a Lipschitz-bounded discriminator function was introduced in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Lately, it was proved that no restriction on the discriminator space leads to the gradient uninformativeness problem <ref type="bibr" target="#b39">[40]</ref>. This means that the gradient of the optimal discriminative function has no information about the real distribution, thus providing useless feedbacks to the generator. Forcing a function to be Lipschitz continuous means controlling how fast it increases and bound the gradients, thus mitigating gradient explosions <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b10">11]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, a method based on weight clipping was proposed to force the discriminator to be 1-Lipschitz. Later, such approach has been improved by adding a gradient penalty (GP) that constraints the gradient norm to be at most 1 <ref type="bibr" target="#b14">[15]</ref>. The latter method is reproposed in several state-of-the-art GANs and combined with other regularization techniques to improve performance, as suggest <ref type="bibr" target="#b23">[24]</ref>. However, being built on the gradients with respect to the inputs, the gradient penalty cannot impose a regularization outside the support of the fake and real data distribution. Moreover, it requires consistent computations. The spectral normalization, instead, directly operates on the weights of the network being free of the support limit and its computations is faster than other methods <ref type="bibr" target="#b24">[25]</ref>. It aims at controlling the Lipschitz constant of the discriminator by constraining the spectral norm of each layer.</p><p>A generic function is -Lipschitz continuous if, for any two points 1 , 2 , the following property holds:</p><formula xml:id="formula_30">( 1 ) − ( 2 ) | 1 − 2 | ≤<label>(29)</label></formula><p>being · the 2 norm. The Lipschitz norm Lip of a function is equal to sup (∇ ( )), where (·) is the spectral norm of the matrix in input, that is, the largest singular value of the matrix.</p><p>For a generic linear layer (ℎ) = Wx + b, the Lipschitz norm is:</p><formula xml:id="formula_31">Lip = sup ℎ (∇ (ℎ)) = sup ℎ (W) = (W)<label>(30)</label></formula><p>Assuming the Lipschitz norm of each layer activation being equal to 1, constraint that is satisfied for many popular activation functions including ReLU and Leaky ReLU <ref type="bibr" target="#b24">[25]</ref>, we can apply the Lipschitz bound to the whole network by following</p><formula xml:id="formula_32">1 • 2 Lip ≤ 1 Lip · 2 Lip .</formula><p>Finally, the SN is defined as¯(</p><formula xml:id="formula_33">W) = W (W)<label>(31)</label></formula><p>and it ensures that the weight matrix W always satisfies the constraint (W) = 1. In <ref type="bibr" target="#b24">[25]</ref> the authors underline that applying the original singular value decomposition algorithm to compute (W) may result in an extremely heavy algorithm. To address the computational complexity, they suggest to estimate the largest singular value via the power iteration method. In order to control the Lipschitz constraint in a QGAN, in this section we explore two methods to define the spectral normalization in the quaternion domain. A first approach aims at normalizing the weights W by operating on each submatrix W 0 , W 1 , W 2 , W 3 independently, by computing the spectral norm separately. That is, through the power iteration method as above, we compute 0 (W 0 ), 1 (W 1 ), 2 (W 2 ), 3 (W 3 ) and then normalize each submatrix with the corresponding norm. This method forces each submatrix to have spectral norm equal to 1. However, it never takes the whole weight matrix W into account. Moreover, the relations among the components of the quaternion matrix is not considered, losing the characteristic property of QNNs.</p><p>The second method, similarly to the real-valued SN, normalizes the whole matrix W together, by imposing the constraint to the complete matrix and not to the singular submatrices. Therefore, the spectral norm is computed by taking the complete weight matrix into account and considering the relations among the quaternion components. However, while the spectral norm is computed as in <ref type="formula" target="#formula_31">(30)</ref>, the normalization step is applied differently from the SN in <ref type="bibr" target="#b30">(31)</ref>. Instead of normalizing the whole matrix as in <ref type="formula" target="#formula_0">(31)</ref>, being the weight matrix W designed by a composition of the submatrices W 0 , W 1 , W 2 , W 3 , we can leverage this quaternion setup to save computational costs and normalize each submatrix W 0 , W 1 , W 2 , W 3 . The normalized subma-tricesW 0, ,W 1, ,W 2, ,W 3, will result in a normalized weight matrix¯(W) with a more efficient computation than normalizing the full matrix W.</p><p>An empirical comparison between the two methods is reported in Section 5. We investigate the two techniques in a plain QGAN and prove that the latter approach is stabler and gains better performance in both the datasets considered. We deem it more appropriate both theoretically and empirically and we use it in our further experiments. From now on, we refer to such approach as the quaternion spectral normalization (QSN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Quaternion Weight Initialization</head><p>Weight initialization has often a crucial role in networks convergence and in the reduction of the risk of vanishing or exploding gradients <ref type="bibr" target="#b8">[9]</ref>. This procedure becomes even more important when dealing with quaternion weights. Indeed, due to the interactions among the elements of the quaternion, the initialization cannot be random nor component-aware. For these reasons, an appropriate initialization has to be introduced.</p><p>First, consider a weight matrix W with E {|W|} = 0. The initialization is based on a normalized pure quaternion generated for each weight submatrix from a uniform distribution in [0, 1]. By using the polar form of a quaternion, we can define the initialization of the weight matrix as</p><formula xml:id="formula_34">W = |W| = |W|(cos( ) + sin( ))<label>(32)</label></formula><p>where each matrix component is initialized as</p><formula xml:id="formula_35">W 0 = cos( ) W 1 = 1 sin( ) W 2 = 2 sin( ) W 3 = 3 sin( )<label>(33)</label></formula><p>where the angle is randomly generated in the interval [− , ] and is randomly sampled in the interval of the standard deviation around zero [− , ]. The standard deviation is set according to the initialization method chosen, either <ref type="bibr" target="#b8">[9]</ref> or <ref type="bibr" target="#b15">[16]</ref>. In the first case, we set = 1/ √︁ 2( + ) whereas in the latter we set = 1/ √ 2 . In both the equations, is the number of neurons in the input layer and the number of neurons in the output layer. The variance of W can be written as:</p><formula xml:id="formula_36">var {W} = E |W| 2 − E {|W|} 2 .<label>(34)</label></formula><p>However, similarly to the QBN in the previous section, in order to reduce the computations, the component E {|W|} 2 can be considered equal to 0 <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. This is equivalent to considering a Q-proper quaternion signal whose augmented covariance matrix has off-diagonal elements equal to 0 and trace equal to 4 2 . Consequently, the variance is computed by considering only the first term of (34) as:</p><formula xml:id="formula_37">var {W} = E |W| 2 = 4 2<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Training</head><p>The forward phase of a QNN is the same as its real-valued counterpart. Therefore, the input flows from the first to the last layer of the network. It may be interesting to note that in eq. (17) the order of the weight and the input can be inverted, thus changing the output of the product, resulting in an inverted QNN <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. For what concerns the backward phase, it worth mentioning that the gradient of a general quaternion loss function L is computed for each component of the quaternion weight matrix W as in the ensuing equation:</p><formula xml:id="formula_38">L W = L W 0 + L W 1ˆ+ L W 2ˆ+ L W 3ˆ.<label>(36)</label></formula><p>Then, the gradient is propagated back following the chain rule. Indeed, as defined in <ref type="bibr" target="#b26">[27]</ref>, the backpropagation of quaternion neural networks is just an extension of the method for their real-valued counterpart. Consequently, QNNs can be easily trained as real-valued networks via backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GAN Architectures in the Quaternion Domain</head><p>The previous section described the main blocks and the framework to build and train a GAN in the quaternion domain. In this section we go further, presenting the complete definition of a plain QGAN in Subsection 4.1 and of an advanced stateof-the-art QGAN composed of complex blocks in Subsection 4.2. First, in order to setting up a QGAN, each input, weight, bias and output has to be manipulated to become a quaternion. Therefore, weight matrices are initialized as composed by the four submatrices, similarly to <ref type="bibr" target="#b16">(17)</ref> and <ref type="bibr" target="#b20">(21)</ref>. Real-valued operations such as multiplications or convolutions in the networks are replaced with their quaternion counterparts, completing the redefinition of the layers in the quaternion domain. The input is handled as a quaternion and processed as a single entity. For images, a pure quaternion is considered as in <ref type="formula" target="#formula_0">(19)</ref>, while for other kind of multidimensional signals, the scalar part is considered too. The initialization of the weights is then applied following the description in Section 3.7. This accurate definition of QGANs grants to design a model with a fewer number of free parameters with respect to the same real-valued model and consequently to save memory and computational requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Vanilla QGAN</head><p>In the original GAN <ref type="bibr" target="#b9">[10]</ref>, both the generator ( ) and the discriminator ( ) are defined by fully connected layers. Due to the limited expressivity of this design with complex data such as images, in <ref type="bibr" target="#b28">[29]</ref> the authors propose to replace dense layers with more suitable operations for this kind of data and to build and by stacking several convolutional layers. State-of-the-art GANs are based on the deep convolutional GAN (DCGAN) <ref type="bibr" target="#b28">[29]</ref>. In particular, the DCGAN increases the spatial dimensionality by means of transposed convolutions in the generator and decreases it in the discriminator with convolutions. Furthermore, this architecture defines batch normalization in every layer except for the last layer of and for the first layer of , in order to let the networks learn the correct statistics of the data distribution.</p><p>By redefining the DCGAN in the quaternion domain (QDCGAN) it is possible to explore the potential of the quaternion algebra in a simple GAN framework. The QDCGAN generator is defined by an initial quaternion fully connected layer and then by interleaving quaternion transposed convolutions with quaternion batch normalization and split ReLU activation functions except for the last layer which ends up with a split Tanh function. The discriminator has the same structure of the generator but with quaternion transposed convolutions replaced by quaternion convolutions to decrease the dimensionality and with a final fully connected quaternion layer that returns as output the real/fake decision by means of a sigmoid split activation. The QDCGAN, as its real-valued counterpart, optimizes the original loss in <ref type="bibr" target="#b10">(11)</ref>.  <ref type="figure">Fig. 2</ref> Quaternion Vanilla GAN architecture. Each parameter including inputs, weights and outputs is a quaternion. The generator (green network) takes a quaternion noise signal and generates a batch of quaternion images with four channels. The discriminator tries to distinguish between fake and real quaternion samples exploiting the properties of quaternion algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Advanced QGAN</head><p>The above presented Vanilla QGAN is just a plain example to give a general idea on how to build GANs in the quaternion domain. In this section, we consider a more advanced model, the spectral normalized GAN (SNGAN) <ref type="bibr" target="#b24">[25]</ref> and we present the steps to define its quaternion counterpart. The quaternion spectral normalized GAN (QSNGAN) is trained in an adversarial fashion through the hinge loss defined in <ref type="bibr" target="#b13">(14)</ref> and <ref type="bibr" target="#b14">(15)</ref> for the discriminator and generator respectively, as suggested in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3]</ref>. The overall architecture of the model is inspired by <ref type="bibr" target="#b2">[3]</ref>. Both the generator and discriminator networks are characterized by quaternion convolutional layers in order to leverage the properties of the Hamilton product. To mitigate the vanishing gradient problem and obtain better performance, a series of residual blocks with upsampling in the generator and downsampling in the disciminator can be adopted <ref type="bibr" target="#b24">[25]</ref>. A scheme of the residual block of the proposed QSNGAN is depicted in <ref type="figure">Fig. 3</ref>  <ref type="figure">Fig. 3</ref> Quaternion residual block (QResBlock) architecture inspired by <ref type="bibr" target="#b24">[25]</ref> and redefined in the quaternion domain. QBN is omitted in the discriminator network and replaced by QSN. Grey blocks means they are used exclusively in teh generator or in the discriminator. The generator considers the umpsampling steps in the residual and in the shortcut pass while the discriminator the average pooling ones, except for the last residual block of the discriminator which keep the dimension invariant. takes in input the two sets of quaternion images with four channels in a first residual block, as illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. The output of the block is the decision on whether they come from the fake or real distribution. In order to guarantee a fair comparison with the SNGAN, we consider a realvalued noise signal in input to the generator and handle it with an initial real-valued fully connected layer. The output of the first layer is then encapsulated in a quaternion signal with a procedure similar to the one considered in Subection 3.3 to handle colored images. The signal is then processed by the quaternion generator up to the last layer, which generates the four-channel fake image. The original SNGAN considers batch normalization in the generator and spectral normalization in the discriminator. We keep the same structure and consider the proposed QBN in <ref type="bibr" target="#b27">(28)</ref> for the first network and the QSN introduced in Section 3.6 for the discriminator. In particular, we exploit the QSN with spectral norm computed over the whole weight matrix, which is theoretically better and ensures stabler results.</p><p>The definition of the SNGAN in the quaternion domain allows to save parameters, as we will explore in the next section. Moreover, the QSNGAN, processing the channels as a single entity through the quaternion convolutions based on the Hamilton product, is able to capture the relations among them and to capture any intra-channel information, which the SNGAN, conversely, loses. The latter property turns into an improved generation ability by the QSNGAN that properly grasps the real data    <ref type="figure">Fig. 3</ref>. The generator outputs a quaternion-valued sample of images that, together with a sample from the real distribution, goes to the input of the discriminator network (bottom). It handles the samples through a series of residual blocks (the first one is illustrutade in <ref type="figure" target="#fig_1">Fig. 4</ref>, the other ones in <ref type="figure">Fig. 3</ref>) up to the last layer which outputs the real or fake decision.</p><p>distribution. The architecture of the proposed QSNGAN is reported in <ref type="figure" target="#fig_3">Fig. 5</ref>. In the scheme, the forward phase flows from left to right for the top network (quaternion generator) and from right to left for the second network (quaternion discriminator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>In order to evaluate the performance of the generative networks, we consider two objective metrics, the Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">[17]</ref> as main metric, as it is more consistent with human evaluations, and the Inception Score (IS) <ref type="bibr" target="#b29">[30]</ref>. The Fréchet inception distance embeds the generated and the real samples into the Inception convolutional features and models the two distributions as Gaussian signals evaluating the empirical means , data and covariances C , C data and then computes the Fréchet distance as:</p><formula xml:id="formula_39">FID( , data ) = || − data || + Tr(C + C data − 2(C C data ) 1/2<label>(37)</label></formula><p>where Tr (·) refers to the trace operation. Being the FID a distance between real and fake distributions, the lower the FID value, the better the generated samples. Instead, the IS considers the inception model to get the conditional distribution ( | ) of the generated samples. IS expects the conditional distribution to have low entropy since the images represent meaningful objects, while the marginal distribution ( ) should have high entropy due to the diversity among the samples. It is defined as:</p><formula xml:id="formula_40">IS( ) = exp E ∼ {KL[ ( | )|| ( )]}<label>(38)</label></formula><p>where KL is the Kullback-Leibler divergence. Conversely to the FID, higher IS values stands for better generated samples. However, IS has some drawbacks since it does not consider the true data distribution and, moreover, it is not able to detect mode collapse, thus we consider the FID score as main metric and the IS in support to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>In order to evaluate the effectiveness of our proposed approach, we conduct a collection of experiments on the unsupervised image generation task. We take two datasets into account: the CelebA-HQ <ref type="bibr" target="#b19">[20]</ref>, which contains 27k images for training and 3k images for testing, and the 102 Oxford Flowers, which contains approximately 7k images for training and a few less then 1k images for testing. We reshape the samples of both the dataset to 128 × 128 and then test the real-valued SNGAN and the proposed QSNGAN. We use the Adam optimizer and keep the same hyper-parameters fixed as in <ref type="bibr" target="#b24">[25]</ref>, i.e., learning rate equal to 0.0002, and the optimizer parameters equal to 1 = 0.0, 2 = 0.9. We just vary the number of critic iterations, considering two experiments with critic iterations equal to 1 and then equal to 5 in order to better investigate the behavior of our QSNGAN, which may have a different balance between generator and discriminator networks with respect to the SNGAN. In every experiment, we fix the batch size to 64 and we perform 100k training iterations for the CelebA-HQ and 50k for the 102 Oxford Flowers. We have also considered to endow the SNGAN and the QSNGAN with a gradient penalty, as in <ref type="formula" target="#formula_0">(13)</ref>, but we did not notice any improvement in the experiments, thus meaning that both the SN and the QSN adequately control the discriminator to be Lipschitz continuous. The QSNGAN generator is a quaternion convolutional network as in <ref type="figure" target="#fig_3">Fig. 5</ref>. The initial fully connected layer, which takes the noise of size 128 in input, is composed of 4 × 4 × 1024 neurons. The following quaternion residual blocks illustrated in <ref type="figure" target="#fig_0">Fig. 3 stack 1024</ref>, 512, 256, 128 and 64 filters. This means that, as an example, the first residual block is built by interleaving QBNs, split ReLUs and quaternion convolutions with 1024 kernels and an upsampling module with scale factor equal to 2. Further, at the end of the last residual connection, we stack a QBN, a split ReLU activation function and a final quaternion convolutional layer of dimension 64 to refine the output image, which is then passed to a split Tanh function to bound it in the interval [−1, 1]. For each quaternion convolution, we fix the kernel size to 3 and the stride and the padding to 1. Conversely, the shortcut in the residual block is composed of an upsampling module and a quaternion convolution with kernel size equal to 1 and null padding. The network built through this procedure has less <ref type="table">Table 1</ref> Summary of number of networks parameters and memory requirements for real-valued SNGAN and its quaternion-valued counterpart QSNGAN models for CelebA-HQ. The proposed method saves more than the 70% of total free parameters and memory disk for model checkpoints. than 10M of free parameters with respect to the 32M parameters of its real-valued counterpart. This means that the checkpoint for inference saves more than the 70% of disk memory, as shown in <ref type="table">Table 1</ref>. The QSNGAN discriminator is still a quaternion convolutional network as in <ref type="figure" target="#fig_3">Fig. 5</ref>, but it is slightly more complex. At the beginning, the real images are encapsulated in a quaternion as depicted in Subsection 3.3, resulting in a batch of four-channel images. Obviously, the images generated by the generator network are already comprised of four channels and defined as quaternions.</p><p>The first residual block of the discriminator in <ref type="figure" target="#fig_1">Fig. 4</ref> is a spectrally-normalized quaternion convolution block with 64 3×3 filters and split ReLU activation functions. The shortcut, instead, as for the generator network, is a 1 × 1 quaternion convolution with padding equal to 0. In this case, however, both the residual and the shortcut part ends with a 2 × 2 split average pooling. The images flow then to a stack of five residual blocks built as in <ref type="figure">Fig. 3</ref> with, respectively, 128, 256, 512, 1024 and 1024 filters. Nevertheless, the residual section of each block has a split average pooling to operate downsampling and the shortcut is comprised of a quaternion convolution and another average pooling. The downsampling procedure is applied in each residual block except for the last one, which is a refiner and leaves the dimensionality unchanged. Every weight is normalized through the QSN introduced in Subsection 3.6. The configurations for kernel size, stride and padding are the same of the generator. At the end of the residual block stack, we apply a split ReLU and a global sum pooling before passing the batch to the final spectrally-normalized fully connected layer which, by means of a sigmoid, returns the real/fake decision. As for the generator, also the quaternion discriminator allows to save parameters while learning the internal relations among channels. This saving is underlined in <ref type="table">Table 1</ref>, which reports the exact number of parameters for the quaternion model and the real-valued one. The quaternion GAN can obtain equal or better results when trained with less parameters since it leverages the properties of quaternion algebra, including the Hamilton product, that allow to capture also the relations among channels and catch more information on the input. Consequently, the training procedure needs less parameters to learn the real distribution and to generate images from it.</p><p>The objective evaluation is reported in <ref type="table">Table 2</ref>. We perform the computations of FID and IS on the test images (3k for the CelebA-HQ and slightly less than 1k for the 102 Oxford Flowers). As shown in <ref type="table">Table 2</ref>, the proposed method stands out in the generation of samples from both the dataset according to the metrics considered. <ref type="table">Table 2</ref> Results summary for the 128 × 128 CelebA-HQ and 102 Oxford Flowers datasets. The proposed QSNGAN obtains a lower FID in each dataset considered. The vlaues of the IS support the FID results. According to the objective metrics, the proposed QSNGAN generates more visually pleasant and diverse samples with respect to the real-valued baseline counterpart. The QSNGAN seems to be more robust to the choice of the hyper-parameter regarding the discriminator iterations (Critic iter) while the real-valued model fails when changing the original setting which fixes the parameter equal to 5. Moreover, the two QSNGANs with critic iterations 1 and 5 score a lower FID with respect to the best configuration of the SNGAN model. The proposed method performs better with one critic per generator iterations, while the real-valued model fails with this configuration. Overall, the QSNGAN seems to be more robust to the choice of the critic iterations with respect to the SNGAN, which is more fragile. The IS strengthen the results obtained with the FID, as it reports higher scores for the proposed method in every dataset. The visual inspection of the generated samples underlines the improved ability of our QSNGAN. <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> show a randomly selected 128 × 128 batch of generated images for the real-valued SNGAN and the proposed QSNGAN, respectively. On one hand, SNGAN seems to be quite unstable and prone to the input noise, thus alternating some good quality images with bad generated ones. Overall, the SNGAN is not always able to distinguish the background from some parts of the character, sometimes confusing attributes such as the neck or the hair as part of the environment, and letting them vanishing. On the other hand, the QSNGAN sample in <ref type="figure" target="#fig_5">Fig. 7</ref> shows visually pleasant images, with a clear distinction between subject and background. It also shows a higher definition of faces attributes, including the most difficult ones, such as eyebrows, beard or skin shades. In addition, colors seem to be more vivid and samples are diverse in terms of pose, genre, expression, and hair color, among others. Concerning the second dataset, the generated samples for the SNGAN are shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, while the batch from the QSNGAN is reported in <ref type="figure" target="#fig_7">Fig. 9</ref>. As it is clear from <ref type="table">Table 2</ref>, the results for this dataset are preliminary but encouraging. Even in this case the proposed approach gains a lower FID and a higher IS than the real-valued model. Additionally, in SNGAN samples pixels are evident and often misleading, thus confusing the flower object with the colored background. On the other hand, the images generated from our QSNGAN contain more distinct subjects. Furthermore, the proposed method better catches every color shade thanks to the quaternion algebra properties, which allow the network learning internal relations among channels without losing intra-channel information.  In conclusion, the proposed quaternion-valued QSNGAN shows an improved ability in capturing the real data distribution by leveraging the quaternion algebra  properties in each experiment we conduct. It can generate better and more vivid samples according to visual inspections and to objective metrics with respect to its real-valued counterpart. Furthermore, the proposed method has less than the 30% of free parameters with respect to the SNGAN which also has worse generation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of Spectral Normalization Methods</head><p>This section reports the tests we conduct to evaluate the two quaternion spectral normalization methods described in Subsection 3.6. In order to investigate the performance of the normalizing approaches, we validate two smaller models with respect to the ones introduced in the previous subsection on the CIFAR10 and STL10 datasets. CIFAR10 contains 50k 32 × 32 images for training and 10k for testing while STL10 has 105k 48 × 48 images in the train split and 8k in the test one.</p><p>We examine three different configurations: the first one does not involve any QSN method, thus the discriminator network is not constrained to be 1-Lipscithz. We run this experiment in order to check the effectiveness of the spectral normalization methods that we propose. The second configuration applies a split computation of the spectral norm for each quaternion component and normalize each weight submatrix W 0 , W 1 , W 2 , W 3 independently. The last approach computes the spectral norm of the whole weight matrix and uses it to normalize each component. Respectively, we refer to these methods as No QSN, QSN Split and QSN Full.</p><p>To assess the performance, we build the same SNGANs presented in <ref type="bibr" target="#b24">[25]</ref> by redefining them in the quaternion domain. We adopt the quaternion core residual blocks we define in the previous section and in <ref type="figure">Fig. 3</ref>, while reducing the model dimension. For CIFAR10, we set up a generator with the initial linear layer 4×4×256 and then pile up three quaternion residual blocks, each one with 256 filters. As before, we end up with a stack of QBN, split ReLU and a quaternion convolution with a final split Tanh to generate the 32 × 32 images in the range [−1, 1]. The discriminator, in which the QSN methods act in each layer, begins with a first residual block ( <ref type="figure" target="#fig_1">Fig. 4)</ref> with 128 filters and then proceeds with three blocks composed of 128 kernels. As in <ref type="figure" target="#fig_3">Fig. 5</ref>, the network ends with a global sum pooling and a fully connected layer with sigmoid to output the decision probability. The so-defined QSNGAN for CIFAR10 is comprised of less than 2M parameters. It is worth noting that the real-valued counterpart presented in <ref type="bibr" target="#b24">[25]</ref> has more than 5M of free parameters.</p><p>The model to generate the 48 × 48 STL10 images is deeper than the previous one and is composed of 5,545,188 parameters. The structure is the same but it contains an initial layer of 6 × 6 × 512 and then the residual blocks with 256, 128 and 64 filters. The final refiner quaternion convolutional layers has 64 kernels. The discriminator, instead, has one residual blocks more than the model for CIFAR10 and the filters are, respectively from the first to the last block, 64, 128, 256, 512, 1024 with a final 512 fully connected layer with sigmoid.</p><p>As we can see in <ref type="table">Table 3</ref>, the unbounded model with no QSN fails in generating images from both CIFAR10 and STL10. Indeed, the FID is much higher than the other approaches. This proves the effectiveness of the proposed QSN full method which computes the spectral norm of each layer taking all the components into <ref type="table">Table 3</ref> Summary results for comparison of the two quaternion spectral normalization methods depicted in Section 3. We consider the SNGAN proposed in <ref type="bibr" target="#b24">[25]</ref> as baseline to define two simple models in the quaternion domain and then test the different QSN approaches. QSN Split refers to the first method that normalizes the submatrices independently while QSN Full stands for the normalization of the whole weight matrix together. No QSN is a model without any spectral normalization method. While the latter fails, the QSN Full generates better images according to the FID in both datasets. account. As a matter of fact, the proposed approach is capable to generate improved quality images in every experiment we conduct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we introduce the family of quaternion-valued GANs (QGANs) that leverages the properties of quaternion algebra. We have rigorously defined each core block employed to build the proposed QGANs, including the quaternion adversarial framework. Moreover, we have provided a meticulous experimental evaluation on different image generation benchmarks to prove the effectiveness of our method. We have shown that the proposed QGAN has an improved generation ability with respect to the real-valued counterpart, according to the FID and IS metrics and to a visual inspection. Moreover, our method saves up to the 75% of free parameters. We reckon that these results lay the foundations for novel deep GANs, thus capturing higher levels of input information and better grasping the real data distribution, while significantly reducing the overall number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Visual explanation of an , , image processed by real and quaternion-valued networks. On the left, the original three-channels image. The image can be processed in two ways: i) As a tensor of independent channels by a standard real-valued convolutional network as on the top of the figure. ii) As a single entity, encapsulating it in a quaternion, and considering internal relations among channels as quaternion-valued convolutional network does in the bottom of the figure. It is worth noting how the real-valued network does not consider any correlation among channels while quaternion ones preserve the relations among channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>First discriminator quaternion residual block (First QResBlock) with quaternion convolutions and average pooling layers to downsample the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fake</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>QSNGAN architecture schema. The generator network (top) takes in input a real-valued signal, processes it with a a fully connected layer and then encapsulates it in a quaternion signal. The residual blocks are depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Randomly generated samples from the real-valued SNGAN on the CelebA-HQ dataset after 100k training iterations. Sometimes this model fails to detect border attributes such as hair and neck which may fade on the background. Indeed, only few samples seem to be visually pleasant while in some other cases the network fails to generate likable images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Randomly generated samples from our QSNGAN on the CelebA-HQ dataset after 100k training iterations. These images are part of the test samples which gained a FID of 29.417 and IS 2.249 ± 0.164. The proposed method is able to generate visually pleasant images, well distinguishing the background from the face. Moreover, we do not observe mode collapse as samples have different attributes such as genre, hair color, pose and smile, among others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8</head><label>8</label><figDesc>Randomly generated samples from the SNGAN model on the 102 Oxford Flowers dataset after 50k training iterations. SNGAN misleads some pixels in the images and depicted objects are not always distinguishable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9</head><label>9</label><figDesc>Randomly generated samples from the proposed QSNGAN on the 102 Oxford Flowers dataset after 50k training iterations. Flowers contain many different colors shades and most of the objects are clearly defined. This set of figures sow the improved generation ability of our proposed method with respect to its real-valued counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. The discriminative network plays a crucial role in GANs training, thus it is more complex with respect to the generator network. It</figDesc><table><row><cell>QBN</cell><cell>split ReLU</cell><cell>Upsampling</cell><cell>QConv</cell><cell>QBN</cell><cell>split ReLU</cell><cell>QConv</cell><cell>AvgPool</cell></row><row><cell></cell><cell cols="2">Upsampling</cell><cell>QConv</cell><cell></cell><cell>AvgPool</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Discriminator collapses and training fails, thus metrics results are not comparable.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>↓</cell><cell></cell><cell>IS ↑</cell></row><row><cell>Model</cell><cell cols="5">Critic iter CelebA-HQ 102 Oxford Flowers CelebA-HQ 102 Oxford Flowers</cell></row><row><cell>SNGAN</cell><cell>1</cell><cell>&gt; 200  *</cell><cell>&gt; 200  *</cell><cell>&lt; 2.000  *</cell><cell>2.797 ± 0.196</cell></row><row><cell></cell><cell>5</cell><cell>34.483</cell><cell>165.058</cell><cell>2.032 ± 0.062</cell><cell>2.977 ± 0.146</cell></row><row><cell>QSNGAN</cell><cell>1</cell><cell>29.417</cell><cell>175.484</cell><cell>2.249 ± 0.164</cell><cell>2.754 ± 0.256</cell></row><row><cell></cell><cell>5</cell><cell>33.068</cell><cell>115.838</cell><cell>2.026 ± 0.082</cell><cell>3.000 ± 0.141</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>.987 ± 0.485</head><label></label><figDesc>91.567 4.031 ± 1.327 4.744 ± 0.643 QSN Split 35.417 75.112 4.7128 ± 1.270 4.455 ± 0.092 QSN Full 31.966 59.611 4.317 ± 0.951 4</figDesc><table><row><cell></cell><cell>↓</cell><cell>IS ↑</cell></row><row><cell>Config</cell><cell>CIFAR10 STL10 CIFAR10</cell><cell>STL10</cell></row><row><cell>No QSN</cell><cell>70.312</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875v3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised GANs via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12146" to="12155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Augmented second-order statistics of quaternion random signals. Signal Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheong</forename><surname>Took</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discrete orthogonal transforms with data representation in composition algebras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chernov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scandinavian Conf. on Image Analysis</title>
		<meeting>Scandinavian Conf. on Image Analysis</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for detection and localization of 3D sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8533" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quaternion involutions and anti-involutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Ell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Sangwine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Appl</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Conf. on Neural Netw. (ĲCNN)</title>
		<meeting><address><addrLine>Rio de Janeiro; Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Int. Conf. on Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularisation of neural networks by enforcing Lipschitz continuity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="416" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A quaternion-valued variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible generative adversarial networks with non-parametric activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Artificial Intelligence and Neural Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">184</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06937v1</idno>
		<title level="m">A review on generative adversarial networks: Algorithms, theory, and applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07360v2</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AlgebraNets</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8107" to="8116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114v10</idno>
		<title level="m">Auto-encoding variational Bayes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large-scale study on regularization and normalization in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957v1</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for heterogeneous image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linarès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of quaternion neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linarès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Quaternion recurrent neural networks. In: Int. Conf. on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A possibility for implementing curiosity and boredom in model-building neural controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Int. Conf. on Simulation of Adaptive Behavior on From Animals to Animats</title>
		<meeting>of the First Int. Conf. on Simulation of Adaptive Behavior on From Animals to Animats<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="222" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial networks are special cases of artificial curiosity (1990) and also closely related to predictability minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A U-Net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8207" to="8216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compressing deep-quaternion neural networks with targeted regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Trans. Intell. Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="176" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Proper and widely linear processing of quaternion random vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vìa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramìrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Santamarìa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3502" to="3515" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quaternions and Caley Numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algebra ans Applications, Mathematics and Its Applications</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<date type="published" when="1997" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural network for color image classification and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="20293" to="20301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
	<note>Int. Conf. on Machine Learning (ICML)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lipschitz generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7584" to="7593" />
		</imprint>
	</monogr>
	<note>Int. Conf. on Machine Learning (ICML)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
