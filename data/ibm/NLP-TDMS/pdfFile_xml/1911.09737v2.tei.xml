<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<email>saurabhsingh@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Krishnan</surname></persName>
							<email>skrishnan@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch Normalization (BN) uses mini-batch statistics to normalize the activations during training, introducing dependence between mini-batch elements. This dependency can hurt the performance if the mini-batch size is too small, or if the elements are correlated. Several alternatives, such as Batch Renormalization and Group Normalization (GN), have been proposed to address this issue. However, they either do not match the performance of BN for large batches, or still exhibit degradation in performance for smaller batches, or introduce artificial constraints on the model architecture.</p><p>In this paper we propose the Filter Response Normalization (FRN) layer, a novel combination of a normalization and an activation function, that can be used as a replacement for other normalizations and activations. Our method operates on each activation channel of each batch element independently, eliminating the dependency on other batch elements. Our method outperforms BN and other alternatives in a variety of settings for all batch sizes. FRN layer performs « 0.7-1.0% better than BN on top-1 validation accuracy with large mini-batch sizes for Imagenet classification using InceptionV3 and ResnetV2-50 architectures. Further, it performs ą 1% better than GN on the same problem in the small mini-batch size regime. For object detection problem on COCO dataset, FRN layer outperforms all other methods by at least 0.3-0.5% in all batch size regimes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Batch normalization (BN) <ref type="bibr" target="#b17">[18]</ref> is a cornerstone of current high performing deep neural network models. One often discussed drawback of BN is its reliance on sufficiently large batch sizes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. When trained with small batch sizes, BN exhibits a significant degradation in performance. This issue has been attributed to training and testing discrepancy arising from BN's reliance on stochastic mini-batches <ref type="bibr" target="#b30">[31]</ref>. As a result, several approaches have been proposed that ameliorate the issues due to stochasticity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> or offer alternatives <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref> by removing batch dependence. How-  ever, these approaches don't match the performance of BN for large batch sizes ( <ref type="figure" target="#fig_1">Figure 1</ref>). Further, either they still exhibit a degradation in performance for smaller batch sizes e.g. Batch Renormalization, or introduce constraints on the model architecture and size e.g. Group Normalization requires number of channels in a layer to be multiples of an ideal group size, such as <ref type="bibr">32.</ref> In this work we propose Filter Response Normalization (FRN) layer, consisting of a normalization and activation function, that eliminates these shortcomings altogether. Our method does not have any batch dependence, as it operates on each activation channel (filter response) of each batch sample independently, and outperforms BN and alternatives in a wide variety of evaluation settings. For example, in <ref type="figure" target="#fig_1">Figure 1</ref>, FRN layer outperforms other approaches by more than 1% at all batch sizes for ResNetV2-50 on ImageNet classification.</p><p>The reliance of BN on large batch sizes hinders the exploration of higher capacity models due to significantly higher memory requirements and imposes limitations on the performance of tasks that need to process larger inputs. For example, object detection and segmentation perform better with higher resolution inputs; similarly, video data tends to be very high dimensional. As a result, these systems are forced to trade-off between model capacity and ability to train with larger batch sizes. As evidenced in <ref type="figure" target="#fig_1">Figure 1</ref>, our method addresses this by maintaining a consistent performance across a range of batch sizes.</p><p>FRN layer consists of two novel components that work together to yield high performance: 1) Filter Response Normalization (FRN), a normalization method that independently normalizes the responses of each filter for each batch element by dividing them by the square root of their uncentered second moment, without performing any mean subtraction, and 2) Thresholded Linear Unit (TLU), a pointwise activation that is parameterized by a learned rectification threshold, allowing for activations that are biased away from zero. FRN layer outperforms BN by more than 0.7-1.0% with large mini-batch sizes for Imagenet classification using InceptionV3 and ResnetV2-50 architectures. Further, it performs ą 1% better than GN on the same problem in the small mini-batch size regime. For object detection on COCO dataset, FRN layer outperforms all other methods by at least 0.3-0.5% in all batch size regimes. Lastly, FRN layer maintains a consistent performance across all the batch sizes that we tested. In summary, the proposed FRN layer does not rely on other batch elements or channels for normalization, yet outperforms BN and other alternatives for all batch sizes and in a variety of settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>1. Filter Response Normalization (FRN), a normalization method that enables models trained with per-channel normalization to achieve high accuracy. 2. Thresholded Linear Unit (TLU), an activation function to use with FRN that further improves accuracy and outperforms BN at all batch sizes without any batch dependency. We refer to this combination as FRN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Several insights and practical considerations that lead to</head><p>the success of the combination of FRN and TLU. 4. A detailed experimental study comparing popular normalization methods on large scale image classification and object detection tasks using a variety of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Normalization of training data has been known to aid in optimization. For example, whitening of inputs is a common practice for training shallow models such as Support Vector Machines and Logistic regression. Similarly, for training deep networks, normalization of inputs and intermediate representations has been recommended for efficient learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Batch Normalization (BN) <ref type="bibr" target="#b17">[18]</ref> accelerates learning and enables training of very deep neural network architectures by stabilizing the intermediate feature distributions. Stabilization is achieved by normalizing each activation channel independently, using the mean and variance statistics computed for that channel over the entire minibatch. However, BN exhibits a dramatic degradation in performance when trained with smaller mini-batches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. Several approaches have been proposed to address this shortcoming, and can be grouped into two major categories: 1) Methods that reduce the train-test discrepancy in batch normalized models, 2) Sample based normalization methods that avoid batch normalization.</p><p>Methods reducing train-test discrepancy in batch normalization. Ioffe <ref type="bibr" target="#b16">[17]</ref> notes that the discrepancy between the statistics that are used for normalization during training and testing may arise from the stochasticity due to small mini-batches and bias due to non-iid samples. They propose Batch Renormalization (BR) to reduce this discrepancy by constraining the mini-batch moments to a specific range, limiting the variation in mini-batch statistics during training.</p><p>A key benefit of this approach is that the test time evaluation scheme of a model trained with BR is exactly the same as that for a model trained with BN. In comparison, EvalNorm <ref type="bibr" target="#b30">[31]</ref> does not modify the training scheme. Instead, it proposes a correction to the normalization statistics for use only during evaluation. The major advantage of this method is that the model does not need to be retrained. However, both these methods still exhibit a degradation in performance for small mini-batches. Another approach is to engineer systems that can circumvent the issue by distributing larger batches across GPUs for tasks that require large inputs <ref type="bibr" target="#b24">[25]</ref>. However, this approach requires considerable GPU infrastructure.</p><p>Methods avoiding normalization using mini-batches. Several approaches sidestep the issues encountered by BN by not relying on the stochastic mini-batch altogether <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Instead, the normalization statistics are computed from the sample itself. Layer Normalization (LN) <ref type="bibr" target="#b1">[2]</ref> computes the normalization statistics from the entire layer i.e. using all the activation channels. In contrast, like BN, Instance Normalization (IN) <ref type="bibr" target="#b34">[35]</ref> computes the normalization statistics for each channel independently, but only from the sample being normalized, as opposed to the entire batch, as BN does. IN was shown to be useful for style transfer applications, but was not successfully applied for recognition. Group Normalization (GN) <ref type="bibr" target="#b35">[36]</ref> fills the middle ground between the two. It computes the normalization statistics over groups of channels. The ideal group size is experimentally determined. While, GN doesn't show performance degradation for smaller batch sizes, it performs worse than BN for larger mini-batches (See <ref type="figure" target="#fig_1">Figure 1</ref> here and <ref type="figure" target="#fig_1">Figure 1</ref> in <ref type="bibr" target="#b35">[36]</ref>). In addition, the size of groups required by GN imposes a constraint on the network size and architecture as every normalized layer needs to have number of channels that are a Other approaches. Weight Normalization <ref type="bibr" target="#b27">[28]</ref> proposes a reparameterization of the filters in terms of a direction and a scale and reports accelerated convergence. Normalization Propagation <ref type="bibr" target="#b0">[1]</ref> uses idealized moment estimates to normalize every layer. Refer to Ren et al. <ref type="bibr" target="#b25">[26]</ref> for a unifying view of various normalization approaches. Divisive normalization (DN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> has been proposed to normalize each activation with a function of the neighboring activations and has been studied in a variety of contexts including density modeling <ref type="bibr" target="#b2">[3]</ref>, image compression <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, sensitivity maximization <ref type="bibr" target="#b6">[7]</ref>, distributed neural representation <ref type="bibr" target="#b28">[29]</ref> and attention <ref type="bibr" target="#b26">[27]</ref>, among others.</p><formula xml:id="formula_0">x ν 2 " ř i x 2 i {N y i " γ xi ? ν 2` `β z i " maxpy i , τ q z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to eliminate the batch dependency in the training of deep neural networks without sacrificing the performance gains of BN at large batch sizes. We start this section with the main details of our proposal and follow that with a discussion of the rationale behind our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Filter Response Normalization with Thresholded Activation</head><p>We will assume for the purpose of exposition that we are dealing with the feed-forward convolutional neural network. We follow the usual convention that the filter responses (activation maps) produced after a convolution operation are a 4D tensor X with shape rB, W, H, Cs, where B is the mini-batch size, W, H are the spatial extents of the map, and C is the number of filters used in convolution. C is also referred to as output channels. Let x " X b,:,:,c P R N , where N " WˆH, be the vector of filter responses for the c th filter for the b th batch point. Let ν 2 " ř i x 2 i {N , be the mean squared norm of x. Then we propose Filter Response Normalization (FRN) as following:</p><p>x "</p><p>x ?</p><formula xml:id="formula_1">ν 2` ,<label>(1)</label></formula><p>where is a small positive constant to prevent division by zero. A few observations are in order about the proposed normalization scheme:</p><p>1. Similar to other normalization schemes, FRN removes the scaling due to both the filter weights and pre-activations. This is known <ref type="bibr" target="#b27">[28]</ref> to remove noisy updates along the direction of the weights and reduce gradient covariance. 2. One of the main differences in our proposal is that we do not remove the mean prior to normalization. While mean subtraction was an important aspect of Batch Normalization, it is arbitrary and without real justification for normalization schemes that are batch independent. 3. Our normalization is done on a per-channel basis. This ensures that all filters (or rows of a weight matrix) have the same relative importance in the final model. 4. At a first glance, FRN appears very similar to Local Response Normalization (LRN) proposed in Krizhevsky et al. <ref type="bibr" target="#b18">[19]</ref>. However, among other differences, LRN does normalization over adjacent channels at the same spatial location, while FRN is a global normalization over the spatial extent.</p><p>As with other schemes, we also perform an affine transform after normalization so that the network can undo the effects of the normalization:</p><formula xml:id="formula_2">y " γx`β,<label>(2)</label></formula><p>where γ and β are learned parameters. The final addition to our FRN layer is the activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Thresholded Linear Unit (TLU)</head><p>Lack of mean centering in FRN can lead to activations having an arbitrary bias away from zero. Such a bias in conjunction with ReLU can have a detrimental effect on learning and lead to poor performance and dead units. We propose to address this issue by augmenting ReLU with a learned threshold τ to yield TLU defined as:</p><formula xml:id="formula_3">z " maxpy, τ q<label>(3)</label></formula><p>Since maxpy, τ q" maxpy´τ, 0q`τ "ReLU py´τ q`τ , the effect of TLU activation is the same as having a shared bias before and after ReLU. However, this does not appear to be identical to absorbing the biases in the previous and subsequent layers based on our experiments. We hypothesize that the form of TLU is more favorable for optimization. TLU significantly improves the performance of models using FRN (see <ref type="table" target="#tab_6">Table 5</ref>), outperforming BN and other alternatives, and leads to our method, FRN layer. <ref type="figure" target="#fig_2">Figure 2</ref> shows the schematic for our proposed FRN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gradients of FRN Layer</head><p>In this section, we derive expressions for the gradients that flow through the network in the presence of the FRN layer. Since all the transformations are performed channel-wise, we only derive the per-channel gradients. Let us assume that somewhere in the network, the activations x are fed to the FRN layer and the output is z (following the transformations described in equations (1), <ref type="bibr" target="#b1">(2)</ref>, and <ref type="formula" target="#formula_3">(3)</ref>). Let f pzq be the mapping that the network applies to z, with gradients Bf Bz flowing backwards. Note that the parameters γ, β and τ are vectors of size num channels, and so the per channel updates are scalar.</p><formula xml:id="formula_4">Bz i Bτ " # 0, if y i ě τ 1, otherwise<label>(4)</label></formula><p>Note that the gradients Bzi Byi are just the same as above, but with the cases reversed. Then the gradient update to τ is of the form</p><formula xml:id="formula_5">Bf Bτ " B ÿ b"1ˆB f Bz b˙T Bz b Bτ ,<label>(5)</label></formula><p>where z b is the vector of per-channel activations of the b th batch point. Gradients w.r.t γ and β are as follows:</p><formula xml:id="formula_6">Bf Bγ , Bf Bβ˙"˜B ÿ b"1 Bf T By bx b , B ÿ b"1</formula><p>Bf By b¸ <ref type="formula">(  6)</ref> Using eqn. <ref type="formula" target="#formula_2">(2)</ref>, we can see that Bf Bx "γ Bf By . Finally, the gradients that flow back from the FRN layer can be written as</p><formula xml:id="formula_7">Bf Bx " 1 ? ν 2` ˆI´xx T N˙B f Bx (7)</formula><p>We make a couple of observations about the gradients. Eqn. <ref type="bibr" target="#b4">(5)</ref> suggests that part of the gradients that get suppressed in a regular ReLU activation are now used to update τ , and in some sense are not wasted. Since ||x|| 2 2 " N , eqn. <ref type="bibr" target="#b6">(7)</ref> shows that the gradients w.r.t to x are orthogonal to x (provided = 0) because pI´xx T {N q projects out the component in the direction ofx. This property is not unique to our normalization, but is known to help in reducing variance of gradients during SGD and benefit optimization <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>In our discussion so far, we have assumed that the filter responses have a large spatial extent of size N " WˆH. However, there are situations in real networks like Incep-tionV3 <ref type="bibr" target="#b32">[33]</ref> and VGG-A <ref type="bibr" target="#b29">[30]</ref>, where some layers produce 1ˆ1 activation maps. In this setting (N "1), for small value of , the proposed normalization as in Equation <ref type="formula" target="#formula_1">(1)</ref> turns into a sign function (see <ref type="figure" target="#fig_3">Figure 3</ref>), and has very small gradients almost everywhere. This will invariable affect the learning adversely. In contrast, higher values of lead to variants of smoother soft sign function that are more amenable to learning. Therefore, appropriate value of becomes crucial for models that are fully connected or lead to 1ˆ1 activation maps. Empirically, we turn into a learnable parameter (initialized at 10´4) for such models. For other models, we use In our experiments, we show that the learnable parameterization is useful for training In-ceptionV3 model where the Auxiliary logits head produces 1ˆ1 activation maps, and for the VGG-A <ref type="bibr" target="#b29">[30]</ref> architecture that uses fully connected layers.</p><p>Since ą 0, we explored two alternative parameterizations to enforce this constraint: absolute value and exponential. While both trained well, the absolute value parameterization "10´6`| l | ( l being a learned parameter), produced consistently better empirical results. Parameterizations of this form are also preferable because the gradient magnitudes for l are independent of the value of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mean Centering</head><p>Batch Normalization was proposed to counter the effects of internal covariate shift during training of a deep neural network. The solution was to keep the statistics of distribution of activations over the data set invariant; and as a practical matter, they choose to normalize the first and second moments of mini-batch at each step. Batch independent alternatives that include mean centering are not justified by any particular consideration, and appear merely to be a legacy of Batch Normalization.</p><p>Consider the example of Instance Normalization (IN). Using the same notation as Section 3.1, IN computes the normalized activations using the channel statistics µ" ř i x i {N and σ 2 " ř i px i´µ q 2 {N as following:</p><p>x " x´µ ?</p><formula xml:id="formula_8">σ 2`<label>(8)</label></formula><p>As the size of the activation map decreases (as is common in the layers closer to the output which are subject to downsampling, or due to the presence of fully connected layers), IN produces zero activations. Layer and Group Normalization are ways to circumvent this issue by normalizing across (all or subset of) channels. Since individual filters are responsible for each channel's activations, normalizing across channels introduces unnecessary interactions in the filter updates. Hence, it appears that the only principled approach is to normalize each channel of the activation map separately without resorting to mean centering. This also has the desirable effect of removing the relative scaling between filters, which is known to greatly aid in optimization. A negative impact of not performing mean centering is that the activations can be biased arbitrarily away from zero, rendering ReLU activation less than ideal. We mitigate this issue by introducing the Thresholded Linear Unit (TLU) in Section 3.1.1. Empirically, the combination of uncentered normalization with the TLU activation outperforms BN and all other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation and Practical Considerations</head><p>FRN is easy to implement in automatic differentiation frameworks. We provide an example implementation using the python API for Tensorflow in Listing 1. Further, for FRN to achieve peak accuracy, we found that care must be exercised with the following practical considerations. Sensitivity of FRN to each of these is architecture dependent.</p><p>Learning Rate Schedule: We found that the more common step decay learning rate (LR) schedule was not optimal for FRN. Instead, continuous LR decay schedules such as cosine decay (without restarts) performed better for all methods and eliminated the need to tune step decay hyper parameters.</p><p>Warm-up: Since FRN does not perform mean centering, we empirically found that certain architectures are more sensitive to the choice of initial LR. Setting a high initial LR causes large updates, that lead to large activations in the early part of the training and result in a slowdown in the learning. This is due to the 1 ? ν 2` factor in the gradient of Bf Bx (see Equation <ref type="formula">(7)</ref>). This happens more often in architectures that employ several max pooling layers, like VGG-A. We address this by using an initial warp-up phase where the LR is slowly increased from 0 to the peak value. Since all our experiments use cosine LR decay schedule, we use a cosine warm-up schedule as well. Note that a warmup phase is quite common and frequently used in training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Learnable : As discussed in Section 3.3, for models that use 1ˆ1 activation maps, it is crucial to turn the into a learned parameter and initialize with a larger value to prevent step function like behavior and enable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method extensively on two tasks: 1) Image classification on Imagenet, and 2) Object detection on COCO. While Image classification is the de-facto standard for evaluation, Object detection typically requires high resolution inputs and is particularly constrained by the large batch size requirement of BN. On Imagenet classification, we show that our method outperforms other normalization methods on three different network architectures. Further, our method does this consistently at all batch sizes we experimented with. Finally, we validate the performance of our method on Object Detection where it outperforms other normalization methods on all batch sizes as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>Dataset: ImageNet classification dataset <ref type="bibr" target="#b7">[8]</ref> consists of 1000 classes. We train on the "1.28M training images and report results on the 50000 validation images. For all models in this section, we resize the images to 299ˆ299 and use data augmentation from <ref type="bibr" target="#b33">[34]</ref> at training time.</p><p>Model architectures: We provide comparisons using three different model architectures: 1) ResnetV2-50 <ref type="bibr" target="#b13">[14]</ref>: Popular model with identity shortcuts, 2) InceptionV3 <ref type="bibr" target="#b31">[32]</ref>: High performing model without identity shortcuts and fully connected layers and, 3) VGG-A <ref type="bibr" target="#b29">[30]</ref>: Feed forward model with a mix of convolutional and fully connected layers. For all models using GN, we use a group size of 32. However, since VGG-A does not use a multiple of 32 filters in all layers, we increase the number of filters to nearest multiple.</p><p>Training: We follow the training setup used by He et al. <ref type="bibr" target="#b12">[13]</ref>. All models are trained using synchronous SGD across 8 GPUs for 300000 steps. Gradients are computed by averaging across all GPUs. For BN, the normalization statistics are computed per GPU. This setup is common for multi-GPU training using synchronous SGD in Tensorflow and PyTorch. An initial learning rate of 0.1ˆbatch size{256 and cosine decay schedule is used. We follow <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> for other implementation details. Results are reported using two image classification metrics: 1) 'Precision@1' measures the accuracy using the highest scoring class (top-1 prediction) while, 2) 'Recall@5' measures the accuracy using top-5 scoring classes.</p><p>Comparison with normalization methods: In <ref type="table" target="#tab_0">Table 1</ref> we compare our method with various normalization methods for   <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> as evident in <ref type="figure" target="#fig_1">Figure 1</ref>. GroupNorm (GN) exhibits a more consistent performance underperforming BN only at the largest batch size. Batch renormalization outperforms GN at the largest two batch sizes but shows a degradation in performance for the smaller batch sizes. Our method, FRN, consistently outperforms all the normalization methods at all batch sizes.</p><p>Analyzing the effect of FRN and TLU: In <ref type="table" target="#tab_4">Table 3</ref> we perform a detailed ablation study of the effect of FRN and TLU. We combine them with various normalization methods -BatchNorm (BN), GroupNorm (GN), LayerNorm (LN) and InstanceNorm (IN), and train models for each combination for two high performing, but different, model architectures -ResnetV2-50 and InceptionV3. We either replace ReLU activation with TLU, or modify the normalization technique to suppress mean centering and dividing by uncentered second moments instead of variance (Equation (1) instead of Equation <ref type="formula" target="#formula_8">(8)</ref>). The corresponding normalization are named with a FRN suffix in <ref type="table" target="#tab_4">Table 3</ref>   per sample methods <ref type="figure">(GN, LN, IN, FRN)</ref>, since the number of activations to be normalized over is relatively small. As a result, normalization layers are typically not applied after FC layers. In this section we evaluate the effect of applying normalization after all the layers irrespective of whether they are FC or convolutional layers. Note that FC layers are the most challenging scenario for FRN since we are normalizing over a single activation (N " 1). We report results for two architectures where the output of FC layers is normalized: 1) InceptionV3 in <ref type="table" target="#tab_0">Table 1</ref> and 2) VGG-A in <ref type="table" target="#tab_5">Table 4</ref>. Note that while ResnetV2-50 also has a FC layer after the global pooling to produce logits, normalization is performed before pooling and is thus not relevant here. InceptionV3 has fully connected layers in an auxiliary logits branch while VGG-A has them in the main network. FRN outperforms all other normalization methods even in this challenging scenario on both the architectures.</p><p>Note that, while training InceptionV3 and VGG-A, it was crucial to use learning rate warm-up (refer Section 3.5) and learned (refer Section 3.3) for FRN to achieve peak performance. FRN underperformed other methods on In-ceptionV3 and failed to learn entirely on VGG-A without warm-up. Other methods were not significantly affected. We discovered that without the warm-up phase, the output of max pooling layers grew to very large magnitudes in first few steps. This saturates the normalized activations (see <ref type="figure">Figure</ref> 3) and prevents learning due to poor flow of gradients.</p><p>Interestingly, for VGG-A, BN performs worse than 'No normalization' at the default learning rate of 0.01. In <ref type="table" target="#tab_5">Table 4</ref> we also report results for models trained with a higher Comparison of TLU with related variants: In <ref type="table" target="#tab_6">Table 5</ref> we compare TLU with three related variants for ResnetV2-50 on ImageNet. All four correspond to different combinations of having a scale κ and bias τ to compute the threshold. First observe that TLU, despite having a less general form, outperforms others. Second, all variants with a learnable threshold outperform BN, which doesn't benefit from it. We conclude that a learnable threshold is necessary for high performance in conjunction with FRN however it doesn't need to be input dependent. Interestingly, while two of the variants correspond to commonly known activations -ReLU and Parametric ReLU (PReLU) <ref type="bibr" target="#b11">[12]</ref>, the third more general form, termed Affine-TLU, outperforms the previous two and has not been explored to the best of our knowledge. Note that Affine-TLU is different from Maxout <ref type="bibr" target="#b9">[10]</ref>, which computes maximum across groups of channels and, unlike Affine-TLU, results in reduced number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection on COCO</head><p>Next, we evaluate our method on the task of Object Detection (OD) and demonstrate that it consistently outperforms other normalization methods at all the batch sizes we evaluated on. Since OD frameworks are typically trained with high resolution inputs, they are limited to using small minibatch sizes. This constraint makes OD an ideal evaluation benchmark for sample based normalization methods that enable training with small batch sizes.  <ref type="bibr" target="#b21">[22]</ref> with 80 object classes. We train using the train2017 set, and evaluate on the 5k images in val2017 (minival) split. We report the standard COCO evaluation metrics of mean average precision with different IoU thresholds, namely AP, AP 50 , AP 75 Lin et al. <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model:</head><p>We use the RetinaNet <ref type="bibr" target="#b23">[24]</ref> object detection framework. RetinaNet is a unified single stage detector that comprises of three conceptual components: 1) A backbone network, with an off-the-shelf architecture, that acts as a convolutional feature extractor for a given high resolution input image, 2) a convolutional object classification sub-network that acts on the features extracted by the backbone network and, 3) a convolutional bounding box regression sub-network. We use a ResnetV1-101 Feature Pyramid Network backbone <ref type="bibr" target="#b22">[23]</ref> and resize the input images to 1024ˆ1024.</p><p>Training: To simplify experimentation and evaluation, we only compare all methods on models trained from scratch. We justify this choice based on conclusions from <ref type="bibr" target="#b14">[15]</ref> that, by training longer, model trained from scratch can catch up with models trained by fine-tuning pre-trained models.</p><p>To ensure this, we start with a baseline fine-tuned model, trained by us at the largest batch size 64, that achieves an AP of 38.3 in 25K training steps (BN˚, <ref type="table" target="#tab_7">Table 6</ref>) and is close to the corresponding result of 39.1 reported in <ref type="bibr" target="#b23">[24]</ref>. Next, we empirically find the nearest multiple of 25K that achieves similar accuracy when training from scratch to be 125K steps (BN, <ref type="table" target="#tab_7">Table 6</ref>). We set 125K as the base number of training steps for the largest batch size. We train our models using 8 GPUs and experiment with batch sizes in {64, 32, 16} leading to {8, 4, 2} images per GPU respectively. For smaller batch size M we set the training steps 125000ˆ64{M and learning rate as base lrˆM {64. We report best performance using base lr P t0.01, 0.05, 0.1u. All models are trained using a momentum of 0.9 and weight decay of 4ˆ10´4.</p><p>Comparison of normalization methods: In <ref type="table" target="#tab_7">Table 6</ref> we observe that FRN outperforms both BN and GN at all batch sizes, further validating our results in the previous section.</p><p>In agreement with the observations from <ref type="table" target="#tab_2">Table 2</ref> both FRN and GN achieve higher accuracy than BN at the evaluated batch sizes. FRN outperforms BN by a significant difference of 0.9 AP points at the largest batch size, and this gap widens to 8.9 AP points at the smallest batch size. Further, FRN consistently achieves higher accuracy than GN.</p><p>Effect of batch size: BN exhibits a dramatic degradation in performance, dropping by 8.5 AP points for the model trained from scratch, as the number of images per GPU is reduced to 2. In comparison, both FRN and GN show a relatively more stable accuracy and degrade by less than 0.6 AP points. Interestingly, the finetuned BN˚model for the smallest batch size performs 2.7 AP points better than the corresponding BN model trained from scratch, indicating that longer training at this batch size is detrimental to the performance of batchnorm. In contrast, FRN maintains a consistent lead for all the metrics across all batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we proposed the FRN layer, a novel combination of Filter Response Normalization (FRN) and a Thresholded activation (TLU) function that eliminates the need for batch dependent training. It outperforms BN in a variety of settings and exhibits a consistently high performance in large as well as small batch training. Further, FRN also outperforms Group Normalization, a leading sample based normalization alternative to BN, in all the explored settings. We also demonstrated the success of FRN in the pathological case of fully connected layers which are typically not normalized. However, since different normalization methods have been successful in different problem domains, e.g. Layer Normalization has been successful in NLP, we leave exploration of these areas with FRN as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our method consistently outperforms other normalization methods, even at the largest batch size where other methods struggle in comparison to Batch Normalization (see inset). The figure reports the validation performance of ResNetV2-50 models trained using 8 GPUs with different batch sizes on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A schematic of the proposed FRN Layer. multiple of the ideal group size determined by GN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Effect of on normalized activations for the case of N " 1. For very small values of , FRN turns into a step function while for higher values it behaves like a softsign function, allowing the gradients to flow. Having a learnable epsilon is crucial in models with fully connected layers or low-dimensional activation maps. a fixed constant value of 10´6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Listing 1 :</head><label>1</label><figDesc>Tensorflow implementation of FRN layer def FRNLayer(x, tau, beta, gamma, eps=1e-6): # x: Input tensor of shape [BxHxWxC]. # tau, beta, gamma: Variables of shape [1, 1, 1, C]. # eps: A scalar constant or learnable variable.</figDesc><table><row><cell># Compute the mean norm of activations per channel.</cell></row><row><cell>nu2 = tf.reduce_mean(tf.square(x), axis=[1, 2],</cell></row><row><cell>keepdims=True)</cell></row><row><cell># Perform FRN.</cell></row><row><cell>x = x * tf.rsqrt(nu2 + tf.abs(eps))</cell></row><row><cell># Return after applying the Offset-ReLU non-linearity.</cell></row><row><cell>return tf.maximum(gamma * x + beta, tau)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FRN layer outperforms BN and other normalization methods for large batch size on Imagenet Classification for ResnetV2-50<ref type="bibr" target="#b13">[14]</ref> and InceptionV3<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Method</cell><cell>ResnetV2 50</cell><cell>InceptionV3</cell></row><row><cell></cell><cell cols="2">P@1 R@5 P@1 R@5</cell></row><row><cell>Batchnorm</cell><cell cols="2">76.21 92.98 78.24 94.07</cell></row><row><cell>BatchRenorm</cell><cell cols="2">75.85 92.90 78.19 94.01</cell></row><row><cell>Groupnorm</cell><cell cols="2">75.67 92.70 78.14 93.98</cell></row><row><cell>Layernorm</cell><cell cols="2">72.75 91.19 76.75 93.37</cell></row><row><cell>Instancenorm</cell><cell cols="2">71.63 90.46 73.93 91.60</cell></row><row><cell cols="3">FRN layer [Ours] 77.21 93.57 78.95 94.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of mini-batch size used for normalization on ImageNet classification for ResnetV2-50<ref type="bibr" target="#b13">[14]</ref>.Ours] 77.21 77.10 77.16 77.18 77.33 77.36 ∆ +1.54 +1.33 +1.02 +1.16 +1.13 +1.43 Ours] 93.62 93.59 93.60 93.49 93.61 93.61 ∆ +0.92 +0.87 +0.71 +0.62 +0.69 +0.88the regular batch size of 32 images/GPU. This results in an effective batch size of 32ˆ8 " 256 and is the most favorable configuration for BN. This is the strongest baseline for image classification and all the alternatives to BN have struggled in this setting, underperforming BN. Even for this large batch size, FRN outperforms all the methods, including BN, with a healthy margin on both the architectures. Key takeaway is that batch dependent training is not necessary for high performance. At this large batch size, the next best performing normalization schemes are BN and BatchRenorm, both of which are batch normalized methods, followed by other sample based normalization methods.Figure 4compares the training and validation 'Precision@1' curves for various normalization methods using the ResnetV2-50 architecture. We observe that FRN layer achieves both higher training and validation accuracies than BN indicating that removal of stochastic batch dependence eases optimization allowing model to train better. The generalization gap, i.e. difference between training and validation accuracy, has also increased. However, improved optimization results in a net performance gain on validation. In comparison, GN also achieves lower training error than BN but performs worse on validation.</figDesc><table><row><cell>0.9</cell></row><row><cell>Images per GPU Ñ Batchnorm Renorm Groupnorm FRN layer [Recall@5 Precision@1 Batchnorm Renorm Groupnorm FRN layer [0K 32 16 8 4 2 1 76.21 75.55 74.04 71.96 65.09 1.58 75.85 75.96 75.59 74.18 70.75 37.55 75.67 75.77 76.14 76.02 76.20 75.93 92.98 92.81 92.12 90.98 86.51 4.00 92.90 92.98 92.80 92.10 89.81 57.18 92.70 92.72 92.89 92.87 92.92 92.73 0.5 0.6 0.7 0.8 Accuracy (Precision @ 1) 0.4 Figure 4: Comparison of training and validation curves 50K 100K 150K 200K 250K 300K Training Steps BatchNorm BatchNorm Train GroupNorm GroupNorm Train FRN Layer FRN Layer Train of various normalization methods for Imagenet Classification using ResnetV2-50 model. ure 1 and Table 2. All methods are trained with 8 GPUs using six different total batch sizes of 8, 16, 32, 64, 128, 256, divided into equal number of images per GPU leading to 1, 2, 4, 8, 16, and 32 images/GPU. BN is known to degrade in performance when the batch size is small</cell></row></table><note>Effect of small number of images per GPU: We study the impact of mini-batch sizes used for normalization (im- ages/GPU) on the performance of various methods in Fig-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation of our method on Imagenet Classification for ResnetV2-50<ref type="bibr" target="#b13">[14]</ref> and InceptionV3<ref type="bibr" target="#b31">[32]</ref>. We evaluate various combinations of our method with existing normalizations. Combinations that include one of our proposals are marked as : : :. Our method, FRN + TLU, at the bottom is marked as[Ours].</figDesc><table><row><cell>Method</cell><cell>ResnetV2-50 InceptionV3</cell></row><row><cell></cell><cell>P@1 R@5 P@1 R@5</cell></row><row><cell>BN + ReLU</cell><cell>76.21 92.98 78.24 94.07</cell></row><row><cell>BN + TLU : : :</cell><cell>76.03 92.94 78.22 94.13</cell></row><row><cell>GN + ReLU</cell><cell>75.67 92.70 78.14 93.98</cell></row><row><cell>GN + TLU : : :</cell><cell>76.59 93.16 78.50 94.18</cell></row><row><cell>GFRN + ReLU : : :</cell><cell>75.93 92.65 78.16 94.03</cell></row><row><cell>GFRN + TLU : : :</cell><cell>76.44 92.80 78.18 94.05</cell></row><row><cell>LN + RELU</cell><cell>72.75 91.19 76.75 93.37</cell></row><row><cell>LN + TLU : : :</cell><cell>73.99 91.60 77.21 93.48</cell></row><row><cell>LFRN + RELU : : :</cell><cell>75.03 92.50 77.62 93.65</cell></row><row><cell>LFRN + TLU: : :</cell><cell>76.17 92.89 78.12 94.02</cell></row><row><cell>IN + ReLU</cell><cell>71.63 90.46 73.93 91.60</cell></row><row><cell>IN + TLU : : :</cell><cell>71.72 90.53 74.81 92.01</cell></row><row><cell>FRN + ReLU : : :</cell><cell>75.24 92.65 77.98 94.02</cell></row><row><cell cols="2">FRN + TLU [Ours] 77.21 93.57 78.95 94.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Model with fully connected layer. We provide a comparison on Imagenet Classification for the VGG-A model that uses two fully connected layers. Top half shows the results for training with an initial learning rate of 0.01 (the default rate). Bottom half shows the results for training with a higher learning rate of 0.1. The base model diverges at this rate, while the model with Batchnorm exhibits instability. FRN and Groupnorm train well, with FRN outperforming all others.</figDesc><table><row><cell>Method</cell><cell>Learning rate</cell><cell>P@1</cell><cell>R@5</cell></row><row><cell>No normalization</cell><cell>0.01</cell><cell>69.04</cell><cell>88.99</cell></row><row><cell>Batchnorm</cell><cell>0.01</cell><cell>67.82</cell><cell>88.11</cell></row><row><cell>Groupnorm</cell><cell>0.01</cell><cell>69.35</cell><cell>89.12</cell></row><row><cell>FRN</cell><cell>0.01</cell><cell>70.04</cell><cell>89.42</cell></row><row><cell>No normalization</cell><cell>0.1</cell><cell cols="2">Diverged Diverged</cell></row><row><cell>Batchnorm</cell><cell>0.1</cell><cell>62.61</cell><cell>84.56</cell></row><row><cell>Groupnorm</cell><cell>0.1</cell><cell>69.94</cell><cell>89.57</cell></row><row><cell>FRN</cell><cell>0.1</cell><cell>71.66</cell><cell>90.69</cell></row><row><cell cols="4">learning rate of 0.1. A warm-up phase was useful for all the</cell></row><row><cell cols="4">models at this learning rate. However, the 'No normaliza-</cell></row><row><cell cols="4">tion' model eventually diverges, while BN shows instability</cell></row><row><cell cols="4">in training (even with warm-up) and performs significantly</cell></row><row><cell cols="4">worse than other methods. In contrast, both FRN and GN</cell></row><row><cell cols="4">benefit from training at higher learning rate and yield im-</cell></row><row><cell cols="4">proved performance with FRN outperforming GN.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of activations in conjunction with FRN on Imagenet Classification for ResnetV2-50. We observe that learnable threshold is key to high performance of our method in comparison to BN, which doesn't benefit from it.</figDesc><table><row><cell>Method</cell><cell>P@1 R@5</cell></row><row><cell>BN + maxpx, 0q (ReLU)</cell><cell>76.21 92.98</cell></row><row><cell>BN + maxpx, τ q (TLU)</cell><cell>76.03 92.94</cell></row><row><cell>FRN + maxpx, 0q (ReLU)</cell><cell>75.24 92.65</cell></row><row><cell>maxpx, κxq (PReLU) [12]</cell><cell>76.43 93.30</cell></row><row><cell cols="2">maxpx, κx`τ q (Affine-TLU) 76.71 93.32</cell></row><row><cell>maxpx, τ q (TLU)</cell><cell>77.21 93.57</cell></row><row><cell cols="2">Experimental setup. We perform experiments on the</cell></row><row><cell>COCO dataset</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Object detection results on COCO. Our method, FRN, outperforms other methods for all batch sizes. Note that while BN shows a dramatic drop in performance for smaller batch sizes, FRN exhibits a comparatively smaller degradation and consistently outperforms GN that also exhibits similarly stable performance. Note that BN˚models were trained by fine-tuning a imagenet pre-trained model, while others are trained from scratch.</figDesc><table><row><cell>Method</cell><cell></cell><cell>AP</cell><cell></cell><cell></cell><cell>AP 50</cell><cell></cell><cell></cell><cell>AP 75</cell></row><row><cell cols="2">imgs/gpu 8</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>2</cell></row><row><cell cols="10">BN˚38.3 37.1 32.9 57.2 55.4 49.1 41.5 40.4 35.9</cell></row><row><cell>BN</cell><cell cols="9">38.7 37.9 30.2 56.6 55.2 44.5 42.1 41.4 32.5</cell></row><row><cell>GN</cell><cell cols="9">39.3 39.0 38.7 57.8 57.5 56.9 42.6 42.3 41.8</cell></row><row><cell>FRN</cell><cell cols="9">39.6 39.5 39.1 58.5 58.4 57.5 43.1 43.3 42.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank Vivek Rathod for help with object detection experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venu</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Govindaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Density modeling of images using a generalized normalization transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend optimized image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Role of inhibition in the specification of orientation selectivity of cells in the cat striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Bonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalization as a canonical neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Normalization of cell responses in cat striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="197" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2999134.2999257" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Yann A Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
	<note type="report_type">Efficient backprop</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04520</idno>
		<title level="m">Normalizing the normalizers: Comparing and extending network normalization schemes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The normalization model of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="185" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A model of neuronal responses in visual area mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evalnorm: Estimating batch normalization statistics for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.00567" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
