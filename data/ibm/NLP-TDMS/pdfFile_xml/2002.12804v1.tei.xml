<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
						</author>
						<title level="a" type="main">UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With welldesigned position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks <ref type="bibr" target="#b19">(Peters et al., 2018;</ref><ref type="bibr" target="#b20">Radford et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2018;</ref><ref type="bibr" target="#b7">Dong et al., 2019;</ref><ref type="bibr" target="#b35">Yang et al., 2019;</ref><ref type="bibr" target="#b13">Lewis et al., 2019;</ref><ref type="bibr" target="#b12">Lan et al., 2019;</ref><ref type="bibr" target="#b21">Raffel et al., 2019)</ref>. After LM pre-training, the obtained model can be fine-tuned to various downstream tasks.</p><p>Two types of language model pre-training objectives are commonly employed to learn contextualized text represen-1 Microsoft Research 2 Harbin Institute of Technology.  <ref type="figure">Figure 1</ref>. Given input x1 · · · x6, the tokens x2, x4, x5 are masked by the special tokens [M] and <ref type="bibr">[P]</ref>. For each example, we jointly train two types of LMs, namely, autoencoding (AE), and partially autoregressive (PAR) masked LMs.</p><p>tations by predicting words conditioned on their context. The first strand of work relies on autoencoding LMs <ref type="bibr" target="#b5">(Devlin et al., 2018;</ref>. For example, the masked language modeling task used by BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> randomly masks some tokens in a text sequence, and then independently recovers the masked tokens by conditioning on the encoding vectors obtained by a bidirectional Transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. The second type of pretraining uses autoregressive modeling <ref type="bibr" target="#b20">(Radford et al., 2018;</ref><ref type="bibr" target="#b13">Lewis et al., 2019;</ref><ref type="bibr" target="#b35">Yang et al., 2019;</ref><ref type="bibr" target="#b21">Raffel et al., 2019)</ref>. Rather than independently predicting words, the probability of a word is dependent on previous predictions. Inspired by <ref type="bibr" target="#b7">(Dong et al., 2019)</ref>, we propose a pseudomasked language model (PMLM) to jointly pre-train a bidi-rectional LM for language understanding (e.g., text classification, and question answering) and a sequence-to-sequence LM for language generation (e.g., document summarization, and response generation). Specifically, the bidirectional model is pre-trained by autoencoding (AE) LMs, and the sequence-to-sequence model is pre-trained by partially autoregressive (PAR) LMs. As shown in <ref type="figure">Figure 1</ref>, the model parameters are shared in two language modeling tasks, and the encoding results of the given context tokens are reused. We use the conventional mask [MASK] (or [M] for short) to represent the corrupted tokens for AE pre-training. In order to handle factorization steps of PAR language modeling, we append pseudo masks [Pseudo] (or [P] for short) to the input sequence without discarding the original tokens. With well-designed self-attention masks and position embeddings, the PMLM can perform the two language modeling tasks in one forward pass without redundant computation of context.</p><p>The proposed method has the following advantages. First, the PMLM pre-trains different LMs in a unified manner, which learns both inter-relations between masked tokens and given context (via AE), and intra-relations between masked spans (via PAR). Moreover, conventional masks used for AE provide global masking information, so that every factorization step of PAR pre-training can access all the position embeddings as in fine-tuning. Second, the unified pre-training framework learns models for both natural language understanding and generation <ref type="bibr" target="#b7">(Dong et al., 2019)</ref>. Specifically, the AE-based modeling learns a bidirectional Transformer encoder, and the PAR objective pre-trains a sequence-to-sequence decoder. Third, the proposed model is computationally efficient in that the AE and PAR modeling can be computed in one forward pass. Because the encoding results of given context are reused for two language modeling tasks, redundant computation is avoided. Fourth, PAR language modeling learns token-to-token, token-tospan, and span-to-span relations during pre-training. By taking spans (i.e., continuous tokens) into consideration, PMLM is encouraged to learn long-distance dependencies by preventing local shortcuts.</p><p>We conduct PMLM pre-training on large-scale text corpora. Then we fine-tune the pre-trained model to a wide range of natural language understanding and generation tasks. Experimental results show that unified pre-training using PMLM improves performance on various benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Backbone Network: Transformer</head><p>First, we pack the embeddings of input tokens</p><formula xml:id="formula_0">{x i } |x| i=1</formula><p>together into H 0 = [x 1 , · · · , x |x| ] ∈ R |x|×d h . Then L stacked Transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> blocks compute the encoding vectors via:</p><formula xml:id="formula_1">H l = Transformer l (H l−1 ), l ∈ [1, L]<label>(1)</label></formula><p>where L is the number of layers. The hidden vectors of the final layer H L = [h L 1 , · · · , h L |x| ] are the contextualized representations of input. Within each Transformer block, multiple self-attention heads aggregate the output vectors of the previous layer, followed by a fully-connected feedforward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Masks</head><p>The output A l of a self-attention head in the l-th Transformer layer is:</p><formula xml:id="formula_2">Q = H l−1 W Q l , K = H l−1 W K l M ij = 0, allow to attend −∞, prevent from attending (2) A l = softmax( QK √ d k + M)(H l−1 W V l )</formula><p>where parameters W Q l , W K l , W V l ∈ R d h ×d k project the previous layer's output H l−1 to queries, keys, and values, respectively. It is worth noting that the mask matrix M ∈ R |x|×|x| controls whether two tokens can attend each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Input Representation</head><p>The inputs of language model pre-training are sequences sampled from large-scale text corpora. We follow the format used by BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. We add a special start-of-sequence token [SOS] at the beginning to get the representation of the whole input. Besides, each text is split into two segments appended with a special end-ofsequence token <ref type="bibr">[EOS]</ref>. The final input format is "[SOS] S1 [EOS] S2 [EOS]", where the segments S1 and S2 are contiguous texts. The vector of an input token is represented by the summation of its token embedding, absolute position embedding, and segment embedding. All the embedding vectors are obtained by lookup in learnable matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unified Language Model Pre-Training</head><p>We propose a pseudo-masked language model (PMLM) to jointly pre-train both autoencoding (Section 3.1.1) and partially autoregressive (Section 3.1.2) LMs. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, PMLM reuses the encoding results of the same example to jointly pre-train both modeling methods by pseudo masking (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-Training Tasks</head><p>We use the masked language modeling (MLM; <ref type="bibr" target="#b5">Devlin et al. 2018</ref>) task to pre-train a Transformer network, which is also known as the cloze task <ref type="bibr" target="#b28">(Taylor, 1953)</ref>. For a given input, we randomly substitute tokens with a special token [MASK]  The bidirectional LM is trained by autoencoding MLM, and the sequence-to-sequence (Seq-to-Seq) LM is trained by partially autoregressive MLM. We use different self-attention masks to control the access to context for each word token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorization Order Probability of Masked Tokens</head><p>Autoencoding (e.g., BERT, and our work) − p(x 2 |x \{2,4,5} )p(x 3 |x \{2,4,5} )p(x 5 |x \{2,4,5} )</p><p>Autoregressive (e.g., GPT, and XLNet) 2 → 4 → 5 5 → 4 → 2 p(x 2 |x \{2,4,5} )p(x 4 |x \{4,5} )p(x 5 |x \{5} ) p(x 5 |x \{2,4,5} )p(x 4 |x \{2,4} )p(x 2 |x \{2} )</p><p>Partially Autoregressive (our work) 2 → 4, 5 4, 5 → 2 p(x 2 |x \{2,4,5} )p(x 4 |x \{4,5} )p(x 5 |x \{4,5} ) p(x 4 |x \{2,4,5} )p(x 5 |x \{2,4,5} )p(x 2 |x \{2} ) <ref type="table">Table 1</ref>. Given input x = x1 · · · x6, the tokens x2, x4, x5 are masked. We compare how to compute p(x2, x4, x5|x \{2,4,5} ) with different factorization orders for autoencoding, autoregressive, and partially autoregressive masked language models.</p><p>(or [M] for short). The training objective is to recover them by conditioning on the output hidden states of Transformer.</p><p>As shown in <ref type="table">Table 1</ref>, we categorize MLMs into autoencoding, autoregressive, and partially autoregressive. Their main difference is how the probability of masked tokens is factorized. In our work, we leverage autoencoding (AE) and partially autoregressive (PAR) modeling for pre-training, which is formally described as follows. It is worth noting that the masked positions are the same for both AE and PAR modeling, but the probability factorization is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">AUTOENCODING MODELING</head><p>The autoencoding method independently predicts the tokens by conditioning on context, which is the same as BERT. Given original input x = x 1 · · · x |x| and the positions of masks M = {m 1 , · · · , m |M | }, the probability of masked tokens is computed by m∈M p(x m |x \M ), where x M = {x m } m∈M , \ is set minus, x \M means all input tokens except the ones that are in M . The autoencoding pre-training loss is defined as:</p><formula xml:id="formula_3">L AE = − x∈D log m∈M p(x m |x \M )<label>(3)</label></formula><p>where D is the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">PARTIALLY AUTOREGRESSIVE MODELING</head><p>We propose to pre-train partially autoregressive MLMs. In each factorization step, the model can predict one or multiple tokens. Let M = M 1 , · · · , M |M | denote factorization order, where M i = {m i 1 , · · · , m i |Mi| } is the set of mask positions in the i-th factorization step. If all factorization steps only contain one masked token (i.e., |M i | = 1), the modeling becomes autoregressive. In our work, we enable a Algorithm 1 Blockwise Masking</p><formula xml:id="formula_4">Input x = x1 · · · x |x| : Input sequence Output M = M1, · · · , M |M | : Masked positions M ← repeat p ← rand int(1, |x|) Randomly sample an index l ← rand int(2, 6) if rand() &lt; 0.4 else 1 if xp, · · · , x p+l−1 has not been masked then M .append({m} p+l−1 m=p ) until |M | j=1 |Mj| ≥ 0.15|x|</formula><p>Masking ratio is 15% return M factorization step to be a span, which makes the LM partially autoregressive. The probability of masked tokens is decomposed as:</p><formula xml:id="formula_5">p(x M |x \M ) = |M | i=1 p(x Mi |x \M ≥i ) (4) = |M | i=1 m∈Mi p(x m |x \M ≥i ) (5) where x Mi = {x m } m∈Mi , and M ≥i = j≥i M j .</formula><p>The partially autoregressive pre-training loss is defined as:</p><formula xml:id="formula_6">L PAR = − x∈D E M log p x M |x \M (6)</formula><p>where E M is the expectation over the factorization distribution. During pre-training, we randomly sample one factorization order M for each input text , rather than computing the exact expectation.</p><p>Blockwise Masking and Factorization Given input sequence x, the masking policy uniformly produces a factorization order M = M 1 , · · · , M |M | for Equation <ref type="formula">(6)</ref>. For the i-th factorization step, the masked position set M i contains one token, or a continuous text span . As described in Algorithm 1, we randomly sample 15% of the original tokens as masked tokens. Among them, 40% of the time we mask a n-gram block, and 60% of the time we mask a token. We then construct a factorization step with the set of masked positions. We repeat the above process until enough masked tokens are sampled. The randomly sampled factorization orders are similar to permutation-based language modeling used by XLNet . However, XLNet only emits predictions one by one (i.e., autoregressive). In contrast, we can generate one token, or a text span at each factorization step (i.e., partially autoregressive).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pseudo-Masked LM</head><p>Equation <ref type="formula">(5)</ref> indicates that factorization steps of partially autoregressive language modeling are conditioned on different context. So if masked language models <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> [M]  are directly used, we have to construct a new cloze instance (as shown in <ref type="figure" target="#fig_2">Figure 3</ref>) for each factorization step, which renders partially autoregressive pre-training infeasible. We propose a new training procedure, named as pseudo-masked language model (PMLM), to overcome the issue.</p><p>For the last example in <ref type="table">Table 1</ref>, <ref type="figure" target="#fig_3">Figure 4</ref> shows how the PMLM conducts partially autoregressive predictions. Rather than replacing the tokens with masks as in vanilla MLMs, we keep all original input tokens unchanged and append pseudo masks to the input sequence. For each masked token, we insert a [Pseudo] (or [P] for short) token with the same position embedding of the corresponding token. The top-layer hidden states of [P] tokens are fed into a softmax classifier for MLM predictions. Notice that positional information in Transformer is encoded by (absolute) position embeddings, while the model components are order-agnostic. In other words, no matter where a token appears in the input sequence, the position of the token is only determined by its position embedding. So we can assign the same position embedding to two tokens, and Transformer treats both of the tokens as if they have the same position.</p><p>Vanilla MLMs allow all tokens to attend to each other, while PMLM controls accessible context for each token according to the factorization order. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the example's factorization order is 4, 5 → 2. When we compute p(x 4 , x 5 |x \{2,4,5} ), only x 1 , x 3 , x 6 and the pseudo masks of x 4 , x 5 are conditioned on. The original tokens of x 4 , x 5 are masked to avoid information leakage, while their pseudo tokens [P] are used as placeholders for MLM predictions. In the second step, the tokens x 1 , x 3 , x 4 , x 5 , x 6  and the pseudo mask of x 2 are conditioned on to compute p(x 2 |x \{2} ). Unlike in the first step, the original tokens of x 4 , x 5 are used for the prediction.</p><p>Self-attention masks (as described in Section 2.1) are used to control what context a token can attend to when computing its contextualized representation. <ref type="figure" target="#fig_4">Figure 5</ref> shows the selfattention mask matrix used for the example of <ref type="figure" target="#fig_3">Figure 4</ref>. The self-attention mask matrix is designed in order to avoid two kinds of information leakage. The first type is explicit leakage, i.e., the masked token can be directly accessed by its pseudo token, which renders the LM prediction trivial. So pseudo tokens [P] are not allowed to attend to the content of "themselves" in a PMLM. The second type is implicit leakage, which implicitly leaks prediction information by multi-step attention propagations. For example, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>, if the context token x 6 has access to x 4 , there is a connected attention flow "x 4 's pseudo mask token → x 6 → x 4 ", which eases the prediction of x 4 . As a result, for each token, we mask the attentions to the tokens that are predicted in the future factorization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unified Pre-Training</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we jointly pre-train bidirectional and sequence-to-sequence LMs with the same input text and masked positions. Both the special tokens [M] and [P] emit predicted tokens. The training objective is to maximize the likelihood of correct tokens, which considers two types of LMs (i.e., autoencoding, and partially autoregressive) in </p><formula xml:id="formula_7">L = L AE + L PAR<label>(7)</label></formula><p>where L AE , L PAR are defined as in Equation <ref type="formula" target="#formula_3">(3)</ref>, and Equation (6) respectively. The proposed method sufficiently reuses the computed hidden states for both LM objectives.</p><p>In addition, experiments in Section 4.6 show that the pretraining tasks are complementary to each other, as they capture both inter-(i.e., between given context and masked tokens) and intra-(i.e., among masked tokens) relations of the input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fine-tuning on NLU and NLG Tasks</head><p>Following <ref type="bibr" target="#b7">(Dong et al., 2019)</ref>, we fine-tune the pre-trained PMLM (with additional task-specific layers if necessary) to both natural language understanding (NLU) and natural language generation (NLG) tasks.</p><p>For NLU tasks, we fine-tune PMLM as a bidirectional Transformer encoder, like BERT. Let us take text classification as an example. Similar to the text format described in Section 2.2, the input is "[SOS] TEXT [EOS]". We use the encoding vector of [SOS] as the representation of input, and then feed it to a randomly initialized softmax classifier (i.e., the task-specific output layer). We maximize the likelihood of the labeled training data by updating the parameters of the pre-trained PMLM and the added softmax classifier.</p><p>For sequence-to-sequence generation tasks, the example is concatenated as "[SOS] SRC [EOS] TGT [EOS]", where SRC and TGT are source and target sequences, respectively. The fine-tuning procedure is similar to pre-training as in Section 3.2. For a source sequence, the dependencies between the tokens are bidirectional, i.e., all the source tokens can attend to each other. In contrast, the target sequence  <ref type="table">Table 3</ref>. Results of BASE-size models on the development set of the GLUE benchmark. We report Matthews correlation coefficient (MCC) for CoLA, Pearson correlation coefficient (PCC) for STS, and accuracy (Acc) for the rest. Metrics of UNILMv2 are averaged over five runs for the tasks. "-rel pos" is the ablation model without relative position bias.</p><p>is produced in an autoregressive manner. So we append a pseudo mask [P] for each target token, and use selfattention masks to perform autoregressive generation. The fine-tuning objective is to maximize the likelihood of the target sequence given source input. It is worth noting that [EOS] is used to mark the end of the target sequence. Once [EOS] is emitted, we terminate the generation process of the target sequence. During decoding, we use beam search to generate the target tokens one by one <ref type="bibr" target="#b7">(Dong et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We employ pseudo-masked language model to conduct unified language model pre-training (UNILMv2), and finetuned the model on both natural language understanding (i.e., question answering, the GLUE benchmark) and generation (i.e., abstractive summarization, and question generation) tasks. Details about hyperparameters and datasets can be found in the supplementary material. In addition, we conducted ablation studies to compare different choices of pre-training objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-Training Setup</head><p>We followed the same model size as BERT BASE <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> for comparison purposes. Specifically, we used a 12-layer Transformer with 12 attention heads. The hidden size was 768, and inner hidden size of feed-forward network was 3072. The weight matrix of the softmax classifier was tied with the token embedding matrix. We also add relative position bias <ref type="bibr" target="#b21">(Raffel et al., 2019)</ref> to attention scores. The whole model contains about 110M parameters.</p><p>For fair comparisons, we report the major results using similar pre-training datasets and optimization hyperparameters as in RoBERTa BASE . We use 160GB text corpora from English Wikipedia, BookCorpus <ref type="bibr" target="#b38">(Zhu et al., 2015)</ref>, OpenWebText 1 , CC-News , and 1 skylion007.github.io/OpenWebTextCorpus</p><p>Stories <ref type="bibr" target="#b29">(Trinh &amp; Le, 2018)</ref>. We follow the preprocess and the uncased WordPiece <ref type="bibr" target="#b34">(Wu et al., 2016)</ref> tokenization used in <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. The vocabulary size was 30, 522.</p><p>The maximum length of input sequence was 512. The token masking probability was 15%. Among masked positions, 80% of the time we replaced the token with masks, 10% of the time with a random token, and keeping the original token for the rest. The block masking (see Algorithm 1) can mask up to 6-gram for one factorization step in partially autoregressive modeling. The batch size was set to 7680. We used Adam <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref> with β 1 = 0.9, β 2 = 0.98, and = 1e-6 for optimization. The peak learning rate was set to 6e-4, with linear warmup over the first 24, 000 steps and linear decay. The weight decay was 0.01. The dropout rate was set to 0.1. We ran the pre-training procedure for 0.5 million steps, which took about 20 days using 64 Nvidia V100-32GB GPU cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Question Answering</head><p>Question answering aims at returning answers for the given question and documents. We conduct experiments on the benchmarks SQuAD v1.1 <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref> and v2.0 <ref type="bibr" target="#b23">(Rajpurkar et al., 2018)</ref>. The model learns to extract answer spans within a passage. We formulate the task as a natural language understanding problem. The input is concatenated as "[SOS] Question [EOS] Passage [EOS]". We add a classification layer on the pre-trained PMLM, which predicts whether each token is the start or end position of an answer span by conditioning on the final outputs of Transformer. For SQuAD v2.0, we use the output vector of [SOS] to predict whether the instance is unanswerable or not.</p><p>The fine-tuning results are presented in <ref type="table">Table 2</ref>, where we report F1 scores and exact match (EM) scores. We compare previous BASE-size models with PMLM. Notice that the publicly available BERT BASE checkpoint <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> is pre-trained on 13GB corpora with 256 batch size, while  <ref type="table">Table 5</ref>. Results on question generation. The first block follows the data split in <ref type="bibr" target="#b8">(Du &amp; Cardie, 2018)</ref>, while the second block is the same as in <ref type="bibr" target="#b37">(Zhao et al., 2018)</ref>. MTR is short for METEOR, and RG for ROUGE. "#Param" indicates the size of pre-trained models. "-rel pos" is the model without relative position bias.</p><p>XLNet BASE and RoBERTa BASE are more directly comparable.</p><p>The results show that UNILMv2 BASE achieves better performance than the other models on both SQuAD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">GLUE Benchmark</head><p>The General Language Understanding Evaluation (GLUE) benchmark    <ref type="table">Table 3</ref> presents the results on GLUE. We compare PMLM with three strong pre-trained models, i.e., BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, XLNet , and RoBERTa , in the single task fine-tuning setting. All the models are in BASE-size for fair comparisons. We observe that the proposed UNILMv2 BASE outperforms both BERT BASE and XLNet BASE across 8 tasks. Comparing to state-of-the-art pre-trained RoBERTa BASE , UNILMv2 BASE obtains the best performance on 6 out of 8 tasks, e.g., 88.4 vs 87.6 (RoBERTa BASE ) in terms of MNLI accuracy, indicating the effectiveness of our UNILMv2 BASE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Abstractive Summarization</head><p>We evaluate the pre-trained PMLM on two abstractive summarization datasets, i.e., XSum <ref type="bibr" target="#b17">(Narayan et al., 2018)</ref>, and the non-anonymized version of CNN/DailyMail <ref type="bibr" target="#b24">(See et al., 2017)</ref>. This is a language generation task, where the texts (such as news articles) are shortened to readable summaries that preserve salient information of the original texts. The pre-trained PMLM is fine-tuned as a sequence-to-sequence model as described in Section 3.4.</p><p>We report ROUGE scores <ref type="bibr" target="#b14">(Lin, 2004)</ref>   <ref type="table">Table 6</ref>. Comparisons between the pre-training objectives. All models are pre-trained over WIKIPEDIA and BOOKCORPUS for one million steps with a batch size of 256. Results in the second block are average over five runs for each task. We report F1 and exact match (EM) scores for SQuAD, and accuracy (Acc) for MNLI and  baselines. We also compare UNILMv2 BASE with state-ofthe-art pre-trained models of both BASE-size and LARGEsize. We focus on the comparisons in the third block because the models contain similar numbers of parameters. BERT-SUMABS <ref type="bibr" target="#b15">(Liu &amp; Lapata, 2019)</ref> fine-tunes a BERT encoder that is pre-trained with an autoencoding objective, concatenating with a randomly initialized decoder. MASS <ref type="bibr" target="#b26">(Song et al., 2019)</ref> and T5 <ref type="bibr" target="#b21">(Raffel et al., 2019)</ref> pre-train encoderdecoder Transformers with masked LM, which relies on the autoregressive pre-training. Although PMLM has the smallest size, we find that UNILMv2 BASE outperforms the other BASE-size pre-trained models on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Question Generation</head><p>We perform evaluations on question generation <ref type="bibr" target="#b8">(Du &amp; Cardie, 2018)</ref>, the task of automatically producing relevant questions that ask for the given answer and context. The input of the sequence-to-sequence problem is defined as the concatenation of a paragraph and an answer. We fine-tune the pre-trained PMLM to predict output questions.</p><p>As shown in <ref type="table">Table 5</ref>, we report BLEU <ref type="bibr">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b0">(Banerjee &amp; Lavie, 2005)</ref>, and ROUGE <ref type="bibr" target="#b14">(Lin, 2004</ref>) scores on two different data splits. Among the compared results, UNILM <ref type="bibr" target="#b7">(Dong et al., 2019)</ref> is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features <ref type="bibr" target="#b8">(Du &amp; Cardie, 2018)</ref>, gated self-attention <ref type="bibr" target="#b37">(Zhao et al., 2018)</ref>, and reinforcement learning <ref type="bibr" target="#b36">(Zhang &amp; Bansal, 2019)</ref>. Results show that UNILMv2 BASE achieves better evaluation metrics compared with UNILM LARGE and several baselines. It is worth noting that UNILMv2 BASE consists of three times fewer parameters than UNILM LARGE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effect of Pre-Training Objectives</head><p>We conduct ablation experiments on using PMLM to implement different pre-training objectives, i.e., autoencoding (AE), autoregressive (AR), partially autoregressive (PAR), and jointly training (AE+AR, and AE+PAR). The evaluations follow the same settings 2 as in BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, so that the results in <ref type="table">Table 6</ref> can be directly compared with each other. Notice that XLNet  is an autoregressive MLM augmented with more advanced relative position embeddings, and long-context memory.</p><p>As shown in <ref type="table">Table 6</ref>, we compare the PMLM-based variants against previous models on question answering (SQuAD; <ref type="bibr" target="#b22">Rajpurkar et al. 2016;</ref>, natural language inference (MNLI; <ref type="bibr" target="#b33">Williams et al. 2018)</ref>, and sentiment classification (SST-2; <ref type="bibr" target="#b25">Socher et al. 2013)</ref>. First, we ablate relative position bias to better compare with BERT, RoBERTa, and BART. On text classification (MNLI and SST-2), the PAR-only objective compares favorably with both AE-only and AR-only objectives, which indicates the effectiveness of the proposed PAR modeling. In comparison, the SQuAD tasks require more precise modeling of spans in order to extract correct answer spans from the input passage, where both AE-only and PAR-only objectives outperform the AR-only objective.</p><p>The results indicate that block masking and factorization are important for LM pre-training. Besides, the settings of jointly training (AE+AR, and AE+PAR) tend to improve the results over using single LM task. Among the five objectives, AE+PAR performs the best with the help of PMLM, which shows that autoencoding and partially autoregressive modelings are complementary for pre-training.</p><p>2 Models were trained for 1M steps with batch size of 256 over English Wikipedia and BookCorpus <ref type="bibr" target="#b38">(Zhu et al., 2015)</ref>. The learning rate of Adam (β1 = 0.9, β2 = 0.999) was set to 1e-4, with linear schedule and warmup over the first 10K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We pre-train a unified language model for language understanding and generation by joint learning bidirectional LM (via AE) and sequence-to-sequence LM (via PAR). We introduce a pseudo-masked language model (PMLM) to efficiently realize the unified pre-training procedure. PMLM is computationally efficient in that AE and PAR can be computed in one forward pass without redundant computation. Besides, the two modeling tasks are complementary to each other. Because conventional masks of AE provide global masking information to PAR, and PAR can learn intra-relations between masked spans. In addition, the proposed PAR pre-training encourages to learn long-distance dependencies by preventing local shortcuts. Experimental results show that PMLM improves the end-task results on several language understanding and generation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameters for Pre-Training</head><p>As shown in <ref type="table">Table 7</ref>, we present the hyperparameters used for pre-training UNILMv2 BASE . We use the same Word-Piece <ref type="bibr" target="#b34">(Wu et al., 2016)</ref> vocabulary and model size as BERT BASE <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. We follow the optimization hyperparameters of RoBERTa BASE    <ref type="table" target="#tab_8">Table 8</ref> summarizes the datasets used for the General Language Understanding Evaluation (GLUE) benchmark .   <ref type="table" target="#tab_9">Table 9</ref> reports the hyperparameters used for fine-tuning UNILMv2 BASE over SQuAD v1.10 <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref> / v2.0 <ref type="bibr" target="#b23">(Rajpurkar et al., 2018)</ref>, and the GLUE benchmark . The hyperparameters are searched on the development sets according to the average performance of five runs. We use the same hyperparameters for both SQuAD question answering datasets. Moreover, we list the hyperparameters used for the GLUE datasets in <ref type="table" target="#tab_9">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GLUE Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset #Train/#Dev/#Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyperparameters for NLU Fine-Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameters for NLG Fine-Tuning</head><p>As shown in <ref type="table" target="#tab_10">Table 10</ref>, we present the hyperparameters used for the natural language generation datasets, i.e., CNN/Dai-lyMail <ref type="bibr" target="#b24">(See et al., 2017)</ref>, XSum <ref type="bibr" target="#b17">(Narayan et al., 2018)</ref>, and SQuAD question generation (QG; <ref type="bibr" target="#b8">Du &amp; Cardie 2018;</ref><ref type="bibr" target="#b37">Zhao et al. 2018</ref>). The total length is set to 512 for QG, and 768 for CNN/DailyMail and XSum. The maximum output length is set to 160 for CNN/DailyMail, and 48 for XSum and QG. The label smoothing <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref> rate is 0.1. During decoding, we use beam search to generate the outputs. Length penalty <ref type="bibr" target="#b34">(Wu et al., 2016)</ref> is also used to score candidates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/DailyMail XSum QG</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of PMLM pre-training. The model parameters are shared across the LM objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparisons between autoencoding (AE), autoregressive (AR), and partially autoregressive (PAR) masked language models. In the example x = x1 · · · x6, the tokens x2, x4, x5 are masked by the special tokens [M] and [P].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Example of the factorization steps 4, 5 → 2. The masks [P] and [M] are assigned with the same position embeddings as the corresponding tokens. Different context is used to compute the hidden states for the pseudo masks of x4, x5 and x2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Self-attention mask of the factorization steps 4, 5 → 2. Both conventional masks [M] and given context (x1, x3, x6) can be attended by all the tokens. one example. The loss is computed via:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>contains various tasks. There are two single-sentence classification tasks, i.e., linguistic acceptability (CoLA;<ref type="bibr" target="#b32">Warstadt et al. 2018)</ref>, and sentiment analysis (SST-2;<ref type="bibr" target="#b25">Socher et al. 2013)</ref>. The text similarity (STS;<ref type="bibr" target="#b3">Cer et al. 2017)</ref> task is formulated as a regression problem. The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI;<ref type="bibr" target="#b4">Dagan et al. 2006;</ref><ref type="bibr" target="#b1">Bar-Haim et al. 2006;</ref><ref type="bibr" target="#b9">Giampiccolo et al. 2007;</ref><ref type="bibr" target="#b2">Bentivogli et al. 2009;</ref><ref type="bibr" target="#b33">Williams et al. 2018)</ref>, question answering (QNLI;<ref type="bibr" target="#b22">Rajpurkar et al. 2016)</ref>, and paraphrase detection (QQP, MRPC;<ref type="bibr" target="#b6">Dolan &amp; Brockett 2005)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Abstractive summarization results on CNN/DailyMail and XSum. The evaluation metric is the F1 version of ROUGE (RG) scores. We also present the number of parameters (#Param) for the methods using pre-trained models.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="2">#Param</cell><cell>CNN/DailyMail RG-1 RG-2 RG-L</cell><cell>XSum RG-1 RG-2 RG-L</cell></row><row><cell cols="2">Without pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LEAD-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.42 17.62 36.67</cell><cell>16.30 1.60 11.95</cell></row><row><cell cols="3">PTRNET (See et al., 2017)</cell><cell></cell><cell></cell><cell>39.53 17.28 36.38</cell><cell>28.10 8.02 21.72</cell></row><row><cell cols="4">Fine-tuning LARGE-size pre-trained models</cell><cell></cell><cell></cell></row><row><cell cols="3">UNILM LARGE (Dong et al., 2019)</cell><cell>340M</cell><cell></cell><cell>43.08 20.43 40.34</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">BART LARGE (Lewis et al., 2019)</cell><cell>400M</cell><cell></cell><cell>44.16 21.28 40.90</cell><cell>45.14 22.27 37.25</cell></row><row><cell cols="3">T5 11B (Raffel et al., 2019)</cell><cell>11B</cell><cell></cell><cell>43.52 21.55 40.69</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Fine-tuning BASE-size pre-trained models</cell><cell></cell><cell></cell></row><row><cell cols="3">MASS BASE (Song et al., 2019)</cell><cell>123M</cell><cell></cell><cell>42.12 19.50 39.01</cell><cell>39.75 17.24 31.95</cell></row><row><cell cols="4">BERTSUMABS (Liu &amp; Lapata, 2019) 156M</cell><cell></cell><cell>41.72 19.39 38.76</cell><cell>38.76 16.33 31.15</cell></row><row><cell cols="3">T5 BASE (Raffel et al., 2019)</cell><cell>220M</cell><cell></cell><cell>42.05 20.34 39.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UNILMv2 BASE</cell><cell></cell><cell>110M</cell><cell></cell><cell>43.16 20.42 40.14</cell><cell>44.00 21.11 36.08</cell></row><row><cell cols="2">-relative position bias</cell><cell></cell><cell>110M</cell><cell></cell><cell>43.45 20.71 40.49</cell><cell>43.69 20.71 35.73</cell></row><row><cell></cell><cell cols="4">#Param BLEU-4 MTR RG-L</cell><cell></cell></row><row><cell cols="2">(Du &amp; Cardie, 2018)</cell><cell>15.16</cell><cell>19.12</cell><cell>-</cell><cell></cell></row><row><cell cols="2">(Zhang &amp; Bansal, 2019)</cell><cell>18.37</cell><cell cols="2">22.65 46.68</cell><cell></cell></row><row><cell>UNILM LARGE</cell><cell>340M</cell><cell>22.78</cell><cell cols="2">25.49 51.57</cell><cell></cell></row><row><cell cols="2">UNILMv2 BASE 110M</cell><cell>24.43</cell><cell cols="2">26.34 51.97</cell><cell></cell></row><row><cell>-rel pos</cell><cell>110M</cell><cell>24.70</cell><cell cols="2">26.33 52.13</cell><cell></cell></row><row><cell cols="2">(Zhao et al., 2018)</cell><cell>16.38</cell><cell cols="2">20.25 44.48</cell><cell></cell></row><row><cell cols="2">(Zhang &amp; Bansal, 2019)</cell><cell>20.76</cell><cell cols="2">24.20 48.91</cell><cell></cell></row><row><cell>UNILM LARGE</cell><cell>340M</cell><cell>24.32</cell><cell cols="2">26.10 52.69</cell><cell></cell></row><row><cell cols="2">UNILMv2 BASE 110M</cell><cell>26.29</cell><cell cols="2">27.16 53.22</cell><cell></cell></row><row><cell>-rel pos</cell><cell>110M</cell><cell>26.30</cell><cell cols="2">27.09 53.19</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Summary of the GLUE benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Hyperparameters used for fine-tuning on SQuAD, and GLUE.</figDesc><table><row><cell></cell><cell>SQuAD v1.1/v2.0</cell><cell>GLUE</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>{16, 32}</cell></row><row><cell>Learning rate</cell><cell>2e-5</cell><cell>{5e-6, 1e-5, 1.5e-5, 2e-5, 3e-5}</cell></row><row><cell>LR schedule</cell><cell></cell><cell>Linear</cell></row><row><cell>Warmup ratio</cell><cell>0.1</cell><cell>{0.1, 0.2}</cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell>{0.01, 0.1}</cell></row><row><cell>Epochs</cell><cell>4</cell><cell>{10, 15}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Hyperparameters used for fine-tuning and decoding on CNN/DailyMail, XSum, and question generation (QG).</figDesc><table><row><cell>Fine-Tuning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate</cell><cell>7e-5</cell><cell>7e-5</cell><cell>2e-5</cell></row><row><cell>Batch size</cell><cell>64</cell><cell>64</cell><cell>48</cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell></cell><cell></cell></row><row><cell>Epochs</cell><cell>14</cell><cell>14</cell><cell>16</cell></row><row><cell>Learning rate schedule</cell><cell>Linear</cell><cell></cell><cell></cell></row><row><cell>Warmup ratio</cell><cell>0.02</cell><cell>0.02</cell><cell>0.1</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>Max input length</cell><cell>608</cell><cell>720</cell><cell>464</cell></row><row><cell>Max output length</cell><cell>160</cell><cell>48</cell><cell>48</cell></row><row><cell>Decoding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Length penalty</cell><cell>0.7</cell><cell>0.6</cell><cell>1.3</cell></row><row><cell>Beam size</cell><cell>5</cell><cell>5</cell><cell>8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Text Analysis Conference</title>
		<meeting>Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similaritymultilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges</title>
		<meeting>the First International Conference on Machine Learning Challenges</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Harvesting paragraph-level questionanswer pairs from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1907" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spanbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ALBERT: A lite bert for selfsupervised learning of language representations. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training Papineni</title>
		<meeting><address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Canada</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1806.02847</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Addressing semantic drift in question generation for semi-supervised question answering. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Paragraph-level neural question generation with maxout pointer and gated self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3901" to="3910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
