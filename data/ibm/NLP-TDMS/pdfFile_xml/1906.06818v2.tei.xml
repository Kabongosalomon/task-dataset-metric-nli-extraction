<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Capsule Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
							<email>adamk@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Whye Teh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Applied</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Oxford Robotics Institute University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Oxford § Google Brain Toronto ∇ DeepMind London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stacked Capsule Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Figure 1: SCAEs learn to explain different object classes with separate object capsules, thereby doing unsupervised classification. Here, we show TSNE embeddings of object capsule presence probabilities for 10000 MNIST digits. Individual points are color-coded according to the corresponding digit class.</p><p>Convolutional neural networks (CNN) work better than networks without weight-sharing because of their inductive bias: if a local feature is useful in one image location, the same feature is likely to be useful in other locations. It is tempting to exploit other effects of viewpoint changes by replicating features across scale, orientation and other affine degrees of freedom, but this quickly leads to cumbersome, high-dimensional feature maps.</p><p>An alternative to replicating features across the non-translational degrees of freedom is to explicitly learn transformations between the natural coordinate frame of a whole object and the natural coordinate frames of each of its parts. Computer graphics relies on such object→part coordinate transformations to represent the geometry of an object in a viewpoint-invariant manner. Moreover, there is strong evidence that, unlike standard CNNs, human vision also relies on coordinate frames: imposing an unfamiliar coordinate frame on a familiar object makes it challenging to recognize the object or its geometry <ref type="bibr" target="#b31">(Rock, 1973;</ref><ref type="bibr" target="#b11">Hinton, 1979)</ref>.</p><p>A neural system can learn to reason about transformations between objects, their parts and the viewer, but each kind of transformation will likely need to be represented differently. An object-partrelationship (OP) is viewpoint-invariant, approximately constant and could be easily coded by learned weights. The relative coordinates of an object (or a part) with respect to the viewer change with the viewpoint (they are viewpointequivariant), and could be easily coded with neural activations 2 .</p><p>With this representation, the pose of a single object is represented by its relationship to the viewer. Consequently, representing a single object does not necessitate replicating neural activations across space, unlike in CNNs. It is only processing two (or more) different instances of the same type of object in parallel that requires spatial replicas of both model parameters and neural activations.</p><p>In this paper we propose the Stacked Capsule Autoencoder (SCAE), which has two stages <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The first stage, the Part Capsule Autoencoder (PCAE), segments an image into constituent parts, infers their poses, and reconstructs the image by appropriately arranging affine-transformed part templates. The second stage, the Object Capsule Autoencoder (OCAE), tries to organize discovered parts and their poses into a smaller set of objects. These objects then try to reconstruct the part poses using a separate mixture of predictions for each part. Every object capsule contributes components to each of these mixtures by multiplying its pose-the object-viewer-relationship (OV)-by the relevant object-part-relationship (OP).</p><p>Stacked Capsule Autoencoders (Section 2) capture spatial relationships between whole objects and their parts when trained on unlabelled data. The vectors of presence probabilities for the object capsules tend to form tight clusters (cf. <ref type="figure">Figure 1</ref>), and when we assign a class to each cluster we achieve state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%), which can be further improved to 67% and 99%, respectively, by learning fewer than 300 parameters (Section 3). We describe related work in Section 4 and discuss implications of our work and future directions in Section 5. The code is available at github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stacked Capsule Autoencoders (SCAE)</head><p>Segmenting an image into parts is non-trivial, so we begin by abstracting away pixels and the partdiscovery stage, and develop the Constellation Capsule Autoencoder (CCAE) (Section 2.1). It uses two-dimensional points as parts, and their coordinates are given as the input to the system. CCAE learns to model sets of points as arrangements of familiar constellations, each of which has been transformed by an independent similarity transform. The CCAE learns to assign individual points to their respective constellations-without knowing the number of constellations or their shapes in advance. Next, in Section 2.2, we develop the Part Capsule Autoencoder (PCAE) which learns to infer parts and their poses from images. Finally, we stack the Object Capsule Autoencoder (OCAE), which closely resembles the CCAE, on top of the PCAE to form the Stacked Capsule Autoencoder (SCAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constellation Autoencoder (CCAE)</head><p>Let {x m | m = 1, . . . , M } be a set of two-dimensional input points, where every point belongs to a constellation as in <ref type="figure">Figure 3</ref>. We first encode all input points (which take the role of part capsules) with Set Transformer <ref type="bibr" target="#b22">(Lee et al., 2019</ref>)-a permutation-invariant encoder h caps based on attention mechanisms-into K object capsules. An object capsule k consists of a capsule feature vector c k , its presence probability a k ∈ [0, 1] and a 3 × 3 object-viewer-relationship (OV) matrix, which represents the affine transformation between the object (constellation) and the viewer. Note that each object capsule can represent only one object at a time. Every object capsule uses a separate multilayer perceptron (MLP) h part k to predict N ≤ M part candidates from the capsule feature vector c k . Each candidate consists of the conditional probability a k,n ∈ [0, 1] that a given candidate part exists, an associated scalar standard deviation λ k,n , and a 3 × 3 object-part-relationship (OP) matrix, which represents the affine transformation between the object capsule and the candidate part 3 . Candidate predictions µ k,n are given by the product of the object capsule OV and the candidate OP matrices. We model all input points as a single Gaussian mixture, where µ k,n and λ k,n are the centres and standard deviations of the isotropic Gaussian components. See Figures 2 and 6 for illustration; formal description follows:</p><formula xml:id="formula_0">OV 1:K , c 1:K , a 1:K = h caps (x 1:M ) predict object capsule parameters,<label>(1)</label></formula><p>OP k,1:N , a k,1:N , λ k,1:</p><formula xml:id="formula_1">N = h part k (c k ) decode candidate parameters from c k 's, (2) V k,n = OV k OP k,n decode a part pose candidate, (3) p(x m | k, n) = N (x m | µ k,n , λ k,n ) turn candidates into mixture components,<label>(4)</label></formula><formula xml:id="formula_2">p(x 1:M ) = M m=1 K k=1 N n=1 a k a k,n i a i j a i,j p(x m | k, n) .<label>(5)</label></formula><p>The model is trained without supervision by maximizing the likelihood of part capsules in Equation (5) subject to sparsity constraints, cf. Section 2.4 and Appendix C. The part capsule m can be assigned to the object capsule k by looking at the mixture component responsibility, that is k = arg max k a k a k,n p(x m | k, n). <ref type="bibr">4</ref> Empirical results show that this model is able to perform unsupervised instance-level segmentation of points belonging to different constellations, even in data which is difficult to interpret for humans. See <ref type="figure">Figure 3</ref> for an example and Section 3.1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part Capsule Autoencoder (PCAE)</head><p>Explaining images as geometrical arrangements of parts requires 1) discovering what parts are there in an image and 2) inferring the relationships of the parts to the viewer (their pose). For the CCAE a part is just a 2D point (that is, a (x, y) coordinate), but for the PCAE each part capsule m has a six-dimensional pose x m (two rotations, two translations, scale and shear), a presence variable d m ∈ [0, 1] and a unique identity. We frame the part-discovery problem as auto-encoding: the encoder learns to infer the poses and presences of different part capsules, while the decoder learns an image template T m for each part <ref type="figure">(Fig. 4</ref>) similar to <ref type="bibr" target="#b34">Tieleman, 2014;</ref><ref type="bibr" target="#b7">Eslami et al., 2016</ref>. If a part exists (according to its presence variable), the corresponding template is affine-transformed with the inferred pose giving T m . Finally, transformed templates are arranged into the image. The PCAE is followed by an Object Capsule Autoencoder (OCAE), which closely resembles the CCAE and is described in Section 2.3.</p><p>Let y ∈ [0, 1] h×w×c be the image. We limit the maximum number of part capsules to M and use an encoder to infer their poses x m , presence probabilities d m , and special features z m ∈ R cz , one per part capsule. Special features can be used to alter the templates in an input-dependent manner (we use them to predict colour, but more complicated mappings are possible). The special features also inform the OCAE about unique aspects of the corresponding part (e. g., occlusion or relation to other parts). Templates T m ∈ [0, 1] ht×wt×(c+1) are smaller than the image y, but have an additional alpha channel which allows occlusion by other templates. We use T a m to refer to the alpha channel and T c m to refer to its colours.</p><p>We allow each part capsule to be used only once to reconstruct an image, which means that parts of the same type are not repeated 5 . To infer part capsule parameters we use a CNN-based encoder followed by attention-based pooling, which is described in more detail in the Appendix E and whose effects on the model performance are analyzed in Section 3.3.</p><p>The image is modelled as a spatial Gaussian mixture, similarly to <ref type="bibr" target="#b8">Greff et al., 2019;</ref><ref type="bibr" target="#b6">Engelcke et al., 2019</ref>. Our approach differs in that we use pixels of the transformed templates (instead of component-wise reconstructions) as the centres of isotropic Gaussian components, but we also use constant variance. Mixing probabilities of different components are proportional to the product of presence probabilities of part capsules and the value of the learned alpha channel for every template. More formally:</p><formula xml:id="formula_3">x 1:M , d 1:M , z 1:M = h enc (y) predict part capsule parameters,<label>(6)</label></formula><p>c m = MLP(z m ) predict the color of the m th template,</p><p>T m = TransformImage(T m , x m ) apply affine transforms to image templates, (8)</p><formula xml:id="formula_5">p y m,i,j ∝ d m T a m,i,j compute mixing probabilities,<label>(9)</label></formula><formula xml:id="formula_6">p(y) = i,j M m=1 p y m,i,j N y i,j | c m · T c m,i,j ; σ 2 y calculate the image likelihood.<label>(10)</label></formula><p>Training the PCAE results in learning templates for object parts, which resemble strokes in the case of MNIST, see <ref type="figure">Figure 4</ref>. This stage of the model is trained by maximizing the image likelihood of Equation (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object Capsule Autoencoder (OCAE)</head><p>Having identified parts and their parameters, we would like to discover objects that could be composed of them 6 . To do so, we use concatenated poses x m , special features z m and flattened templates T m (which convey the identity of the part capsule) as an input to the OCAE, which differs from the CCAE in the following ways. Firstly, we feed part capsule presence probabilities d m into the OCAE's encoder-these are used to bias the Set Transformer's attention mechanism not to take absent points into account. Secondly, d m s are also used to weigh the part-capsules' log-likelihood, so that we do not take log-likelihood of absent points into account. This is implemented by raising the likelihood of the m th part capsule to the power of d m , cf. Equation <ref type="formula" target="#formula_2">(5)</ref>. Additionally, we stop the gradient on all of OCAE's inputs except the special features to improve training stability and avoid the problem of collapsing latent variables; see e. g., <ref type="bibr" target="#b29">Rasmus et al., 2015</ref>. Finally, parts discovered by the PCAE have independent identities (templates and special features rather than 2D points). Therefore, every part-pose is explained as an independent mixture of predictions from object-capsules-where every object capsule makes exactly M candidate predictions V k,1:M , or exactly one candidate prediction per part. Consequently, the part-capsule likelihood is given by,</p><formula xml:id="formula_7">p(x 1:M , d 1:M ) = M m=1 K k=1 a k a k,m i a i j a i,j p(x m | k, m) dm .<label>(11)</label></formula><p>The OCAE is trained by maximising the part pose likelihood of Equation <ref type="formula" target="#formula_0">(11)</ref>, and it learns to discover further structure in previously identified parts, leading to learning sparsely-activated object capsules, see <ref type="figure">Figure 5</ref>. Achieving this sparsity requires further regularization, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Achieving Sparse and Diverse Capsule Presences</head><p>Stacked Capsule Autoencoders are trained to maximise pixel and part log-likelihoods (L ll = log p(y) + log p(x 1:M )). If not constrained, however, they tend to either use all of the part and object capsules to explain every data example or collapse onto always using the same subset of capsules, regardless of the input. We want the model to use different sets of part-capsules for different input examples and to specialize object-capsules to particular arrangements of parts. To encourage this, we impose sparsity and entropy constraints. We evaluate their importance in Section 3.3.</p><p>We first define prior and posterior object-capsule presence as follows. For a minibatch of size B with K object capsules and M part capsules we define a minibatch of prior capsule presence a prior 1:K with dimension <ref type="bibr">[B, K]</ref> and posterior capsule presence a posterior 1:</p><formula xml:id="formula_8">K,1:M with dimension [B, K, M ] as, a prior k = a k max m a m,k , a posterior k,m = a k a k,m N (x m | m, k) ,<label>(12)</label></formula><p>respectively; the former is the maximum presence probability among predictions from object capsule k while the latter is the unnormalized mixing proportion used to explain part capsule m. the sum of object capsule presence probabilities for a given example. If we assume that training examples contain objects from different classes uniformly at random and we would like to assign the same number of object capsules to every class, then each class would obtain K /C capsules. Moreover, if we assume that only one object is present in every image, then K /C object capsules should be present for every input example, which results in the sum of presence probabilities of B /C for every object capsule. To this end, we minimize,</p><formula xml:id="formula_9">L prior = 1 B B b=1 ( u b − K /C) 2 + 1 K K k=1 (u k − B /C) 2 .<label>(13)</label></formula><p>Posterior Sparsity Similarly, we experimented with minimizing the within-example entropy of capsule posterior presence H(v k ) and maximizing its between-example entropy H </p><formula xml:id="formula_10">( v b ),</formula><formula xml:id="formula_11">L posterior = 1 K K k=1 H(v k ) − 1 B B b=1 H( v b ) .<label>(14)</label></formula><p>Our ablation study has shown, however, that the model can perform equally well without these posterior sparsity constraints, cf. Section 3.3. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the schematic architecture of SCAE. We optimize a weighted sum of image and part likelihoods and the auxiliary losses. Loss weight selection process, as well as the values used for experiments, are detailed in Appendix A.</p><p>In order to make the values of presence probabilities (a k , a k,m and d m ) closer to binary we inject uniform noise ∈ [−2, 2] into logits, similar to <ref type="bibr" target="#b34">Tieleman, 2014</ref>. This forces the model to predict logits that are far from zero to avoid stochasticity and makes the predicted presence probabilities close to binary. Interestingly, it tends to work better in our case than using the Concrete distribution <ref type="bibr" target="#b26">(Maddison et al., 2017)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>The decoders in the SCAE use explicitly parameterised affine transformations that allow the encoders' inputs to be explained with a small set of transformed objects or parts. The following evaluations show how the embedded geometrical knowledge helps to discover patterns in data. Firstly, we show that the CCAE discovers underlying structures in sets of two-dimensional points, thereby performing instance-level segmentation. Secondly, we pair an OCAE with a PCAE and investigate whether the resulting SCAE can discover structure in real images. Finally, we present an ablation study that shows which components of the model contribute to the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discovering Constellations</head><p>We create arrangements of constellations online, where every input example consists of up to 11 two-dimensional points belonging to up to three different constellations (two squares and a triangle) as well as binary variables indicating the presence of the points (points can be missing). Each constellation is included with probability 0.5 and undergoes a similarity transformation, whereby it is randomly scaled, rotated by up to 180°and shifted. Finally, every input example is normalised such that all points lie within [−1, 1] 2 . Note that we use sets of points, and not images, as inputs to our model.</p><p>We compare the CCAE against a baseline that uses the same encoder but a simpler decoder: the decoder uses the capsule parameter vector c k to directly predict the location, precision and presence probability of each of the four points as well as the presence probability of the whole corresponding constellation. Implementation details are listed in Appendix A.1.</p><p>Both models are trained unsupervised by maximising the part log-likelihood. We evaluate them by trying to assign each input point to one of the object capsules. To do so, we assign every input point to the object capsule with the highest posterior probability for this point, cf. Section 2.1, and compute segmentation accuracy (i. e., the true-positive rate).</p><p>The CCAE consistently achieves 7 below 4% error with the best model achieving 2.8% , while the best baseline achieved 26% error using the same budget for hyperparameter search. This shows that wiring in an inductive bias towards modelling geometric relationships can help to bring down the error by an order of magnitude-at least in a toy setup where each set of points is composed of familiar constellations that have been independently transformed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Class Discovery in Images</head><p>We now turn to images in order to assess if our model can simultaneously learn to discover parts and group them into objects. To allow for multimodality in the appearance of objects of a specific class, we typically use more object capsules than the number of class labels. It turns out that the vectors of presence probabilities form tight clusters as shown by their TSNE embeddings (Maaten and <ref type="bibr" target="#b25">Hinton, 2008)</ref> in <ref type="figure">Figure 1</ref>-note the large separation between clusters corresponding to different digits, and that only a few data points are assigned to the wrong clusters. Therefore, we expect object capsules presences to be highly informative of the class label. To test this hypothesis, we train SCAE on MNIST, SVHN 8 and CIFAR10 and try to assign class labels to vectors of object capsule presences. This is done with one of the following methods: LIN-MATCH: after finding 10 clusters 9 with KMEANS we use bipartite graph matching <ref type="bibr" target="#b21">(Kuhn, 1955)</ref> to find the permutation of cluster indices that minimizes the classification error-this is standard practice in unsupervised classification, see e. g., <ref type="bibr" target="#b19">Ji et al., 2018</ref>; LIN-PRED: we train a linear classifier with supervision given the presence vectors; this learns K × 10 weights and 10 biases, where K is the number of object capsules, but it does not modify any parameters of the main model.</p><p>In agreement with previous work on unsupervised clustering <ref type="bibr" target="#b19">(Ji et al., 2018;</ref><ref type="bibr" target="#b15">Hu et al., 2017;</ref><ref type="bibr" target="#b14">Hjelm et al., 2019;</ref><ref type="bibr" target="#b10">Haeusser et al., 2018)</ref>, we train our models and report results on full datasets (TRAIN, VALID and TEST splits). The linear transformation used in LIN-PRED variant of our method is trained on the TRAIN split of respective datasets while its performance on the TEST split is reported.</p><p>We used an PCAE with 24 single-channel 11 × 11 templates for MNIST and 24 and 32 three-channel 14 × 14 templates for SVHN and CIFAR10, respectively. We used sobel-filtered images as the reconstruction target for SVHN and CIFAR10, as in Jaiswal et al., 2018, while using the raw pixel intensities as the input to PCAE. The OCAE used 24, 32 and 64 object capsules, respectively. Further details on model architectures and hyper-parameter tuning are available in Appendix A. All results are presented in <ref type="table" target="#tab_2">Table 1</ref>. SCAE achieves state-of-the-art results in unsupervised object classification on MNIST and SVHN and under-performs on CIFAR10 due to the inability to model backgrounds, which is further discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation study</head><p>SCAEs have many moving parts; an ablation study shows which model components are important and to what degree. We train SCAE variants on MNIST as well as a padded-and-translated 40 × 40 version of the dataset, where the original digits are translated up to 6 pixels in each direction. Trained models are tested on TEST splits of both datasets; additionally, we evaluate the model trained on the 40 × 40 MNIST on the TEST split of AFFNIST dataset. Testing on AFFNIST shows whether the model can generalise to unseen viewpoints. This task was used by <ref type="bibr" target="#b30">Rawlinson et al., 2018</ref> to evaluate Sparse Unsupervised Capsules, which achieved 90.12% accuracy. SCAE achieves 92.2 ± 0.59%, which indicates that it is better at viewpoint generalisation. We choose the LIN-MATCH performance metric, since it is the one favoured by the unsupervised classification community.</p><p>Results are split into several groups and shown in <ref type="table" target="#tab_3">Table 2</ref>. We describe each group in turn. Group a) shows that sparsity losses introduced in Section 2.4 increase model performance, but that the posterior loss might not be necessary. Group b) checks the influence of injecting noise into logits for presence probabilities, cf. Section 2.4. Injecting noise into part capsules seems critical, while noise in object capsules seems unnecessary-the latter might be due to sparsity losses. Group c) shows that using similarity (as opposed to affine) transforms in the decoder can be restrictive in some cases, while not allowing deformations hurts performance in every case.</p><p>Group d) evaluates the type of the part-capsule encoder. The LINEAR encoder entails a CNN followed by a fully-connected layer, while the CONV encoder predicts one feature map for every capsule parameter, followed by global-average pooling. The choice of part-capsule encoder seems <ref type="bibr">8</ref> We note that we tie the values of the alpha channel T a m and the color values T c m which leads to better results in the SVHN experiments. 9 All considered datasets have 10 classes. Additionally, e) using Set Transformer as the object-capsule encoder is essential. We hypothesise that it is due to the natural tendency of Set Transformer to find clusters, as reported in <ref type="bibr" target="#b22">Lee et al., 2019</ref>. Finally, f) using special features z m seems not less important-presumably due to effects the high-level capsules have on the representation learned by the primary encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Capsule Networks Our work combines ideas from Transforming Autoencoders <ref type="bibr" target="#b12">(Hinton, Krizhevsky, et al., 2011)</ref> and EM Capsules <ref type="bibr" target="#b13">(Hinton, Sabour, et al., 2018)</ref>. Transforming autoencoders discover affine-aware capsule instantiation parameters by training an autoencoder to reconstruct an affine-transformed version of the original image. This model uses an additional input that explicitly represents the transformation, which is a form of supervision. By contrast, our model does not need any input other than the image.</p><p>Both EM Capsules and the preceding Dynamic Capsules <ref type="bibr" target="#b32">(Sabour et al., 2017)</ref> use the poses of parts and learned part→object relationships to vote for the poses of objects. When multiple parts cast very similar votes, the object is assumed to be present, which is facilitated by an interactive inference (routing) algorithm. Iterative routing is inefficient and has prompted further research <ref type="bibr" target="#b35">(Wang and Liu, 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2018;</ref><ref type="bibr" target="#b24">Li et al., 2018)</ref>. In contrast to prior work, we use objects to predict parts rather than vice-versa; therefore we can dispense with iterative routing at inference time-every part is explained as a mixture of predictions from different objects, and can have only one parent. This regularizes the OCAE's encoder to respect the single parent constraint when learning to group parts into objects.</p><p>Additionally, since it is the objects that predict parts, the part poses can have fewer degrees-of-freedom than object poses (as in the CCAE). Inference is still possible because the OCAE encoder makes object predictions based on all the parts. This is in contrast to each individual part making its own prediction, as was the case in previous works on capsules.</p><p>A further advantage of our version of capsules is that it can perform unsupervised learning, whereas previous capsule networks used discriminative learning. <ref type="bibr" target="#b30">Rawlinson et al., 2018</ref> is a notable exception and used the reconstruction MLP introduced in Sabour et al., 2017 to train Dynamic Capsules without supervision. Their results show that unsupervised training for capsule-conditioned reconstruction helps with generalization to AFFNIST classification; we further improve on their results, cf. Section 3.3.</p><p>Unsupervised Classification There are two main approaches to unsupervised object category detection in computer vision. The first one is based on representation learning and typically requires discovering clusters or learning a classifier on top of the learned representation. <ref type="bibr" target="#b7">Eslami et al., 2016;</ref><ref type="bibr" target="#b20">Kosiorek et al., 2018</ref> use an iterative procedure to infer a variable number of latent variables, one for every object in a scene, that are highly informative of the object class, while <ref type="bibr" target="#b8">Greff et al., 2019;</ref> perform unsupervised instance-level segmentation in an iterative fashion. While similar to our work, these approaches cannot decompose objects into their constituent parts and do not provide an explicit description of object shape (e. g., templates and their poses in our model).</p><p>The second approach targets classification explicitly by minimizing mutual information (MI)-based losses and directly learning class-assignment probabilities. IIC <ref type="bibr" target="#b19">(Ji et al., 2018)</ref> maximizes an exact estimator of MI between two discrete probability vectors describing (transformed) versions of the input image. DeepInfoMax <ref type="bibr" target="#b14">(Hjelm et al., 2019)</ref> relies on negative samples and maximizes MI between the predicted probability vector and its input via noise-contrastive estimation <ref type="bibr" target="#b9">(Gutmann and Hyvärinen, 2010)</ref>. This class of methods directly maximizes the amount of information contained in an assignment to discrete clusters, and they hold state-of-the-art results on most unsupervised classification tasks. MI-based methods suffer from typical drawbacks of mutual information estimation: they require massive data augmentation and large batch sizes. This is in contrast to our method, which achieves comparable performance with batch size no bigger than 128 and with no data augmentation.</p><p>Geometrical Reasoning Other attempts at incorporating geometrical knowledge into neural networks include exploiting equivariance properties of group transformations <ref type="bibr" target="#b3">(Cohen and Welling, 2016)</ref> or new types of convolutional filters <ref type="bibr" target="#b27">(Oyallon and Mallat, 2015;</ref><ref type="bibr" target="#b5">Dieleman et al., 2016)</ref>. Although they achieve significant parameter efficiency in handling rotations or reflections compared to standard CNNs, these methods cannot handle additional degrees of freedom of affine transformations-like scale. <ref type="bibr" target="#b23">Lenssen et al., 2018</ref> combined capsule networks with group convolutions to guarantee equivariance and invariance in capsule networks. Spatial Transformers (ST; <ref type="bibr" target="#b17">Jaderberg et al., 2015)</ref> apply affine transformations to the image sampling grid while steerable networks <ref type="bibr" target="#b4">(Cohen and Welling, 2017;</ref><ref type="bibr" target="#b16">Jacobsen et al., 2017)</ref> dynamically change convolutional filters. These methods are similar to ours in the sense that transformation parameters are predicted by a neural network but differ in the sense that ST uses global transformations applied to the whole image while steerable networks use only local transformations. Our approach can use different global transformations for every object as well as local transformations for each of their parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The main contribution of our work is a novel method for representation learning, in which highly structured decoder networks are used to train one encoder network that can segment an image into parts and their poses and another encoder network that can compose the parts into coherent wholes. Even though our training objective is not concerned with classification or clustering, SCAE is the only method that achieves competitive results in unsupervised object classification without relying on mutual information (MI). This is significant since, unlike our method, MI-based methods require sophisticated data augmentation. It may be possible to further improve results by using an MI-based loss to train SCAE, where the vector of capsule probabilities could take the role of discrete probability vectors in IIC <ref type="bibr" target="#b19">(Ji et al., 2018)</ref>. SCAE under-performs on CIFAR10, which could be because of using fixed templates, which are not expressive enough to model real data. This might be fixed by building deeper hierarchies of capsule autoencoders ( e. g., complicated scenes in computer graphics are modelled as deep trees of affine-transformed geometric primitives) as well as using input-dependent shape functions instead of fixed templates-both of which are promising directions for future work. It may also be possible to make a much better PCAE for learning the primary capsules by using a differentiable renderer in the generative model that reconstructs pixels from the primary capsules.</p><p>Finally, the SCAE could be the ' <ref type="figure">figure'</ref> component of a mixture model that also includes a versatile 'ground' component that can be used to account for everything except the figure. A complex image could then be analyzed using sequential attention to perceive one figure at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Constellation Experiments</head><p>The CCAE uses a four-layer Set Transformer as its encoder. Every layer has four attention heads, 128 hidden units per head, and is followed by layer norm <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. The encoder outputs three 32-dimensional vectors-one for each object capsule. The decoder uses a separate neural net for each object capsule to predict all parameters used to model its points: this includes four candidate part predictions per capsule for a total of 12 candidates. In this experiment, each object→part relationship OP is just a 2-D offset in the object's frame of reference (instead of a 3 × 3 matrix) and it is affine transformed by the corresponding OV matrix to predict the 2-D point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Image Experiments</head><p>We use a convolutional encoder for part capsules and a set transformer encoder <ref type="bibr" target="#b22">(Lee et al., 2019)</ref> for object capsules. Decoding from object capsule to part capsules is done with MLPs, while the input image is reconstructed with affine-transformed learned templates. Details of the architectures we used are available in <ref type="table" target="#tab_4">Table 3</ref>. For SVHN and CIFAR10, we use normalized sobel-filtered images as the target of the reconstruction to emphasize the shape importance. <ref type="figure">Figure 7</ref> in Appendix B shows examples of SVHN and CIFAR10 reconstruction. The filtering procedure is as follows: 1) apply sobel filtering, 2) subtract the median color, 3) take the absolute value of the image, 4) normalize for image values to be ∈ [0, 1].</p><p>All models are trained with the RMSProp optimizer <ref type="bibr" target="#b33">(Tieleman and Hinton, 2012</ref>) momentum = .9 and = (10 * batch_size) −2 . Batch size is 64 for constellations and 128 for all other datasets. The learning rate was equal to 10 −5 for MNIST and constellation experiments (without any decay), while we run a hyperparameter search for SVHN and CIFAR10: we searched learning rates in the range of 5 * 10 −5 to 5 * 10 −4 and exponential learning rate decay of 0.96 every 10 3 or 3 * 10 3 weight updates. Learning rate of 10 −4 was selected for both SVHN and CIFAR10, the decay steps was 10 3 for SVHN and 3 * 10 3 for CIFAR10. The LIN-PRED accuracy on a validation set is used as a proxy to select the best hyperparameters-including weights on different losses, reported in <ref type="table" target="#tab_5">Table 4</ref>. Models were trained for up to 3 * 10 5 iterations on single Tesla V100 GPUs, which took 40 minutes for constellation experiments and less than a day for CIFAR10. Third row shows the reconstruction if we use the object predictions for the Part poses instead of Part poses themselves for reconstruction. The templates in this model has the same number of channels as the image, but they have converged to black and white templates and the reconstruction do not have color diversity. The SCAE model is trained completely unsupervised but the reconstructions tend to focus on the center digit in SVHN and filter the rest of the clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Constellation Capsule Sparsity</head><p>We noticed that we can get better instance segmentation results in the constellation experiment when we add an additional sparsity loss, which says that every active object capsule should explain at least two parts. We say that an object capsule has 'won' a part if it has the highest posterior mixing probability for that part among other object capsules. We then create binary labels for each of object capsules, where the label is 1 if the capsule wins at least two parts and it is 0 otherwise. The final loss takes the form of binary cross-entropy between the generated label and the prior capsule presence. This loss is used only for the stand-alone constellation model experiments on point data, cf. Sections 2.1 and 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Modelling Deformable Objects</head><p>Each object capsule votes for part capsules by contributing Gaussian votes to mixture models, where the centers of these Gaussians are a product of an object-viewer OV matrix and an objectpart OP matrix, cf. Equations (1) to (4). Importantly, the OP k,n matrices are a sum of a static component OP static k,n , which represents the mean shape of an object, and a dynamic component OP dynamic k,n = MLP(c k ), which is a function of data, and can model deformations in objects' shape. If an object capsule is to specialise to a specific object class, it should learn its mean shape. Therefore we discourage large deformations, which also prevents an object capsule from modelling several objects at once. Concretely, we add the weighted Frobenius norm of the deformation matrix α||OP dynamic k,n || 2 F to the loss, where α is a weight set to a high value, typically α = 10 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Part Capsule Encoder with Attention-based Pooling</head><p>The PCAE encoder consists of a CNN followed by a bottom-up attention mechanism based on globalaverage pooling, which we call attention-based pooling. When global-average pooling is typically used, a feature map with d channels is averaged along its spatial dimensions resulting into a ddimensional vector. This is useful for e. g., counting features of a particular type, which can be useful for classification. In our case, we wanted to predict pose and existence of a particular part, with the constraint that this part can exist at at most one location in the image. In order to support this, we predict a d + 1 dimensional feature map, where the additional dimension represents softmax logits for attention. Finally, we compute the weighted average of the feature map along its spatial dimensions, where the weights are given by the softmax. This allows to predict different part parameters at different locations and weigh them by the corresponding confidence.</p><p>Concretely, for every part capsule k, we use the CNN to predict a feature map e k of 6 (pose) + 1 (presence) + c z (special features) capsule parameters with spatial dimensions h e × w e , as well as a single-channel attention mask a k . The final parameters for that capsule are computed as i j e k,i,j softmax(a) k,i,j , where softmax is along the spatial dimensions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Stacked Capsule Autoencoder (SCAE): (a) part capsules segment the input into parts and their poses. The poses are then used to reconstruct the input by affine-transforming learned templates. (b) object capsules try to arrange inferred poses into objects, thereby discovering underlying structure. SCAE is trained by maximizing image and part log-likelihoods subject to sparsity constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Stroke-like templates learned on MNIST (left) as well as sobel-filtered SVHN (middle) and CIFAR10 (right). For SVHN they often take the form of double strokes due to sobel filtering. MNIST (a) images, (b) reconstructions from part capsules in red and object capsules in green, with overlapping regions in yellow. Only a few object capsules are activated for every input (c) a priori (left) and even fewer are needed to reconstruct it (right). The most active capsules (d) capture object identity and its appearance; (e) shows a few o f the affine-transformed templates used for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Prior sparsity Let u k = B b=1 a prior b,k the sum of presence probabilities of the object capsule k among different training examples, and u b = K k=1 a prior b,k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where H is the entropy, and where v k and v b are the the normalized versions of k,m a posterior b,k,m and b,m a posterior b,k,m , respectively. The final loss reads as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>SCAE architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 3: Unsupervised segmentation of points belonging to up to three constellations of squares and triangles at different positions, scales and orientations. The model is trained to reconstruct the points (top row) under the CCAE mixture model. The bottom row colours the points based on the parent with the highest posterior probability in the mixture model. The right-most column shows a failure case. Note that the model uses sets of points, not pixels, as its input; we use images only to visualize the constellation arrangements.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Unsupervised classifi-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cation results in % with (stan-dard deviation) are averaged</cell><cell>Method</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>SVHN</cell></row><row><cell>over 5 runs. Methods based on mutual information are shaded.</cell><cell cols="2">KMEANS (Haeusser et al., 2018) 53.49 AE (Bengio et al., 2007)  § 81.2</cell><cell>20.8 31.4</cell><cell>12.5 -</cell></row><row><cell>Results marked with  † use</cell><cell>GAN (Radford et al., 2016)  §</cell><cell>82.8</cell><cell>31.5</cell><cell>-</cell></row><row><cell>data augmentation, ∇ use IM-AGENET-pretrained features in-stead of images, while  § are taken from Ji et al., 2018. We</cell><cell>IMSAT (Hu et al., 2017)  †,∇ IIC (Ji et al., 2018)  §, † ADC (Haeusser et al., 2018)  †</cell><cell>98.4 (0.4) 98.4 (0.6) 98.7 (0.6)</cell><cell>45.6 (0.8) 57.6 (5.0) 29.3 (1.5)</cell><cell>57.3 (3.9) -38.6 (4.1)</cell></row><row><cell>highlight the best results and</cell><cell>SCAE (LIN-MATCH)</cell><cell cols="3">98.7 (0.35) 25.01 (1.0) 55.33 (3.4)</cell></row><row><cell>those that are are within its 98%</cell><cell>SCAE (LIN-PRED)</cell><cell cols="3">99.0 (0.07) 33.48 (0.3) 67.27 (4.5)</cell></row><row><cell>confidence interval according</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>to a two-sided t test.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on MNIST. All used model components contribute to its final performance.</figDesc><table><row><cell></cell><cell>Method</cell><cell>MNIST</cell><cell cols="2">40 × 40 MNIST AFFNIST</cell></row><row><cell></cell><cell>full model</cell><cell>95.3 (4.65)</cell><cell>98.7 (0.35)</cell><cell>92.2 (0.59)</cell></row><row><cell></cell><cell>a) no posterior sparsity</cell><cell>97.5 (1.55)</cell><cell>95.0 (7.20)</cell><cell>85.3 (11.67)</cell></row><row><cell>AFFNIST results show out-of-distribution gener-</cell><cell cols="3">no prior sparsity no prior/posterior sparsity 84.7 (3.01) 72.4 (22.39) 88.2 (6.98) 82.0 (5.46)</cell><cell>71.3 (5.46) 59.0 (5.66)</cell></row><row><cell>alization properties and</cell><cell>b) no noise in object caps</cell><cell>96.7 (2.30)</cell><cell>98.5 (0.12)</cell><cell>93.5 (0.38)</cell></row><row><cell>come from a model trained</cell><cell>no noise in any caps</cell><cell>93.1 (5.09)</cell><cell>78.5 (22.69)</cell><cell>64.1 (26.74)</cell></row><row><cell>on 40 × 40 MNIST. Num-</cell><cell>no noise in part caps</cell><cell>93.9 (7.16)</cell><cell>82.8 (24.83)</cell><cell>70.7 (25.96)</cell></row><row><cell>bers represent average % and (standard deviation) over 10 runs. We highlight</cell><cell>c) similarity transforms no deformations</cell><cell cols="2">97.5 (1.55) 87.3 (21.48) 87.2 (18.54) 95.9 (1.59)</cell><cell>88.9 (1.58) 79.0 (22.44)</cell></row><row><cell>the best results and those</cell><cell>d) LINEAR part enc</cell><cell>98.0 (0.52)</cell><cell>63.2 (31.47)</cell><cell>50.8 (26.46)</cell></row><row><cell>that are are within its</cell><cell>CONV part enc</cell><cell>97.6 (1.22)</cell><cell>97.8 (.98)</cell><cell>81.6 (1.66)</cell></row><row><cell>98% confidence interval according to a two-sided t test.</cell><cell>e) MLP enc for object caps f) no special features</cell><cell>27.1 (9.03) 90.7 (2.25)</cell><cell>36.3 (3.70) 58.7 (31.60)</cell><cell>25.29 (3.69) 44.5 (21.71)</cell></row><row><cell cols="5">not to matter much for within-distribution performance; however, our attention-based pooling (cf.</cell></row><row><cell cols="5">Appendix E) does achieve much higher classification accuracy when evaluated on a different dataset,</cell></row><row><cell cols="2">showing better generalisation to novel viewpoints.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Architecture details. S in the last column means that the entry is the same as for SVHN. ReLu nonlinearities except for presence probabilities, for which we use sigmoids. (128:2) for a CNN means 128 channels with a stride of two. All kernels are 3 × 3. For set transformer (1-16)-256 means one attention head, 16 hidden units and 256 output units; it uses layer normalization<ref type="bibr" target="#b0">(Ba et al., 2016)</ref> as in the original paper<ref type="bibr" target="#b22">(Lee et al., 2019)</ref> but no dropout. All experiments (apart from constellations) used 16 special features per part capsule.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Constellation MNIST</cell><cell>SVHN</cell><cell>CIFAR10</cell></row><row><cell cols="2">num templates N/A</cell><cell>24</cell><cell>24</cell><cell>32</cell></row><row><cell>template size</cell><cell>N/A</cell><cell>11 × 11</cell><cell>14 × 14</cell><cell>S</cell></row><row><cell>num capsules</cell><cell>3</cell><cell>24</cell><cell>32</cell><cell>64</cell></row><row><cell>part CNN</cell><cell>N/A</cell><cell cols="3">2x(128:2)-2x(128:1) 2x(128:1)-2x(128:2) S</cell></row><row><cell cols="3">set transformer 4x(4-128)-32 3x(1-16)-256</cell><cell>3x(2-64)-128</cell><cell></cell></row></table><note>S We use</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Loss weights values. The within and between quantifiers in sparsity losses corresponds to different terms of Equations(13)and(14). Sample SVHN and Cifar10 reconstructions. First row shows Sobel filtered target image. Second row shows the reconstruction from Part Capsule Layer directly.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Constellation MNIST SVHN CIFAR10</cell></row><row><cell>part ll weight</cell><cell>1</cell><cell>1</cell><cell>2.56</cell><cell>2.075</cell></row><row><cell>image ll weight</cell><cell>N/A</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>prior within sparsity</cell><cell>1</cell><cell>1</cell><cell>0.22</cell><cell>0.17</cell></row><row><cell>prior between sparsity</cell><cell>1</cell><cell>1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>posterior within sparsity</cell><cell>0</cell><cell>10</cell><cell>8.62</cell><cell>1.39</cell></row><row><cell cols="2">posterior between sparsity 0</cell><cell>10</cell><cell>0.26</cell><cell>7.32</cell></row><row><cell>too-few-active-capsules</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This may explain why accessing perceptual knowledge about objects, when they are not visible, requires creating a mental image of the object with a specific viewpoint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Deriving these matrices from capsule feature vectors allows for deformable objects, see Appendix D for details.4  We treat parts as independent and evaluate their probability under the same mixture model. While there are no clear 1:1 connections between parts and predictions, it seems to work well in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We could repeat parts by using multiple instances of the same part capsule. 6 Discovered objects are not used top-down to refine the presences or poses of the parts during inference. However, the derivatives backpropagated via OCAE refine the lower-level encoder network that infers the parts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This result requires using an additional sparsity loss described in Appendix C; without it the CCAE achieves around 10% error.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We would like to thank Sandy H. Huang for help with editing the manuscript and making <ref type="figure">Figure 2</ref>. Additionally, we would like to thank S. M. Ali Eslami and Danijar Hafner for helpful discussions throughout the project. We also thank Hyunjik Kim, Martin Engelcke, Emilien Dupont and Simon Kornblith for feedback on initial versions of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy Layer-wise Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MONet: Unsupervised Scene Decomposition and Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1901.11390</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Group Equivariant Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploiting Cyclic Symmetry in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1602.02660</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Parker</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1907.13052</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08575</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-Object Representation Learning with Iterative Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1903.00450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Noise-contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associative Deep Clustering: Training a Classification Network with No Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some Demonstrations of the Effects of Structural Descriptions in Mental Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transforming Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artifical Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix Capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Deep Representations by Mutual Information Estimation and Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Discrete Representations via Information Maximizing Self-augmented Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic steerable blocks in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1706.00598</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025v1</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Capsulegan: Generative adversarial capsule network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Invariant Information Distillation for Unsupervised Image Segmentation and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1807.06653</idno>
		<ptr target="http://arxiv.org/abs/1807.06653" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequential Attend, Infer, Repeat: Generative modelling of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01794</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00825</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Group Equivariant Capsule Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Libuschewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural Network Encapsulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1808.03749</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Roto-Translation Scattering for Object Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning with Ladder Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sparse Unsupervised Capsules Generalize Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rawlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kowadlo</surname></persName>
		</author>
		<idno type="arXiv">CoRR.arXiv:1804.06094</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Orientation and form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic Routing Between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Optimizing Neural Networks That Generate Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto, Canada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An Optimization View on Dynamic Routing Between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast Dynamic Routing Based on Weighted Kernel Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Artificial Intelligence and Robotics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
