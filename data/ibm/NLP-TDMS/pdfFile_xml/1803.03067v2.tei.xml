<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 COMPOSITIONAL ATTENTION NETWORKS FOR MACHINE REASONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 COMPOSITIONAL ATTENTION NETWORKS FOR MACHINE REASONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Reasoning, the ability to manipulate previously acquired knowledge to draw novel inferences or answer new questions, is one of the fundamental building blocks of the intelligent mind. As we seek to advance neural networks beyond their current great success with sensory perception towards tasks that require more deliberate thinking, conferring them with the ability to move from facts to conclusions is thus of crucial importance. To this end, we consider here how best to design a neural network to perform the structured and iterative reasoning necessary for complex problem solving.</p><p>Q: Do the block in front of the tiny yellow cylinder and the tiny thing that is to the right of the large green shiny object have the same color? A: No Concretely, we develop a novel model that we apply to the CLEVR task <ref type="bibr" target="#b15">(Johnson et al., 2017a)</ref> of visual question answering (VQA). VQA <ref type="bibr" target="#b3">(Antol et al., 2015;</ref><ref type="bibr" target="#b11">Gupta, 2017</ref>) is a challenging multimodal task that requires responding to natural language questions about images. However, <ref type="bibr" target="#b0">Agrawal et al. (2016)</ref> show how the first generation of successful VQA models tends to acquire only superficial comprehension of both the image and the question, exploiting dataset biases rather than capturing a sound perception and reasoning process that would lead to the correct answer <ref type="bibr">(cf. Sturm (2014)</ref>). CLEVR was created to address this problem. As illustrated in figure 1, the dataset features unbiased, highly compositional questions that require an array of challenging reasoning skills, such as transitive and logical relations, counting and comparisons, without allowing any shortcuts around such reasoning.</p><p>However, deep learning approaches often struggle to perform well on tasks with a compositional and structured nature <ref type="bibr" target="#b8">(Garnelo et al., 2016;</ref><ref type="bibr" target="#b19">Lake et al., 2017)</ref>. Most neural networks are essentially very large correlation engines that will hone in on any statistical, potentially spurious pattern that allows them to model the observed data more accurately. The depth, size and statistical nature that allows them to cope with noisy and diverse data often limits their interpretability and hinders their capacity <ref type="figure">Figure 2</ref>: Model Overview. The MAC network consists of an input unit, a core recurrent network and an output unit. (1) The input unit transforms the raw image and question into distributed vector representations.</p><p>(2) The core recurrent network reasons sequentially over the question by decomposing it into a series of operations (control) that retrieve information from the image (knowledge base) and aggregate the results into a recurrent memory. to perform explicit and sound inference procedures that are vital for problem solving tasks. To mitigate this issue, some recent approaches adopt symbolic structures, resembling the expression trees of programming languages, that compose neural modules from a fixed predefined collection <ref type="bibr" target="#b1">(Andreas et al., 2016a;</ref><ref type="bibr" target="#b16">Johnson et al., 2017b)</ref>. However, they consequently rely on externally provided structured representations and functional programs, brittle handcrafted parsers or expert demonstrations, and require relatively complex multi-stage reinforcement learning training schemes. The rigidity of these models' structure and the use of an inventory of specialized operation-specific modules ultimately undermines their robustness and generalization capacities.</p><p>Seeking a balance between the versatility and robustness of end-to-end neural approaches on the one hand and the need to support more explicit and structured reasoning on the other, we propose the MAC network, a novel fully differentiable architecture for reasoning tasks. Our model performs structured and explicit reasoning by sequencing a new recurrent Memory, Attention and Composition (MAC) cell. The MAC cell was deliberately designed to capture the inner workings of an elementary, yet general-purpose reasoning step, drawing inspiration from the design principles of computer architectures. The cell explicitly separates out memory from control, both represented recurrently, and consists of three operational units that work in tandem to perform a reasoning step: the control unit updates the control state to attend at each iteration to some aspect of a given question; the read unit extracts information out of a knowledge base, guided by the control and memory states; and the write unit integrates the retrieved information into the memory state, iteratively computing the answer. This universal design of the MAC cell serves as a structural prior that encourages the network to solve problems by decomposing them into a sequence of attention-based reasoning operations that are directly inferred from the data, without resorting to any strong supervision. With self-attention connections between the cells, the MAC network is capable of representing arbitrarily complex acyclic reasoning graphs in a soft manner, while still featuring a physically sequential structure and end-to-end differentiabillity, amenable to training simply by backpropagation. We demonstrate the model's quantitative and qualitative performance on the CLEVR task and its associated datasets. The model achieves state-of-the-art accuracy across a variety of reasoning tasks and settings, both for the primary dataset as well as the more difficult human-authored questions. Notably, it performs particularly well on questions that involve counting and aggregation skills, which tend to be remarkably challenging for other VQA models <ref type="bibr" target="#b24">(Santoro et al., 2017;</ref><ref type="bibr" target="#b13">Hu et al., 2017;</ref><ref type="bibr" target="#b16">Johnson et al., 2017b)</ref>. Moreover, we show that the MAC network learns rapidly and generalizes effectively from an order of magnitude less data than other approaches. Finally, extensive ablation studies and error analysis demonstrate MAC's robustness, versatility and generalization capacity. These results highlight the significance and value of imposing strong structural priors to guide the network towards compositional reasoning. The model contains structures that encourage it to explicitly perform a chain of operations that build upon each other, allowing MAC to develop reasoning skills from the ground up, realizing the vision of an algebraic, compositional model of inference as proposed by <ref type="bibr" target="#b6">Bottou (2014)</ref>. Although each cell's functionality has only a limited range of possible continuous behaviors, geared to perform a simple reasoning operation, when chained together in a MAC network, the whole system becomes expressive and powerful. TensorFlow implementation of the model is available at https://github.com/stanfordnlp/mac-network. The MAC recurrent cell consists of a control unit, read unit, and write unit, that operate over dual control and memory hidden states. The control unit successively attends to different parts of the task description (question), updating the control state to represent at each timestep the reasoning operation the cell intends to perform. The read unit extracts information out of a knowledge base (here, image), guided by the control state. The write unit integrates the retrieved information into the memory state, yielding the new intermediate result that follows from applying the current reasoning operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE MAC NETWORK</head><p>A MAC network is an end-to-end differentiable architecture primed to perform an explicit multi-step reasoning process, by stringing together p recurrent MAC cells, each responsible for performing one reasoning step. Given a knowledge base K (for VQA, an image) and a task description q (for VQA, a question), the model infers a decomposition into a series of p reasoning operations that interact with the knowledge base, iteratively aggregating and manipulating information to perform the task at hand. It consists of three components: (1) an input unit, (2) the core recurrent network, composed out of p MAC cells, and (3) an output unit, all described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE INPUT UNIT</head><p>The input unit transforms the raw inputs given to the model into distributed vector representations. Naturally, this unit is tied to the specifics of the task we seek to perform. For the particular case of VQA, it receives a question and an image and processes each of them respectively:</p><p>The question string, of length S, is converted into a sequence of learned word embeddings that is further processed by a d-dimensional biLSTM yielding: (1) contextual words: a series of output states cw 1 , . . . , cw S that represent each word in the context of the question, and (2) the question representation: q = ← − − cw 1 , − − → cw S , the concatenation of the final hidden states from the backward and forward LSTM passes. Subsequently, for each step i = 1, . . . , p, the question q is transformed through a learned linear transformation into a position-aware vector q i = W d×2d i q+b d i , representing the aspects of the question that are relevant to the i th reasoning step.</p><p>The image is first processed by a fixed feature extractor pre-trained on ImageNet <ref type="bibr" target="#b23">(Russakovsky et al., 2015)</ref> that outputs conv4 features from ResNet101 , matching prior work for CLEVR <ref type="bibr" target="#b13">(Hu et al., 2017;</ref><ref type="bibr" target="#b24">Santoro et al., 2017;</ref><ref type="bibr" target="#b22">Perez et al., 2017)</ref>. The resulting tensor is then passed through two CNN layers with d output channels to obtain a final image representation, the knowledge base K H×W ×d = {k d h,w | H,W h,w=1,1 }, where H = W = 14 are the height and width of the processed image, corresponding to each of its regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">THE MAC CELL</head><p>The MAC cell is a recurrent cell designed to capture the notion of an atomic and universal reasoning operation and formulate its mechanics. For each step i = 1, . . . , p in the reasoning process, the i th cell maintains dual hidden states: control c i and memory m i , of dimension d, initialized to learned parameters m 0 and c 0 , respectively.</p><p>The control c i represents the reasoning operation the cell should accomplish in the i th step, selectively focusing on some aspect of the question. Concretely, it is represented by a soft attention-based weighted average of the question words cw s ; s = 1, . . . , S. The memory m i holds the intermediate result obtained from the reasoning process up to the i th step, computed recurrently by integrating the preceding hidden state m i−1 with new information r i retrieved from the image, performing the i th reasoning operation c i . Analogously to the control, r i is a weighted average over its regions {k h,w | H,W h,w=1,1 }. Building on the design principles of computer organization, the MAC cell consists of three operational units: control unit CU, read unit RU and write unit WU, that work together to accomplish tasks by performing an iterative reasoning process: The control unit identifies a series of operations, represented by a recurrent control state; the read unit extracts relevant information from a given knowledge base to perform each operation, and the write unit iteratively integrates the information into the cell's memory state, producing a new intermediate result.</p><formula xml:id="formula_0">cq i = W d×2d [ci−1, qi] + b d (c1) cai,s = W 1×d (cq i cw s) + b 1 (c2.1) cv i,s = softmax(cai,s) (c2.2) ci = S s=1 cv i,s · cw s (c2.3)</formula><p>Through their operation, the three units together impose an interface that regulates the interaction between the control and memory states. Specifically, the control state, which is a function of the question, guides the integration of content from the image into the memory state only through indirect means: soft-attention maps and sigmoidal gating mechanisms. Consequently, the interaction between these two modalities -visual and textual, or knowledge base and query -is mediated through probability distributions only. This stands in stark contrast to common approaches that fuse the question and image together into the same vector space through linear combinations, multiplication, or concatenation. As we will see in section 4, maintaining a strict separation between the representational spaces of question and image, which can interact only through interpretable discrete distributions, greatly enhances the generalizability of the network and improves its transparency.</p><p>In the following, we describe the cell's three components: control, read and write units, and detail their formal specification. Unless otherwise stated, all the vectors are of dimension d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">THE CONTROL UNIT</head><p>The control unit (see <ref type="figure" target="#fig_3">figure 4</ref>) determines the reasoning operation that should be performed at each step i, attending to some part of the question and updating the control state c i accordingly. It receives the contextual question words cw 1 , . . . , cw S , the question position-aware representation q i , and the control state from the preceding step c i−1 and consists of two stages:</p><p>1. First, we combine q i and c i−1 through a linear transformation into cq i , taking into account both the overall question representation q i , biased towards the i th reasoning step, as well as the preceding reasoning operation c i−1 . This allows the cell to base its decision for the i th reasoning operation c i on the previously performed operation c i−1 .</p><p>2. Subsequently, we cast cq i onto the space of the question words. Specifically, this is achieved by measuring the similarity between cq i and each question word cw s and passing the result through a softmax layer, yielding an attention distribution over the question words cw 1 , . . . , cw S . Finally, we sum the words according to this distribution to produce the reasoning operation c i , represented in terms of the question words.</p><p>The casting of cq i onto question words serves as a form of regularization that restricts the space of the valid reasoning operations by anchoring them back in the original question words, and due to the use of soft attention may also improve the MAC cell transparency, since we can interpret the control state content and the cell's consequent behavior based on the words it attends to.</p><formula xml:id="formula_1">I i,h,w =[W d×d m mi−1 + b d m ] [W d×d k k h,w + b d k ] (r1) I i,h,w = W d×2d [I i,h,w , k h,w ] + b d (r2) ra i,h,w = W d×d (ci I i,h,w ) + b d (r3.1) rv i,h,w = softmax(ra i,h,w ) (r3.2) ri = H,W h,w=1,1 rv i,h,w · k h,w (r3.3)</formula><p>Figure 5: The Read Unit (RU) architecture. The read unit retrieves information from the knowledge base that is necessary for performing the current reasoning operation (control) and potentially related to previously obtained intermediate results <ref type="bibr">(memory)</ref>. It extracts the information by performing a two-stage attention process over the knowledge base elements. See section 2.2.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">THE READ UNIT</head><p>For the i th step, the read unit (see <ref type="figure">figure 5</ref>) inspects the knowledge base (the image) and retrieves the information r i that is required for performing the i th reasoning operation c i . The content's relevance is measured by an attention distribution rv i that assigns a probability to each element in the knowledge base k d h,w , taking into account the current reasoning operation c i and the prior memory m i−1 , the intermediate result produced by the preceding reasoning step. The attention distribution is computed in several stages:</p><p>1. First, we compute the direct interaction between the knowledge-base element k h,w and the memory m i−1 , resulting in I i,h,w . This term measures the relevance of the element to the preceding intermediate result, allowing the model to perform transitive reasoning by considering content that now seems important in light of information obtained from the prior computation step.</p><p>2. Then, we concatenate the element k h,w to I i,h,w and pass the result through a linear transformation, yielding I i,h,w . This allows us to also consider new information that is not directly related to the prior intermediate result, as sometimes a cogent reasoning process has to combine together independent facts to arrive at the answer (e.g., for a logical OR operation, set union and counting).</p><p>3. Finally, aiming to retrieve information that is relevant for the reasoning operation c i , we measure its similarity to each of the interactions I i,h,w and pass the result through a softmax layer. This produces an attention distribution over the knowledge base elements, which we then use to compute a weighted average over them -r i . To give an example of the read unit operation, consider the question in <ref type="figure" target="#fig_4">figure 6</ref>, which refers to the purple cylinder in the image. Initially, no cue is provided to the model to attend to the cylinder, since no direct mention of it is given in the question. Instead, the model approaches the question in steps: in the first iteration it attends to the "tiny blue block", updating m 1 accordingly to the visual representation of the block. At the following step, the control unit realizes it should now look for "the sphere in front" of the block, storing that in c 2 . Then, when considering both m 1 and c 2 , the read unit realizes it should look for "the sphere in front" (c 2 ) of the blue block (stored in m 1 ), thus finding the cyan sphere and updating m 2 . Finally, a similar process repeats in the next iteration, allowing the model to traverse from the cyan ball to the final objective -the purple cylinder, and answer the question correctly. </p><formula xml:id="formula_2">m inf o i = W d×2d [ri, mi−1] + b d (w1) saij = softmax W 1×d (ci cj) + b 1 (w2.1) m sa i = i−1 j=1 saij · mj (w2.2) m i = W d×d s m sa i + W d×d p m inf o i + b d (w2.3) c i = W 1×d ci + b 1 (w3.1) mi = σ c i mi−1 + 1 − σ c i m i (w3.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">THE WRITE UNIT</head><p>The write unit (see <ref type="figure" target="#fig_5">figure 7</ref>) is responsible for computing the i th intermediate result of the reasoning process and storing it in the memory state m i . Specifically, it integrates the information retrieved from the read unit r i with the preceding intermediate result m i−1 , guided by the i th reasoning operation c i . The integration proceeds in three steps, the first mandatory while the others are optional 1 :</p><p>1. First, we combine the new information r i with the prior intermediate result m i−1 by a linear transformation, resulting in m inf o i . 2. Self-Attention (Optional). To support non-sequential reasoning processes, such as trees or graphs, we allow each cell to consider all previous intermediate results, rather than just the preceding one m i−1 : We compute the similarity between the i th operation c i and the previous ones c 1 , . . . , c i−1 and use it to derive an attention distribution over the prior reasoning steps sa i,j for j = 0, . . . , i − 1. The distribution represents the relevance of each previous step j to the current one i, and is used to compute a weighted average of the memory states, yielding m sa i , which is then combined with m inf o i to produce m i . Note that while we compute the attention based on the control states, we use it to average over the memory states, in a way that resembles Key-Value Memory Networks <ref type="bibr" target="#b21">(Miller et al., 2016)</ref>. 3. Memory Gate (Optional). Not all questions are equally complex -some are simpler while others are more difficult. To allow the model to dynamically adjust the reasoning process length to the given question, we add a sigmoidal gate over the memory state that interpolates between the previous memory state m i−1 and the new candidate m i , conditioned on the reasoning operation c i . The gate allows the cell to skip a reasoning step if necessary, passing the previous memory value further along the network, dynamically reducing the effective length of the reasoning process as demanded by the question. The output unit predicts the final answer to the question based on the question representation q and the final memory m p , which represents the final intermediate result of the reasoning process, holding relevant information from the knowledge base. 2 For CLEVR, where there is a fixed set of possible answers, the unit processes the concatenation of q and m p through a 2-layer fully-connected softmax classifier that produces a distribution over the candidate answers. 3 RELATED WORK There have been several prominent models that address the CLEVR task. By and large they can be partitioned into two groups: module networks, which in practice have all used the strong supervision provided in the form of structured functional programs that accompany each data instance, and large, relatively unstructured end-to-end differentiable networks that complement a fairly standard stack of CNNs with components that aid them in performing reasoning tasks. In contrast to modular approaches <ref type="bibr" target="#b1">(Andreas et al., 2016a;</ref><ref type="bibr" target="#b19">b;</ref><ref type="bibr" target="#b13">Hu et al., 2017;</ref><ref type="bibr" target="#b16">Johnson et al., 2017b)</ref>, our model is fully differentiable and does not require additional supervision, making use of a single computational cell chained in sequence rather than a collection of custom modules deployed in a rigid tree structure. In contrast to augmented CNN approaches <ref type="bibr" target="#b24">(Santoro et al., 2017;</ref><ref type="bibr" target="#b22">Perez et al., 2017)</ref>, we suggest that our approach provides an ability for relational reasoning with better generalization capacity, higher computational efficiency and enhanced transparency. These approaches and other related work are discussed and contrasted in more detail in the supplementary material in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">THE OUTPUT UNIT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our model on the recent CLEVR task for visual reasoning <ref type="bibr" target="#b15">(Johnson et al., 2017a)</ref>. The dataset consists of rendered images featuring 3D-objects of various shapes, materials, colors and sizes, coupled with machine-generated compositional multi-step questions that measure performance on an array of challenging reasoning skills such as following transitive relations, counting objects and comparing their properties. Each question is also associated with a tree-structured functional program that was used to generate it, specifying the reasoning operations that should be performed to compute the answer.</p><p>In the following experiments, our model's training is cast as a supervised classification problem to minimize the cross-entropy loss of the predicted candidate answer out of the 28 possibilities. The model uses a hidden state size of d = 512 and, unless otherwise stated, length of p = 12 MAC cells. 3 While some prior work uses the functional programs associated with each question as additional supervisory information at training time (see table 1), we intentionally do not use these structured representations to train our model, aiming to infer coherent reasoning strategies directly from the question and answer pairs in an end-to-end approach.</p><p>We first perform experiments on the primary 700k dataset. As shown in table 1, our model outperforms all prior work both in overall accuracy, as well as in each of the categories of specific reasoning skills. In particular, for the overall performance, we achieve 98.94% accuracy, more than halving the error rate of the best prior model, FiLM <ref type="bibr" target="#b22">(Perez et al., 2017)</ref>.</p><p>Counting and Numerical Comparison. In particular, our performance on questions about counting and numerical comparisons is significantly higher than existing models, which consistently struggle on these question types. Again, we nearly halve the corresponding error rate. These are significant results, as counting and aggregations are known to be particularly challenging in the area of VQA <ref type="bibr" target="#b7">(Chattopadhyay et al., 2017)</ref>. In contrast to CNNs, using attention enhances our model's ability to perform reasoning operations such as counting that pertain to the global aggregation of information across different regions of the image. We analyze our model's performance on the CLEVR-Humans dataset <ref type="bibr" target="#b16">(Johnson et al., 2017b)</ref>, consisting of natural language questions collected through crowdsourcing. As such, the dataset has a diverse vocabulary and linguistic variations, and it also demands more varied reasoning skills. Since the training set is relatively small, comprising 18k samples, we use it to finetune a model pre-trained on the primary CLEVR dataset, following prior work. In order to gain insight into the nature of the mistakes our model makes, we perform an error analysis for the CLEVR and CLEVR-Humans datasets (See <ref type="figure" target="#fig_0">figure 10)</ref>. Overall, we see that most of the errors in the CLEVR dataset are either off-by-one counting mistakes or result from heavy object occlusions. For CLEVR-Humans, we observe many errors that involve new reasoning skills that the model has not been trained for, such as ones that relate to physical properties (stability and reflections), relative distances and amounts, commonalities and uniqueness of objects, or negative questions. See appendix B for further details. Nevertheless, the model does respond correctly to many of the questions that fall under these reasoning skills, as illustrated in figures 11 and 16, and so we speculate that the errors the model makes stem in part from the small size of the CLEVR-Human dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLEVR HUMANS AND ERROR ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPUTATIONAL AND DATA EFFICIENCY</head><p>We examine the learning curves of MAC and compare them to previous models 4 : specifically, FiLM <ref type="bibr" target="#b22">(Perez et al., 2017)</ref>, the strongly-supervised PG+EE <ref type="bibr" target="#b16">(Johnson et al., 2017b)</ref>, and stacked-attention  In contrast, we achieve higher accuracy in 6 epochs, 9.5 hours overall, leading to a 10x reduction in training time.</p><p>In order to study the ability of MAC to generalize from a smaller amount of data, we explore its performance on subsets of CLEVR, sampled at random from the original 700k dataset. As shown in <ref type="figure" target="#fig_7">figure 9</ref>, MAC outperforms the other models by a wide margin: For 50% of the data, equivalent to 350k samples, other models obtain accuracies ranging between 70% and 93%, while our model achieves 97.6%. The gap becomes larger as the dataset size reduces: for 25% of the data, equivalent to 175k samples, the performance of other models is between 50% and 77%, while MAC maintains a high 94.3% accuracy.</p><p>Finally, for just 10% of the data, amounting to 70k samples, our model is the only one to generalize well, with performance of 85.5% on average, whereas the other leading models fail, achieving 49.0%-54.9%. Note that, as pointed out by <ref type="bibr" target="#b15">Johnson et al. (2017a)</ref>, a simple baseline that predicts the most frequent answer for each question type already achieves 42.1%, suggesting that answering only half of the questions correctly means that the other models barely learn to generalize from this smaller subset. These results demonstrate the robustness and generalization capacity of our architecture and its key role as a structural prior guiding MAC to learn the intended reasoning skills. To gain better insight into the relative contribution of the design choices we made, we perform extensive ablation studies. See <ref type="figure" target="#fig_7">figure 9</ref> and appendix C for accuracies and learning curves. The experiments demonstrate the robustness of the model to hyperparameter variations such as network dimension and length, and shed light on the significance of various aspects and components of the model, as discussed below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATIONS</head><p>Question Attention. The ablations show that using attention over the question words (see section 2.2.1) is highly effective in accelerating learning and enhancing generalization capacity. Using the complete question q instead of the attention-based control state leads to a significant drop of 18.5% in the overall accuracy. Likewise, using unconstrained recurrent control states, without casting them back onto the question words space (step (3) in section 2.2.1) leads to a 6x slowdown in the model convergence rate. These results illustrate the importance and usefulness of decomposing the question into a series of simple operations, such that a single cell is faced with learning the semantics of one or a few words at a time, rather than grasping all of the question at once. They provide evidence for the efficacy of using attention as a regularization mechanism, by restricting the input and output spaces of each MAC cell.</p><p>Control and Memory. Maintaining separation between control and memory proves to be another key property that contributes significantly to the model's accuracy, learning rate and data efficiency.</p><p>We perform experiments for a variant of the MAC cell in which we maintain one hidden state that <ref type="figure" target="#fig_0">Figure 14</ref>: Attention maps produced by MAC which provide some evidence for the ability of the model to perform counting and summation of small numbers. Note how the first iterations focus on the key structural question words "many" and "or" that inform the model of the required reasoning operation it has to perform.</p><p>plays both the roles of the control and memory, iteratively attending and integrating information from both the question and the image. While this approach achieves a final accuracy of 93.75%, it leads to a sharp drop in the convergence rate, as shown in <ref type="figure" target="#fig_7">figure 9</ref>, and a 20.2% reduction in the final accuracy for a smaller 10% subset of CLEVR. The results make a strong case for our model's main design choice, namely, splitting the computation into two dual paths: one that decomposes the linguistic information and another that reconstructs the corresponding visual information.</p><p>The design choices discussed above were found to be the most significant to the model's overall accuracy, convergence rate and generalization. Other design choices that were found beneficial include <ref type="formula">(1)</ref> predicting the final answer based on both the final memory state and the question (see section 2.3), and (2) considering knowledge base elements directly (step (2) in section 2.2.2), resulting in 19.8% and 11.1% improvement for a 10% subset of CLEVR, respectively. Please refer to appendix C for further discussion and results. First, we observe that both the linguistic and visual attentions of the model are very focused on specific terms or regions in the image, and commonly refer to concrete objects ("the shiny red cube" or the "metallic cylinder") or question structural keywords ("or", "and" or "how many"). More importantly, the attention maps give evidence of the ability of the model to capture the underlying semantic structure of the question, traversing the correct transitive relations between the objects it refers to. For instance, we see in <ref type="figure" target="#fig_0">figure 13</ref> how the model explicitly decomposes the question into the correct reasoning steps: first identifying the green ball, then focusing on the red cylinder that is located left of the ball, and finally attending to the yellow cylinder. In the second step, note how the model attends only to the relevant red cylinder and not to other red rubber things, correctly resolving the indirect reference in the question. This shows strong evidence of the ability of the model to perform transitive reasoning, integrating information from prior steps that allows it to focus only on the relevant objects, even when they are not mentioned explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">INTERPRETABILITY</head><p>In <ref type="figure" target="#fig_0">figure 14</ref>, we further see how the model interprets a multi-step counting question, apparently summing up the amounts of two referenced object groups to produce the correct overall count. These observations suggest that the model infers and effectively performs complex reasoning processes in a transparent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced the Memory, Attention and Composition (MAC) network, an end-to-end differentiable architecture for machine reasoning. The model solves problems by decomposing them into a series of inferred reasoning steps that are performed successively to accomplish the task at hand. It uses a novel recurrent MAC cell that aims to formulate the inner workings of a single universal reasoning operation by maintaining a separation between memory and control. These MAC cells are chained together to produce explicit and structured multi-step reasoning processes. We demonstrate the versatility, robustness and transparency of the model through quantitative and qualitative studies, achieving state-of-the-art results on the CLEVR task for visual reasoning, and generalizing well even from a 10% subset of the data. The experimental results further show that the model can adapt to novel situations and diverse language, and generate interpretable attention-based rationales that reveal the underlying reasoning it performs. While CLEVR provides a natural testbed for our approach, we believe that the architecture will prove beneficial for other multi-step reasoning and inference tasks, including reading comprehension, textual question answering, and real-world VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We wish to thank Justin Johnson, Aaron Courville, Ethan Perez, Harm de Vries, Mateusz Malinowski, Jacob Andreas, and the anonymous reviewers for the helpful suggestions, comments and discussions. Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462 for supporting this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL A IMPLEMENTATION AND TRAINING DETAILS</head><p>We train our model using Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2014)</ref>, with a learning rate of 10 −4 and a batch size of 64. We use gradient clipping, and employ early stopping based on the validation accuracy, resulting in a training process of 10-20 epochs, equivalent to roughly 15-30 hours on a single Maxwell Titan X GPU. Word vectors have dimension 300 and were initialized randomly using a standard uniform distribution. The exponential moving averages of the model weights are maintained during training, with a decay rate of 0.999, and used at test time instead of the raw weights. We use variational dropout of 0.15 across the network, along with ELU as non-linearity, which, in our experience, accelerates training and performs favorably compared to the more standard ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ERROR ANALYSIS</head><p>To gain insight into the model's failure cases, we perform error analysis for the CLEVR and CLEVR-Humans datasets. For CLEVR, we see that many of the errors arise from object occlusions, which may make it harder for the model to recognize the objects' material or shape. Most of the other errors are off-by-one counting mistakes, oftentimes for questions that ask to sum up two groups of objects (see examples in <ref type="figure" target="#fig_0">figure 16</ref>). Interestingly, we noticed that when the model is required to count objects that are heavily occluded, it may lead the model to slightly underestimate the correct number of objects, suggesting that it may perform some sort of "continuous" counting rather than discrete.</p><p>For CLEVR-Humans, the errors made by the model are more diverse. About half of the errors result from questions about reasoning skills that the model has not been exposed to while training on CLEVR. These include questions about physical properties: lighting and shadows, reflections and objects stability ("How many of the items are casting a shadow?", "Can a ball stay still on top of one another?"); relative distances ("How many objects. . . are almost touching?", "What color is the sphere positioned closest to. . . ", "What object is in between. . . "); relative amounts ("Are half the items. . . ", "Are the objects mainly. . . "); commonalities ("What color are the identical objects?", "What shape do the items that . . . have in common?"); and negative questions, which refer to objects that do not maintain some property ("How many items do not. . . "). In addition, we observed some cases where the model misinterprets unseen words, capturing plausible but incorrect semantics: for instance, in some cases it interpreted a "caramel" object as yellow whereas the original question referred to a brown one, or considered a cylinder to be "circle" while the question referred to the spheres only. We believe that these errors may arise from diversity in the semantics of these words across the dataset, making the model learn those potentially valid but incorrect interpretations. Similarly to CLEVR, we observed some off-by-one errors in CLEVR-Humans. Finally, in one fifth of the cases, the errors result from faulty or ambiguous questions, which may mistakenly regard a cyan object as blue or mention references that cannot be uniquely resolved to a specific object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ABLATION STUDIES</head><p>Based on the validation set, we have conducted an ablation study for MAC to better understand the impact of each of its components on the overall performance. We have tested each setting for the primary 700K CLEVR dataset as well as on a 10% subset of it. See table 2, <ref type="figure" target="#fig_0">figure 9 and figure 15</ref> for final accuracies and training curves. The following discussion complements the main conclusions presented in section 4.3:</p><p>Network Length. We observe a positive correlation between the network length and its performance, with significant improvements up to length p = 8. These results stand out from other multi-hop architectures that tend to benefit from a lower number of iterations, commonly 2-3 only <ref type="bibr" target="#b18">Kumar et al., 2016)</ref>, and suggest that MAC makes effective use of the recurrent cells to perform compositional reasoning.</p><p>Weight Sharing. Weight sharing across the p cell instances has also proven to be useful both for the primary CLEVR task as well as for settings with limited data. In contrast to alternative approaches that apply specialized modules for different parts of the reasoning, these results provide some evidence for the ability of the same MAC cell to adapt its behavior to the task at hand and demonstrate different behaviors as inferred from the context.</p><p>Control Unit. We have performed several ablations in the control unit architecture to identify its contribution to the model behavior and performance. First, we observe that, as would be expected, an ablated model that reasons over the image alone with no access to the question, performs poorly, achieving accuracy of 51.1%. As discussed in section 4.3, the ablations further show the importance of applying attention over the question words to decompose it into an explicit sequence of steps. Finally, we find that using the "contextual words" -the output states of a biLSTM processing the question -results in better performance and faster learning than attending directly to the learned word vectors. This implies that the model benefits from interpreting the word semantics and entailed behaviors in the broader context of the question rather than as a sequence of independent entities.</p><p>Write Unit. The basic MAC write unit integrates new information r i with the previous memory state m i−1 through a linear transformation (step (1) in section 2.2.3). In this experiment, we explore other variants of the unit. We begin by measuring the impact of the self-attention and gating mechanisms, both aiming to reduce long-range dependencies in the reasoning process. Compared to the basic MAC model, which achieves 98.94% on the validation set, self-attention yields an accuracy of 99.23%, memory gating -99.36%, and adding both results in 99.48%. While we can see some gain from using these components for CLEVR, we speculate that they may prove more useful for tasks that necessitate longer or more complex reasoning processes over larger knowledge bases.</p><p>Next, we examine ablated write unit variants that assign the newly retrieved information r i (or its linear transformation) to m i directly, ignoring the prior memory content m i−1 . Notably, the results show that in fact such variants are only slightly worse than the default basic write unit, reducing accuracy by 0.4% only. We perform further experiments in which we compute the new memory state m i by averaging the retrieved information r i with the previous memory state m i−1 using a sigmoidal gate alone. This architecture results in equivalent performance to that of the standard basic write unit variant.</p><p>Gate Bias Initialization. Finally, we test the impact of the gate bias (step (3) in section 2.2.3), initializing it to either −1, 0 or 1. Intuitively, initialization of −1 amounts to biasing the model to retain previous memory states and thereby shortening the effective reasoning process, while initialization of 1 is equivalent to using all new intermediate results derived by each of the cells. The experiments show that a bias of 1 is optimal for training on the full dataset while 0 is ideal for settings of limited data (training on 10% of the data). These results demonstrate that when enough data is available, the MAC network benefits from utilizing its full capacity, whereas biasing the model towards using less cells helps to mitigate overfitting when data is more scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RELATED WORK</head><p>In this section we provide detailed discussion of related work. Several models have been applied to the CLEVR task. These can be partitioned into two groups, module networks that use the strong supervision provided as a tree-structured functional program associated with each instance, and end-to-end, fully differentiable networks that combine a fairly standard stack of CNNs with components that aid them in performing reasoning tasks. We also discuss the relation of MAC to other approaches, such as memory networks and neural computers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 MODULE NETWORKS</head><p>The modular approach <ref type="bibr" target="#b1">(Andreas et al., 2016a;</ref><ref type="bibr" target="#b19">b;</ref><ref type="bibr" target="#b13">Hu et al., 2017;</ref><ref type="bibr" target="#b16">Johnson et al., 2017b)</ref> first translates a given question into a tree-structured action plan, aiming to imitate the question underlying structural representation externally provided as strong supervision. Then, it constructs a tailor-made network that progressively executes the plan over the image. The network is composed of discrete units selected out of a fixed collection of predefined "modules", each responsible for an elementary reasoning operation, such as identifying an object's color, filtering them for their shape, or comparing their amounts. Each module has its own set of learned parameters <ref type="bibr" target="#b16">(Johnson et al., 2017b)</ref>, or even a hand-crafted design <ref type="bibr" target="#b1">(Andreas et al., 2016a</ref>) that guides it towards its intended behavior.</p><p>Overall, this approach makes discrete choices at two levels: the identity of each module -the behavior it should learn among a fixed set of possible behavior types, and the network layout -the way in which the modules are wired together to compute the answer. The model differentiability is thus confined to the boundaries of a single module.</p><p>Several key differences exist between our approaches. First, MAC replaces the fixed and specialized modules inventory with one universal cell that adapts its operation to the task at hand, selected from a continuous range of reasoning behaviors. Therefore, in contrast to module networks, our cell can be applied across all the reasoning steps, sharing both its parameters and architecture. Second, we replace the dynamic recursive tree structures with a sequential topology, augmented by soft attention mechanisms, inspired by <ref type="bibr" target="#b5">Bahdanau et al. (2015)</ref>. This confers the network with the capacity to represent arbitrarily complex Directed Acyclic Graphs (DAGs) in a soft way, while still having efficient and readily deployed physical sequential structure. Together, these relaxations allow us to effectively train our model end-to-end by backpropagation alone, whereas module networks demand more involved training schemes that rely on the strongly-supervised programs at the first stage, and on various reinforcement learning (RL) techniques at the second. Finally, since the only source of supervision in training our model arises from the answer to each question, MAC is free to acquire more robust and adaptive reasoning strategies from the bottom up -inferred directly form the data, rather than trying to imitate the behaviors dictated by brittle parsers or closed domain functional programs, and is thus more applicable to real-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 AUGMENTED CONVOLUTIONAL NEURAL NETWORKS</head><p>Alternative approaches for the CLEVR task that do not rely on the provided programs as a strong supervision signal are <ref type="bibr" target="#b24">Santoro et al. (2017)</ref> and <ref type="bibr" target="#b22">Perez et al. (2017)</ref>. Both complement standard multi-layer Convolutional Neural Networks (CNNs) with components that aid them in handling compositional and relational questions.</p><p>Relation Networks. <ref type="bibr" target="#b24">Santoro et al. (2017)</ref> appends a Relation Network (RN) layer to the CNN. This layer inspects all pairs of pixels in the image, thereby enhancing the network capacity to reason over binary relations between objects. While this approach is very simple and elegant conceptually, it suffers from quadratic computational complexity, in contrast to our approach, which is linear. But beyond that, closer inspection reveals that this direct pairwise comparison might be unnecessary. Based on the analogy suggested by <ref type="bibr" target="#b24">Santoro et al. (2017)</ref>, according to which pixels are equivalent to objects and their pairwise interactions to relations, an RN layer attempts to grasp the induced graph between objects all at once in one shallow and broad layer. Conversely, our attention-based model proceeds in steps, iteratively comparing the image to a memory state that had aggregated information from the image in prior iterations. By the same analogy, MAC traverses a narrow and deep reasoning "path" that progressively follows transitive relations. Consequently, our model exhibits a relational capacity while circumventing the computational inefficiency.</p><p>FiLM. <ref type="bibr" target="#b22">Perez et al. (2017)</ref> proposes a model for visual reasoning that interleaves standard CNN layers with linear layers, reminiscent of layer normalization techniques <ref type="bibr" target="#b4">(Ba et al., 2016;</ref><ref type="bibr" target="#b14">Ioffe &amp; Szegedy, 2015)</ref>. Each of these layers, called FiLM, is conditioned on the question, which is translated into matching bias and variance terms that tilt the layer's activations to reflect the specifics of the given question, thus influencing the computation done over the image. Similarly to our model, this approach features distant modulation between the question and the image, where the former can affect the latter only through constrained means. However, since the same normalization is applied across all the activations homogeneously, agnostic to both their spatial location as well as their feature values, FiLM does not allow the question to differentiate between regions in the image based on their semantics -the objects or concepts they represent.</p><p>This stands in stark contrast to our attention-based model, which readily allows and actually encourages the question to inform the model about relevant regions to focus on. As supported by section 4, this more selective interaction between the question and the image facilitates learning and increases the model's generalizability. Indeed, since attention is commonly used in models designed for standard VQA <ref type="bibr" target="#b3">(Antol et al., 2015;</ref><ref type="bibr" target="#b11">Gupta, 2017;</ref><ref type="bibr" target="#b20">Lu et al., 2016;</ref>, it is reasonable to assume that it would be beneficial to incorporate such methods into visual reasoning systems for the CLEVR task as well. In fact, attention mechanisms should be especially useful for multi-step reasoning questions such as those present in CLEVR. Such questions refer to several relations between different objects in the image and feature compositional structure that may be approached one step at a time. Thus, it should be beneficial for a cogent responder to have the capacity to selectively focus on one or some objects at each step, traversing the relevant relational links one after the other, both at the image level, and at the question level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 MEMORY AND ATTENTION</head><p>Our architecture draws inspiration from recent research on mechanisms for neural memory and attention <ref type="bibr" target="#b18">(Kumar et al., 2016;</ref><ref type="bibr" target="#b26">Xiong et al., 2016;</ref><ref type="bibr" target="#b9">Graves et al., 2014;</ref>. <ref type="bibr" target="#b18">Kumar et al. (2016)</ref> and <ref type="bibr" target="#b26">Xiong et al. (2016)</ref> propose the Dynamic Memory Network (DMN) model that proceeds in an iterative process, attending to relevant information from a given knowledge base, which is then successively accumulated into the model's memory state. However, the DMN views the question as one atomic unit, whereas our model decomposes it into a multi-step action plan that informs each cell of its specific objective. Another key difference is the distant interaction between the question and the knowledge base that characterizes our model. Conversely, DMN fuses their representations together into the same vector space. <ref type="bibr" target="#b9">Graves et al. (2014;</ref>) complement a neural network with an external memory it can interact with through the means of soft attention. Similarly to our approach, the model consists of a controller that performs read and write operations over a fixed-size memory array. However, in contrast to <ref type="bibr" target="#b9">Graves et al. (2014;</ref>, we employ a recurrent memory structure, where each MAC cell is associated with its own memory state. Rather than reading and writing iteratively into multiple slots in a shared memory resource, each cell creates a new memory, building upon the contents of the prior ones. This allows us to avoid potential issues of content blurring due to multiple global write operations, while still supporting the emergence of complex reasoning processes that progressively interact with preceding memories and intermediate results to accomplish the task at hand.   Figure 18: Attention maps produced by a MAC network of length 6, providing evidence for the ability of the model to track transitive relations, and perform logical operations, counting and summation. Note how the first iterations focus on the key structural question words "many" and "or" that serve as indicators for the model of the required reasoning operation it has to perform. Also note how the model correctly sums up two object groups in the second example, while correctly accounting for the intersection between them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A CLEVR example. Color added for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(3) The output classifier computes the final answer using the question and the final memory state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The MAC cell architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The Control Unit (CU) architecture. The control unit attends at each iteration to some part of the question, by applying soft attention over the question words, and updates the control state accordingly. The unit's inputs and outputs are in bold. See section 2.2.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Attention maps produced by a MAC network of length 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The Write Unit (WU) architecture. The write unit integrates the information retrieved from the knowledge base into the recurrent memory state, producing a new intermediate result mi that corresponds to the reasoning operation ci. See section 2.2.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The output unit. A classifier that predicts an answer based on the question and the final memory state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>From left to right: (1) Learning curve of MAC and alternative approaches (accuracy / epoch). (2) Models' performance as a function of the CLEVR subset size used for training, ranging from 1% to 100%. (3),(4) Learning curves for ablated MAC variants. See section 4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Error distribution for CLEVR and CLEVR-Humans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>As shown in table 1, our model achieves state-of-the-art performance on CLEVR-Humans both before and after fine-tuning. It surpasses the nextbest model by 5.6% percent, achieving 81.5%. The results substantiate the model's robustness against linguistic variations and noise as well as its ability to adapt to new and more diverse vocabulary and reasoning skills. The soft attention performed over the question allows the model to focus on the words that are most critical to answer the question while paying less attention to irrelevant linguistic variations. See figure 11, and figures 16 and 17 in the appendix for examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>CLEVR-Humans examples showing the model performs novel reasoning skills that do not appear in CLEVR, including: obstructions, object uniqueness, relative distances, superlatives and new concepts.networks (SA) (Johnson et al., 2017b; Yang et al., 2016). As shown in figure 9, our model learns significantly faster than other approaches. While we do not have learning curves for the recent Relation Network model, Santoro et al. (2017) report 1.4 million iterations (equivalent to 125 epochs) to achieve 95.5% accuracy, whereas our model achieves a comparable accuracy after only 3 epochs, yielding a 40x reduction in the length of the training process. Likewise, Perez et al. (2017) report a training time of 4 days, equivalent to 80 epochs, to reach accuracy of 97.7%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Model performance as a function of the network length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Attention maps produced by MAC, showing how it tracks transitive relations between objects.To obtain better insight into the underlying reasoning processes MAC learns to perform, we study visualizations of the attention distributions produced by the model during its iterative computation, and provide examples in figures 13, 14, 17, and 18. Examining the sequence of attention maps over the image and the question reveals several qualitative patterns and properties that characterize MAC's mode of operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Learning curves for ablated MAC variants (accuracy / epoch). See appendix C for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>The first five rows show examples of the final attention map produced by the model for CLEVR-Human questions, demonstrating the ability of the model to perform novel reasoning skills and cope with new concepts that have not been introduced in CLEVR. These include in particular: obstructions, object uniqueness, relative distances, superlatives and new terms. The final row shows examples from CLEVR with object occlusions and summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Attention maps produced by MAC networks of lengths 4 and 6, providing evidence for the ability of the model to track transitive relations and perform logical operations. Note how the model tends to proceed from the end of the question backwards, tracking the relevant objects iteratively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CLEVR and CLEVR-Humans Accuracy by baseline methods, previous methods, and our method (MAC). For CLEVR-Humans, we show results before and after fine-tuning. (*) denotes use of extra supervisory information through program labels. ( † ) denotes use of data augmentation. ( ‡ ) denotes training from raw pixels.</figDesc><table><row><cell>Model</cell><cell>CLEVR</cell><cell>Count</cell><cell>Exist</cell><cell>Compare</cell><cell>Query</cell><cell>Compare</cell><cell>Humans</cell><cell>Humans</cell></row><row><cell></cell><cell>Overall</cell><cell></cell><cell></cell><cell>Numbers</cell><cell>Attribute</cell><cell>Attribute</cell><cell>before FT</cell><cell>after FT</cell></row><row><cell>Human (Johnson et al., 2017b)</cell><cell>92.6</cell><cell>86.7</cell><cell>96.6</cell><cell>86.5</cell><cell>95.0</cell><cell>96.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Q-type baseline (Johnson et al., 2017b)</cell><cell>41.8</cell><cell>34.6</cell><cell>50.2</cell><cell>51.0</cell><cell>36.0</cell><cell>51.3</cell><cell>-</cell><cell>-</cell></row><row><cell>LSTM (Johnson et al., 2017b)</cell><cell>46.8</cell><cell>41.7</cell><cell>61.1</cell><cell>69.8</cell><cell>36.8</cell><cell>51.8</cell><cell>27.5</cell><cell>36.5</cell></row><row><cell>CNN+LSTM (Johnson et al., 2017b)</cell><cell>52.3</cell><cell>43.7</cell><cell>65.2</cell><cell>67.1</cell><cell>49.3</cell><cell>53.0</cell><cell>37.7</cell><cell>43.2</cell></row><row><cell cols="2">CNN+LSTM+SA+MLP (Johnson et al., 2017a) 73.2</cell><cell>59.7</cell><cell>77.9</cell><cell>75.1</cell><cell>80.9</cell><cell>70.8</cell><cell>50.4</cell><cell>57.6</cell></row><row><cell>N2NMN* (Hu et al., 2017)</cell><cell>83.7</cell><cell>68.5</cell><cell>85.7</cell><cell>84.9</cell><cell>90.0</cell><cell>88.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PG+EE (9K prog.)* (Johnson et al., 2017b)</cell><cell>88.6</cell><cell>79.7</cell><cell>89.7</cell><cell>79.1</cell><cell>92.6</cell><cell>96.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PG+EE (18K prog.)* (Johnson et al., 2017b)</cell><cell>95.4</cell><cell>90.1</cell><cell>97.3</cell><cell>96.5</cell><cell>97.4</cell><cell>98.0</cell><cell>54.0</cell><cell>66.6</cell></row><row><cell>PG+EE (700K prog.)* (Johnson et al., 2017b)</cell><cell>96.9</cell><cell>92.7</cell><cell>97.1</cell><cell>98.7</cell><cell>98.1</cell><cell>98.9</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN+LSTM+RN  † ‡ (Santoro et al., 2017)</cell><cell>95.5</cell><cell>90.1</cell><cell>97.8</cell><cell>93.6</cell><cell>97.9</cell><cell>97.1</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN+GRU+FiLM (Perez et al., 2017)</cell><cell>97.7</cell><cell>94.3</cell><cell>99.1</cell><cell>96.8</cell><cell>99.1</cell><cell>99.1</cell><cell>56.6</cell><cell>75.9</cell></row><row><cell>CNN+GRU+FiLM  ‡ (Perez et al., 2017)</cell><cell>97.6</cell><cell>94.3</cell><cell>99.3</cell><cell>93.4</cell><cell>99.3</cell><cell>99.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MAC</cell><cell>98.9</cell><cell>97.1</cell><cell>99.5</cell><cell>99.1</cell><cell>99.5</cell><cell>99.5</cell><cell>57.4</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracies for ablated MAC models, measured for the validation set after training on the full CLEVR dataset (left) and 10% subset of it (right).</figDesc><table><row><cell>Model</cell><cell>Standard CLEVR</cell><cell>10% CLEVR</cell></row><row><cell>MAC</cell><cell>98.9</cell><cell>84.5</cell></row><row><cell>state dimension 256</cell><cell>98.4</cell><cell>76.3</cell></row><row><cell>state dimension 128</cell><cell>97.6</cell><cell>77.0</cell></row><row><cell>unshared weights</cell><cell>97.8</cell><cell>67.5</cell></row><row><cell>attention over word vectors</cell><cell>98.3</cell><cell>61.4</cell></row><row><cell>w/o word-attention</cell><cell>95.3</cell><cell>63.2</cell></row><row><cell>question vector as control</cell><cell>80.7</cell><cell>65.0</cell></row><row><cell>w/o control</cell><cell>55.6</cell><cell>51.5</cell></row><row><cell>w/o memory-control separation</cell><cell>93.9</cell><cell>64.7</cell></row><row><cell>w/o direct KB elements</cell><cell>98.4</cell><cell>73.4</cell></row><row><cell>retrieved → memory</cell><cell>98.2</cell><cell>84.5</cell></row><row><cell>W · retrieved + b → memory</cell><cell>98.5</cell><cell>83.7</cell></row><row><cell>only memory gate</cell><cell>99.3</cell><cell>83.1</cell></row><row><cell>w/ self-attention</cell><cell>99.2</cell><cell>83.2</cell></row><row><cell>w/ memory gate</cell><cell>99.4</cell><cell>83.1</cell></row><row><cell>w/ self-attention and memory gate</cell><cell>99.5</cell><cell>85.5</cell></row><row><cell>gate bias 0</cell><cell>98.7</cell><cell>84.9</cell></row><row><cell>gate bias 1</cell><cell>99.4</cell><cell>68.5</cell></row><row><cell>gate bias −1</cell><cell>99.0</cell><cell>77.1</cell></row><row><cell>prediction w/o question</cell><cell>97.8</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>What color is the metallic cylinder in front of the silver cylinder? A: cyan Q: What is the object made of hiding behind the green cube? A: rubber Q: What is the color of the ball that is farthest away? A: blue What shape are most of the shiny items? A: sphere Q: What is the tan object made of? A: rubber Q: Are half the items shown green? A: yes (no) Q: What color object is biggest? A: blue Q: Which shape is a different color from the others? A: cylinder Q: How many other objects are the same size as the blue ball? A: 7 Q: What is the shape of the object that is to the left of the red rubber cube and behind the metallic cylinder? A: sphere (cylinder)Q: How many tiny objects are green things or red objects? A: 4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Q: How many matte cubes are</cell></row><row><cell></cell><cell></cell><cell></cell><cell>there? A: 2</cell></row><row><cell>Q: How many spheres are</cell><cell>Q: How many square objects are</cell><cell>Q: What object is to the far</cell><cell>Q: Are the yellow blocks the</cell></row><row><cell>pictured? A: 4</cell><cell>in the picture? A: 4</cell><cell>right? A: cube</cell><cell>same? A: no</cell></row><row><cell>Q: What shape is the smallestt</cell><cell>Q: What object looks like a</cell><cell>Q: Can a ball stay still on top of</cell><cell>Q: What color is the center</cell></row><row><cell>object in this image? A: sphere</cell><cell>caramel? A: cube</cell><cell>one another? A: yes (no)</cell><cell>object? A: blue</cell></row><row><cell>Q: How many gray objects are</cell><cell>Q: How many small objects are</cell><cell>Q: What color is the largest</cell><cell>Q:</cell></row><row><cell>shown? A: 3</cell><cell>rubber? A: 2</cell><cell>cube? A: yellow</cell><cell></cell></row></table><note>Q:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both self-attention connections as well as the memory gate serve to reduce long-term dependencies. However, note that for the CLEVR dataset we were able to maintain almost the same performance with the first step only, and so we propose the second and third ones as optional extensions of the basic write unit, and explore their impact on the model's performance in section 4.3.2 Note that some questions refer to important aspects that do not have counterpart information in the knowledge base, and thus considering both the question and the memory is critical to answer them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We initialize the word embeddings of our model to random vectors using a uniform distribution. In an earlier version of this work, we used pretrained GloVe vectors, but found that they did not improve the performance for CLEVR and led to only a marginal improvement for CLEVR-Humans.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For previous models, we use the author's original publicly available implementations. All the models were trained with an equal batch size of 64 (as in the original implementations) and using the same hardware -a single Maxwell Titan X GPU per model. To make sure the results are statistically significant, we run each model multiple (10) times, and plot the averages and confidence intervals.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05518</idno>
		<title level="m">Towards deep symbolic reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay Kumar</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03865</idno>
		<title level="m">Survey of visual question answering: Datasets and techniques</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07871</idno>
		<title level="m">FiLM: Visual reasoning with a general conditioning layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple method to determine if a music information retrieval system is a horse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1636" to="1644" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
