<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Victoria University of Wellington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
							<email>david.balduzzi@vuw.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="institution">Victoria University of Wellington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bastiaan</forename><surname>Kleijn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Victoria University of Wellington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
							<email>mengjie.zhang@ecs.vuw.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="institution">Victoria University of Wellington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain adaptation</term>
					<term>domain generalization</term>
					<term>feature learning</term>
					<term>kernel methods</term>
					<term>scatter</term>
					<term>object recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation. (Research and Innovation) in the Faculty of Engineering. His current research interests include evolutionary computation, particularly genetic programming, particle swarm optimization, and learning classifier systems with application areas of image analysis, multiobjective optimization, classification with unbalanced data, feature selection and reduction, and job shop scheduling. He has published over 350 academic papers in refereed international journals and conferences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Supervised learning is perhaps the most popular task in machine learning and has recently achieved dramatic successes in many applications such as object recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, speech recognition <ref type="bibr" target="#b3">[4]</ref>, and machine translation <ref type="bibr" target="#b4">[5]</ref>. These successes derive in large part from the availability of massive labeled datasets such as PASCAL VOC2007 <ref type="bibr" target="#b5">[6]</ref> and ImageNet <ref type="bibr" target="#b6">[7]</ref>. Unfortunately, obtaining labels is often a time-consuming and costly process that requires human experts. Furthermore, the process of collecting samples is prone to dataset bias <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, i.e., a learning algorithm trained on a particular dataset generalizes poorly across datasets. In object recognition, for example, training images may be collected under specific conditions involving camera viewpoints, backgrounds, lighting conditions, and object transformations. In such situations, the classifiers obtained with learning algorithms operating on samples from one dataset cannot be directly applied to other related datasets. Developing learning algorithms that are robust to label scarcity and dataset bias is therefore an important and compelling problem.</p><p>Domain adaptation <ref type="bibr" target="#b9">[10]</ref> and domain generalization <ref type="bibr" target="#b10">[11]</ref> have been proposed to overcome the fore-mentioned issues. In this context, a domain represents a probability distribution from which the samples are drawn and is often equated with a dataset. The domain is usually divided into two different types: the source domain and the target domain, to distinguish between a domain with labeled samples and a domain without labeled samples. These two domains are related but different, which limits the applicability of standard supervised learning models on the target domain. In particular, the basic assumption in standard supervised learning that training and test data come from the same distribution is violated.</p><p>The goal of domain adaptation is to produce good models on a target domain, by training on labels from the source domain(s) and leveraging unlabeled samples from the target domain as supplementary information during training. Domain adaptation has demonstrated significant successes in various applications, such as sentiment classification <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, visual object recognition <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and WiFi localization <ref type="bibr" target="#b17">[18]</ref>.</p><p>Finally, the problem of domain generalization arises in situations where unlabeled target samples are not available, but samples from multiple source domains can be accessed. Examples of domain generalization applications are automatic gating of flow cytometry <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref> and visual object recognition <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>The main practical issue is that several state-of-the-art domain adaptation and domain generalization algorithms for object recognition result in optimization problems that are inefficient to solve <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Therefore, they may not be suitable in situations that require a real-time learning stage. Furthermore, although domain adaptation and domain generalization are closely related problems, domain adaptation algorithms cannot in general be applied directly to domain generalization, since they rely on the availability of (unlabeled) samples from the target domain. It is highly desirable to develop algorithms that can be computed more efficiently, are compatible with both domain adaptation and domain generalization, and provides state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Goals and Objectives</head><p>To address the fore-mentioned issues, we propose a fast unified algorithm for reducing dataset bias that can be used for both domain adaptation and domain generalization. The basic idea of our algorithm is to learn representations as inputs to a classifier that are invariant to dataset bias. Intuitively, the learnt representations should incorporate four requirements: (i) separate points with different labels and (ii) separate the data as a whole (high variance), whilst (iii) not separating points sharing a label and (iv) reducing mismatch between the two or more domains. The main contributions of this paper are as follows:</p><p>• The first contribution is scatter, a simple geometric function that quantifies the mean squared distance of a distribution from its centroid. We show that the above four requirements can be encoded through scatter and establish the relationship with Linear Dis-arXiv:1510.04373v2 [cs.CV] <ref type="bibr" target="#b25">26</ref> Jul 2016</p><p>criminant Analysis <ref type="bibr" target="#b23">[24]</ref>, Principal Component Analysis, Maximum Mean Discrepancy <ref type="bibr" target="#b24">[25]</ref> and Distributional Variance <ref type="bibr" target="#b18">[19]</ref>. • The second contribution is a fast scatter-based feature learning algorithm that can be applied to both domain adaptation and domain generalization problems, Scatter Component Analysis (SCA), see Algorithm 1. To the best of our knowledge, SCA is the first multi-purpose algorithm applicable across a range of domain adaptation and generalization tasks. The SCA optimization reduces to a generalized eigenproblem that admits a fast and exact solution on par with Kernel PCA <ref type="bibr" target="#b25">[26]</ref> in terms of time complexity. • The third contribution is the derivation of a theoretical bound for SCA in the case of domain adaptation. Our theoretical analysis shows that domain scatter controls the generalization performance of SCA. We demonstrate that domain scatter controls the discrepancy distance under certain conditions. The discrepancy distance has previously been shown to control the generalization performance of domain adaptation algorithms <ref type="bibr" target="#b26">[27]</ref>. We performed extensive experiments to evaluate the performance of SCA against a large suite of alternatives in both domain adaptation and domain generalization settings. We found that SCA performs considerably faster than the prior state-of-the-art across a range of visual object cross-domain recognition, with competitive or better performance in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization of the Paper</head><p>This paper is organized as follows. Section 2 describes the problem definitions and reviews existing work on domain adaptation and domain generalization. Sections 3 and 4 describes our proposed tool and also the corresponding feature learning algorithm, Scatter Component Analysis (SCA). The theoretical domain adaptation bound for SCA is then presented in Section 5. Comprehensive evaluation results and analyses are provided in Sections 6 and 7. Finally, Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND LITERATURE REVIEW</head><p>This section establishes the basic definitions of domains, domain adaptation, and domain generalization. It then reviews existing work in domain adaptation and domain generalization, particularly in the area of computer vision and object recognition.</p><p>A domain is a probability distribution P XY on X × Y, where X and Y are the input and label spaces respectively. For the sake of simplicity, we equate P XY with P. The terms domain and distribution are used interchangeably throughout the paper. Let S = {x i , y i } n i=1 ∼ P be an i.i.d. sample from a domain. It is convenient to use the notationP for the corresponding empirical distributionP(x, y) = 1 n n i=1 δ (xi,yi) (x, y), where δ is the Dirac delta. We define domain adaptation and domain generalization as follows.</p><p>Definition 1 (Domain Adaptation). Let P s and P t be a source and target domain respectively, where P s = P t . Denote by It is instructive to compare these two related definitions. The main difference between domain adaptation and domain generalization is on the availability of the unlabeled target samples. Both have the same goal: learning a labeling function f : X → Y that performs well on the target domain. In practice, domain generalization requires m &gt; 1 to work well although m = 1 might not violate Definition 2. Note that domain generalization can be exactly reduced to domain adaptation if m = 2 and P t X ∈ ∆. Domain adaptation and domain generalization have recently attracted great interest in machine learning. We present a review of recent literature that is organized into two parts: i) domain adaptation and ii) domain generalization.</p><formula xml:id="formula_0">S s = {x s i , y s i } ns i=1 ∼ P s and S t u = {x t i } nt i=1 ∼ P t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Adaptation</head><p>Earlier studies on domain adaptation focused on natural language processing, see, e.g., <ref type="bibr" target="#b27">[28]</ref> and references therein. Domain adaptation has gained increasing attention in computer vision for solving dataset bias in object recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and object detection <ref type="bibr" target="#b32">[33]</ref>. The reader is encouraged to consult the recent survey in visual domain adaptation <ref type="bibr" target="#b33">[34]</ref> for a more comprehensive review. We classify domain adaptation algorithms into three categories: i) the classifier adaptation approach, ii) the selection/reweighting approach, and iii) the feature transformation-based approach.</p><p>The classifier adaptation approach aims to learn a good, adaptive classifier on a target domain by leveraging knowledge from source or auxiliary domains <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Adaptive Support Vector Machines (A-SVMs) <ref type="bibr" target="#b34">[35]</ref> utilize auxiliary classifiers to adapt a primary classifier that performs well on a target domain, where the optimization criterion is similar to standard SVMs. The Domain Adaptation Machine (DAM) <ref type="bibr" target="#b35">[36]</ref> employs both a domain-dependent regularizer based on a smoothness assumption and a sparsity regularizer in Least-Squares SVMs <ref type="bibr" target="#b39">[40]</ref>. Recently, a multi-instance learning based classifier for action and event recognition, trained on weakly labeled web data, was proposed <ref type="bibr" target="#b38">[39]</ref>.</p><p>The reweighting/selection approach reduces sample bias by reweighting or selecting source instances that are 'close' to target instances -selection can be considered as the 'hard' version of reweighting. The basic idea has been studied under the name of covariate shift <ref type="bibr" target="#b40">[41]</ref>. Gong et al. <ref type="bibr" target="#b41">[42]</ref> applied a convex optimization strategy to select some source images that are maximally similar to the target images according to Maximum Mean Discrepancy <ref type="bibr" target="#b24">[25]</ref> referred to as landmarks. The landmarks are then used to construct multiple auxiliary tasks as a basis for composing domain-invariant features. Transfer Joint Matching (TJM) <ref type="bibr" target="#b14">[15]</ref> uses a reweighting strategy as a regularizer based on 2,1 -norm structured sparsity on the source subspace bases.</p><p>The feature transformation-based approach is perhaps the most popular approach in domain adaptation. Daume III <ref type="bibr" target="#b42">[43]</ref> proposed a simple feature augmentation method by replicating the source and target data, both are in R d , as well as zero-padding such that the resulting features are in R 3d . Li et al. <ref type="bibr" target="#b43">[44]</ref> extended the method for the case of heterogeneous features, i.e., source and target features have different dimensionality, by introducing a common subspace learnt via the standard SVM formulation. A subspace learning-based algorithm, Transfer Component Analysis (TCA) and its semi-supervised version SSTCA <ref type="bibr" target="#b44">[45]</ref>, utilizes the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b45">[46]</ref> to minimize dataset bias in WiFi localization and text classification applications. Metric learning-based domain adaptation approaches have been proposed <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b46">[47]</ref>, which were early studies in object recognition on the Office dataset. The idea of extracting 'intermediate features' to minimize dataset bias by projecting data onto multiple intermediate subspaces was also considered. Sampling Geodesic Flow (SGF) <ref type="bibr" target="#b47">[48]</ref> and Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b28">[29]</ref> generate multiple subspaces via an interpolation between the source and the target subspace on a Grassmann manifold -a point on the manifold is a subspace. Subspace Alignment (SA) <ref type="bibr" target="#b30">[31]</ref> transforms a source PCA subspace into a new subspace that is well-aligned to a target PCA subspace without requiring intermediate subspaces.</p><p>A recent method called Correlation Alignment (CORAL) facilitates adaptive features by aligning the source and target covariance matrices <ref type="bibr" target="#b48">[49]</ref>. Other subspace learning-based methods such as Transfer Sparse Coding (TSC) <ref type="bibr" target="#b22">[23]</ref> and Domain Invariant Projection (DIP) <ref type="bibr" target="#b49">[50]</ref> make use of MMD, following TCA, to match the source and target distributions in the feature space. One of the methods proposed in <ref type="bibr" target="#b50">[51]</ref> follows a similar intuition by using Hellinger distance as an alternative to MMD. Algorithms based on hierarchical non-linear features or deep learning are also capable of producing powerful domain adaptive features <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>Several works have addressed Probably Approximately Correct theoretical bounds for domain adaptation. Ben-David et al. <ref type="bibr" target="#b56">[57]</ref> presented the first theoretical analysis of domain adaptation, that is, an adaptation bound in classification tasks based on the d A -distance <ref type="bibr" target="#b57">[58]</ref>. Mansour et al. <ref type="bibr" target="#b26">[27]</ref> extended this work in several ways built on Rademacher complexity <ref type="bibr" target="#b58">[59]</ref> and the discrepancy distance, as an alternative to d A -distance. In this paper, we provide a domain adaptation bound for our new algorithm based on the latter analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain Generalization</head><p>Domain generalization is a newer line of research than domain adaptation. Blanchard et al. <ref type="bibr" target="#b10">[11]</ref> first studied this issue and proposed an augmented SVM that encodes empirical marginal distributions into the kernel for solving automatic gating of flow cytometry. A feature projection-based algorithm, Domain-Invariant Component Analysis (DICA) <ref type="bibr" target="#b18">[19]</ref>, was then introduced to solve the same problem. DICA extends Kernel PCA <ref type="bibr" target="#b25">[26]</ref> by incorporating the distributional variance to reduce the dissimilarity across domains and the central subspace <ref type="bibr" target="#b59">[60]</ref> to capture the functional relationship between the features and their corresponding labels.</p><p>Domain generalization algorithms also have been used in object recognition. Khosla et al. <ref type="bibr" target="#b20">[21]</ref> proposed a multi-task max-margin classifier, which we refer to as Undo-Bias, that explicitly encodes dataset-specific biases in feature space. These biases are used to push the dataset-specific weights to be similar to the global weights. Fang et al. <ref type="bibr" target="#b19">[20]</ref> developed Unbiased Metric Learning (UML) based on a learning-to-rank framework. Validated on weakly-labeled web images, UML produces a less biased distance metric that provides good object recognition performance. Xu et al. <ref type="bibr" target="#b21">[22]</ref> extended an exemplar-SVM <ref type="bibr" target="#b60">[61]</ref> to domain generalization by adding a nuclear norm-based regularizer that captures the likelihoods of all positive samples. The proposed model is referred to as LRE-SVM that provides the state-ofthe-art performance. More recently, an autoencoder based algorithm to extract domain-invariant features via multi-task learning has been proposed <ref type="bibr" target="#b61">[62]</ref>.</p><p>Although both domain adaptation and domain generalization have the same goal (reducing dataset bias), the approaches are generally not compatible to each other -domain adaptation methods cannot be directly applied to domain generalization or vice versa. To our best knowledge, only LRE-SVM can be applied to both domain adaptation and domain generalization. The domain generalization algorithm formulation as in DICA, Undo-Bias, or UML typically does not allow to take into account unlabeled data from the target domain. Furthermore, several state-of-the-art domain adaptation and domain generalization algorithms such as TJM and LRE-SVM, require the solution of a computationally complex optimization that induces high complexity in time. In this work, we establish a fast algorithm that overcomes the above issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCATTER</head><p>We work in a feature space that is a reproducing kernel Hilbert space (RKHS) H. The main motivation is to transform original inputs onto H, which is high or possibly infinite dimensional space, with the hope that the new features are linearly separable. The most important property of RKHS is perhaps to allow a computationally feasible transformation onto H by virtue of the kernel trick. </p><formula xml:id="formula_1">|L x [h]| = |h(x)| ≤ λ h H .<label>(1)</label></formula><p>It follows that there is a function φ : X → H (referred to as the canonical feature map) satisfying:</p><formula xml:id="formula_2">L x [h] = h, φ(x) = h(x) for all h ∈ H and x ∈ X . (2)</formula><p>Consequently, for each t ∈ X , one can write</p><formula xml:id="formula_3">φ(t), φ(x) =: κ(t, x).</formula><p>The function κ : X × X → R is referred to as the reproducing kernel.</p><p>Expression <ref type="formula" target="#formula_1">(1)</ref> is the weakest condition that ensures the existence of an inner product and also the ability to evaluate each function in H at every point in the domain, while (2) provides more useful notion in practice. Before introducing scatter, it is convenient to first represent domains as points in RKHS using the mean map <ref type="bibr" target="#b62">[63]</ref>:</p><p>Definition 4 (Mean map). Suppose that X is equipped with a kernel, and that H is the corresponding RKHS with feature map φ : X → H. Let ∆ X denote the set of probability distributions on X . The mean map takes distributions on X to points in H:</p><formula xml:id="formula_4">µ : ∆ X → H : P → E x∼P φ(x) =: µ P .</formula><p>Geometrically, the mean map is the centroid of the image of the distribution under φ. We define scatter as the variance of points in the image around its centroid:</p><formula xml:id="formula_5">Definition 5 (Scatter). The scatter of distribution P on X relative to φ is Ψ φ (P) := E x∼P µ P − φ(x) 2 H</formula><p>where · H is the norm on H.</p><p>The scatter of a domain cannot be computed directly; instead it is estimated from observations. The scatter of a finite set of observations {x 1 , . . . , x n } is computed with respect to the empirical distribution</p><formula xml:id="formula_6">P(x) := 1 n n i=1 δ xi (x) where δ xi (x) = 1 if x i = x 0 else.</formula><p>We provide a theorem that shows how the difference between the true scatter and a finite sample estimate decreases with the sample size.</p><p>Theorem 1 (Scatter Bound). Suppose P is a true distribution over all samples of size n andP is its empirical distribution. Further suppose that φ(x) 2 ≤ M for all x ∈ X . Then, with probability</p><formula xml:id="formula_7">≥ 1 − δ, Ψ φ (P) − Ψ φ (P) ≤ M 2 log( 2 δ ) n .</formula><p>Proof. See Supplementary material.</p><p>Note that the right hand site of the bound is smaller for lower values of M and higher values of n. Furthermore, if κ is in the form of Gaussian kernel, the bound only depends on n since M = 1.</p><p>We provide an example for later use. If the input space is a vector space and φ is the identity then it follows immediately that Lemma 2 (Total variance as scatter). The scatter of the set of pdimensional points (in a matrix form) X = [x 1 , . . . , x n ] ∈ R n×p relative to the identity map φ(x) := x, is the total variance:</p><formula xml:id="formula_8">Ψ(X) = 1 n Tr(X −X) (X −X) = Tr Cov(X),</formula><p>where Tr(·) denotes the trace operation andX = [x, . . . ,x] with x = n i=1 x i . We utilize scatter to formulate a feature learning algorithm referred to as Scatter Component Analysis (SCA). Specifically, scatter quantifies requirements needed in SCA to develop an effective solution for both domain adaptation and generalization, which will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SCATTER COMPONENT ANALYSIS (SCA)</head><p>SCA aims to efficiently learn a representation that improves both domain adaptation and domain generalization. The strategy is to convert the observations into a configuration of points in feature space such that the domain mismatch is reduced. SCA then finds a representation of the problem (that is, a linear transformation of feature space) for which (i) the source and target domains are similar and (ii) elements with the same label are similar; whereas (iii) elements with different labels are well separated and (iv) the variance of the whole data is maximized. Each requirement can be quantified through scatter that leads to four consequences: (i) domain scatter, (ii) between-class scatter, (iii) within-class scatter, and (iv) total scatter.</p><p>The remainder of the subsection defines the above four scatter quantities in more detail (along the way relating the terms to principal component analysis, the maximum mean discrepancy, and Fisher's linear discriminant) and describes the SCA's learning algorithm. We will also see that SCA can be easily switched to either domain adaptation or domain generalization by modifying the configuration of the input domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Total Scatter</head><p>Given m domains P 1 X , . . . , P m X on X , we define the total domain as the meanP</p><formula xml:id="formula_9">X = 1 m m d=1 P d X .</formula><p>The total scatter is then defined by total scatter = Ψ φ (P X ) .</p><p>(</p><p>It is worth emphasizing that this definition is general in the sense that it covers both domain adaptation (m = 2 and one of them is the target domain) and domain generalization (m &gt; 2). Total scatter is estimated from data as follows. Let X = [x 1 , ..., x n ] ∈ R n×p be the matrix of unlabeled samples from all m domains (n = m d=1 n d , where n d is the number of examples in the d-th domain). Given a feature map φ : R p → H corresponding to kernel κ, define a set of functions arranged in a column vector Φ = [φ(x 1 ), ..., φ(x n )] . After centering {φ(x i )} n i=1 by subtracting the mean, the covariance matrix is</p><formula xml:id="formula_11">Cov(Φ) = 1 n Φ Φ. By Lemma 2, Ψ φ P X = Tr Cov(Φ).<label>(4)</label></formula><p>We are interested in the total scatter after applying a linear transform to a finite relevant subspace W : H → R k . To avoid the direct computation of φ : X → H, which could be expensive or undoable, we use the kernel trick. Let Z = ΦW ∈ R n×k be the n transformed feature vectors and</p><formula xml:id="formula_12">[K] ij = [ΦΦ ] ij = [κ(x i , x j )].</formula><p>After fixing B ∈ R n×k such that W = Φ B, the total transformed scatter is</p><formula xml:id="formula_13">Ψ B•φ P X = Tr ( 1 n B KKB).<label>(5)</label></formula><p>We remark that, in our notation, Kernel Principal Component Analysis (KPCA) <ref type="bibr" target="#b25">[26]</ref> corresponds to the optimization problem</p><formula xml:id="formula_14">max Ψ B•φ P X .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Scatter</head><p>Suppose we are given m domains P 1 X , . . . , P m X on X . We can think of the set {µ P 1 X , . . . , µ P m X } ⊂ H as a sample from some latent distribution on domains. Equipping the sample with the empirical distribution and computing scatter relative to the identity map on H yields domain scatter:</p><formula xml:id="formula_15">Ψ {µ P 1 X , . . . , µ P m X } = 1 m m i=1 μ − µ P i 2 ,<label>(7)</label></formula><p>whereμ = 1 m m i=1 µ P i . Note that domain scatter coincides with the distributional variance introduced in <ref type="bibr" target="#b18">[19]</ref>. Domain scatter is also closely related to the Maximum Mean Discrepancy (MMD), used in some domain adaptation algorithms <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Definition 6. Let F be a set of functions f : X → R. The maximum mean discrepancy between domains P and Q is</p><formula xml:id="formula_16">MMD F [P, Q] := sup f ∈F E P [f (x)] − E Q [f (x)] .</formula><p>The MMD measures the extent to which two domains resemble one another from the perspective of function class F. The following theorem relates domain scatter to MMD given two domains, where the case of interest is bounded linear functions on the feature space:</p><p>Lemma 3 (Scatter recovers MMD). The scatter of domains P and Q on X is their (squared) maximum mean discrepancy:</p><formula xml:id="formula_17">Ψ({µ P , µ Q }) = 1 4 MMD 2 F [P, Q], where F = {f : X → R | f is linear and f F ≤ 1}.</formula><p>In particular, if φ is induced by a characteristic kernel on X then</p><formula xml:id="formula_18">Ψ({µ P , µ Q }) = 0 if and only if P = Q.</formula><p>Proof. Note that the theorem involves two levels of probability distributions: (i) the domains P and Q on X , and (ii) the empirical distribution on F that assigns probability p = 1 2 to the points µ P and µ Q , and p = 0 to everything else. Letμ = 1 2 (µ P + µ Q ). By Definition 5,</p><formula xml:id="formula_19">Ψ({µ P , µ Q }) = 1 2 μ − µ P 2 F + 1 2 μ − µ Q 2 F = 1 4 µ P − µ Q 2 F .</formula><p>The result follows from Theorem 2.2 of <ref type="bibr" target="#b24">[25]</ref>.</p><p>Lemma 3 also tells that the domain scatter is a valid metric if the kernel on X is characteristic <ref type="bibr" target="#b64">[65]</ref>. The most important example of a characteristic kernel is the Gaussian RBF kernel, which is the kernel used in the theoretical results and experiments below. We also remark that MMD can be estimated from observed data with bound provided in <ref type="bibr" target="#b65">[66]</ref>, which is analogous to Theorem 1.</p><p>Domain scatter in a transformed feature space in R k is estimated as follows. Suppose we have m samples</p><formula xml:id="formula_20">S d u = {x d i } n d i=1 ∼ P d X .</formula><p>Recall that Z = ΦW = K B, where Z = [z 1 , . . . , z n ] contains projected samples from all domains: z i = W φ(x i ) and</p><formula xml:id="formula_21">K =    K 11 · · · K 1m . . . . . . . . . K m1 · · · K mm    ∈ R n×n (8) is the corresponding kernel matrix, where [K kl ] ij = κ(x k i , x l j )</formula><p>. By some algebra, the domain scatter is</p><formula xml:id="formula_22">Ψ B {µPd X } m d=1 = Tr(B KLKB),<label>(9)</label></formula><p>where</p><formula xml:id="formula_23">L ∈ R n×n is a coefficient matrix with [L kl ] ij = m−1 m 2 n 2 k if k = l, and − 1 m 2 n k n l otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Class Scatter</head><p>For each class k ∈ {1, . . . , C}, let P l X|k denote the conditional distribution on X induced by the total labeled domain P l XY = 1 q q j=1 P j XY when Y = k (the number of labeled domains q does not necessarily equal to the number of source domains m). We define the within-class scatter and between-class scatter as</p><formula xml:id="formula_24">Ψ φ (P l X|k ) within-class-k scatter and Ψ {µ P l X|k=1 , . . . , µ P l X|k=C } between-class scatter .<label>(10)</label></formula><p>The class scatters are estimated as follows.</p><formula xml:id="formula_25">Let S w k = φ(x j ) xj ∈k denote the n k -tuple of source samples in class k. The centroid of S w k is µ k = 1 n k xi∈k φ(x i ). Furthermore, let S b = (µ 1 , . . . , µ |C| ) denote the n-tuple of all class centroids where centroid k appears n k times in S b . The centroid of S b is then the centroid of the source domain:μ s = 1 n |C| k=1 n k µ k . It follows that the within-class scatter is Ψ φ P l X|y k = Tr   n k j=1 (φ(x jk ) − µ k ) (φ(x jk ) − µ k )  </formula><p>and the between-class scatter is</p><formula xml:id="formula_26">Ψ µPl X|y k C k=1 = Tr n k (µ k −μ)(µ k −μ)</formula><p>. The right-hand sides of the above equations are the classical definitions of within-and between-class scatter <ref type="bibr" target="#b23">[24]</ref>. The classical linear discriminant is thus a ratio of scatters</p><formula xml:id="formula_27">Fisher's linear discriminant = Ψ µPl X|y k C k=1 C k=1 Ψ φ P l X|y k .</formula><p>Maximizing Fisher's linear discriminant increases the separation of the data points with respect to the class clusters.</p><p>Given a linear transformation W : H → R k , it follows from Lemma 2 that the class scatters in the projected feature spaceH are</p><formula xml:id="formula_28">Ψ B µPl X|y k C k=1 = Tr(W Cov(S b )W) = Tr(B P s B),<label>(11)</label></formula><formula xml:id="formula_29">C k=1 Ψ B•φ P s X|y k = C k=1 Tr(W Cov(S w k )W) = Tr(B Q s B),<label>(12)</label></formula><p>where</p><formula xml:id="formula_30">P s = C k=1 n k (m k −m)(m k −m) ,<label>(13)</label></formula><formula xml:id="formula_31">Q s = C k=1 K k H k K k ,<label>(14)</label></formula><p>with</p><formula xml:id="formula_32">m k = 1 n k n k j=1 κ(·, x jk ),m = 1 n n j=1 κ(·, x j ), [K k ] ij = [κ(x ik , x jk )]</formula><p>, and the centering matrix H k = I n k − 1 n k 1 n k 1 n k , where I n k denotes a n k × n k identity matrix and 1 n k ∈ R n k denotes a vector of ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Algorithm</head><p>Here we formulate the SCA's learning algorithm by incorporating the above four quantities. The objective of SCA is to seek a representation by solving an optimization problem in the form of the following expression</p><formula xml:id="formula_33">sup {total scatter} + {between-class scatter} {domain scatter} + {within-class scatter} .<label>(15)</label></formula><p>Using <ref type="formula" target="#formula_13">(5)</ref>, <ref type="formula" target="#formula_22">(9)</ref>, <ref type="bibr" target="#b10">(11)</ref>, and (12), the above expression can then be specified in more detail:</p><formula xml:id="formula_34">argmax B Ψ B•φ P X + Ψ B µPl X|y k C k=1 Ψ B µPd X } m d=1 + C k=1 Ψ B•φ P l X|y k .<label>(16)</label></formula><p>Maximizing the numerator encourages SCA to preserve the total variability of the data and the separability of classes. Minimizing the denominator encourages SCA to find a representation for which the source and target domains are similar, and source samples sharing a label are similar.</p><p>Objective function. We reformulate <ref type="bibr" target="#b15">(16)</ref> in three ways. First, we express it in terms of linear algebra. Second, we insert hyperparameters that control the trade-off between scatters as one scatter quantity could be more important than others in a particular case. Third, we impose the constraint that W W = B KB is small to control the scale of the solution.</p><p>Explicitly, SCA finds a projection matrix</p><formula xml:id="formula_35">B = [b 1 , b 2 , ..., b k ] that solves the constrained optimization argmax B∈R n×k Tr B ( (1−β) n KK + βP)B Tr B (δKLK + Q + K)B ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_36">P = P s 0 ns×nt 0 nt×ns 0 nt×nt , Q = Q s 0 ns×nt 0 nt×ns 0 nt×nt ,</formula><p>and β, δ &gt; 0 are the trade-off parameters controlling the total and between-class scatter, and domain scatter respectively. Observe that the above optimization is invariant to rescaling B → αB. Therefore, optimization (17) can be rewritten as</p><formula xml:id="formula_37">argmax B∈R n×k Tr B ( (1 − β) n KK + βP)B<label>(18)</label></formula><formula xml:id="formula_38">s.t. Tr B (δKLK + Q + K)B = 1,</formula><p>which results in Lagrangian</p><formula xml:id="formula_39">J(B) = Tr(B ( (1 − β) n KK + βP)B) − Tr((B (δKLK + Q + K)B − I k )Λ).<label>(19)</label></formula><p>To solve <ref type="bibr" target="#b16">(17)</ref>, set the first derivative ∂J(B) ∂B = 0, inducing the generalized eigenproblem</p><formula xml:id="formula_40">(1 − β) n KK + βP B * = δKLK + K + Q B * Λ,<label>(20)</label></formula><p>where Λ = diag(λ 1 , ..., λ k ) are the k leading eigenvalues and B = [b 1 , ..., b k ] contains the corresponding eigenvectors. <ref type="bibr" target="#b0">1</ref> Algorithm 1 provides a complete summary of SCA. </p><formula xml:id="formula_41">⊂ S u ). Construct a kernel matrix [K t ] ij = κ(x i , t j ), ∀x i ∈ S u , t j ∈ S t u .</formula><p>The extracted features are given by</p><formula xml:id="formula_42">Z t = K t B * Λ − 1 2 Output: • Optimal transformation matrix B * ∈ R n×k ; • Feature matrix Z t ∈ R nt×k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Relation to other methods</head><p>SCA is closely related to a number of feature learning and domain adaptation methods. To see this, let us observe Lagrangian <ref type="bibr" target="#b18">(19)</ref>. Setting the hyper-parameters β = δ = 0 and Q = 0 in (19) recovers KPCA. Setting β = 1 and δ = 0 recovers the Kernel Fisher Discriminant (KFD) method <ref type="bibr" target="#b66">[67]</ref>. KFD with linear kernel is equivalent to Fisher's linear discriminant, which is the basis of a domain adaptation method for object detection proposed in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Setting β = 0 and Q = 0 (that is, ignoring class separation) yields a new algorithm: unsupervised Scatter Component Analysis (uSCA), which is closely related to TCA. The difference between the two algorithms is that TCA constrains the total variance and regularizes the transform, whereas uSCA trades-off the total variance and constrains the transform (recall that W W should be small) motivated by Theorem 1. It turns out that uSCA consistently outperforms TCA in the case of domain adaptation, see Section 6.</p><p>Eliminating the term B KB from the denominator in (17) from uSCA yields TCA <ref type="bibr" target="#b44">[45]</ref>. The semi-supervised extension SSTCA of TCA differs markedly from SCA. Instead of incorporating withinand between-class scatter into the objective function, SSTCA incorporates a term derived from the Hilbert-Schmidt Independence Criterion that maximizes the dependence of the embedding on labels.</p><p>uSCA is essentially equivalent to unsupervised Domain Invariant Component Analysis (uDICA) in the case of two domains <ref type="bibr" target="#b18">[19]</ref>. However, as for SSTCA, supervised DICA incorporates label-information differently from SCA -via the notion of a central subspace. In particular, supervised DICA requires that all data points are labeled, and so it cannot be applied in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Computational Complexity</head><p>Here we analyze the computation complexity of the SCA algorithm. Suppose that we have m domains with n 1 , . . . , n m are the number of samples for each domain (m &gt; 2 covers the domain generalization case). Denote the total number of samples by n = n 1 + . . . + n m and the number of leading eigenvectors by k n. Computing the matrices K, L, P, and Q takes O(n 2 ) (Line 1 at Algorithm 1). Hence, the total complexity of SCA after solving the eigendecomposition problem (Line 2) takes O(kn 2 ), or quadratic in n. This complexity is similar to that of KPCA and Transfer Component Analysis <ref type="bibr" target="#b44">[45]</ref>.</p><p>In comparison to Transfer Joint Matching (TJM) <ref type="bibr" target="#b14">[15]</ref>, the prior state-of-the-art domain adaptation algorithm for object recognition, TJM uses an alternating eigendecomposition procedure in which T iterations are needed. Using our notation, the complexity of TJM is O(T kn 2 ), i.e., TJM is T times slower than SCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Hyper-parameter Settings</head><p>Before reporting the detailed evaluation results, it is important to explain how SCA hyper-parameters were tuned. The formulation of SCA described in Section 4 has four hyper-parameters: 1) the choice of the kernel, 2) the number of subspace bases k, 3) the betweenclass and total scatters trade-off β, and 4) the domain scatter δ,. Tuning all those hyper-parameters using a standard strategy, e.g., a grid-search, might be impractical due to two reasons. The first is of the computational complexity. The second, which is crucial, is that cross-validating a large number of hyper-parameters may worsen the generalization on the target domain, since labeled samples from the target domain are not available.</p><p>Our strategy to deal with the issue is to reduce the number of tunable hyper-parameters. For the kernel selection, we chose the RBF kernel exp(</p><formula xml:id="formula_43">− a−b 2 2 σ 2 ), ∀a, b ∈ X ,</formula><p>where the kernel bandwidth σ was set analytically to the median distance between samples in the aggregate domain following <ref type="bibr" target="#b65">[66]</ref>,</p><formula xml:id="formula_44">σ = median( a − b 2 2 ), ∀a, b ∈ S s ∪ S t .<label>(21)</label></formula><p>For domain adaptation, δ was fixed at 1. Thus, only two hyperparameters remain tunable: k and β. For domain generalization, β was set at 1, i.e., the total scatter was eliminated, and δ was allowed to be tuned -the number of tunable hyper-parameters remains unchanged. The configuration is based on an empirical observation that setting 0 &lt; β &lt; 1 is no better (if not worse) than β = 1 in terms of both the cross-validation and test performance for domain generalization cases. In all evaluations, we used 5-fold cross validation using source labeled data to find the optimal k and β. We found that this strategy is sufficient to produce good SCA models for both domain adaptation and generalization cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS OF ADAPTATION PERFORMANCE</head><p>We derive a bound for domain adapation that shows how the MMD controls generalization performance in the case of the squared loss (y, y ) = (y − y ) 2 . Despite the widespread use of the MMD for domain adaptation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b67">[68]</ref>, to the best of our knowledge, this is the first generalization bound. The main idea is to incorporate the MMD (that is, domain scatter) into the adaptation bound proven for the discrepancy distance <ref type="bibr" target="#b26">[27]</ref>. A generalization bound for domain generalization in terms of domain scatter is given in <ref type="bibr" target="#b18">[19]</ref>, see remark 1. Let Hyp := {h : X → Y} denote a hypothesis class of functions from X to Y where X is a compact set. Given a loss function defined over pairs of labels : Y × Y → R + and a distribution D over X , let L D h, h = E x∼D [ (h(x), h (x))] denote the expected loss for any two hypotheses h, h ∈ Hyp. We consider the case where the hypothesis set Hyp is a subset of an RKHS H.</p><p>We first introduce discrepancy distance, disc Hyp (P, Q), which measures the difference between two distributions P and Q.</p><p>Definition 7 (Discrepancy Distance <ref type="bibr" target="#b26">[27]</ref>). Let Hyp ⊂ {f : X → Y} be a set of functions mapping from X to Y. The discrepancy distance between two distributions P and Q over X is defined by</p><formula xml:id="formula_45">disc(P, Q) = sup h,h ∈Hyp L P (h, h ) − L Q (h, h )<label>(22)</label></formula><p>The discrepancy is symmetric and satisfies the triangle inequality, but it does not define a distance in general: ∃P = Q such that disc Hyp (P, Q) = 0 <ref type="bibr" target="#b26">[27]</ref>.</p><p>If we assume that we have a universal kernel <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, i.e. H = C(X ) as topological spaces, and the loss is the squared loss <ref type="bibr" target="#b70">[71]</ref> then the discrepancy is a metric. The most important example of a universal kernel is the Gaussian RBF kernel, which is the kernel used in the experiments below.</p><p>The main step of the proof is to find a relationship between domain scatter and the discrepancy distance. We are able to do so in the special case where the kernel is universal and the loss is the meansquare error. The main technical challenge is that the discrepancy distance is quadratic in the hypotheses (involving terms of the form h(x) 2 and h(x)h (x)) whereas the MMD is linear. We therefore need to bound the effects of the multiplication operator:</p><p>Definition 8 (Multiplication Operator). Let C(X ) be the space of continuous functions on the compact set X equipped with the supremum norm · ∞ . Given g ∈ C(X ), define the multiplication operator as the bounded linear operator M g : C(X ) → C(X ) given by</p><formula xml:id="formula_46">M g (h)(x) = g(x)h(x).</formula><p>Note that a general RKHS is not closed under the multiplication operator <ref type="bibr" target="#b71">[72]</ref>. However, since the kernel is universal, it follows that H is closed under multiplication since the space of continuous functions C(X ) is closed under multiplication. Moreover, we can define the sup-norm · ∞ on H using its identification with C(X ).</p><p>The following Lemma upper bounds the norm of multiplication operator, which will be useful to prove our main theorem.</p><formula xml:id="formula_47">Lemma 4. Given g, h ∈ H, where H is equipped with a universal kernel, it holds that M g (h) H = g · h H ≤ g ∞ · f H .</formula><p>Proof. Straightforward calculation. The Lemma requires a universal kernel since g · h H is only defined if g · h ∈ H.</p><p>We now show that the domain scatter of two distributions upperbounds the discrepancy distance.</p><p>Lemma 5 (Domain scatter bounds discrepancy). Let H be an RKHS with a universal kernel. Suppose that (y, y ) = (y − y ) 2 is the square loss, and consider the hypothesis set</p><formula xml:id="formula_48">Hyp = {f ∈ H : f H ≤ 1 and f ∞ ≤ r},</formula><p>where r &gt; 0 is a constant Let P and Q be two domains over X . Then the following inequality holds:</p><formula xml:id="formula_49">disc (P, Q) discrepancy ≤ 8r Ψ φ ({µ P , µ Q }) domain scatter .<label>(23)</label></formula><p>Proof. See Supplementary material.</p><p>Lemma 5 allows us to relate domain scatter to generalization bounds for domain adaptation proven in <ref type="bibr" target="#b26">[27]</ref>. Before stating the bounds, we introduce Rademacher complexity <ref type="bibr" target="#b58">[59]</ref>, which measures the degree to which a class of functions can fit random noise. This measure is the basis of bounding the empirical loss and expected loss. </p><formula xml:id="formula_50">R S (G) = E σ sup g∈G 1 n n i=1 σ i g(z i ) ,<label>(24)</label></formula><p>where σ = (σ 1 , . . . , σ n ) are Rademacher variables, with σ i s independent uniform random variables taking values in {−1, +1}.</p><p>The Rademacher complexity over all samples of size n is</p><formula xml:id="formula_51">R n (G) = E S R S (G) .<label>(25)</label></formula><p>The supplementary material discusses how to associate a family of functions to a loss function, and provides a useful Rademacher bound. We now have all the ingredients to derive domain adaptation bounds in terms of domain scatter.</p><p>Let f P and f Q be the true labeling functions on domain P and Q respectively, and h * P := argmin h∈Hyp L P (h, f P ) and h * Q := argmin h∈Hyp L Q (h, f Q ) be the minimizers. For a successful domain adaptation, we shall assume that L P (h * P , h * Q ) is small. The following theorem provides a domain adaptation bound in terms of scatter (recall that the MMD is a special case of scatter by Lemma 3).</p><p>Theorem 6 (Adaptation bounds with domain scatter). Let Hyp be a family of functions mapping from X to R, S P X = (x t 1 , . . . , x t ns ) ∼ P and S Q X = (x t 1 , . . . , x t nt ) ∼ Q be a source and target sample respectively. Let the rest of the assumptions be as in Lemma 5 and Theorem 8 in the supplementary material. For any hypothesis h ∈ Hyp, with probability at least 1 − δ, the following adaptation bound holds:</p><formula xml:id="formula_52">regret on target domain L Q (h, f Q ) − L Q (h * Q , f Q ) ≤ empirical loss LP(h, h * P ) + Rademacher complexity 2qR S P X (Hyp) + 3B log 2 δ 2n t O(1/ √ sample size) + 8r Ψ φ ({µ Q , µ P }) domain scatter + L P (h * P , h * Q ) deviation of optimal solns<label>(26)</label></formula><p>Proof. Fix h ∈ Hyp. Since the square loss is symmetric and obeys the triangle inequality, Theorem 8 in <ref type="bibr" target="#b26">[27]</ref> (see supplementary material) implies that</p><formula xml:id="formula_53">L Q (h, f Q ) − L Q (h * Q , f Q ) ≤ L P (h, h * P ) + disc (Q, P) +L P (h * P , h * Q ).<label>(27)</label></formula><p>The result then follows by Lemma 5 combined with the Rademacher bound in the supplementary material.</p><p>It is instructive to compare Theorem 6 above with Theorem 9 in <ref type="bibr" target="#b26">[27]</ref>, which is the analog if we expand disc l (Q, P) in <ref type="formula" target="#formula_15">(27)</ref> with its empirical measure. It is also straightforward to rewrite the bound in term of the empirical scatter Ψ φ ({µP, µQ}) by applying Theorem 1. The significance of Theorem 6 is twofold. First, it highlights that the scatter Ψ φ ({µ P , µ Q }) controls the generalization performance in domain adaptation. Second, the bound shows a direct connection between scatter (also MMD) and the domain adaptation theory proposed in <ref type="bibr" target="#b26">[27]</ref>. Note that the bound might not be useful for practical purposes, since it is loose and pessimistic as they hold for all hypotheses and all possible data distributions.</p><p>Remark 1 (The role of scatter in domain generalization). Theorem 5 of <ref type="bibr" target="#b18">[19]</ref> shows that the domain scatter (or, alternatively, the distributional variance) is one of the key terms arising in a generalization bound in the setting of domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT I : DOMAIN ADAPTATION</head><p>The first set of experiments evaluated the domain adaptation performance of SCA on synthetic data and real-world object recognition tasks. The synthetic data was designed to understand the behavior of the learned features compared to other algorithms, whereas the real-world images were utilized to verify the performance of SCA.</p><p>The experiments are divided into two parts. Section 6.1 visualizes performance on synthetic data. Section 6.2 evaluates performance on a range of cross-domain object recognition tasks with a standard yet realistic hyper-parameter tuning. Some additional results with a tuning protocol established in the literature are also reported in the supplementary material for completeness. ) was used for all algorithms. All tunable hyper-parameters were selected according to 1-nearest neighbor's test accuracy. We compare features extracted from Kernel Principal Component Analysis (KPCA), Semi-Supervised Transfer Component Analysis (SSTCA) <ref type="bibr" target="#b44">[45]</ref>, Transfer Joint Matching (TJM) <ref type="bibr" target="#b14">[15]</ref>, and SCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synthetic data</head><p>The top row of <ref type="figure" target="#fig_2">Figure 1</ref> illustrates how the features extracted from the MMD-based algorithms (SSTCA, TJM, and SCA) reduce the domain mismatch. Red and blue colors indicate the source and target domains, respectively. Good features for domain adaptation should have a configuration of which the red and blue colors are mixed. This effect can be seen in features extracted from SSTCA, TJM, and SCA, which indicates that the domain mismatch is successfully reduced in the feature space. In classification, domain adaptive features should also have a certain level of class separability. The bottom row highlights a major difference between SCA and the other algorithms in terms of the class separability: the SCA features are more clustered with respect to the classes, with more prominent gaps among clusters. This suggests that it would be easier for a simple function to correctly classify SCA features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Real world object recognition</head><p>We summarize the complete domain adaptation results over a range of cross-domain object recognition tasks. Several real-world image datasets were utilized such as handwritten digits (MNIST <ref type="bibr" target="#b72">[73]</ref> and USPS <ref type="bibr" target="#b73">[74]</ref>) and general objects (MSRC <ref type="bibr" target="#b74">[75]</ref>, VOC2007 <ref type="bibr" target="#b5">[6]</ref>, Caltech-256 <ref type="bibr" target="#b75">[76]</ref>, Office <ref type="bibr" target="#b15">[16]</ref>). Three cross-domain pairs were constructed from these datasets: USPS+MNIST, MSRC+VOC2007, and Of-fice+Caltech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Data Setup</head><p>The USPS+MNIST pair consists of raw images subsampled from datasets of handwritten digits. MNIST contains 60,000 training images and 10,000 test images of size 28 × 28. USPS has 7,291 training images and 2,007 test images of size 16 × 16 <ref type="bibr" target="#b72">[73]</ref> . The pair was constructed by randomly sampling 1,800 images from USPS and 2,000 images from MNIST. Images were uniformly rescaled to size 16 × 16 and encoded into feature vectors representing the grayscale pixel values. Two SOURCE → TARGET classification tasks were constructed: USPS → MNIST and MNIST → USPS.</p><p>The MSRC+VOC2007 pair consist of 240-dimensional images that share 6 object categories: "aeroplane", "bicycle","bird", "car", "cow", and "sheep" taken from the MSRC and VOC2007 <ref type="bibr" target="#b5">[6]</ref> datasets. The pair was constructed by selecting all 1,269 images in MSRC and 1,530 images in VOC2007. As in <ref type="bibr" target="#b22">[23]</ref>, features were extracted from the raw pixels as follows. First, images were uniformly rescaled to be 256 pixels in length. Second, 128-dimensional dense SIFT (DSIFT) features were extracted using the VLFeat open source package <ref type="bibr" target="#b76">[77]</ref>. Finally, a 240-dimensional codebook was created using K-means clustering to obtain the codewords.</p><p>The Office+Caltech consists of 2,533 images of ten categories (8 to 151 images per category per domain), that forms four domains: (A) AMAZON, (D) DSLR, (W ) WEBCAM, and (C) CALTECH. AMAZON images were acquired in a controlled environment with studio lighting. DSLR consists of high resolution images captured by a digital SLR camera in a home environment under natural lighting. WEBCAM images were acquired in a similar environment to DSLR, but with a low-resolution webcam. Finally, CALTECH images were collected from Google Images <ref type="bibr" target="#b75">[76]</ref>. Taking all possible source-target combinations yields 12 cross-domain datasets denoted by A → W, A → D, A → C, . . . , C → D. We used two types of extracted features from these datasets that are publicly available: SURF-BoW 2 <ref type="bibr" target="#b15">[16]</ref> and DeCAF 6 3 <ref type="bibr" target="#b52">[53]</ref>. SURF-BoW features were extracted using SURF <ref type="bibr" target="#b77">[78]</ref> and quantized into 800-bin histograms with codebooks computed by K-means on a subset of AMAZON images. The final histograms were standardized to have zero mean and unit standard deviation in each dimension. Deep Convolutional Activation Features (DeCAF) were constructed by <ref type="bibr" target="#b52">[53]</ref> using the deep convolutional neural network architecture in <ref type="bibr" target="#b0">[1]</ref>. The model inputs are the mean-centered raw RGB pixel values that are forward propagated through 5 convolutional layers and 3 fully-connected layers. We used the outputs from the 6th layer as the features, leading to 4, 096 dimensional DeCAF 6 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Baselines and Protocol</head><p>We compared the classification performance of the following algorithms: 1) a classifier on raw features (Raw), 2) KPCA, 3) Transfer Component Analysis (TCA) <ref type="bibr" target="#b44">[45]</ref>, 4) SSTCA, 5) Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b28">[29]</ref>, 6) Transfer Sparse Coding (TSC) <ref type="bibr" target="#b22">[23]</ref>, 7) Subspace Alignment (SA) <ref type="bibr" target="#b30">[31]</ref>, 8) TJM <ref type="bibr">[15], 9)</ref> unsupervised Scatter Component Analysis, and 10) SCA. For a realistic setting, the tunable hyper-parameters were selected via 5-fold cross validation, according to labels from source domains only.</p><p>The above feature learning algorithms were evaluated on three different classifiers: 1) 1-nearest neighbor (1NN), 2) support vector machines with linear kernel (L-SVM) <ref type="bibr" target="#b78">[79]</ref>, and 3) domain adaptation machines (DAM) <ref type="bibr" target="#b35">[36]</ref>. 1NN and L-SVM are the standard off-theshelf classifiers, while DAM is specifically designed for domain adaptation. DAM is an extension of SVM that incorporates a domaindependent regularization to encourage the target classifier sharing similar prediction values with the source classifiers. We also utilize the linear kernel for DAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Classification Accuracy with 1-Nearest Neighbor</head><p>We first report the classification accuracy of the competing algorithms according to 1NN classifier. The goal is to clearly highlight the adaptation impact induced purely from the representations, since 1NN basically just measures the distance between features. <ref type="table" target="#tab_2">Table 1</ref> summarizes the classification accuracy on the USPS+MNIST and MSRC+VOC2007 pairs. We can see that SCA is the best model on average, while the prior state-of-the-art TJM is the second best. Other domain adaptation algorithms (TCA, SSTCA, GFK, and TSC) do not perform well, even worse than one without adaptation strategy: KPCA. Surprisingly, the unsupervised version of our algorithm, uSCA, has the highest accuracy on two MSRC+VOC2007 cases. This indicates that the label incorporation does not help improve domain adaptation on the MSRC+VOC2007, while it clearly does on the USPS+MNIST. Furthermore, SCA and uSCA always provide improvement over the raw features, while other algorithms, including TJM, fail to do so in MNIST → USPS case.    Surprisingly, SSTCA, which also incorporates label information during training, does not perform competitively. The first possible explanation is that SCA directly improves class separability, whereas SSTCA maximizes a dependence criterion that relates indirectly to separability. The second is that SSTCA incorporates the manifold regularization that requires a similarity graph, i.e., affinity matrix. This graph is parameterized by k-nearest neighbor with l 2 distance, which might not be suitable in these cases.</p><p>The results on the Office+Caltech pair are summarized in <ref type="table" target="#tab_3">Table 2</ref> (SURF-BoW) and <ref type="table" target="#tab_4">Table 3</ref> (DeCAF 6 ). In general, DeCAF 6 induces stronger discriminative performance than SURF-BoW features, since DeCAF 6 with 1NN only has already provided significantly better performance. SCA consistently has the best average performance on both features, slightly better than the prior state-of-the-art, TJM. On SURF-BoW, SCA is the best model on 3 out of 12 cases and the second best on other 4 cases. The trend on DeCAF 6 is better -SCA has the best performance on 5 out of 12 cases, while comes second on other 6 cases. Although the closest competitor, TJM, has the highest number of individual best cross-domain performance, it requires higher computational complexity than SCA, see Section 6.2.5 below.</p><p>Recall that the algorithms' hyper-parameters used to produce all the above results were tuned using labels from the source domain only. This is the only valid tuning protocol for the unsupervised domain adaptation setting. Nevertheless, some of the best results established in the literature were obtained using the hyper-parameter tuning on target labels. For completeness, we also report the results under this tuning-on-target protocol in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Classification Accuracy with L-SVM and DAM</head><p>Next we report the results with L-SVM and DAM as the base classifiers for the feature learning algorithms. For succinctness, we compare the performance of five algorithms: KPCA, SSTCA, SA, TJM, and SCA, presented in <ref type="figure" target="#fig_4">Figure 2</ref>. The bar chart shows the average accuracies relative to the performance on Raw features (indicated by line y = 0 in red); the numbers alongside the bars indicate the absolute accuracies. <ref type="table" target="#tab_5">Table 4</ref> summarizes the absolute accuracies on Raw features. In general, all feature learning algorithms rectify the domain adaptation performances over Raw features, except in two cases: SA on the Office+Caltech with SURF-BoW features and KPCA on the Office+Caltech with DeCAF 6 features. Considering the absolute accuracies, we find that the best average performances on each dataset are still provided by SCA, a similar trend as in the 1NN results. This confirms the effectiveness of SCA regardless of the classifier choice, at least, among 1NN, L-SVM, and DAM.</p><p>Let us now compare the absolute average performance of L-SVM and DAM with the performance of 1NN. L-SVM and DAM evidently provide a considerable performance improvement only on the Office+Caltech dataset. Their performances on less powerful features, that is, the features extracted from the MNIST+USPS and MSRC+VOC, are even worse than 1NN. A useful lesson from this finding is that one should make use better features to take the real benefit of more advanced classifiers in the context of domain adaptation.</p><p>Finally, we seek to investigate the performance impact induced by DAM in comparison to L-SVM. DAM is expected to provide a better performance, since it is specifically designed for domain adaptation. From <ref type="table" target="#tab_5">Table 4</ref> we can see that DAM outperforms L-SVM when operating on Raw features. Surprisingly, that is not always the case when a feature learning algorithm is applied. Moreover, L-SVM always produces higher performance gain relative to Raw features.than DAM. This could be attributed to overfitting considering that DAM has more hyper-parameters than L-SVM. That is, combining DAM with a feature learning algorithm complicates the whole processsrecall that the hyper-parameter selection is based on a validation on source data. <ref type="table" target="#tab_6">Table 5</ref> compares the average runtime performance of SCA with some other algorithms: KPCA, TCA, SA, TSC, and TJM, on the MNIST+USPS, MSRC+VOC, and Office+Caltech (with DeCAF 6 ). All algorithms were executed with MATLAB R2014b by a machine with Intel Core i5-240 CPU, Arch Linux 64-bit OS, and 4GB RAM. Note that KPCA, TCA, and SCA basically utilizes the same optimization procedure: a single iteration of the eigenvalue decomposition. TJM requires several iterations of the eigenvalue decomposition with an additional gradient update in each iteration, while TSC solves the dictionary learning and sparse coding with an iterative procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.5">Runtime Performance</head><p>In general, SCA is significantly faster than TJM, the closest competitor in accuracy, and TSC. Specifically, SCA is 3 to 6× faster than TJM, and &gt; 50× faster than TSC. SCA runs at about the same speed as TCA on the MNIST+USPS and MSRC+VOC, and the same speed as SA on the Office+Caltech. In several other cases, SCA performs slower than KPCA, TCA, and SA. Note that KPCA, TCA, and SA are less competitive in accuracy compared to SCA and TJM so that the runtime gap is less interesting to be concerned about. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENT II : DOMAIN GENERALIZATION</head><p>In the second set of experiments, we show that our proposed algorithm is also applicable for domain generalization and achieves state-of-the-art performance on some object and action recognition datasets. We evaluated our algorithms on three cross-domain datasets: the VLCS, Office+Caltech, and IXMAS <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data setup</head><p>The first cross-domain dataset, which we refer to as the VLCS consists of images from PASCAL VOC2007 (V) <ref type="bibr" target="#b5">[6]</ref>, LabelMe (L) <ref type="bibr" target="#b80">[81]</ref>, Caltech-101 (C) <ref type="bibr" target="#b75">[76]</ref>, and SUN09 (S) <ref type="bibr" target="#b81">[82]</ref> datasets, each of which represents one domain. These datasets share five object categories: bird, car, chair, dog, and person. Each domain in the VLCS dataset was divided into a training set (70%) and a test set (30%) by random selection from the overall dataset. The detailed training-test configuration for each domain is summarized in the supplementary material. We employed the DeCAF 6 features <ref type="bibr" target="#b52">[53]</ref> with dimensionality of 4,096 as inputs to the algorithms. These features are publicly available. <ref type="bibr" target="#b3">4</ref> The second cross-domain dataset is the Office+Caltech dataset, see Section 6.2.1 for a detailed explanation about this dataset. We also used DeCAF 6 features extracted from this dataset. <ref type="bibr" target="#b4">5</ref> The third dataset is the IXMAS dataset <ref type="bibr" target="#b79">[80]</ref> that contains videos of the 11 actions, recorded with different actors, cameras, and viewpoints. This dataset has been used as a benchmark for evaluating human action recognition models. To simulate the domain generalization problem, we followed the setup proposed in <ref type="bibr" target="#b21">[22]</ref>: only frames from five actions were utilized (check watch, cross arms, scratch head, sit down, and get up) with domains represented represented by camera viewpoints (Cam 0, Cam 1, ..., Cam 4). The task is to learn actions from particular camera viewpoints and classify actions on unseen viewpoints. In the experiment, we used the dense trajectories features <ref type="bibr" target="#b82">[83]</ref> extracted from the raw frames and applied K-means clustering to build a codebook with 1,000 clusters for each of the five descriptors: trajectory, HOG, HOF, MBHx, MBHy. The bag-ofwords features were then concatenated forming a 5,000 dimensional features for each frame.  <ref type="table" target="#tab_5">Table 4</ref> for the exact numbers.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Baselines and Protocol</head><p>We compared our algorithms with the following baselines: : a structural metric learning-based algorithm that aims to learn a less biased distance metric for classification tasks. The initial tuning proposal for this method was using a set of weaklylabeled data retrieved from querying class labels to search engine. However, here we tuned the hyper-parameters using the same kfold cross-validation strategy as others for a fair comparison. • DICA <ref type="bibr" target="#b18">[19]</ref>: a kernel feature extraction method for domain generalization. DICA has three tunable hyper-parameters. • LRE-SVM <ref type="bibr" target="#b21">[22]</ref>: a non-linear exemplar-SVMs model with a nuclear norm regularization to impose a low-rank likelihood matrix. LRE-SVM has four hyper-parameters (λ 1 , λ 2 , C 1 , C 2 ) that require tuning. Undo-Bias, UML, and LRE-SVM are the prior state-of-the-art domain generalization algorithms for object recognition tasks. Note that Undo-Bias, DICA, and UML cannot be applied to the domain adaptation setting -they do not all</p><p>We used 1-nearest neighbor (1NN) as the base classifier for all feature learning-based algorithms: KPCA, DICA, uSCA/uDICA, and SCA. The tunable hyper-parameters were selected according to labels from source domains. For all kernel-based methods, the kernel function is the RBF kernel, k(a, b) = exp(− a−b 2 σ 2 ), with a kernel bandwidth σ computed by median heuristic. Note that the unsupervised DICA (uDICA) is almost identical to uSCA in this case. The only difference is that uSCA has a control parameter δ &gt; 0 for the domain scatter/distributional variance term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results on the VLCS Dataset</head><p>On this dataset, we first conducted the standard training-test evaluation using 1-nearest neighbor (1NN), i.e., learning the model on a training set from one domain and testing it on a test set from another domain, to check the groundtruth performance and also to identify the existence of the dataset bias. The groundtruth evaluation results are summarized in <ref type="table" target="#tab_7">Table 6</ref>. In general, the dataset bias indeed exists despite the use of the state-of-the-art deep convolution neural network features DeCAF 6 . For example, the average crossdomain performance, i.e., "Mean others", is 56.63%, which is 25% drop from the corresponding in-domain performance: 75.96%. In particular, Caltech-101 has the highest bias, while LabelMe is the least biased dataset indicated by the largest and smallest performance drop, respectively.</p><p>We then evaluated the domain generalization performance over seven cross-domain recognition tasks. The complete results are summarized in <ref type="table" target="#tab_8">Table 7</ref>. We can see that SCA is the best model on 5 out of 7 tasks, outperforms the prior state-of-the-art, LRE-SVM. It almost always has better performance than the 'raw' baseline, except when Caltech-101 is the target domain. On average, SCA is about 2% better than its closest competitor on this dataset, Undo-Bias. The VLCS cross-domain recognition is a hard task in general, since the best model (SCA) only provides &lt; 4% average improvement over the raw baseline. Furthermore, three algorithms, two of which are the domain generalization-based methods (uSCA, DICA), cannot achieve even better performance than the raw baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results on the Office+Caltech Dataset</head><p>We evaluated our algorithms on several cross-domain cases constructed from the Office+Caltech dataset. The detailed evaluation results on four cases with DeCAF 6 are reported in <ref type="table" target="#tab_9">Table 8</ref>. We do not report other cross-domain cases that are possibly constructed from this dataset, such as A, D, C → W and A, W, C → D, since 1NN on Raw features has already provided high accuracies (&gt; 95%).</p><p>The closest competitor to SCA is LRE-SVM. Although LRE-SVM performs best on average, SCA has the best performance on three out of four cross-domain cases and comes second on average. The only case when SCA underperforms LRE-SVM is that of D, W → A, C. Note that the LRE-SVM algorithm is more complex than SCA both in the optimization procedure and in the number of tunable hyper-parameters.</p><p>However, the unsupervised version of our algorithm, uSCA, which is the same as uDICA <ref type="bibr" target="#b18">[19]</ref> in the domain generalization case, cannot compete with the state-of-the-art models. It is only slightly better than KPCA on average. This suggests that incorporating labeled information from source domains during feature learning does improve domain generalization on the Office+Caltech cases. <ref type="table" target="#tab_10">Table 9</ref> summarizes the classification accuracies on the IXMAS dataset over three cross-domain cases. We can see that the standard baselines (Raw, KPCA) cannot match other algorithms with domain generalization strategies. In this dataset, SCA has the best performance on two out of three cases and on average. In particular, SCA is significantly better than others on Cam 2,3,4 → Cam 0,1 case. LRE-SVM remains the closest competitor of SCA -it has the second best average performance with one best cross-domain case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Results on the IXMAS dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Runtime Performance</head><p>Next we report the average (training) runtime performance over all cross-domain recognition tasks in each dataset. All algorithms were executed using the same software and machine as described in Section 6.2.5. From <ref type="table" target="#tab_2">Table 10</ref>, we can see that the runtime of SCA is on par with KPCA and DICA, which is expected since they utilize the same optimization procedure: a single run with a generalized eigenvalue decomposition.</p><p>SCA is significantly faster than some prior state-of-the-art domain generalization methods (Undo-Bias, UML, and LRE-SVM). For example, on the VLCS dataset, Undo-Bias, UML, and LRE-SVM require ∼ 30 minutes, while SCA only needs ∼ 5 minutes average training time. An analogous trend can also be seen in the case of Office+Caltech and IXMAS datasets. This outcome indicates that SCA is better suited for domain generalization tasks than the competing algorithms if a training stage in real time is required. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>The scatter-based objective function is a straightforward way to encode the relevant structure of the domain adaptation and domain generalization problems. SCA uses variances between subsets of the data to construct a linear transformation that dampens unimportant distinctions (within labels and between domains) and amplifies useful distinctions (between labels and overall variability). Extensive experiments on several cross-domain image datasets show that SCA is much faster than competing algorithms and provides the state-of-the-art performance on both domain adaptation and domain generalization. Our theoretical analysis shows that scatter with two input domains, i.e., domain scatter, provides generalization bounds for domain adaptation <ref type="bibr" target="#b26">[27]</ref>. In the setting of domain generalization, recall remark 1, prior work has shown that the distributional variance (which is a special case of scatter) arises as one of the terms controlling generalization performance <ref type="bibr" target="#b18">[19]</ref>. Scatter is thus a unifying quantity that controls generalization performance in domain adaptation and generalization.</p><p>SCA is a natural extension of Kernel PCA, Kernel Fisher Discriminant and TCA. In contrast, many domain adaptation methods use objective functions that combine the total variance and MMD with quantities that are fundamentally different in kind such as the graph Laplacian <ref type="bibr" target="#b22">[23]</ref>, sparsity constraints <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, the Hilbert-Schmidt independence criterion <ref type="bibr" target="#b44">[45]</ref> or the central subspace <ref type="bibr" target="#b18">[19]</ref>. SCA can easily be extended to semi-supervised domain adaptation, by incorporating target labels into the class scatters. Finally, we remark that it should be possible to speed up SCA for large-scale problems using random features <ref type="bibr" target="#b83">[84]</ref>.</p><p>In general, dataset bias remains far from solved. Existing algorithms perform satisfactorily (≥ 80% accuracy) only in several crossdomain tasks, even when using powerful feature extraction methods such as DeCAF 6 (for images) and dense trajectory-based features (for videos). Using less powerful features (raw pixels or SURF-BoW) is clearly unsatisfactory. Thus, it is crucial to develop more fundamental feature learning algorithms that significantly reduce dataset bias in a wide range of situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 (</head><label>3</label><figDesc>Reproducing Kernel Hilbert Space). Let X be an arbitrary set and H ⊂ {f : X → R} a Hilbert space of functions on X . Define the evaluation functional L x : H → R by L x [h] := h(x), ∀h ∈ H. Then H is a reproducing kernel Hilbert space (RKHS) if the functional L x is always bounded: i.e. for all x ∈ X there exists an λ &gt; 0 such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 9 (</head><label>9</label><figDesc>Rademacher Complexity). Let G be a family of functions mapping from X × Y to [a, b] and S = (z 1 , ..., z n ) ∈ X × Y be a fixed sample of size n. The empirical Rademacher complexity of G with respect to the sample S iŝ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1</head><label>1</label><figDesc>depicts synthetic data that consists of two dimensional data points under three classes with six clusters. The data points in each cluster were generated from a Gaussian distribution x c i ∼ N (µ c , σ c ), where µ c and σ c is the mean and standard deviation of the cth cluster. The RBF kernel k(a, b) = exp(−</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Visualization. Projections of synthetic data onto the first two leading eigenvectors. Numbers in brackets indicate the classification accuracy on the target using 1-nearest neighbor (1NN). The top and bottom rows show the domains and classes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>L-SVM and DAM average performance accuracy (%) relative to the performance on Raw features. The numbers on the top or bottom of the bars show the absolute accuracy. The red line indicates the Raw baseline performance, see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>X samples drawn from both domains. The task of domain adaptation is to learn a good labeling function f P t : X → Y given S s and S t u as the training examples.</figDesc><table /><note>Definition 2 (Domain Generalization). Let ∆ = {P 1 , . . . , P m } be a set of m source domains and P t / ∈ ∆ be a target domain. Denote by S d = {x d i , y d i } n d i=1 ∼ P d samples drawn from m source domains. The task of domain generalization is to learn a labeling function f Pt : X → Y given S d , ∀d = 1, ..., m as the training examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Scatter Component Analysis Input: • Sets of training datapoints S d u = {x d i } n d i=1 , ∀d = 1, . . . , m and their corresponding matrices X = X 1 ; . . . ; X m ∈ R n×p , where X d = [x d 1 , . . . , x d n d ] ; • Training labels y l = [y 1 1 , . . . , y 1 n1 , . . . , y q 1 , . . . , y q nq ] ∈ R n ; • Hyper-parameters β, δ &gt; 0; kernel bandwidth σ; • Number of subspace bases k;1: Construct kernel matrix K from X, matrices L, P and Q based on (8),<ref type="bibr" target="#b12">(13)</ref>,<ref type="bibr" target="#b13">(14)</ref>, and (18); 2: Apply the centering operation K ← K−1 n K−K1 n +1 n K1 n , where n = m d=1 n d and [1 n ] ij := 1 n ; 3: Obtain the transformation B * and its corresponding eigenvalues Λ by solving the generalized eigendecomposition problem in Eq.<ref type="bibr" target="#b19">(20)</ref> and selecting the k leading eigenvectors; 4: Target feature extraction: Let S u =</figDesc><table><row><cell>m d=1 S d u be the to-u be a target sample (for domain tal training sample and S t adaptation, S t u</cell></row></table><note>1. In the implementation, a numerically more stable variant is obtained by using (20) using δKLK + K + Q + I, where &gt; 0 is a fixed small constant.Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Accuracy % on the USPS+MNIST and MSRC+VOC2007 datasets. Bold-red and bold-black indicate the best and second best performance.</figDesc><table><row><cell>Dataset</cell><cell>Raw</cell><cell>KPCA</cell><cell>TCA</cell><cell>SSTCA</cell><cell>GFK</cell><cell>TSC</cell><cell>SA</cell><cell>TJM</cell><cell>uSCA</cell><cell>SCA</cell></row><row><cell>USPS → MNIST</cell><cell>34.80</cell><cell>42.55</cell><cell>41.75</cell><cell>40.07</cell><cell cols="4">43.50 40.95 41.50 52.65</cell><cell>44.86</cell><cell>48.00</cell></row><row><cell>MNIST → USPS</cell><cell>63.06</cell><cell>62.61</cell><cell>59.44</cell><cell>60.13</cell><cell cols="3">61.22 59.56 63.95</cell><cell>62.00</cell><cell>64.67</cell><cell>65.11</cell></row><row><cell>MSRC → VOC</cell><cell>28.63</cell><cell>29.35</cell><cell>31.70</cell><cell>30.95</cell><cell cols="3">30.63 28.80 30.90</cell><cell>32.48</cell><cell>33.14</cell><cell>32.75</cell></row><row><cell>VOC → MSRC</cell><cell>41.06</cell><cell>47.12</cell><cell>45.78</cell><cell>46.06</cell><cell cols="3">44.47 40.58 46.88</cell><cell>46.34</cell><cell>49.80</cell><cell>48.94</cell></row><row><cell>Avg.</cell><cell>41.89</cell><cell>45.51</cell><cell>44.67</cell><cell>44.30</cell><cell cols="4">44.96 42.47 45.81 48.37</cell><cell>48.12</cell><cell>48.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Accuracy % on the Office+Caltech images with SURF-BoW features. 1NN was used as the base classifier.</figDesc><table><row><cell>Dataset</cell><cell>Raw</cell><cell>KPCA</cell><cell>TCA</cell><cell>SSTCA</cell><cell>GFK</cell><cell>TSC</cell><cell>SA</cell><cell>TJM</cell><cell>uSCA</cell><cell>SCA</cell></row><row><cell>A → W</cell><cell>29.83</cell><cell>31.86</cell><cell>25.08</cell><cell>28.15</cell><cell cols="3">39.32 32.95 37.63</cell><cell>33.56</cell><cell>32.88</cell><cell>33.90</cell></row><row><cell>A → D</cell><cell>25.48</cell><cell>33.76</cell><cell>31.21</cell><cell>32.25</cell><cell>28.66</cell><cell cols="2">33.14 34.49</cell><cell>35.67</cell><cell>33.85</cell><cell>34.21</cell></row><row><cell>A → C</cell><cell>26.00</cell><cell>37.04</cell><cell>33.93</cell><cell>32.48</cell><cell cols="2">39.27 34.42</cell><cell>37.80</cell><cell>37.58</cell><cell>37.13</cell><cell>38.29</cell></row><row><cell>W → A</cell><cell>22.96</cell><cell>29.44</cell><cell>22.86</cell><cell>25.56</cell><cell cols="3">34.03 27.91 34.34</cell><cell>29.85</cell><cell>30.41</cell><cell>30.48</cell></row><row><cell>W → D</cell><cell cols="2">59.24 89.81</cell><cell>65.61</cell><cell>80.81</cell><cell>84.71</cell><cell>83.27</cell><cell>80.89</cell><cell>86.62</cell><cell>89.81</cell><cell>92.36</cell></row><row><cell>W → C</cell><cell>19.86</cell><cell>27.60</cell><cell>23.06</cell><cell>25.39</cell><cell>28.76</cell><cell>28.62</cell><cell>28.76</cell><cell>29.72</cell><cell>28.52</cell><cell>30.63</cell></row><row><cell>D → A</cell><cell>28.50</cell><cell>31.00</cell><cell>30.17</cell><cell>29.16</cell><cell>32.25</cell><cell cols="2">31.00 34.24</cell><cell>30.06</cell><cell>31.00</cell><cell>33.72</cell></row><row><cell>D → W</cell><cell>63.39</cell><cell>84.41</cell><cell>64.75</cell><cell>78.90</cell><cell>80.34</cell><cell>85.13</cell><cell>82.37</cell><cell>90.85</cell><cell>84.41</cell><cell>88.81</cell></row><row><cell>D → C</cell><cell>26.27</cell><cell>27.78</cell><cell>28.05</cell><cell>28.05</cell><cell>29.12</cell><cell>28.59</cell><cell>31.17</cell><cell>30.72</cell><cell>27.78</cell><cell>32.32</cell></row><row><cell>C → A</cell><cell>23.70</cell><cell>40.40</cell><cell>41.02</cell><cell>40.67</cell><cell>41.75</cell><cell>39.21</cell><cell>41.34</cell><cell>45.41</cell><cell>40.40</cell><cell>43.74</cell></row><row><cell>C → W</cell><cell>25.76</cell><cell>31.53</cell><cell>23.39</cell><cell>26.62</cell><cell cols="2">36.61 29.97</cell><cell>32.20</cell><cell>33.90</cell><cell>29.15</cell><cell>33.56</cell></row><row><cell>C → D</cell><cell>25.48</cell><cell>40.76</cell><cell>34.49</cell><cell>36.45</cell><cell>40.13</cell><cell cols="2">35.37 42.86</cell><cell>40.31</cell><cell>42.04</cell><cell>39.49</cell></row><row><cell>Avg.</cell><cell>31.37</cell><cell>42.12</cell><cell>35.29</cell><cell>38.17</cell><cell>42.91</cell><cell>40.80</cell><cell>43.21</cell><cell>43.67</cell><cell>42.28</cell><cell>44.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Accuracy % on the Office+Caltech images with DeCAF 6 features. 1NN was used as the base classifier.</figDesc><table><row><cell>Dataset</cell><cell>Raw</cell><cell>KPCA</cell><cell>TCA</cell><cell>SSTCA</cell><cell>GFK</cell><cell>TSC</cell><cell>SA</cell><cell>TJM</cell><cell>uSCA</cell><cell>SCA</cell></row><row><cell>A → W</cell><cell>57.29</cell><cell>67.80</cell><cell>71.86</cell><cell>70.73</cell><cell>68.47</cell><cell>68.54</cell><cell>68.81</cell><cell>72.54</cell><cell cols="2">73.22 75.93</cell></row><row><cell>A → D</cell><cell>64.97</cell><cell>80.89</cell><cell>78.34</cell><cell>80.13</cell><cell>79.62</cell><cell>79.33</cell><cell>78.34</cell><cell>85.99</cell><cell>79.43</cell><cell>85.35</cell></row><row><cell>A → C</cell><cell>70.35</cell><cell>74.53</cell><cell>74.18</cell><cell>72.25</cell><cell>76.85</cell><cell cols="2">74.91 80.05</cell><cell>78.45</cell><cell>74.62</cell><cell>78.81</cell></row><row><cell>W → A</cell><cell>62.53</cell><cell>69.42</cell><cell>79.96</cell><cell>75.65</cell><cell>75.26</cell><cell>73.65</cell><cell>77.77</cell><cell>82.46</cell><cell>79.52</cell><cell>86.12</cell></row><row><cell>W → D</cell><cell>98.73</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>W → C</cell><cell>60.37</cell><cell>65.72</cell><cell>72.57</cell><cell>69.30</cell><cell>74.80</cell><cell cols="3">73.27 74.89 79.61</cell><cell>72.81</cell><cell>74.80</cell></row><row><cell>D → A</cell><cell>62.73</cell><cell>80.06</cell><cell>88.20</cell><cell>87.30</cell><cell>85.80</cell><cell>84.26</cell><cell>82.67</cell><cell>91.34</cell><cell>88.71</cell><cell>89.98</cell></row><row><cell>D → W</cell><cell>89.15</cell><cell>98.31</cell><cell>97.29</cell><cell>97.56</cell><cell>98.64</cell><cell cols="2">97.29 99.32</cell><cell>98.31</cell><cell>98.31</cell><cell>98.64</cell></row><row><cell>D → C</cell><cell>52.09</cell><cell>75.16</cell><cell>73.46</cell><cell>74.45</cell><cell>74.09</cell><cell>76.11</cell><cell>75.69</cell><cell>80.77</cell><cell>74.98</cell><cell>78.09</cell></row><row><cell>C → A</cell><cell>85.70</cell><cell>88.73</cell><cell>89.25</cell><cell>88.90</cell><cell>88.41</cell><cell cols="3">88.52 89.46 89.67</cell><cell>88.52</cell><cell>89.46</cell></row><row><cell>C → W</cell><cell>66.10</cell><cell>77.29</cell><cell>80.00</cell><cell>81.22</cell><cell>80.68</cell><cell>80.15</cell><cell>75.93</cell><cell>80.68</cell><cell>76.27</cell><cell>85.42</cell></row><row><cell>C → D</cell><cell>74.52</cell><cell>86.62</cell><cell>83.44</cell><cell>84.56</cell><cell>84.56</cell><cell>86.62</cell><cell>83.44</cell><cell>87.26</cell><cell>86.62</cell><cell>87.90</cell></row><row><cell>Avg.</cell><cell>70.38</cell><cell>80.38</cell><cell>82.38</cell><cell>81.84</cell><cell>82.44</cell><cell>81.72</cell><cell>82.20</cell><cell>85.59</cell><cell>82.75</cell><cell>85.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 Average</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">accuracy (%) on Raw features.</cell><cell></cell></row><row><cell>Dataset</cell><cell>1-NN</cell><cell>L-SVM</cell><cell>DAM</cell></row><row><cell>MNIST+USPS, MSRC+VOC</cell><cell>41.89</cell><cell>38.23</cell><cell>40.45</cell></row><row><cell>Office+Caltech (SURF-BoW)</cell><cell>31.37</cell><cell>43.98</cell><cell>47.42</cell></row><row><cell>Office+Caltech (DeCAF 6 )</cell><cell>70.38</cell><cell>82.66</cell><cell>85.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Average runtime (seconds) over all cross-domain tasks in each dataset.</figDesc><table><row><cell>Dataset</cell><cell>KPCA</cell><cell>TCA</cell><cell>SA</cell><cell>TJM</cell><cell>TSC</cell><cell>SCA</cell></row><row><cell>MNIST+USPS</cell><cell>9.83</cell><cell cols="5">41.55 0.75 269.44 3072.25 42.74</cell></row><row><cell>MSRC+VOC</cell><cell>3.23</cell><cell cols="5">20.92 0.69 127.35 2051.05 36.86</cell></row><row><cell>Office+Caltech</cell><cell>0.84</cell><cell>3.03</cell><cell>8.35</cell><cell>29.65</cell><cell>1070.98</cell><cell>8.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>The groundtruth 1NN accuracy % of five-class classification when training on one dataset (the left-most column) and testing on another (the upper-most row). The bold black numbers indicate in-domain performance, while the plain black indicate cross-domain performance. "Self" refers to training and testing on the same dataset, same as the bold black numbers and "mean others" refers to the average performance over all cross-domain cases.</figDesc><table><row><cell>Training/Test</cell><cell cols="4">VOC2007 LabelMe Caltech-101 SUN09</cell><cell>Self</cell><cell>Mean others Percent drop (</cell><cell>(Self−MeanOthers) * 100 Self</cell><cell>)</cell></row><row><cell>VOC2007</cell><cell>72.46</cell><cell>52.45</cell><cell>89.17</cell><cell>60.00</cell><cell>72.46</cell><cell>67.20</cell><cell>∼ 7%</cell></row><row><cell>LabelMe</cell><cell>54.99</cell><cell>63.74</cell><cell>79.72</cell><cell>46.90</cell><cell>63.74</cell><cell>60.54</cell><cell>∼ 5%</cell></row><row><cell>Caltech-101</cell><cell>53.70</cell><cell>44.79</cell><cell>99.53</cell><cell>44.87</cell><cell>99.53</cell><cell>47.49</cell><cell>∼ 52%</cell></row><row><cell>SUN09</cell><cell>51.63</cell><cell>50.69</cell><cell>50.71</cell><cell>68.12</cell><cell>68.12</cell><cell>51.01</cell><cell>∼ 25%</cell></row><row><cell>Mean others</cell><cell>53.44</cell><cell>49.31</cell><cell>73.19</cell><cell>50.59</cell><cell>75.96</cell><cell>56.63</cell><cell>∼ 25%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Domain generalization performance accuracy (%) on the VLCS dataset with DeCAF 6 features as inputs. The accuracy of all feature learning-based algorithms: Raw, KPCA, uSCA, DICA, SCA is according to 1-nearest neighbor (1NN) classifier. Bold red and bold black indicate the best and the second best performance, respectively.</figDesc><table><row><cell cols="2">Source Target</cell><cell>1NN</cell><cell>L-SVM</cell><cell cols="2">KPCA Undo-Bias</cell><cell>UML</cell><cell cols="3">LRE-SVM uSCA DICA</cell><cell>SCA</cell></row><row><cell>L,C,S</cell><cell>V</cell><cell>57.26</cell><cell>58.44</cell><cell>60.22</cell><cell>54.29</cell><cell>56.26</cell><cell>60.58</cell><cell>58.54</cell><cell>59.62</cell><cell>64.36</cell></row><row><cell>V,C,S</cell><cell>L</cell><cell>52.45</cell><cell>55.21</cell><cell>51.94</cell><cell>58.09</cell><cell>58.50</cell><cell>59.74</cell><cell>54.08</cell><cell>51.82</cell><cell>59.60</cell></row><row><cell>V,L,S</cell><cell>C</cell><cell>90.57</cell><cell>85.14</cell><cell>90.09</cell><cell>87.50</cell><cell>91.13</cell><cell>88.11</cell><cell>85.14</cell><cell>78.30</cell><cell>88.92</cell></row><row><cell>V,L,C</cell><cell>S</cell><cell>56.95</cell><cell>55.23</cell><cell>55.03</cell><cell>54.21</cell><cell>58.49</cell><cell>54.88</cell><cell>55.63</cell><cell>55.33</cell><cell>59.29</cell></row><row><cell>C,S</cell><cell>V,L</cell><cell>55.08</cell><cell>55.58</cell><cell>55.64</cell><cell>59.28</cell><cell>56.47</cell><cell>55.04</cell><cell>53.98</cell><cell>50.90</cell><cell>59.50</cell></row><row><cell>C,L</cell><cell>V,S</cell><cell>52.60</cell><cell>51.80</cell><cell>50.70</cell><cell>55.80</cell><cell>54.72</cell><cell>52.87</cell><cell>49.05</cell><cell>55.47</cell><cell>55.96</cell></row><row><cell>V,C</cell><cell>L,S</cell><cell>56.62</cell><cell>59.99</cell><cell>54.66</cell><cell>62.35</cell><cell>55.49</cell><cell>58.84</cell><cell>55.89</cell><cell>58.08</cell><cell>60.77</cell></row><row><cell>Avg.</cell><cell></cell><cell>60.22</cell><cell>60.20</cell><cell>59.47</cell><cell>61.65</cell><cell>61.58</cell><cell>61.44</cell><cell>58.90</cell><cell>58.50</cell><cell>64.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Domain generalization performance accuracy (%) on the Office+Caltech dataset with DeCAF 6 features as inputs.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell>1NN</cell><cell>L-SVM</cell><cell cols="2">KPCA Undo-Bias</cell><cell>UML</cell><cell cols="3">LRE-SVM uSCA DICA</cell><cell>SCA</cell></row><row><cell>W, D, C</cell><cell>A</cell><cell>85.39</cell><cell>91.34</cell><cell>89.14</cell><cell>90.98</cell><cell>91.02</cell><cell>91.87</cell><cell>89.46</cell><cell>90.40</cell><cell>92.38</cell></row><row><cell>A,W,D</cell><cell>C</cell><cell>73.73</cell><cell>84.95</cell><cell>75.87</cell><cell>85.95</cell><cell>84.59</cell><cell>86.38</cell><cell>77.15</cell><cell>84.33</cell><cell>86.73</cell></row><row><cell>A,C</cell><cell>D,W</cell><cell>67.92</cell><cell>81.86</cell><cell>78.99</cell><cell>80.49</cell><cell>82.29</cell><cell>84.59</cell><cell>78.10</cell><cell>79.65</cell><cell>85.84</cell></row><row><cell>D,W</cell><cell>A,C</cell><cell>67.09</cell><cell>77.94</cell><cell>68.84</cell><cell>69.98</cell><cell>79.54</cell><cell>81.17</cell><cell>71.74</cell><cell>69.73</cell><cell>75.54</cell></row><row><cell>Avg.</cell><cell></cell><cell>72.28</cell><cell>84.02</cell><cell>77.71</cell><cell>81.85</cell><cell>84.36</cell><cell>86.00</cell><cell>79.11</cell><cell>81.02</cell><cell>85.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Domain generalization performance accuracy (%) on the IXMAS dataset with dense trajectory-based features.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell>1NN</cell><cell>L-SVM</cell><cell cols="2">KPCA Undo-Bias</cell><cell>UML</cell><cell cols="3">LRE-SVM uSCA DICA</cell><cell>SCA</cell></row><row><cell>Cam 0,1</cell><cell>Cam 2,3,4</cell><cell>58.24</cell><cell>73.26</cell><cell>67.77</cell><cell>69.03</cell><cell>74.14</cell><cell>79.96</cell><cell>66.67</cell><cell>65.93</cell><cell>80.59</cell></row><row><cell>Cam 2,3,4</cell><cell>Cam 0,1</cell><cell>20.33</cell><cell>84.07</cell><cell>41.21</cell><cell>60.56</cell><cell>63.79</cell><cell>80.15</cell><cell>51.09</cell><cell>78.02</cell><cell>85.16</cell></row><row><cell>Cam 0,1,2,3</cell><cell>Cam 4</cell><cell>39.56</cell><cell>67.03</cell><cell>59.34</cell><cell>56.84</cell><cell>60.37</cell><cell>74.97</cell><cell>61.54</cell><cell>62.64</cell><cell>70.33</cell></row><row><cell>Avg.</cell><cell></cell><cell>39.38</cell><cell>72.59</cell><cell>56.10</cell><cell>62.14</cell><cell>66.10</cell><cell>78.36</cell><cell>59.77</cell><cell>68.86</cell><cell>78.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1}, ∀k = 1, . . . , C, where C is the number of classes. At the prediction stage, given a test instance (x,ŷ) we computedŶ := {k|∀k = 1, . . . , C : f ub k = 1}. Finally, we verified whetherŷ ∈Ŷ .</figDesc><table /><note>• 1NN: 1-nearest neighbor classifier.• L-SVM: SVM classifier with linear kernel.• KPCA [26]: Kernel Principal Component Analysis.• Undo-Bias [21]: a multi-task SVM-based algorithm for undoing dataset bias. Three hyper-parameters (λ, C 1 , C 2 ) require tuning. Since the original formulation was designed for binary classifi- cation, we performed the following setup for multi-class classi- fication purposes. We trained C individual Undo-Bias classifiers f ubk : R d → {−1,• UML [20]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10</head><label>10</label><figDesc>Average domain generalization runtime (seconds) over all cross-domain recognition tasks in each dataset.</figDesc><table><row><cell>Dataset</cell><cell>KPCA</cell><cell>Undo-Bias</cell><cell>DICA</cell><cell>UML</cell><cell>LRE-SVM</cell><cell>SCA</cell></row><row><cell>VLCS</cell><cell>201.99</cell><cell>1, 925.54</cell><cell cols="2">336.92 1, 652.67</cell><cell>2, 161.60</cell><cell>300.94</cell></row><row><cell>Office+Caltech</cell><cell>6.53</cell><cell>589.50</cell><cell>17.90</cell><cell>413.25</cell><cell>695.31</cell><cell>18.49</cell></row><row><cell>IXMAS</cell><cell>0.50</cell><cell>49.14</cell><cell>0.79</cell><cell>57.67</cell><cell>65.47</cell><cell>0.96</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. http://www-scf.usc.edu/ ∼ boqinggo/da.html 3. http://vc.sce.ntu.edu.sg/transfer learning domain adaptation/domain adaptation home.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. http://www.cs.dartmouth.edu/ ∼ chenfang/proj page/FXR iccv13/index.php 5. http://vc.sce.ntu.edu.sg/transfer learning domain adaptation/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ACKNOWLEDGMENTS</head><p>The authors would like to thank Zheng Xu for sharing the extracted dense trajectories features from the IXMAS dataset, and also Chen Fang for sharing the Unbiased Metric Learning code and useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van-Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-04" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dataset issues in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward Category-Level Object Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4170</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbiased Look at Dataset Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="120" to="128" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2178" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marginalized Denoising Autoencoders for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient Learning of Domain-invariant Image Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer Joint Matching for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting Visual Category Models to New Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized domain-adaptive dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1187" to="1192" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain Generalization via Invariant Feature Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Undoing the Damage of Dataset Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="158" to="171" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting Low-Rank Structure from Latent Domains for Domain Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer Sparse Coding for Robust Image Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="404" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Use of Multiple Measurements in Taxonomic Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by Kernel Maximum Mean Discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear Component Analysis as a Kernel Eigenvalue Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Literature Survey on Domain Adaptation of Statistical Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://sifaka.cs.uiuc.edu/jiang4/domainadaptation/survey" />
	</analytic>
	<monogr>
		<title level="j">UIUC, Tech. Rep</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geodesic Flow Kernel for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frustratingly Easy NBNN Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Domain Adaptation Using Subspace Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continuous Manifold Based Adaptation for Evolving Visual Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of 15th Int Conf on Multimedia</title>
		<meeting>of 15th Int Conf on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiples sources: A domain-dependent regularization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for action and event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Benchmarking least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">A K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dedene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé-Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1134" to="1148" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Kernel Method for the Two-Sample-Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What You Saw is Not What You Get: Domain Adaptation Using Asymmetric Kernel Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1785" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Object Recognition: An Unsupervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="2058" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation by Domain Invariant Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain Adaptation on the Statistical Manifold</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2481" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DLID: Deep Learning for Domain Adaptation by Interpolating between Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Domain adaptive neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI: Trends in AI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8862</biblScope>
			<biblScope unit="page" from="898" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Analysis of Representations for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detecting Change in Data Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VLDB Conference</title>
		<meeting>VLDB Conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Central Subspace Dimensionality Reduction Using Covariance Operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="670" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ensemble of Exemplar-SVMs for Object Detection and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A Hilbert Space Embedding for Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ALT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings and metrics on probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1517" to="1561" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing IX</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">DAML: Domain Adaptation Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the influence of the kernel on the consistency of support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="93" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Universal Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2651" to="2667" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Domain adaptation and sample bias correction theory and algorithm for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">519</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Smooth Operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<ptr target="http://authors.library.caltech.edu/7694" />
	</analytic>
	<monogr>
		<title level="j">California Inst. of Tech</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Annual Workshop on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Free viewpview action recognition using motion history volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">LabelMe: A database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="157" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Random Features for Large-Scale Kernel Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
