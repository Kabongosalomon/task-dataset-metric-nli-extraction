<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Key Laboratory of Science and Technology on Multi-Spectral Information Processing</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boshen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Key Laboratory of Science and Technology on Multi-Spectral Information Processing</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Key Laboratory of Science and Technology on Multi-Spectral Information Processing</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Key Laboratory of Science and Technology on Multi-Spectral Information Processing</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taidong</forename><surname>Yu</surname></persName>
							<email>taidongyu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Key Laboratory of Science and Technology on Multi-Spectral Information Processing</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Tianyi Zhou</surname></persName>
							<email>joey.tianyi.zhou@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IHPC, A*STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff2">
								<orgName type="department">CSE Department</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet-50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the emergence of low-cost depth camera, 3D hand and body pose estimation from a single depth image draws much attention from computer vision community with wide-range application scenarios (e.g., HCI and AR) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Despite recent remarkable progress <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>, it is still a challenging task due to the issues of dramatic pose variation, high similarity among the different joints, self-occlusion, etc <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref>. *Fu Xiong and Boshen Zhang devote the equal contribution.</p><p>†Yang Xiao is the corresponding author (Yang_Xiao@hust.edu.cn).  <ref type="figure">Figure 1</ref>. The main idea of anchor-based 3D pose estimation paradigm within A2J. The densely set anchor points predict the positions of joints with weighted aggregation. The different joints possess the different informative anchor points of high weights (i.e., &gt; 0.02), which reveals A2J's adaptive characteristics.</p><p>Most of state-of-the-art 3D hand and body pose estimation approaches rely on deep learning technology. Nevertheless, they still suffer from some defects. First, encoderdecoder based FCN manners <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref> are generally trained with non-adaptive ground-truth Gaussian heatmap for different joints and with relatively high computational burden. Meanwhile, most of them cannot be fully end-to-end trained towards 3D pose estimation task <ref type="bibr" target="#b34">[35]</ref>. Secondly, 3D CNN models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref> are difficult to train with costly voxelizing procedure, due to the large number of convolutional parameters. Additionally, point-set based approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> require some extra time-consuming preprocessing treatments (e.g., point sampling).</p><p>Thus, we attempt to address 3D hand and body pose estimation problem using a novel anchor-based approach termed Anchor-to-Joint regression network (A2J). The proposed A2J network has end-to-end learning ability. The key idea of A2J is to predict 3D joint position by aggregating the estimation results of multiple anchor points, in spirit of ensemble learning to enhance generalization ability. Specifically, the anchor points can be regarded as the local regressors towards the joints from different viewpoints and distances. They are densely set on depth image to capture the global-local spatial context information together. Each of them will contribute to regress the positions of all the joints, but with different weights. The joint is localized by aggregating the outputs of all the anchor points. Since different joints may share the same anchor points, the articulated characteristics among them can be well maintained. For a specific joint, not all of the anchor points contribute equally. Accordingly, an anchor proposal procedure is proposed to discover the informative anchor points towards the certain joint by weight assignment. During training, both factors of estimation error of anchor points and spatial layout of informative anchor points are concerned. In particular, the picked up informative anchor points are encouraged to uniformly surround the corresponding joint to alleviate overfitting. Accordingly, the main idea of the proposed anchor-based 3D pose estimation paradigm within A2J is shown in <ref type="figure">Fig. 1</ref>. We can see that, generally different joints possess different informative anchor points. Furthermore, the visible "index tip" joint holds few informative anchor points. While, the invisible "index mid" joint and the "palm" joint on the relatively flat area possess much more ones, in order to capture richer spatial contexts. This actually reveals A2J's adaptive property.</p><p>Technically, A2J network consists of 3 branches driven by 2D CNN backbone network (i.e., ResNet-50 <ref type="bibr" target="#b20">[21]</ref>) without deconvolutional layers. In particular, the 3 branches take charges of predicting in-plain offsets between the anchor points and joints, estimating depth value of the joints, and informative anchor point proposal respectively. The main reasons to build A2J on 2D CNN for 3D pose estimation lie in 3 folders: (1) 3D information is already involved in depth image, using 2D CNN can still reveal 3D characteristics of the original depth image data; (2) compared to 3D CNN and point-set network, 2D CNN can be pre-trained on large-scale datasets (e.g., ImageNet <ref type="bibr" target="#b8">[9]</ref>), which may help to enhance its visual pattern capturing capacity for depth image; (3) 2D CNN is of high running efficiency without time-consuming 3D convolution operation and preprocessing procedures (e.g., voxelizing and point sampling).</p><p>A2J is experimented on 3 hand datasets (i.e., HANDS 2017 <ref type="bibr" target="#b47">[48]</ref>, NYU <ref type="bibr" target="#b36">[37]</ref>, and ICVL <ref type="bibr" target="#b35">[36]</ref>) and 2 body pose datasets (i.e., ITOP <ref type="bibr" target="#b19">[20]</ref> and K2HPD <ref type="bibr" target="#b41">[42]</ref>) to verify its superiority. The experiments reveal that, both for 3D hand and body pose estimation tasks A2J generally outperforms the state-of-the-art methods on effectiveness and efficiency simultaneously. Meanwhile, A2J can online run with the high speed around 100 FPS on a single NVIDIA 1080Ti GPU.</p><p>The main contributions of this paper include:</p><p>• A2J: an anchor-based regression network for 3D hand and body estimation from a single depth image. It is of endto-end learning capacity;</p><p>• An informative anchor proposal approach is proposed, concerning the joint position prediction error and anchor spatial layout simultaneously;</p><p>• 2D CNN without deconvolutional layers is used to drive A2J to ensure high running efficiency.</p><p>A2J's code is available at https://github.com/ zhangboshen/A2J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The existing 3D hand and body pose estimation approaches can be mainly categorized into non-deep learning and deep learning based groups. The state-of-the-art nondeep learning based ones <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref> generally follow the 2-step technical pipeline of first extracting hand-crafted feature, and then executing classification or regression. One main drawback is that, hand-crafted feature is often not representative enough. This tends to lead non-deep learning based method to be inferior to deep learning based manner. Since the proposed A2J falls into deep learning group, next we will introduce and discuss this paradigm from the perspectives of 2D and 3D deep learning respectively.</p><p>2D deep learning based approach. Due to end-to-end working manner, deep learning technology holds strong fitting ability for visual pattern characterization. 2D CNN has already achieved great success for 2D pose estimation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Recently it has also been introduced to 3D domain, resorting to global regression <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> or local detection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39]</ref> ways. The global regression manner cannot well maintain local spatial context information due to the global feature aggregation operation within fully-connected layers. Local detection based paradigm of promising performance generally chooses to address this problem via encoder-decoder model (e.g., FCN), setting local heatmap for each joint. Nevertheless, heatmap setting is still not adaptive for the different joints. And, the deconvolution operation is time consuming. Furthermore, most of the encoder-decoder based methods cannot be fully end-to-end trained <ref type="bibr" target="#b43">[44]</ref>.</p><p>3D deep learning based approach. To better reveal the 3D property within depth image for performance enhancement, one recent research trend is to resort to 3D deep learning. The paid efforts can be generally categorized into 3D CNN based and point-set based families. 3D CNN based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref> voxelizes the depth image into volumetric representation (e.g., occupancy grid models <ref type="bibr" target="#b23">[24]</ref>). 3D convolution or deconvolution operation is  then executed to capture 3D visual characteristics. However, 3D CNN is relatively hard to tune due to the large number of convolutional parameters. Meanwhile, 3D voxelization operation also leads to high computational burden both on memory storage and running time. Another way for 3D deep learning is point-set network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>, transferring depth image into point cloud as input. Nevertheless some time-consuming procedures (e.g., point sampling and KNN search) are required <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>, which weakens running efficiency. Accordingly, A2J belongs to 2D deep learning based group. The dense anchor points capture the global-local spatial context information in ensemble way, without using computationally expensive deconvolutional layers. 2D CNN is used as the backbone network for high running efficiency, also aiming to transfer knowledge from RGB domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A2J: Anchor-to-Joint Regression Network</head><p>The main technical pipeline of A2J is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. And, the symbols within A2J are defined in <ref type="table" target="#tab_0">Table 1</ref>. A2J consists of 2D backbone network (i.e., ResNet-50), and 3 functional branches: in-plain offset estimation branch, depth estimation branch, and anchor proposal branch. The 3 branches predict O j (a), D j (a), and P j (a) respectively.</p><p>Within A2J, anchor points are densely set up on the input depth images with stride S t = 4 pixels to capture the global-local spatial context information as in <ref type="figure" target="#fig_2">Fig. 3</ref>. Essentially, each of them serves as the local regressor to predict the 3D position of all the joints via in-plain offset predic- tion branch and depth estimation branch. For certain joint, it is finally localized by aggregating the outputs of all the anchor points. Concerning that maybe not all the anchor points contribute equally to certain joint, the anchor points will be assigned weights via anchor proposal branch to discover the informative ones. As consequence, the in-plain position and depth value of joint j can be achieved as the weighted average of the outputs of all anchor points as:</p><formula xml:id="formula_0">  Ŝ j = a∈AP j (a) (S (a) + O j (a)) D j = a∈AP j (a)D j (a) ,<label>(1)</label></formula><p>whereŜ j andD j indicate the estimated in-plain position and depth value of joint j;P j (a) can be regarded as the normalized weight of anchor point a towards joint j across all anchor points, and is acquired using softmax by:</p><formula xml:id="formula_1">P j (a) = e Pj (a) a∈A e Pj (a) .<label>(2)</label></formula><p>It is worthy noting that, the anchor point a withP j (a) &gt; 0.02 will be regarded as the informative anchor points for joint j. The selected informative anchor points can reveal A2J's adaptive characteristics as in <ref type="figure">Fig. 1</ref>. Joint position estimation loss and anchor point surrounding loss are used to supervise A2J's end-to-end training. Under their joint supervision, informative anchor points with the spatial layout that surrounds the joint will be picked up to enhance generalization ability. Next, we will illustrate the proposed A2J regression network and its learning procedure in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A2J regression network</head><p>Here, the 3 functional branches and backbone network within A2J will be illustrated in details respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">In-plain offset and depth estimation branches</head><p>Essentially, these 2 branches play the role of predicting the 3D positions of joints. Since in-plain position estimation and depth estimation are of different properties, we choose to execute them separately. Specifically, one is to estimate O j (a) between anchor points and joints. And, the other is to estimate D j (a) towards joints. As in <ref type="figure">Fig. 4</ref>, they are built upon the output feature map of regression trunk within backbone network to involve semantic feature. Four 3 × 3 intermediate convolutional layers (with BN and ReLU) are consequently set to aggregate richer local context information without reducing in-plain size. Since the feature map is a 16× downsampling of the input depth image on in-plain size (illustrated in Sec. 3.1.3) and anchor point setting stride S t = 4 as in <ref type="figure" target="#fig_2">Fig. 3</ref>, one feature map point corresponds to 4 × 4 = 16 anchor points on depth image. An output convolutional layer with the feature map in-plain size is then set towards all the 16 corresponding anchor points in columnwise manner. Suppose K joints exist, in-plain offset estimation branch is of 16 × K × 2 output channels. And, depth estimation branch is of 16 × K × 1 output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Anchor proposal branch</head><p>This branch discovers informative anchor points for the certain joint by weight assignment as Eqn. 2. As in <ref type="figure">Fig. 5</ref>, anchor proposal branch is built upon the output feature map of common trunk within backbone network to involve relatively fine feature. As the 2 branches introduced in Sec. 3.1.1, 4 intermediate convolutional layers and 1 output convolutional layer are consequently set for predicting P j (a) for the anchor points without losing in-plain size. Accordingly, the output layer of this branch is of 16 × K × 1 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Backbone network architecture</head><p>ResNet-50 <ref type="bibr" target="#b20">[21]</ref> pre-trained on ImageNet is used as the backbone network. In particular, layers 0-3 correspond to the common trunk in <ref type="figure" target="#fig_1">Fig. 2</ref>. And, layer 4 corresponds to regression trunk. Some modifications are executed to make ResNet-50 more suitable for pose estimation. First, the convolutional stride in layer 4 is set to 1. Consequently, the output feature map of layer 4 is a 16× downsampling of the input depth image on in-plain size. Compared with the raw ResNet-50 with 32× downsampling, more fine spatial information can be maintained in this way. Meanwhile, the  convolution operation within layer 4 is revised as the dilated convolution with a dilation of 2 to enlarge receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning procedure of A2J</head><p>To generate input of A2J, we follow <ref type="bibr" target="#b25">[26]</ref> and use center points to crop the hand region from depth image. For body pose, we follow <ref type="bibr" target="#b10">[11]</ref> and use bounding box to crop the body region. For joint j, in-plain target T i j denotes the 2D ground-truth in pixel coordinate transformed according to the cropped region. To make T i j and depth target T d j be in comparable magnitude, we transform the ground-truth depth Z j of joint j as:</p><formula xml:id="formula_2">T d j = µ(Z j − θ),<label>(3)</label></formula><p>where µ and θ are the transformation parameters. For hand pose µ is set to 1, and θ is set to the depth of center points. For body pose µ is set to 50 and θ is set as 0, since we do not have depth center. During test, the prediction result will be warpped back to world coordinate. A2J is then trained under the joint supervision of 2 loss functions: joint position estimation loss and informative anchor point surrounding loss. Next, we will illustrate these 2 loss functions in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Joint position estimation loss</head><p>Within A2J, the anchor points serve as the local regressors to predict the 3D position of joints in ensemble way. This objective loss can be formulated as:</p><formula xml:id="formula_3">loss 1 = α j∈J L τ1 ( a∈AP j (a)(S(a) + O j (a)) − T i j ) + j∈J L τ2 ( a∈AP j (a)D j (a) − T d j ),<label>(4)</label></formula><p>where α = 0.5 is the factor to balance in-plain offset and depth estimation task; T i j and T d j are the in-plain and depth targets position of joint j; and L τ (·) is the smooth L1 like loss function <ref type="bibr" target="#b30">[31]</ref> given by:</p><formula xml:id="formula_4">L τ (x) = 1 2τ x 2 , for |x| &lt; τ, |x| − τ 2 , otherwise.<label>(5)</label></formula><p>In Eqn. 4, τ 1 is set to 1 and τ 2 is set to 3 since the depth value is relatively noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Informative anchor point surrounding loss</head><p>To enhance the generalization ability of A2J, we intend to let the picked up informative anchor points locate around the joints, in spirit of observing the joints from multiple viewpoints simultaneously. Hence, the informative anchor point surrounding loss is defined by us as:</p><formula xml:id="formula_5">loss 2 = j∈J L τ1 ( a∈AP j (a)S(a) − T i j ).<label>(6)</label></formula><p>To reveal its effectiveness, we show the informative anchor point spatial layouts with and without using it both for hand and body pose cases in <ref type="figure" target="#fig_5">Fig. 6</ref>. It can be seen that, informative anchor point surrounding loss can essentially help to alleviate viewpoint bias. Its quantitative effectiveness will also be verified in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">End-to-end training</head><p>The 2 loss functions above jointly supervise the end-to-end learning procedure of A2J, which is formulated as:</p><formula xml:id="formula_6">loss = λloss 1 + loss 2 ,<label>(7)</label></formula><p>where loss is the loss in all; and λ = 3 is the weight factor to balance loss 1 and loss 2 .   <ref type="table">Table 2</ref>. Performance comparison on HANDS 2017 dataset <ref type="bibr" target="#b47">[48]</ref>. "SEEN" and "UNSEEN" denote the cases whether the test subjects are involved in training set. "AVG" indicates the result over all subjects. And, " * " means the ensemble of 10 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Mean error (mm) FPS DISCO <ref type="bibr" target="#b0">[1]</ref> 20.7 -Hand3D <ref type="bibr" target="#b9">[10]</ref> 17.6 30 DeepModel <ref type="bibr" target="#b50">[51]</ref> 17.04 -JTSC <ref type="bibr" target="#b11">[12]</ref> 16.8 -Global-to-Local <ref type="bibr" target="#b22">[23]</ref> 15.60 50 Lie-X <ref type="bibr" target="#b44">[45]</ref> 14.51 -REN-4x6x6 <ref type="bibr" target="#b17">[18]</ref> 13.39 -REN-9x6x6 <ref type="bibr" target="#b17">[18]</ref> 12.69 -DeepPrior++ <ref type="bibr" target="#b27">[28]</ref> 12.24 30 Pose-REN <ref type="bibr" target="#b6">[7]</ref> 11.81 -HandPointNet <ref type="bibr" target="#b13">[14]</ref> 10.  <ref type="table">Table 3</ref>. Performance comparison on NYU dataset <ref type="bibr" target="#b36">[37]</ref>. "Mean error" indicates the average 3D distance error.</p><p>uation criteria. For body, Percent of Detected Joints (PDJ) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20]</ref> and mean average precision (mAP) with 10-cm rule <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20]</ref> are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation details</head><p>A2J network is implemented using PyTorch. The input depth image is cropped and resized to a fixed resolution (i.e., 176 × 176 for hand, and 288 × 288 for body). Random in-plain rotation and random scaling for both in-plain and depth dimension are executed for data augment. Random Gaussian noise is also randomly added with the probability of 0.5 for data augment. We use Adam as the optimizer. The learning rate is set to 0.00035 with a weight decay of 0.0001 in all cases. A2J is trained on NYU for 34 epochs with a learning rate decay by 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art methods</head><p>HANDS 2017 dataset: A2J is compared with the stateof-the-art 3D hand pose estimation methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref> , particularly. The performance comparison is listed in Table 2. It can be observed that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Mean error (mm) FPS LRF <ref type="bibr" target="#b35">[36]</ref> 12.58 -DeepModel <ref type="bibr" target="#b50">[51]</ref> 11.56 -Hand3D <ref type="bibr" target="#b9">[10]</ref> 10.9 30 CrossingNets <ref type="bibr" target="#b39">[40]</ref> 10.2 90.9 Cascade <ref type="bibr" target="#b33">[34]</ref> 9.9 -JTSC <ref type="bibr" target="#b11">[12]</ref> 9.16 -DeepPrior++ <ref type="bibr" target="#b27">[28]</ref> 8.1 30 REN-4x6x6 <ref type="bibr" target="#b17">[18]</ref> 7.63 -REN-9x6x6 <ref type="bibr" target="#b17">[18]</ref> 7.31 -DenseReg <ref type="bibr" target="#b38">[39]</ref> 7.3 27.8 Pose-REN <ref type="bibr" target="#b6">[7]</ref> 6.79 -HandPointNet <ref type="bibr" target="#b13">[14]</ref> 6.935 48 P2P <ref type="bibr" target="#b16">[17]</ref> 6.328 41.8 V2V * <ref type="bibr" target="#b25">[26]</ref> 6.286 3.5 A2J (Ours) 6.461 105.06 <ref type="table">Table 4</ref>. Performance comparison on ICVL dataset <ref type="bibr" target="#b35">[36]</ref>. "Mean error" indicates the average 3D distance error.</p><p>• On this challenging million-scale dataset, A2J consistently outperforms the other approaches both from the perspectives of effectiveness and efficiency. This essentially verifies the superiority of our proposition;</p><p>• It is worthy noting that, A2J is significantly superior to the others with the remarkable margin (2.05 at least) towards the "UNSEEN" test case. This phenomenon essentially demonstrates the generalization ability of A2J;</p><p>• V2V * is the strongest competitor of A2J, but with 10 models ensemble. As a consequence, it is much slower than A2J with only a single model.</p><p>NYU and ICVL datasets: We compare A2J with the state-of-the-art 3D hand pose estimation methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref> on this 2 datasets specifically. The experimental results are given in <ref type="table">Table 3</ref>, 4 on the average 3D distance error. Meanwhile, the percentage of success frames over different error thresholds and the error of each joint are also given in <ref type="figure" target="#fig_6">Fig. 7</ref>. We can summarize that:</p><p>• A2J is superior to the other methods in most cases both on accuracy and efficiency. The exceptional case is that, A2J is slightly inferior to V2V * and P2P on ICVL dataset on accuracy but with much higher running efficiency;</p><p>• Concerning the good tradeoff between effectiveness and efficiency, A2J essentially takes advantage over the state-of-the-art 3D hand pose estimation approaches.</p><p>ITOP dataset: We also compare A2J with the state-ofthe-art 3D body pose estimation manners <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref> on this dataset. The performance comparison is listed in <ref type="table">Table 5</ref>. We can see that:</p><p>• A2J is significantly superior to the other ones both for front-view and top-view tracks, except V2V * . The performance gap is 3.1 at least for front-view case, and 5 at least for top-view case. This reveals that A2J is also applicable to 3D body pose estimation, as well as 3D hand task;</p><p>• A2J is inferior to V2V * . However, V2V * actually consists of 10 models ensemble . Thus, compared with A2J mAP (front-view) mAP (top-view)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>RF <ref type="bibr" target="#b32">[33]</ref> RTW <ref type="bibr" target="#b49">[50]</ref> IEF <ref type="bibr" target="#b4">[5]</ref> VI <ref type="bibr" target="#b19">[20]</ref> CMB <ref type="bibr" target="#b40">[41]</ref> REN-9x6x6 <ref type="bibr" target="#b17">[18]</ref> V2V * <ref type="bibr" target="#b25">[26]</ref> A2J (Ours) RF <ref type="bibr" target="#b32">[33]</ref> RTW <ref type="bibr" target="#b49">[50]</ref> IEF <ref type="bibr" target="#b4">[5]</ref> VI <ref type="bibr" target="#b19">[20]</ref> REN-9x6x6 <ref type="bibr">[</ref>  <ref type="table">Table 5</ref>. Performance comparison on ITOP 3D body pose estimation dataset <ref type="bibr" target="#b19">[20]</ref>. with single model it is of much lower running efficiency.</p><p>K2HPD dataset: Since this body pose dataset only provides the pixel-level in-plain ground-truth, the depth estimation branch within A2J is removed accordingly. We also compare A2J with the state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref>. The performance comparison is given in <ref type="table" target="#tab_6">Table 6</ref>. It can be observed that:</p><p>• A2J outperforms the other methods by large margins consistently, corresponding to the difference PDJ thresholds. In average, the performance gap is 10.8 at least. This demonstrates that, A2J is also applicable to 2D case;</p><p>• It is worthy noting that, with the decrease of PDJ threshold the advantage of A2J will be enlarged remarkably. This reveals the fact that, A2J is essentially superior to more accurate body pose estimation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study 4.3.1 Component effectiveness analysis</head><p>The component effectiveness analysis within A2J is executed on NYU <ref type="bibr" target="#b36">[37]</ref> (hand), and ITOP <ref type="bibr" target="#b19">[20]</ref> dataset (body). We will investigate the effectiveness of anchor proposal branch, informative anchor point surrounding loss, and configuration of in-plain offset and depth estimation branches.</p><p>The results are listed in <ref type="table">Table 7</ref>. It can be observed that:</p><p>• Without using anchor proposal branch, performance will drop remarkably especially for body pose. This verifies our point that, not all the anchor points contribute equally to the certain joints. Actually, anchor point adaptivity is A2J's essential property to leverage performance;</p><p>• Without using informative anchor point surrounding loss, performance will drop especially for body pose. This demonstrate that, informative anchor point spatial layout is an essential issue that should be concerned towards generalization ability;</p><p>• When estimating in-plain offset and depth value in one branch, performance will drop to some degree. This may be caused by the fact that, in-plain offset and depth value holds different physical characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effectiveness of anchor-based paradigm</head><p>To verify the effectiveness of anchor-based 3D pose estimation paradigm, we compare A2J with the global regression based manner <ref type="bibr" target="#b37">[38]</ref> and FCN-based approach <ref type="bibr" target="#b43">[44]</ref>. Since FCN model is generally used to predict in-plain joint position, this ablation study is executed on K2HPD dataset <ref type="bibr" target="#b41">[42]</ref>  only with in-plain ground-truth annotation. Global regression manner encodes depth image with 2D CNN, and then regresses in-plain human joint position using fullyconnected layers. FCN model is built following <ref type="bibr" target="#b43">[44]</ref>. ResNet-50 <ref type="bibr" target="#b20">[21]</ref> is employed as the backbone network for them, which is the same as A2J for fair comparison. PDJ (0.05) is used as the evaluation criteria. The performance comparison is listed in <ref type="table">Table 8</ref>. We can see that:</p><p>• Our proposed anchor-based paradigm significantly outperforms the other 2 ones, when using the same ResNet-50 backbone network. We think 2 main reasons lie. First, compared with global regression based manner local spatial context information can be better maintained within A2J. Meanwhile, compared with FCN model A2J possess anchor point adaptivity towards the certain joint;</p><p>• A2J runs faster than FCN model, but slower than global regression way. However, its performance advantage over global regression paradigm is significant, actually with better tradeoff between effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Effectiveness of the pre-training</head><p>One reason for why we build A2J on 2D CNN is that, it can be pre-trained on the large-scale RGB visual datasets (e.g., ImageNet) for knowledge transfer. To verify this point, we compare the performance of A2J with and without pre-training on ImageNet on NYU (hand) and ITOP (body) datasets. The performance comparison is listed in <ref type="table" target="#tab_9">Table 9</ref>. It can be observed that, both for hand and body pose cases pre-training A2J on ImageNet can indeed help to leverage the performance.   <ref type="table" target="#tab_0">Table 10</ref>. Performance comparison among the backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Backbone network comparison</head><p>The comparison among the different backbone networks is further studied. As shown in <ref type="table" target="#tab_0">Table 10</ref>, we compare the performance of 3 backbone networks (i.e., ResNet-18, ResNet-34 and ResNet-50). It can be summarized that:</p><p>• Deeper network can achieve better results, but with relatively slower running efficiency. However, the performance gap among the different backbones is not huge;</p><p>• It is worthy noting that, even using ResNet-18 A2J still can generally achieve the state-of-the-art performance and with extremely fast running speed of 192.25 FPS. This reveals the applicability of A2J towards high real-time running demanding application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative evaluation</head><p>Some qualitative results of A2J on NYU <ref type="bibr" target="#b36">[37]</ref> and ITOP (front-view) <ref type="bibr" target="#b19">[20]</ref> datasets are shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. We can see that, generally A2J works well both for 3D hand and body pose estimation. The failure cases are mainly caused by the serious self-occlusion and dramatic pose variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Running speed analysis</head><p>The average online running speed of A2J for 3D hand pose estimation is 105.06 FPS, including 1.5 ms for reading and warpping image, and 8.0 ms for network forward propagation and post-processing on a single NVIDIA 1080Ti GPU. The running speed for 3D body pose estimation is 93.78 FPS, including 0.4 ms for reading and warpping image, and 10.2 ms for network forward propagation and postprocessing. This reveals A2J's real-time running capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, an anchor-based 3D articulated pose estimation approach for single depth image termed A2J is proposed. Within A2J anchor points are densely set up on depth image to capture the global-local spatial context information, and predict joint's position in ensemble way. Meanwhile, informative anchor points are extracted to reveal A2J's adaptive characteristics towards the different joints. A2J is built on 2D CNN without using computational expensive deconvolutional layers. The wide-range experiments demonstrate A2J's superiority both from the perspectives of effectiveness and efficiency. In future work, we will seek the more effective way to fuse the anchor points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The main technical pipeline of A2J. A2J consists of backbone network and 3 functional branches. The backbone network is built on ResNet-50. And, the 3 branches are in-plain offset prediction branch, depth estimation branch, and anchor proposal branch.SymbolDefinition AAnchor point set. aAnchor point a ∈ A. J Joint set. j Joint j ∈ J. K Number of joints.S(a)In-plain position of anchor point a.Pj (a)Response of anchor a towards joint j.Oj (a)Predicted in-plain offset towards joint j from anchor point a. Dj (a) Predicted depth value of joint j by anchor point a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The densely set anchor points on depth image. They will serve for predicting the positions of all joints in ensemble way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>In-plain offset and depth estimation branches. They both contain 4 intermediate convolutional layers with 256 channels, and 1 output convolutional layer (with 16 × K × 2 or 16 × K × 1 channels). W and H indicate width and height of the input depth image. d means dimensionality. Anchor proposal branch with 4 intermediate convolutional layers with 256 channels, and 1 output convolutional layer with 16 × K × 1 channels. W and H indicate width and height of the input depth image. d means dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Body cases from ITOP front-view dataset Effectiveness of anchor point surrounding loss. Grey dot denotes anchor point. Red dot indicate informative anchor point. Green arrow represent in-plain offset. Yellow square corresponds to ground-truth joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of A2J with state-of-the-art methods. Left: the percentage of success frames over different error thresholds. Right: 3D distance errors per hand keypoints. Top: NYU dataset. Bottom: ICVL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on ITOP front-view dataset Qualitative results of A2J. Ground-truth is shown in red, and the predicted pose is in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table /><note>Symbol definition within A2J.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison on K2HPD dataset<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Effectiveness of pre-training A2J on ImageNet.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>ResNet-18</cell><cell>ResNet-34</cell><cell>ResNet-50</cell></row><row><cell>NYU</cell><cell>error</cell><cell>9.32</cell><cell>9.01</cell><cell>8.61</cell></row><row><cell></cell><cell>FPS</cell><cell>192.25</cell><cell>144.63</cell><cell>105.06</cell></row><row><cell>ITOP</cell><cell>mAP</cell><cell>87.1</cell><cell>87.8</cell><cell>88.0</cell></row><row><cell>front-view</cell><cell>FPS</cell><cell>167.19</cell><cell>122.47</cell><cell>93.78</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disco nets: Dissimilarity coefficients networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Qi</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03416</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03416</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02224</idno>
		<title level="m">Hand3d: Hand pose estimation using 3d neural network</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task, multi-domain learning: application to semantic segmentation and pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Trémeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time human pose tracking from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="738" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3593" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards good practices for deep 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07248</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>IEEE International Conference on Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4512" to="4516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards viewpoint invariant 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boya</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth-images-based pose estimation using regression forests and graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end global to local cnn learning for hand pose recovery in depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09606</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Holistic planimetric prediction to local volumetric prediction for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04758</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<meeting>IEEE International Conference on Computer Vision Workshop (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">840</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular real-time 3d articulated hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE-RAS International Conference on Humanoid Robots (ICHR)</title>
		<meeting>IEEE-RAS International Conference on Humanoid Robots (ICHR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crossing nets: Combining gans and vaes with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional memory blocks for depth data representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human pose estimation from depth images via inference embedded multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM on Multimedia Conference (ACM MM)</title>
		<meeting>ACM on Multimedia Conference (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lie-x: Depth image based articulated object pose estimation, tracking, and action recognition on lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lakshmi Narasimhan Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="454" to="478" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accurate 3d pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Liu Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="731" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Depthbased 3d hand pose estimation: From current achievements to future goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02237</idno>
		<title level="m">The 2017 hands in the million challenge on 3d hand pose estimation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bighand2. 2m benchmark: Hand pose dataset and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4866" to="4874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><surname>Ho Yub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochahn</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Seok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Dong</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
