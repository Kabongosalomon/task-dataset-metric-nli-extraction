<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Aware Human-Action Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-16">16 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
							<email>pingyu@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
							<email>changyou@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Aware Human-Action Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-16">16 Aug 2020</date>
						</imprint>
					</monogr>
					<note>2 P. Yu et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>action generation</term>
					<term>graph convolutional net- work</term>
					<term>self-attention</term>
					<term>generative adversarial networks (GAN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs (SA-GCNs) to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, wellcapturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods. The code to reproduce our analysis is available at https://github.com/PingYuiris/SA-GCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs (SA-GCNs) to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, wellcapturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods. The code to reproduce our analysis is available at https://github.com/PingYuiris/SA-GCN.</p><p>Keywords: action generation, graph convolutional network, self-attention, generative adversarial networks (GAN)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the development of skeleton-based action generation, which has been applied in a variety of applications, such as action classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref>, action prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> and human-centric video generation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. Action generation is still a challenging problem since small deviations in one frame can cause confusion in the entire sequence.</p><p>One of the most successful methods for skeleton-based action generation considers skeleton-based action generation as a standard video generation problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref>. Specifically, the method naively treats skeleton joints as image pixels and sequential actions as videos, without considering the rich structure information among both joints and action frames. The video-generation based methods may produce distorted actions when applied to skeleton generation, if prior structure knowledge is not well leveraged. A first step to consider structure information into action generation is to represent a skeleton as a graph structure to characterize the spatial relations between joints in each frame based on graph convolution networks (GCN) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>. However, most existing GCN methods do not have the flexibility to process continuous sequential graphs data. This poses a new challenge: how to construct a representation to effectively incorporate both temporal and spatial structures into action generation?</p><p>Generally speaking, there are two classes of methods with GCN to model action structure information: (i) Full connection: an entire action sequence is7890-=-098765 considered as a graph. Each node of the current frame is connected with the corresponding nodes in all the past frames. This construction, however, is computationally very inefficient (if ever possible at all). Moreover, the model could be highly redundant since many frames are similar to each other. (ii) Spatial-temporal graph convolutional networks <ref type="bibr" target="#b43">[44]</ref>: a graph convolution is first applied to intra-frame skeletons, whose extracted features are then applied with a 1D convolution layer to capture temporal information. This method typically requires weight sharing among all nodes, and the ability to model temporal information is somewhat weak.</p><p>We advocate that a better solution should be proposed to leverage skeleton structures and gather information from action sequences more efficiently. In this paper, we propose Self-Attention based Graph Convolutional Networks (SA-GCN) to build generic representations <ref type="figure" target="#fig_4">Fig. 1</ref>: Comparisons of the construction of action graphs with our proposed method (3rd tow) and two standard methods (1st and 2nd rows) to encode temporal information. First row (full connection): the left-hand joint gather information from all left-hand joint of past frames; similar to the right-hand joint. Second row (ST-GCN ): a 1D convolution of kernel size k is used to encode temporal information. Both the left and right hands could encode information from past k frames with share weights. Third row (SA-GCN ): both the left-and right-hand joints learn to encode information from a selected left-hand joints based on the attention scores.</p><p>for skeleton sequences. Our SA-GCN aims at building a sparse global graph for each action sequence to achieve both computational efficiency and modelling efficacy. Specifically, for a given frame, the proposed SA-GCN first calculates self-attention scores for other frames. Based on the attention scores, top k past frames with the most significant scores are selected to be connected to the current frame to construct inter-frame connections. Within each frame, the joints are connected as the original skeleton representation. To demonstrate the differences between our construction and the aforementioned two constructions, <ref type="figure" target="#fig_4">Fig. 1</ref> illustrates a sequence of samples in terms of every three consecutive frames on the Human 3.6m dataset SittingDown sequence. As illustrated in the figure, our method can be considered as an adaptive scheme to construct an action graph, with each node assigning a trainable weight instead of a shared weight as in other methods.</p><p>The major contributions of this work are summarized in three aspects:</p><p>-We propose SA-GC layer, a generic graph-based formulation to encode structure information into action modelling efficiently.</p><p>Our method is the first sparse and adaptive scheme to encode past frame information for action generation. -By efficiently leveraging action structure information, our model can generate high-quality long-range action sequences with pure Gaussian noise and provided labels as inputs without pretraining. -Our model is evaluated on two standard large datasets for skeleton-based action generation, achieving superior and stable performance compared with previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries &amp; Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention Model</head><p>Attention models have become increasingly popular in capturing long-term global dependencies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, self-attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref> mimics human visual attention, allowing a model to focus on crucial regions and to learn the correlation among elements in the same sequence. <ref type="bibr" target="#b37">[38]</ref> proposes a non-local operation as a kind of attention on capturing long-range dependencies in videos. <ref type="bibr" target="#b32">[33]</ref> develops the transformer model, which is solely based on attention and achieves state-of-the-art on machine translation. Thus, self-attention can typically lead to a better representation learning.</p><p>One key advance of our proposed model compared with previous ones is that we adopt self attention to efficiently encode frame-wise correlations by inheriting all merits of the self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Skeleton-Based Action Generation</head><p>The task of action generation differs from action prediction <ref type="bibr" target="#b2">[3]</ref> in that no past intermediate sub-sequence is provided. Directly generating human actions from noise is considered more challenging. The problem has been well studied in early works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, which applied switching linear models to generate stochastic human motions. These models, however, required a large amount of data to fit a model and are difficult to find an appropriate number of switching states. Later on, the Restricted Boltzmann Machine <ref type="bibr" target="#b29">[30]</ref> and Gaussian-process latent variable models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> were applied. But they still can not scale to massive amounts of data. The rapid development of deep generative models has brought the idea of recurrent-neural-network (RNN) based Variational Autoencoder (VAE) <ref type="bibr" target="#b19">[20]</ref> and Generative Adversarial Net (GAN) models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>. These models are scalable and usually can generate actions with better quality. The aforementioned methods still have some limitations, which mainly lie in two aspects. Firstly, spatial relationships among body joints and temporal dynamics along continuous frames have not been well explored. Secondly, these models often require an expensive pre-training phase to capture intra-frame constraints, including the two most recent state-of-the-art works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>. By contrast, Our work moves beyond these limitations and can be trained from scratch to generate high-quality motions. GCNs have been achieving encouraging results <ref type="bibr" target="#b43">[44]</ref>. In general, they can be categorized into two types: spectral approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> and spatial approaches <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b25">26]</ref>. The spectral GCN operates on the Fourier domain (locality) with convolution to produce a spectral representation of graphs. The spatial GCN, by contrast, directly applies convolution on the spatially distributed nodes. This work is in the spirit of spatial GCNs and incorporates new ideas of GCNs to fit the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Convolutional Network</head><p>In particular, to model longterm dependent motion dynamics, we are aware of ideas from graph pruning <ref type="bibr" target="#b46">[47]</ref> and jump connection <ref type="bibr" target="#b42">[43]</ref>, which respectively allows one to extract structure representation more efficiently and to build deeper graph convolutional layers. In terms of GCN-based human motion modelling, the most related work is ST-GCN <ref type="bibr" target="#b43">[44]</ref>, which applies a spatial GCN to a different task of action recognition. This method applied a GCN layer for intra-frame skeletons and then used 1D convolution layer for gathering information in temporal space. All nodes in a frame share weights on the temporal space and could only attend limited range of information, depending on the kernel size of the 1D convolution layer. We will compare our method with ST-GCN (for action generation) in Section 4.6.</p><p>3 Structure-Aware Human-Action Generation Different from the video-generation task, the skeleton-based action generation contains huge amounts of structure information, e.g., intra-frame structural joints information and inter-frame motion dynamics. Directly treating skeleton frames as images will lose most of these structure information, leading to the distortion of some skeletal frames. Moreover, in the context of skeleton-based actions, where only limited positional information is provided, differences between two continuous frames are virtually impossible to be observed. To address these issues, we propose to incorporate GCNs to encode the rich structural information, with additional consideration to reduce computational burden by using self-attention to automatically learn a sparse action graph. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the overall framework of our model for action generation. It follows the GAN framework of video generation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9]</ref>, which consists of an action generator G and a dual discriminator: one video-based discriminator D V and one frame-based discriminator D F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Overview of the SA-GCN</head><p>Generator For simplicity, we assume the sequence length to be T . Our action generator starts with a RNN with an input at each time as the concatenation of a Gaussian random noise z and an embedded class representation of a label y. The outputs of the RNN layer are denoted as [o 0 , o 1 , o 2 , ..., o T −1 ]. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>, we consider outputting residuals instead of the exact coordinates of different joints, i.e., c 0 = o 0 ,</p><formula xml:id="formula_0">c 1 = o 1 + c 0 , ..., c T −1 = o T −1 + c T −2 .</formula><p>The output of the RNN will go through three linear transformations before being fed as the input of the newly proposed SA-GC layer, which will be detailed in Section 3.2.</p><p>The SA-GC layer The key component of our framework is a newly defined self-attention based graph convolutional layers (SA-GC layers), as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Specifically, we denote the input of the SA-GC layers as a feature vector H ∈ R T ×N . Through a self attention layer <ref type="bibr" target="#b32">[33]</ref>, the output are a new representation H in ∈ R T ×N ×1 and a learned masked attention score matrix S mask ∈ R T ×T . This self attention layer is followed by 5 GC layers. Each GC layer takes last layer's hidden state vector and masked adjacency matrixÃ s as the input. The hidden states, which are outputs of the 5 GC layers, are defined respectively as H <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_1">∈ R T ×N ×32 , H (2) ∈ R T ×N ×64 , H (3) ∈ R T ×N ×64 , H (4) ∈ R T ×N ×128 and H (5) ∈ R T ×N ×128</formula><p>. Furthermore, the ResNet mechanism <ref type="bibr" target="#b14">[15]</ref> is applied on each two SA-GC layers, i.e., we add the output of the first SA-GC layer to the third SA-GC layer, and the output of the third SA-GC layer to the final output. Detailed operations of the SA-GC layer are described in Section 3.2.</p><p>Dual discriminator The video-based discriminator D V takes a sequence of actions and the corresponding labels as the input. The frame-based discriminator D F randomly selects k f rame frames of an input sequence and the corresponding labels as the input. Both discriminators output either real or fake. In this paper, we apply the conditional GAN objective formulation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>:</p><formula xml:id="formula_2">L = min G max D F ,D V E x∼p(x) [log D F (x|y))] + E z∼p(z) [log(1 − D F (G(z|y)))]+ E x∼p(x) [log D V (x|y)] + E z∼p(z) [log(1 − D V (G(z|y))))]<label>(1)</label></formula><p>where p(x) defines the ground truth distribution, p(z) is the standard Gaussian distribution and y is the one-hot class indicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Action Graph Construction</head><p>In this section, we describe detailed construction of the action graph, which is used in our SA-GCN module. Note a skeleton sequence is usually represented by 2D or 3D coordinates of human joints in each frame. The inter-frame action is the static skeleton in spatial space, and the inter-frame action is the movement in temporal space. To capture the temporal correlation, previous work has applied 1D convolution for learning skeleton representing by concatenating coordinate vectors of all joints in one frame. In our framework, as stated before, we propose to construct a connected graph for a whole action sequence, and learn a sparse interframe connection by adopting self-attention learning. Particularly, we construct an undirected graph G=(V, E) on a whole action sequence of T frames, each consists of N joints. Here, the node set V = {v ti |t = 0, . . . , T − 1, i = 1, . . . N } includes all joints of a skeleton sequence.</p><p>Explanation of the SA-GC layer <ref type="figure" target="#fig_0">Fig. 2</ref> shows the detailed implementation of the SA-GC layer. Our SA-GC layer consists of one self attention layer and 5 graph convolution (GC) layers.   the construction, we detail the pipeline of the construction with an example illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>The Self attention layer Similar to standard self attention <ref type="bibr" target="#b32">[33]</ref>, our self-attention layer takes a feature vector H ∈ R T ×N as input, and outputs a self-attention matrix S mask , representing how much influence of the past frames on the current frame. <ref type="figure" target="#fig_2">Fig. 4</ref> shows one of our generated Direction sequences and its corresponding attention score vector's heat map for the last frame (the 49th frame). After the self attention layer, we select top 5 past frames with the highest attention scores (only keep 6 elements in each row of the S mask matrix). As we could see from the example in <ref type="figure" target="#fig_2">Fig. 4</ref>, the selected 5 past frames have the highest similarity with the 49th frame. The skeleton in the 49th frame keeps the red arm up and keeps the blue arm bent down. Looking back to past frames, frames before the 21st lift up its red arm. Frames between the 24th to the 31st frame have the similar blue arm pose as the 49th frame. Our attention identifies frames 17th, 20th, 24th, 30th and 31st as the most relevant frames to be attended according to the learned attention matrix S mask .</p><p>The GC layers As illustrated, the self-attention layer is followed by 5 GC layers. After selection, we will connect each node of the 49th frame with the corresponding node in the selected 5 frames and assign edge weights with the corresponding self-attention scores. <ref type="figure" target="#fig_3">Fig. 5</ref> shows information passing path through our SA-GC layer at the node neck. The left plot of <ref type="figure" target="#fig_3">Fig. 5</ref> shows that after one GC layer, the 49th neck node can gather information from neck nodes of five selected frames and four neighbor nodes in its own frame. The right plot of <ref type="figure" target="#fig_3">Fig. 5</ref> shows that after the two SA-GC layers, the 49th neck node can gather information for the five nodes of the selected past four frames and seven nodes of its own frame. It is worth noting that nodes in different frame will have distinct attention score for edges in both spatial space and temporal space, thus they will have their particular edge weights through our SA-GC layer.</p><p>Implementing self-attention based GCN In our case, we consider all joints in an action sequence, ending up with a 2D adjacent matrix with both row size and column size N * T . To this end, we first use A ∈ R N ×N to denote the adjacent matrix of intra-frame, which is constructed by strictly following the structure of a skeleton, e.g., the "head" node is connected to the "neck" node. After adding self connections I, the intra-frame adjacency matrix will bē A = A + I. We then define an initial adjacency matrix of a whole sequence as:Ã =     Ā I · · · I IĀ · · · I . . . . . . . . .</p><formula xml:id="formula_3">I I · · ·Ā      (N * T )×(N * T ) ,<label>(2)</label></formula><p>where I is used to represent connecting each node with all of the corresponding nodes in the temporal space, (N * T )×(N * T ) means A is a 2D matrix with both row size and column size N * T , both N and T are numbers, * means multiply operation. The adjacency matrixÃ essentially means each node in one frame is connected to the corresponding node in the temporal space. At the same time, it also connects to the neighboring nodes in spatial space encoded byĀ. Next, we propose to use self-attention to prune the action graph. The idea is to learn a set of attention scores encoding the relevance of each frame w.r.t. the current frame, and only choose the top-K frames in the temporal space. Specifically, we adopt a similar implementation of the scaled dot-product attention as in <ref type="bibr" target="#b32">[33]</ref>. The input of the self-attention layer is represented as H {h 0 , h 1 , · · · , h T −1 }, where h t ∈ R N represents the hidden state vector at time t with N nodes. Following the self-attention in <ref type="bibr" target="#b32">[33]</ref>, Q, K and V are given as:</p><formula xml:id="formula_4">Q = W q H , K = W k H , V = W v H ,<label>(3)</label></formula><p>where W q , W k and W v are projection weights. The attention score S ∈ R T ×T and the attention layer's output H in are calculated as:</p><formula xml:id="formula_5">S = softmax QK T ; H in = SV<label>(4)</label></formula><p>In the task of action generation, we need to modify S as a masked attention S mask which prevents current frame from attending to subsequent frames</p><formula xml:id="formula_6">S mask =     s0,0 0 · · · 0 s1,0 s1,1 · · · 0 . . . . . . . . . sT −1,0 sT −1,1 · · · sT −1,T −1     T ×T ,<label>(5)</label></formula><p>where the element s m,n denotes the n-th frame's influence on the m-th frame and values in the upper triangle are all equal to 0. To enforce the pruning, we further select the top K scores in each row of the S mask and set the other elements to be 0. Note that, if the number of non-zero elements in some rows is less than K, we will keep all the non-zero elements. Finally, the adjacent matrix is constructed as </p><formula xml:id="formula_7">A s = S mask Ã     s0,0 * Ā 0 · · · 0 s1,0 * I s1,1 * Ā · · · 0 . . . . . . . . .</formula><p>Consequently, the output (before activation) of the self-attention based graph convolutional layer becomes:</p><formula xml:id="formula_9">H (1) = D −1Ã s H in W ,<label>(7)</label></formula><p>where D ii = jÃ ij s represents diagonal node degree matrix for normalizingÃ s , H (1) is the hidden state after first GC layer in <ref type="figure" target="#fig_0">Fig. 2</ref>. We will conduct graph convolution operation in equation 7 using sameÃ s for five times. The output of the fifth GC layer in SA-GC layer is H <ref type="bibr" target="#b4">(5)</ref> . After a linear layer, we get the output of the generator, which is a generated sequence x ∈ R T ×N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments to evaluate the proposed method on two standard skeleton-based human-action benchmarks, the Human-3.6m dataset <ref type="bibr" target="#b15">[16]</ref> and the NTU RGB+D dataset <ref type="bibr" target="#b28">[29]</ref>. Several stateof-the-art methods are used for comparison, including <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40]</ref>. Following <ref type="bibr" target="#b39">[40]</ref>, the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b11">[12]</ref> is adopted to measure the quality of generated actions. Further, we pre-train a classifier on training set to test the recognition accuracy of generated actions. We also conduct human evaluation on the Amazon Mechanical Turk (AMT) to access the perceptual quality of generated sequences. To examine the functionality of each component of the proposed model, we also perform detailed ablation studies on the Human-3.6m dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Human-3.6m Following the same pre-processing procedure in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>, 50 Hz video frames are down-sampled to <ref type="bibr" target="#b15">16</ref> Hz to obtain representative and larger variation 2D human motions. The joint information consists of 2-D locations of 15 major body joints. Ten distinctive classes of actions are used in the following experiments, including sitting, sitting down, discussion, walking, greeting, direction, phoning, eating, smoking and posing.</p><p>NTU RGB+D This dataset contains 56,000 video clips on 60 classes performed by 40 subjects and recorded with 3 different camera views. Compared with Human-3.6m, it can provide more samples in each class and much more intra-class variations. We select ten classes of motions and obtain their 2-D coordinates of 25 body joints following the same setting in <ref type="bibr" target="#b39">[40]</ref>, including drinking water, jump up, make phone call, hand waving, standing up, wear jacket, sitting down, throw, cross hand in front and kicking something. We then apply two commonly used benchmarks for a further evaluation in the ten classes: (i)cross-view : the training set contains actions captured by two cameras and remaining data are left for testing. (ii)cross-subject: action clips performed by 20 subjects are randomly picked for training and another 20 subjects are reserved for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>Following <ref type="bibr" target="#b39">[40]</ref>, we set the action sequence length for both datasets to be 50. The image discriminator randomly selects 20 frames from every generated sequence and training sequences as the input. The SA-GC layer selects top 5 past frames to construct an adjacency matrixÃ s . We set batch size for training to be 100, for testing to be 1000, and the learning rate to be 0.0002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Maximum Mean Discrepancy The MMD metric is based on a twosample test to measure the similarity between two distributions P(x) and Q(y), based on samples x ∼ P(x) and y ∼ Q(y). It is widely used to measure the quality of generated samples compared with real data in deep generative model <ref type="bibr" target="#b48">[49]</ref> and Bayesian sampling <ref type="bibr" target="#b13">[14]</ref>. The metric has also been applied to evaluate the similarity between generated actions and the ground truth in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>, which has been proved consistent with human evaluation. As motion dynamics are in the form of sequential data points, we denote MMD avg as the average MMD over each frame and MMD seq to denote the MMD over whole sequences.</p><p>Recognition Accuracy Apart from using MMD to evaluate the model performance, we also pre-train a recognition network on the training data to compute the classification accuracy of generated samples. The recognition network exactly follows the video discriminator except for the last softmax layer. This evaluation metrics can examine whether the conditional generated samples are actually residing in the same manifold as the ground truth and can be correctly recognized. Details are given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>We compare our method with six baselines. We first consider the model in <ref type="bibr" target="#b41">[42]</ref>, which can be used to generate long-term skeletonbased actions in an end-to-end manner. This includes three training alternatives: end-to-end (E2E), E2E prediction with visual analogy network (EPVA) and EPVA with adversarial loss (adv-EPVA). The second baseline <ref type="bibr" target="#b12">[13]</ref> is based on VAE, called the SkeletonVAE, which improves previous motion generation methods significantly. Finally, two most recent strong baselines are considered, including the previous state-of-the-art method <ref type="bibr" target="#b6">[7]</ref> and an improved version <ref type="bibr" target="#b39">[40]</ref> with an auxiliary classifier. The latter utilizes a Single Pose Training stage and a Pose Sequence Generation stage to produce high-quality motions. These two baselines are respectively referred to as SkeletonGAN and c-SkeletonGAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative results</head><p>Our SA-GCN model shows superior quantitative results in terms of both MMD and recognition accuracies on the two datasets, compared with related baseline models.</p><p>Human-3.6m <ref type="table" target="#tab_1">Table 1</ref> shows MMD results of our model and the baselines on Human-3.6m. With structure information considered, our model achieves significant performance gains over all baselines, which even without the need of an inefficient pre-training stage. The recognition accuracies are reported in <ref type="table" target="#tab_2">Table 2</ref>. Similarly, our model consistently outperforms three baselines by a large margin. Please note none information of the generated actions are used in the pretrained classifier, thus we avocate that the relatively low recognition accuracies are indeed reasonable. On the other hand, this also indicates that existing action generation models are still far from satisfactory.</p><p>NTU RGB+D This dataset is more challenging, which contains more body joints and action variations. In the experiments, we find that three models (E2E, EPVA, adv-EPVA <ref type="bibr" target="#b41">[42]</ref>) fail to generate any interpretable action sequences. As a result, we only present MMD results for the other three baselines in <ref type="table" target="#tab_3">Table 3</ref>. Again, the proposed method performs the best among all models under cross-view and cross-subject settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results</head><p>We present some generated actions in Human-3.6m dataset and NTU RGB+D dateset in <ref type="figure">Fig. 7</ref>(first and third row) and <ref type="figure">Fig. 6</ref> respectively. It is easy to see that our model can generate very realistic and easily recognizable actions. We also plot  action trajectories on a projected space by t-SNE <ref type="bibr" target="#b22">[23]</ref> for each generated action class on the Human-3.6m dataset in <ref type="figure" target="#fig_7">Fig. 9</ref>. It is observed that a group of actions, i.e., directions, discussion, greeting, are close to each other, and so is the group sitting, sitting down, eating; while actions smoking and sitting down are far away. These are consistent with what we have observed in the ground truth.</p><p>Smooth action generation Humans are capable of switching two actions very smoothly and naturally. For instance, a person can show others directions and walking at the same time. In this part, we verify that our model is expressive enough to perform such transitions as humans do. We use <ref type="bibr" target="#b7">(8)</ref> to produce a smooth action transition between action classes y 1 and y 2 with a smoothing parameter λ ∈ [0, 1]. We generate 100 video clips with every mix and apply t-SNE <ref type="bibr" target="#b22">[23]</ref> to project the averaged sequences to a 1D mani-   fold. The histogram of various mixed actions is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. As we decrease λ, the mode (action) gradually moves from directions towards walking, meaning that our model can produce very smooth transitions when interpolating between the two actions. <ref type="figure">Fig. 7</ref> illustrates as randomly selected samples.</p><p>y mix = λy 1 + (1 − λ)y 2 ; x mix = G(z; y mix ), z ∼ N (0, 1) (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Our key innovation in our model is the SA-GC layer. As a result, we conduct detailed experiments to verify the effectiveness and usefulness of our self-attention based graph convolutional layer on Human 3.6m dataset. Since the self-attention layer has already been proved to be effective for sequential data, we keep the self-attention layer for all the following baselines. Without special mentioning, we keep all the other parts of the model to be the same.</p><p>Baseline 1: replace GCN layers with CNN layers We replace 5 GCN layers with 5 CNN layers using the same hidden dimension and kernel size.</p><p>Baseline 2: without the inter-frame A matrix Based on our model, we drop the attention connections to past frames. That setting is the same as setting our top k to be 0 in our SA-GC layer. Under this Baseline, each frame in the sequence will be an independent graph for graph convolutional layer.</p><p>Baseline 3: replace self-attention based GCN layers with the ST-GC layers <ref type="bibr" target="#b43">[44]</ref> The ST-GC layer leverage graph convolution for skeleton-based action recognition. Each ST-GC layer combines one graph convolution layer for learning intra-frame skeleton and one 1D convolutional layer for feature aggregation in the temporal space. The Fully Connected model described in <ref type="figure" target="#fig_4">Fig. 1</ref> is not applicable and can not scale to long sequences because it demands excessive amount of memory and computational resources. The results of above three baselines are shown in <ref type="table" target="#tab_4">Table 4</ref>. Comparing with base-line2 and baseline3, we can see that adding the adjacency matrix makes the model harder to train compared with CNN. However, our proposed self-attention can mitigate the difficulties and surpass standard CNN method on the skeleton based action generation task with much lower MMD scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Human Evaluation</head><p>We finally conduct perceptual human evaluations in the AMT platform. Four models are trained on the Human-3.6m dataset, including SkeletonVAE, Skeleton-GAN, c-SkeletonGAN and our SA-GCN. We then sample 100 action clips for each of the 10 action classes; 140 workers were asked to evaluate the quality of the generated sequences and score them in a range from 1 to 5. A higher score indicates a more realistic action clip. We only inform them of the action class and one real action clip to ensure proper judgements. The design detail is given in the Appendix. <ref type="table" target="#tab_5">Table 5</ref> demonstrates that our model is significantly better than other baselines in human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have presented the self-attention graph convolutional network (SA-GCN) to efficiently encode structure information into skeleton-based human action generation. Self-attention can capture long-range dependencies in continuous action sequences and learn to prune the dense action graph for efficient training. Further, the graph convolution is applied to seamlessly encode both spatial joints information and temporal dynamics information into the model. Based on these ideas, our model directly transforms noises to high-quality action sequences and can be trained end-toend. On two standard human action datasets, we observe a significant improvement of generation quality in terms of both quantitative and qualitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments Results</head><p>We further show some action samples on both the Human-3.6m dataset <ref type="bibr" target="#b15">[16]</ref> and the NTU RGB+D dataset <ref type="bibr" target="#b28">[29]</ref>. We sample one frame in terms of every two consecutive frames to show the whole sequence of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-3.6m</head><p>We show ten classes of action sequences: direction, discussion, eating, greeting, phoning, posing, sitting, sitting down, smoking and walking in <ref type="figure" target="#fig_0">Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14,  Fig. 15, Fig. 16, Fig. 17, Fig. 18</ref> and <ref type="figure" target="#fig_4">Fig. 19</ref> on Human-3.6m dataset. For each action class, we present three generated action sequences from random initialization.</p><p>NTU RGB+D We show ten classes of action sequences: drinking water, jumping up, kicking something, making phone call, sitting down, standing up, throwing, hand waving, wearing jacket and crossing hand in front in <ref type="figure" target="#fig_0">Fig. 20, Fig. 21, Fig. 22, Fig. 23, Fig. 24</ref>, <ref type="figure" target="#fig_0">Fig. 25, Fig. 26, Fig. 27, Fig. 28</ref> and <ref type="figure" target="#fig_0">Fig. 29</ref> on NTU RGB+D dataset. For each action class, we present two generated action sequences (the top two lines) for cross-view and two generated action sequences (the bottom two lines) for cross-subject from random initialization. <ref type="figure" target="#fig_4">Fig. 10</ref>: direction: this character is directing traffic. <ref type="figure" target="#fig_4">Fig. 11</ref>: discussion: this character is discussing issues with others. <ref type="figure" target="#fig_0">Fig. 12</ref>: eating: this character is sitting on the chair and having its lunch.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An illustration of the SA-GC layer.Ã andÃs are two adjacency matrices detailed in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The overall framework of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The pipeline of SA-GC layer. The top line shows frames out of every three consecutive frames from Human 3.6 Direction class. The heat map under these samples represent the corresponding attention scores for the 49th frame. The bottom line shows the top 5 frames with the highest attention scores. Green circles and orange circles show similarity between selected frames and our target frame (the 49th frame).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Information passing through SA-GC layers at the node "neck".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>sT − 1</head><label>1</label><figDesc>,0 * I sT −1,1 * I · · · sT −1,T −1 * Ā</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 Fig. 7 :</head><label>57</label><figDesc>Generated sequences of directions, walking and a mixed action with λ = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Histogram of mixed actions where each mode represents an action with a smoothing term λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Action trajectories onHuman-3.6m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :Fig. 14 :Fig. 15 :Fig. 16 :Fig. 17 :Fig. 18 :</head><label>131415161718</label><figDesc>greeting: this character is waving hands and greeting with other people. phoning: this character is making a phone call with other people. posing: this character is making some exaggerated poses to take photos. sitting: this character is sitting down on a chair. sitting down: this character is sitting down on the ground. smoking: this character is holding a cigarette in one hand and occasionally smokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 19 :Fig. 20 :Fig. 21 :</head><label>192021</label><figDesc>walking: this character is walking. drinking water : this character is holding a water bottle in one hand while drinking water. jumping up: this character is jumping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 22 :Fig. 23 :Fig. 24 :</head><label>222324</label><figDesc>kicking something: this character is kicking something. making phone call : this character is raising his mobile phone with one hand and is making a phone call. sitting down: this character is sitting down.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 25 :Fig. 26 :Fig. 27 :</head><label>252627</label><figDesc>standing up: this character is standing up. throwing: this character is throwing a ball. hand waving: this character is waving hands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 28 :</head><label>28</label><figDesc>wearing jacket: this character is wearing jacket with its two arms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 29 :</head><label>29</label><figDesc>crossing hand in front: the final position for this action is making this character's arm crossing in front.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model comparisons in terms of MMD on Human-3.6m.</figDesc><table><row><cell>Models</cell><cell cols="3">Pretrain MMDavg ↓ MMDseq ↓</cell></row><row><cell>E2E [42]</cell><cell>No</cell><cell>0.991</cell><cell>0.805</cell></row><row><cell>EPVA [42]</cell><cell>No</cell><cell>0.996</cell><cell>0.806</cell></row><row><cell>adv-EPVA [42]</cell><cell>No</cell><cell>0.977</cell><cell>0.792</cell></row><row><cell>SkeletonVAE [13]</cell><cell>No</cell><cell>0.452</cell><cell>0.467</cell></row><row><cell>SkeletonGAN [7]</cell><cell>Yes</cell><cell>0.419</cell><cell>0.436</cell></row><row><cell cols="2">c-SkeletonGAN [40] Yes</cell><cell>0.195</cell><cell>0.218</cell></row><row><cell>Ours</cell><cell>No</cell><cell>0.146</cell><cell>0.134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Action recognition accuracy on the generated actions on the Human-3.6m.</figDesc><table><row><cell>Models</cell><cell cols="2">Direct Discuss Eat Greet Phone Pose Sit SitD Smoke Walk Average</cell></row><row><cell cols="2">SkeletonVAE 0.37</cell><cell>0.01 0.51 0.47 0.10 0.03 0.17 0.33 0.01 0.01 0.201</cell></row><row><cell cols="2">SkeletonGAN 0.35</cell><cell>0.29 0.72 0.66 0.46 0.09 0.32 0.71 0.14 0.02 0.376</cell></row><row><cell cols="2">c-SkeletonGAN 0.34</cell><cell>0.44 0.57 0.56 0.52 0.25 0.67 1.00 0.50 0.03 0.488</cell></row><row><cell>SA-GCN</cell><cell>0.42</cell><cell>0.40 0.78 0.55 0.72 0.61 0.95 0.79 0.52 0.18 0.593</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model comparisons in terms of MMD on NTU RGB+D. Randomly selected samples on NTU RGB+D dataset. Top: sitting down from cross-subject, Bottom: phoning from cross-view.</figDesc><table><row><cell>Models</cell><cell cols="2">cross-view</cell><cell cols="2">cross-subject</cell></row><row><cell></cell><cell cols="4">MMDavg ↓ MMDseq ↓ MMDavg ↓ MMDseq ↓</cell></row><row><cell>SkeletonVAE [13]</cell><cell>1.079</cell><cell>1.205</cell><cell>0.992</cell><cell>1.136</cell></row><row><cell>SkeletonGAN [7]</cell><cell>0.999</cell><cell>1.311</cell><cell>0.698</cell><cell>0.788</cell></row><row><cell>c-SkeletonGAN [40]</cell><cell>0.371</cell><cell>0.398</cell><cell>0.338</cell><cell>0.402</cell></row><row><cell>SA-GCN</cell><cell>0.316</cell><cell>0.335</cell><cell>0.285</cell><cell>0.299</cell></row><row><cell>Sitting Down</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Phoning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results.</figDesc><table><row><cell cols="3">Baselines MMDavg ↓ MMDseq ↓</cell></row><row><cell>Baseline 1</cell><cell>0.240</cell><cell>0.222</cell></row><row><cell>Baseline 2</cell><cell>0.915</cell><cell>0.922</cell></row><row><cell>Baseline 3</cell><cell>0.580</cell><cell>0.595</cell></row><row><cell>Ours</cell><cell>0.152</cell><cell>0.142</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>AMT Evaluations</figDesc><table><row><cell>Models</cell><cell>Evaluation Score↑</cell></row><row><cell>SkeletonVAE</cell><cell>2.401</cell></row><row><cell>SkeletonGAN</cell><cell>2.731</cell></row><row><cell>c-SkeletonGAN</cell><cell>3.157</cell></row><row><cell>SA-GCN</cell><cell>3.925</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.09561" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid dynamical models of human motion for the recognition of human gaits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="114" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09763</idno>
		<title level="m">Pixelsnail: An improved autoregressive generative model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<title level="m">Efficient video generation on complex datasets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A recurrent variational autoencoder for human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yearsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stein variational gradient descent without gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02775</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human action generation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kiasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Moirangthem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10416</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BNMW CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alice: Towards understanding adversarial learning for joint distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5495" to="5503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and inference in parametric switching linear dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="981" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two distributed-state models for generating high-dimensional time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1025" to="1068" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Topologically-constrained latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3332" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimizing walking controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimizing walking controllers for uncertain inputs and environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Catanzaro, B.: Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning diverse stochastic human-action generators by learning smooth latent transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10150</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining recurrent neural networks and adversarial training for human motion synthesis and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04768</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skeleton-aided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Feature quantization improves GAN training. ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-adversarially learned bayesian sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5893" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
