<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Face Manipulation Detection Through Ensemble of CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol√≤</forename><surname>Bonettini</surname></persName>
							<email>nicolo.bonettini@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><forename type="middle">Daniele</forename><surname>Cannas</surname></persName>
							<email>edoardodaniele.cannas@polimi.it</email>
							<affiliation key="aff1">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mandelli</surname></persName>
							<email>sara.mandelli@polimi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bondi</surname></persName>
							<email>luca.bondi@polimi.it</email>
							<affiliation key="aff3">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Bestagini</surname></persName>
							<email>paolo.bestagini@polimi.it</email>
							<affiliation key="aff4">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Tubaro</surname></persName>
							<email>stefano.tubaro@polimi.it</email>
							<affiliation key="aff5">
								<orgName type="institution">DEIB Politecnico di Milano Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Face Manipulation Detection Through Ensemble of CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-deepfake</term>
					<term>video forensics</term>
					<term>deep learning</term>
					<term>atten- tion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last few years, several techniques for facial manipulation in videos have been successfully developed and made available to the masses (i.e., FaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in video sequences with incredibly realistic results and a very little effort. Despite the usefulness of these tools in many fields, if used maliciously, they can have a significantly bad impact on society (e.g., fake news spreading, cyber bullying through fake revenge porn). The ability of objectively detecting whether a face has been manipulated in a video sequence is then a task of utmost importance.</p><p>In this paper, we tackle the problem of face manipulation detection in video sequences targeting modern facial manipulation techniques. In particular, we study the ensembling of different trained Convolutional Neural Network (CNN) models. In the proposed solution, different models are obtained starting from a base network (i.e., EfficientNetB4) making use of two different concepts: (i) attention layers; (ii) siamese training. We show that combining these networks leads to promising face manipulation detection results on two publicly available datasets with more than 119000 videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Over the past few years, huge steps forward in the field of automatic video editing techniques have been made. In particular, great interest has been shown towards methods for facial manipulation <ref type="bibr" target="#b0">[1]</ref>. Just to name an example, it is nowadays possible to easily perform facial reenactment, i.e., transferring the facial expressions from one video to another one <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. This enables to change the identity of a speaker with very little effort.</p><p>Systems and tools for facial manipulations are now so advanced that even users without any previous experience in photo retouching and digital arts can use them. Indeed, code and libraries that work in an almost automatic fashion are more and more often made available to the public for free <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. On one hand, this technological advancement opens the door to new artistic possibilities (e.g., movie making, visual effect, visual arts, etc.). On the other hand, unfortunately, it also eases the generation of video forgeries by malicious users.</p><p>Fake news spreading and revenge porn are just a few of the possible malicious applications of advanced facial manipulation technology in the wrong hands. As the distribution of these kinds of manipulated videos indubitably leads to serious and dangerous consequences (e.g., diminished trust in media, targeted opinion formation, cyber bullying, etc.), the ability of detecting whether a face has been manipulated in a video sequence is becoming of paramount importance <ref type="bibr" target="#b5">[6]</ref>.</p><p>Detecting whether a video has been modified is not a novel issue per se. Multimedia forensics researchers have been working on this topic since many years, proposing different kinds of solutions to different problems <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. For instance, in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> the authors focus on studying the coding history of videos. The authors of <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> focus on localizing copy-move forgeries with block-based or dense techniques. In <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, different methods are proposed to detect frame duplication or deletion.</p><p>All the above-mentioned methods work according to a common principle: each non-reversible operation leaves a peculiar footprint that can be exposed to detect the specific editing. However, forensics footprints are often very subtle and hard to detect. This is the case of videos undergoing excessive compression, multiple editing operations at once, or strong downsampling <ref type="bibr" target="#b7">[8]</ref>. This is also the case of very realistic forgeries operated through methods that are hard to formally model. For this reason, modern facial manipulation techniques are very challenging to detect from the forensic perspective <ref type="bibr" target="#b15">[16]</ref>. As a matter of fact, many different face manipulation techniques exist (i.e., there is not a unique model explaining these forgeries). Moreover, they often operate on small video regions only (i.e., the face or part of it, and not the full frame). Finally, these kinds of manipulated videos are typically shared through social platforms that apply resizing as well as coding steps, further hindering classic forensic detectors performance.</p><p>In this paper, we tackle the problem of detecting facial manipulation operated through modern solutions. In particular, we focus on all the manipulation techniques reported in <ref type="bibr" target="#b16">[17]</ref> (i.e., deepfakes, Face2Face, FaceSwap and NeuralTextures) and in the Facebook DFDC started on Kaggle in December 2019 <ref type="bibr" target="#b17">[18]</ref>. Within this context, we study the possibility of using an ensemble of different CNN trained models. We consider EfficientNetB4 <ref type="bibr" target="#b18">[19]</ref> and propose a modified version of it obtained by adding an attention mechanism <ref type="bibr" target="#b19">[20]</ref>. Moreover, for each network, we investigate two different training strategies, one of which is based on the siamese paradigm.</p><p>As one of the big challenges is to be able to run a forensic detector in real-world scenarios, we develop our solution keeping computational complexity at bay. Specifically, we consider the strong hardware and time constraints imposed by the DFDC <ref type="bibr" target="#b17">[18]</ref>. This means that the proposed solution must be able to analyze 4 000 videos in less than 9 hours using at most a single NVIDIA P100 GPU. Moreover, the trained models must occupy less than 1GB of disk space.</p><p>Evaluation is performed on two disjoint datasets: FF++ <ref type="bibr" target="#b16">[17]</ref>, which has been recently proposed as a public benchmark; DFDC <ref type="bibr" target="#b17">[18]</ref>, which has been released as part of the DFDC Kaggle competition. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts a few examples of faces extracted from the two datasets, reporting pristine and manipulated samples. Results show that the proposed attentionbased modification as well as the siamese training strategy help the ensemble system in outperforming the baseline reported in FF++ on both datasets. Moreover, the proposed attentionbased solution provides interesting insights on which part of each frame drives face manipulation detection, thus enabling a small step forward towards the explainability of the network results.</p><p>The rest of the paper is structured as follows. Section II reports a literature review of the latest related work. Section III reports all the details about the proposed method. Section IV details the experimental setup. Section V collects all the achieved results. Finally, Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Multiple video forensics techniques have been proposed for a variety of tasks in the last few years <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. However, since the forensics community has become aware of the potential social risks introduced by the latest facial manipulation techniques, many detection algorithms have been proposed to detect this kind of forgeries <ref type="bibr" target="#b15">[16]</ref>.</p><p>Some of the proposed techniques focus on a CNN-based frame-by-frame analysis. For instance, MesoNet is proposed in <ref type="bibr" target="#b20">[21]</ref>. This is a relatively shallow CNN with the goal of detecting fake faces. The authors of <ref type="bibr" target="#b16">[17]</ref> have shown that this network is outperformed by XceptionNet retrained on purpose.</p><p>Alternative techniques exploit also the temporal evolution of video frames through Long Short-Term Memory (LSTM) analysis. This is the case of <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref>, which first extract a series of frame-based features, and then put them together with a recurrent mechanism.</p><p>Other methods leverage specific processing traces. This is the case of <ref type="bibr" target="#b23">[24]</ref>, where the authors exploit the fact that deepfake donor faces are warped in order to realistically stick to the host video. They therefore propose a detector that captures warping traces.</p><p>In order to overcome the limitation of pixel analysis, other techniques are based on a semantic analysis of the frames. In <ref type="bibr" target="#b24">[25]</ref>, a technique that learns to distinguish natural and fake head pose is proposed. Conversely, the authors of <ref type="bibr" target="#b25">[26]</ref> focus on inconsistent lighting effects. Alternatively, <ref type="bibr" target="#b26">[27]</ref> reports a methodology based on eye blinking analysis. Indeed, the first generation of deepfake videos was showing some eye artifacts that could be captured with this method. Unfortunately, the more the manipulation techniques produce realistic results, the less semantic methods work.</p><p>Finally, other techniques provide additional localization information. The authors of <ref type="bibr" target="#b27">[28]</ref> propose a multi-task learning method that provides a detection score together with a segmentation mask. Alternatively, in <ref type="bibr" target="#b28">[29]</ref>, an attention mechanism is proposed.</p><p>Inspired by the state of the art, in this paper we focus on network ensembles, proposing a solution that works on multiple datasets and is sufficiently lightweight according to DFDC competition rules <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we describe our proposed method for video face manipulation detection, i.e., given a video frame, to detect whether faces are real (pristine) or fake.</p><p>The proposed method is based on the concept of ensembling. Indeed, it is well-known that model ensembling may lead to better prediction performance. We therefore focus on investigating whether and how it is possible to train different CNN-based classifiers to capture different high-level semantic information that complement one another, thus positively contributing to the ensemble for this specific problem.</p><p>To do so, we consider as starting point the EfficientNet family of models, proposed in <ref type="bibr" target="#b18">[19]</ref> as a novel approach for the automatic scaling of CNNs. This set of architectures achieves better accuracy and efficiency with respect to other state-of-the-art CNNs, and actually revealed to be very useful to fulfil hardware and time constraints imposed by DFDC. Given an EfficientNet architecture, we propose to follow two paths to make the model beneficial for the ensambling. On one hand, we propose to include an attention mechanism, which also provides the analyst with a method to infer which portion of the investigated video is more informative for the classification process. On the other hand, we investigate how siamese training strategies can be included into the learning process for extrapolating additional information about the data.</p><p>In the following, more details are provided about Efficient-Net architecture with the proposed attention mechanism and the network training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EfficientNet and attention mechanism</head><p>Among the family of EfficientNet models, we choose the EfficientNetB4 as the baseline for our work, motivated by the good trade-off offered by this architecture in terms of dimensions (i.e., number of parameters), run time (i.e., FLOPS cost) and classification performance. As reported in <ref type="bibr" target="#b18">[19]</ref>, with 19 millions of parameters and 4.2 billions of FLOPS, Effi-cientNetB4 reaches the 83.8% top-1 accuracy on the ImageNet <ref type="bibr" target="#b29">[30]</ref> dataset. On the same dataset, XceptionNet, used as face manipulation detection baseline method by the authors of <ref type="bibr" target="#b16">[17]</ref>, reaches the 79% top-1 accuracy at the expense of 23 millions parameters and 8.4 billions FLOPS.</p><p>EfficientNetB4 architecture is represented within the blue block in <ref type="figure">Fig. 2</ref>, where all layers are defined using the same nomenclature introduced in <ref type="bibr" target="#b18">[19]</ref>.</p><p>The input to the network is a squared color image I, i.e., in our experiments, the face extracted from a video frame. As a matter of fact, authors of <ref type="bibr" target="#b16">[17]</ref> recommend to track face information instead of using the full frame as input to the network for increasing the classification accuracy. Moreover, faces can be easily extracted from frames using any of the widely available face detectors proposed in the literature <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The network output is a feature vector of 1792 elements, defined as f (I). The final score related to the face is the result of a classification layer.</p><p>The proposed variant of the standard EfficientNetB4 architecture is inspired by the several contributions in the natural language processing and computer vision fields that make use of attention mechanisms. Works such as the transformer <ref type="bibr" target="#b19">[20]</ref> and residual attention networks <ref type="bibr" target="#b32">[33]</ref> show how it is possible for a neural network to learn which part of its input (being an image or a sequence of words) is more relevant for accomplishing the task at hand. In the context of video deepfake detection, it would be of great benefit to discover which portion of the input gave the network more information for its decision making process. We thus explicitly implement an attention mechanism similar to the one already exploited by the EfficientNet itself, as well as to the self-attention mechanisms presented in <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref>: 1) we select the feature maps extracted by the Efficient-NetB4 up to a certain layer, chosen such that these features provide sufficient information on the input frame without being too detailed or, on the contrary, too unrefined. To this purpose, we select the output features at the third MBConv block which have size 28√ó28√ó56; 2) we process the feature maps with a single convolutional layer with kernel size 1 followed by a Sigmoid activation function to obtain a single attention map; 3) we multiply the attention map for each of the feature maps at the selected layer. For clarity's sake, the attention-based module is depicted in the red block of <ref type="figure">Fig. 2</ref>.</p><p>On one hand, this simple mechanism enables the network to focus only on the most relevant portions of the feature maps, on the other hand it provides us with a deeper insight on which parts of the input the network assumes as the most informative. Indeed, the obtained attention map can be easily mapped to the input sample, highlighting which elements of it have been given more importance by the network. The result of the attention block is finally processed by the remaining layers of EfficientNetB4. The whole training procedure can be executed end-to-end, and we call the resulting network EfficientNetB4Att.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network training</head><p>We train each model according to two different training paradigms: (i) end-to-end, and; (ii) siamese. The former represents a more classical training strategy, also used as evaluation metrics in the contest of DFDC. The latter aims at exploiting the generalization capabilities offered by the networks in order to obtain a feature descriptor that privileges the similarity between samples belonging to the same class. The ultimate goal is to learn a representation in the encoding space of the network's layers that well separates samples (i.e., faces) of the real and fake class.</p><p>1) End-to-end training: We feed the network with a sample face, and the network returns a face-related score≈∑. Notice that this score is not passed through a Sigmoid activation function yet. The weights update is led by the commonly used LogLoss function</p><formula xml:id="formula_0">L L = ‚àí 1 N N i=1 [y i log (S(≈∑ i )) + (1 ‚àí y i ) log (1 ‚àí S(≈∑ i ))] ,<label>(1)</label></formula><p>where≈∑ i represents the i-th face score, y i ‚àà {0, 1} the related face label. Specifically, label 0 is associated with faces coming from real pristine videos and label 1 with fake videos. N is the total number of faces used for training and S (¬∑) is the Sigmoid function.</p><p>2) Siamese training: Inspired by computer vision works that generate local feature descriptors using CNNs, we adopt the triplet margin loss, first proposed in <ref type="bibr" target="#b34">[35]</ref>. Recalling that f (I) is the non-linear encoding obtained by the network for an input face I (see <ref type="figure">Fig. 2</ref>), being ¬∑ 2 the L 2 norm, the triplet margin loss is defined as</p><formula xml:id="formula_1">L T = max(0, ¬µ + Œ¥ + ‚àí Œ¥ ‚àí ),<label>(2)</label></formula><p>with</p><formula xml:id="formula_2">Œ¥ + = f (I a ) ‚àí f (I p ) 2 , Œ¥ ‚àí = f (I a ) ‚àí f (I n ) 2</formula><p>and ¬µ is a strictly positive margin. In this case I a , I p and I n are, respectively:</p><p>‚Ä¢ I a the anchor sample (i.e., a real face); ‚Ä¢ I p a positive sample, belonging to the same class as I a (i.e., another real face); ‚Ä¢ I n a negative sample, belonging to a different class than I a (i.e., a fake face).</p><p>We then finalize the training by finetuning a simple classification layer on top of the network, following the end-to-end approach described before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section we report all the details regarding the used datasets and experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We test the proposed method on two different datasets: FF++ <ref type="bibr" target="#b16">[17]</ref>; DFDC <ref type="bibr" target="#b17">[18]</ref>.</p><p>FF++ is a large-scale facial manipulation dataset generated using automated state-of-the-art video editing methods. In detail, two classical computer graphics approaches are used, i.e., Face2Face <ref type="bibr" target="#b1">[2]</ref> and FaceSwap <ref type="bibr" target="#b4">[5]</ref>, together with two learning-based strategies, i.e., DeepFakes <ref type="bibr" target="#b3">[4]</ref> and NeuralTextures <ref type="bibr" target="#b2">[3]</ref>. Every method is applied to 1000 high quality pristine videos downloaded from YouTube, manually selected to present nearly front-facing subjects without occlusions. All the sequences contain at least 280 frames. Eventually, a database of more than 1.8 million images from 4000 manipulated videos is built. In order to simulate a realistic setting, videos are compressed using the H.264 codec. High quality as well as low quality videos are generated using a constant rate quantization parameter equal to 23 and 40, respectively.</p><p>DFDC is the training dataset released for the homologous Kaggle challenge. It is composed by more than 119 000 video sequences, created specifically for this challenge, representing both real and fake videos. The real videos are sequences of actors taking into account diversity in several axes (gender, skintone, age, etc.) recorded with arbitrary backgrounds to bring visual variability. The fake videos are created starting from the real ones and applying different DeepFake techniques, e.g., different face swap algorithms. Notice that we do not know the precise algorithms used to generate fake videos, since for the time being the complete dataset (i.e., with the public and private testing sequences and possibly an explanation of the creation procedure) has not been released yet. The sequence length is roughly 300 frames, and the classes are strongly unbalanced towards the fake one, counting roughly 100 000 fakes and 19 000 reals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Networks</head><p>In our experiments, we consider the following networks:</p><p>‚Ä¢ XceptionNet, since it is the best performing model used in <ref type="bibr" target="#b16">[17]</ref>, thus being the natural yardstick for our experimental campaign; ‚Ä¢ EfficentNetB4, as it achieves better accuracy and efficiency than other existing methods <ref type="bibr" target="#b18">[19]</ref>; ‚Ä¢ EfficentNetB4Att, which should discriminate relevant parts of the face sample from irrelevant ones. Each model is trained and tested separately over both the considered datasets. Specifically, regarding FF++, we consider only videos generated with constant rate quantization equal to 23. XceptionNet is trained using the same approach of <ref type="bibr" target="#b16">[17]</ref>, whereas the two EfficientNet models are trained following the end-to-end as well as the siamese fashion described in Section III-B. In doing so, we end up with 4 trained models: Effi-cientNetB4 and EfficientNetB4Att which are trained with the classical end-to-end approach, together with EfficientNetB4ST and EfficientNetB4AttST, trained using the siamese strategy. All these EfficientNetB4-derived models can contribute to the final ensembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Setup</head><p>We adopt a different split policy for each dataset. We split DFDC according to its folder structure, using the first 35 folders for training, folders from 36 to 40 for validation and the last 10 folders for testing. Regarding FF++, we use a similar split as in <ref type="bibr" target="#b16">[17]</ref> selecting 720 videos for training, 140 for validation and 140 for test from the pool of original sequences taken from YouTube. The corresponding fake videos are assigned to the same split. All the results are shown on the test sets.</p><p>In our experiments, we only consider a limited number of frames for each video. In training phase, this choice is motivated by two main considerations: (i) when using a really small amount of frames per video, there is a strong tendency to overfit; (ii) increasing the number of frames does not improve  performances in a justifiable manner. This phenomenon can be noticed in <ref type="figure" target="#fig_1">Fig. 3</ref>, which reports training and validation losses as a function of training iterations, selecting a variable amount of frames per video. It is worth noting that the minimum validation loss does not improve selecting 15 frames per video instead of 32, however choosing 32 frames per video helps to prevent overfitting. For testing, we should also take into account the hardware and time constraints imposed by the DFDC challenge. With this in mind, we limit the number of analyzed frames from each sequence to 32 for both training and testing phases. Even in this setting, the dimensions of the datasets remain remarkable: for the FF++, we end up with roughly 1.6 million images, while for the DFDC with 3.4 million frames.</p><p>In this perspective, we can further reduce the amount of data processed by the networks by recalling that not all the frame information is useful for the deepfake detection process <ref type="bibr" target="#b16">[17]</ref>. Indeed, we can mainly focus our analysis on the region where the face of the subject is located. Consequently, as a pre-processing step, we extract from each frame the faces of the scene subjects using the BlazeFace extractor <ref type="bibr" target="#b31">[32]</ref>, that, in our experiments, proved to be faster than the MTCNN detector <ref type="bibr" target="#b30">[31]</ref> used by the authors of <ref type="bibr" target="#b16">[17]</ref>. In case more than one face is detected, we keep the face with the best confidence score. The resulting input for the networks is the squared color image I introduced in section III, of size 224 √ó 224 pixel.</p><p>During training and validation, to make our models more robust, we perform data augmentation operations on the input faces. In particular, we randomly apply downscaling, horizontal flipping, random brightness contrast, hue saturation, noise addition and finally JPEG compression. Specifically, we resort to Albumentation <ref type="bibr" target="#b35">[36]</ref> as our data-augmentation library, while we use Pytorch <ref type="bibr" target="#b36">[37]</ref> as Deep Learning framework. We train the models using Adam <ref type="bibr" target="#b37">[38]</ref> optimizer with hyperparameters equal to Œ≤ 1 = 0.9, Œ≤ 2 = 0.999, = 10 ‚àí8 , and initial learning rate equal to 10 ‚àí5 .</p><p>Independently from the used training strategy, given the size of the datasets, we never train our networks for a complete epoch. Specifically:</p><p>‚Ä¢ for the end-to-end training, we either train for a maximum of 20k iterations, indicating as iteration the processing of a batch of 32 faces (16 real, 16 fake) taken randomly and evenly across all the videos of the train split, or until reaching a plateau on the validation loss. Validation of the model in this context is performed every 500 training iterations, on 6000 samples taken again evenly and randomly across all videos of the validation set. The initial learning rate is reduced of a 0.1 factor if the validation loss does not decrease after 10 validation routines (5000 training iterations), and the training is stopped when we reach a minimum learning rate of 1 √ó 10 ‚àí10 ; ‚Ä¢ for the siamese training, the feature extractor is trained using the same number of iterations, validation routine and learning rate scheduling of the end-to-end training.</p><p>The main difference lies in the different loss function used (as explained in Section III), and in the composition of the batch, which in this case is made by 12 triplets of samples (6 real-fake-fake, 6 fake-fake-real) selected across all videos of the set considered. Regarding the parameter ¬µ in (2), we set it to 1 after some preliminary experiments. The fine-tuning of the classification layer is then executed in a successive step following the end-toend training paradigm with the hyperparameters specified above.</p><p>We finally run our experiments on a machine equipped with an Intel Xeon E5-2687W-v4 and a NVIDIA Titan V. The code to replicate our tests is freely available at https://github.com/ polimi-ispl/icpr2020dfdc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section we collect all the results obtained during our experimental campaign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EfficientNetB4Att explainability</head><p>In order to show the effectiveness of the attention mechanism in extracting the most informative content of faces, we evaluate the attention map computed on a few faces of FF++. Referring to <ref type="figure">Fig. 2</ref>, we select the output of the Sigmoid layer in the attention block, which is a 2D map with size 28 √ó 28. Then, we up-scale it to the input face size (224 √ó 224), and superimpose this to the input face. Results are reported in <ref type="figure" target="#fig_2">Fig. 4</ref>. It is worth noting that this simple attention mechanism enables to highlight the most detailed portion of faces, e.g., eyes, mouth, nose and ears. On the contrary, flat regions (where gradients are small) are not informative for the network. As a matter of fact, it has been shown several times that artifacts of deepfake generation methods are mostly localized around facial features <ref type="bibr" target="#b15">[16]</ref>. For instance, roughly modeled eyes and teeth, showing excessively white regions, are still the main trademarks of these methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Siamese features</head><p>In order to understand whether the features produced by the encoding of the network when trained in siamese fashion are discriminatory for the task, we computed a projection over a reduced space using the well known algorithm t-SNE <ref type="bibr" target="#b38">[39]</ref>. In <ref type="figure" target="#fig_3">Fig. 5</ref> we show the projection obtained by means of EfficientNetB4Att starting from 20 FF++ videos. We can clearly see how frames of the same videos clusters into small sub-regions. More importantly, all the real samples cluster into the top region of the chart, whereas the fake samples are in the bottom region. Frames of the same videos clusters into smaller sub-regions. This justifies the choice to adopt this particular training paradigm in addition to the classical endto-end approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture independence</head><p>As we want to understand whether the different networks can be used in an ensemble, we explore whether the scores extracted by each model are independent to some extent.</p><p>In <ref type="figure" target="#fig_4">Fig. 6</ref>, all plots outside of the main diagonal show that different networks provide slightly different scores for each frame. Indeed, the point clouds do not perfectly align on a shape that can be easily described by a simple relation. This motivates us in using the different trained models in an ensemble way. If all networks were perfectly correlated, this would not be reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Face manipulation detection capability</head><p>In this section, we report the average results achieved by the baseline network (i.e., XceptionNet) and the 4 proposed models (i.e., EfficientNetB4, EfficientNetB4Att, EfficientNetB4ST and EfficientNetB4AttST). We also verify our guess behind the use of an ensemble, specifically combining two, three or even all the proposed models. In this case, the final score associated with a face is simply computed as the average between the scores returned by the single models.</p><p>In <ref type="table" target="#tab_2">Table I</ref> we report the AUC (computed binarizing the network output with different thresholds) and LogLoss obtained in our experiments. Results are provided in a per-frame fashion. Analyzing these results, it is worth noting that the strategy of model ensembling generally awards in terms of performances. As somehow expected, best top-3 results are always reached by a combination of 2 or more networks, meaning that network fusion helps both the accuracy of the deepfake detection (estimated by means of AUC) and the quality of the detection (estimated by means of LogLoss measure). Indeed, on both datasets, LogLoss and AUC are always better than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Kaggle results</head><p>In order to gain a deeper insight on the proposed solution performance, we also participated to the DFDC challenge on Kaggle <ref type="bibr" target="#b17">[18]</ref> as ISPL team. The ultimate goal of the competition was to build a system able to tell whether a video is real or fake. The DFDC dataset used in this paper represents the training dataset released by the competition host, while the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Being able to detect whether a video contains manipulated content is nowadays of paramount importance, given the significant impact of videos in everyday life and in mass communications. In this vein, we tackle the detection of facial manipulation in video sequences, targeting classical computer graphics as well as deep learning generated fake videos.</p><p>The proposed method takes inspiration from the family of EfficientNet models and improves upon a recently proposed solution, investigating an ensemble of models trained using two main concepts: (i) an attention mechanism which generates a human comprehensible inference of the model, increasing the learning capability of the network at the same time; (ii) a triplet siamese training strategy which extracts deep features from data to achieve better classification performances.</p><p>Results evaluated over two publicly available datasets containing almost 120 000 videos reveals the proposed ensemble strategy as a valid solution for the goal of facial manipulation detection.</p><p>Future work will be devoted to the embedding of temporal information. As a matter of fact, intelligent voting schemes when more frames are analyzed at once might lead to an increased accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sample faces extracted from FF++ and DFDC datasets. For each pristine face, we show a corresponding fake sample generated from it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Training and validation loss curves for XceptionNet on FF++, while varying the number of frames per video (FPV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Effect of the attention on faces under analysis. Given some faces to analyze (top row), the attention network tends to select regions like eyes, mouth and nose (bottom row). Faces have been extracted from FF++ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>t-SNE visualization of features obtained by EfficientNetB4Att with siamese training. Faces have been extracted from FF++ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Pair-plot showing the score distribution for real (orange ‚Ä¢) and fake (blue ‚Ä¢) samples for each pair of networks on FF++ (a) and DFDC (b) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Blue block: EfficientNetB4 model. If the red block is embedded into the network, an attention mechanism is included in the model, defining the proposed EfficientNetB4Att architecture.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">EfficientNetB4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x</cell><cell></cell><cell>4x</cell><cell></cell><cell>4x</cell><cell></cell><cell>6x</cell><cell></cell><cell>6x</cell><cell></cell><cell>8x</cell><cell></cell><cell>2x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I</cell><cell>224 x 224 x 3</cell><cell>Conv 3 x 3</cell><cell>112 x 112 x 48</cell><cell>MBConv1 3 x 3</cell><cell>112 x 112 x 24</cell><cell>MBConv6 3 x 3</cell><cell>56 x 56 x 32</cell><cell>MBConv6 5 x 5</cell><cell>28 x 28 x 56</cell><cell>MBConv6 3 x 3</cell><cell>14 x 14 x 112</cell><cell>MBConv6 5 x 5</cell><cell>14 x 14 x 160</cell><cell>MBConv6 5 x 5</cell><cell>7 x 7 x 272</cell><cell>MBConv6 3 x 3</cell><cell>7 x 7 x 448</cell><cell>Conv 1 x 1 &amp;</cell><cell>Pooling &amp; FC</cell><cell>1792</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f (I)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attention</cell><cell>28 x 28 x 56</cell><cell>Conv 1x1</cell><cell>28 x 28 x 1</cell><cell>Sigmoid</cell><cell>28 x 28 x 1</cell><cell>28 x 28 x 56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I AREA</head><label>I</label><figDesc>UNDER THE CURVE (AUC) AND LOGLOSS OBTAINED WITH DIFFERENT NETWORK COMBINATIONS OVER ALL THE DATASETS. TOP-3 RESULTS PER COLUMN IN BOLD, BASELINE IN ITALICS.evaluation is performed over two different testing datasets: (i) the public test dataset; (ii) the private test dataset. Participants were not aware of the composition of those datasets (e.g., the provenance of the sequences, the techniques used for generating fakes, etc.), apart from the number of videos in public test set, which is roughly 4000. The final solution proposed by our team was an ensemble of the 4 proposed models, which led us to top 3% on the leaderboard computed against the public test set. For the time being, the leaderboard computed over the private test set has not been disclosed yet.</figDesc><table><row><cell>Xception</cell><cell>EfficientNet</cell><cell></cell><cell cols="2">AUC</cell><cell cols="2">LogLoss</cell></row><row><cell>Net</cell><cell>B4 B4ST B4Att</cell><cell>B4AttST</cell><cell>FF++</cell><cell>DFDC</cell><cell>FF++</cell><cell>DFDC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9273</cell><cell>0.8784</cell><cell cols="2">0.3844 0.4897</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9382</cell><cell>0.8766</cell><cell>0.3777</cell><cell>0.4819</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9337</cell><cell>0.8658</cell><cell>0.3439</cell><cell>0.5075</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9360</cell><cell>0.8642</cell><cell>0.3873</cell><cell>0.5133</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9293</cell><cell>0.8360</cell><cell>0.3597</cell><cell>0.5507</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9413</cell><cell>0.8800</cell><cell>0.3411</cell><cell>0.4687</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9428</cell><cell>0.8785</cell><cell>0.3566</cell><cell>0.4731</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9421</cell><cell>0.8729</cell><cell>0.3370</cell><cell>0.4739</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9423</cell><cell>0.8760</cell><cell>0.3371</cell><cell>0.4770</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9393</cell><cell>0.8642</cell><cell>0.3289</cell><cell>0.4977</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9390</cell><cell>0.8625</cell><cell>0.3515</cell><cell>0.4997</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9441</cell><cell>0.8813</cell><cell>0.3371</cell><cell>0.4640</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9432</cell><cell>0.8769</cell><cell>0.3269</cell><cell>0.4684</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9433</cell><cell>0.8751</cell><cell>0.3399</cell><cell>0.4717</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9426</cell><cell>0.8719</cell><cell>0.3304</cell><cell>0.4800</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9444</cell><cell>0.8782</cell><cell>0.3294</cell><cell>0.4658</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3d face reconstruction, tracking, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="523" to="550" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deepfakes github</title>
		<ptr target="https://github.com/deepfakes/faceswap" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Faceswap</title>
		<ptr target="https://github.com/MarekKowalski/FaceSwap/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protecting world leaders against deep fakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision of the unseen: Current trends and challenges in digital image and video forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An overview on video forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fontani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information forensics: An overview of the first decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="200" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Codec and gop identification in double compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2298" to="2310" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video integrity verification and gop size estimation via generalized variation of prediction footprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vzquez-Padn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fontani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shullani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prez-Gonzlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1815" to="1830" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local tampering detection in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A patchmatchbased dense-field algorithm for video copymove detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Damiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="669" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal forensics and anti-forensics for motion compensated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1315" to="1329" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A video forensic technique for detecting frame deletion and insertion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fontani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6226" to="6230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Media forensics and deepfakes: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FaceForensics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>R√∂ssler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepfake Detection Challenge (DFDC)</title>
		<ptr target="https://deepfakedetectionchallenge.ai/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, (ICML) 2019, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MesoNet: a compact facial video forgery detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepfakes: a new threat to face recognition? assessment and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<idno>abs/1812.08685</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepfake Video Detection Using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G√ºera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exposing deepfake videos by detecting face warping artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exposing deep fakes using inconsistent head poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting visual artifacts to expose deepfakes and face manipulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Matern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Applications of Computer Vision Workshops (WACVW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In ictu oculi: Exposing AI created fake videos by detecting eye blinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task learning for detecting and segmenting manipulated facial images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the detection of digital face manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stehouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Blazeface: Sub-millisecond neural face detection on mobile gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bazarevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raveendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05047" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K V I I A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch√©-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14126980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v9/vandermaaten08a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
