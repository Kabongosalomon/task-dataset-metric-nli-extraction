<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Second-Order Semantic Dependency Parsing with End-to-End Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxian</forename><surname>Huang</surname></persName>
							<email>huangjx@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Second-Order Semantic Dependency Parsing with End-to-End Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a graph. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of edges. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both algorithms as recurrent layers of a neural network and therefore can train the parser in an end-to-end manner. Our experiments show that our approach achieves stateof-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic dependency parsing <ref type="bibr">(Oepen et al.)</ref> aims to produce graph-structured semantic dependency representations of sentences instead of treestructured syntactic dependency parses. Existing approaches to semantic dependency parsing can be classified as graph-based approaches and transition-based approaches. In this paper, we investigate graph-based approaches which score each possible parse of a sentence by factorizing over its parts and search for the highest-scoring parse.</p><p>Previous work in graph-based syntactic dependency parsing has shown that higher-order parsing generally outperforms first-order parsing <ref type="bibr">(Mc-Donald and Pereira, 2006;</ref><ref type="bibr" target="#b2">Carreras, 2007;</ref><ref type="bibr" target="#b8">Koo and Collins, 2010;</ref><ref type="bibr" target="#b9">Ma and Zhao, 2012)</ref>. While a first-order parser scores dependency edges independently, a higher-order parser takes relationships between two or more edges into consideration. However, most of the previous algorithms for higher-order syntactic dependency tree parsing are not applicable to semantic dependency * Corresponding Author graph parsing, and designing efficient algorithms for higher-order semantic dependency graph parsing is nontrivial. In addition, it becomes a common practice to use neural networks to compute features and scores of parse graph components, which ideally requires backpropagation of parsing errors through the higher-order parsing algorithm, adding to the difficulty of designing such an algorithm.</p><p>In this paper, we propose a novel graph-based second-order semantic dependency parser. Given an input sentence, we use a neural network to compute scores for both first and second-order parts of parse graphs and then apply either mean field variational inference or loopy belief propagation to approximately find the highest-scoring parse graph. Both algorithms are iterative inference algorithms and we show that they can be unfolded as recurrent layers of a neural network with each layer representing the computation in one iteration of the algorithms. In this way, we can construct an end-to-end neural network that takes in a sentence and outputs the approximate marginal probability of every possible dependency edge. During training, we maximize the probability of the gold parses by using standard gradient-based methods. Our experiments show that our approach achieves state-of-the-art performance in semantic dependency parsing and outperforms our baseline with 0.3% and 0.4% labeled F1 score and previous state-of-the-art model with 1.3% and 1.4% labeled F1 score for in-domain and out-of-domain test sets respectively. Our approach shows more advantage over the baseline when there are fewer training data and when parsing longer sentences.  <ref type="bibr" target="#b14">(Oepen et al., 2015)</ref> with an additional out-of-domain dataset (the Brown corpus). A semantic dependency parse is different from a syntactic dependency parse in that the dependency edges are annotated with semantic relations (e.g., agent and patient) and form a directed acyclic graph instead of a tree. The Broad-Coverage Semantic Dependency Parsing provides three different formalisms: DM, PAS and PSD. Previous work has found that PAS is the easiest to learn and PSD is the most difficult as it has the largest set of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semantic Dependency Parsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our model architecture (shown in <ref type="figure">Figure 1</ref>) follows that of <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref>. Given an input sentence, we first compute word representations using a BiLSTM, which are then fed into two parallel modules, one for predicting the existence of every edge and the other for predicting the label of every edge. The label-prediction module makes predictions of each edge independently and hence is a first-order decoder. The edge-prediction module is what our approach differs from that of <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref>. The module scores both first and second-order parts and then goes through multiple recurrent inference layers to predict edge existence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Part Scoring</head><p>Given a sentence with n words [w 1 , w 2 , ..., w n ], we feed a BiLSTM with their word embeddings and POS tag embeddings.</p><formula xml:id="formula_0">o i = e (word) i ⊕ e (postag) i R = BiLSTM(O)</formula><p>where o i is the concatenation (⊕) of the word and POS tag embeddings of word w i , O represents [o 1 , . . . , o n ], and R = [r 1 , . . . , r n ] represents the output from the BiLSTM. To score first-order parts (edges) in both the edge-prediction module and the label-prediction module, we use two single-layer feedforward networks (FNNs) to compute a head representation and a dependent representation for each word and then apply a biaffine function to compute the scores of edges and labels.</p><formula xml:id="formula_1">Biaff(v 1 , v 2 ) := v T 1 Uv 2 + b h (edge-head) i = FNN (edge-head) (r i ) h (edge-dep) i = FNN (edge-dep) (r i ) h (label-head) i = FNN (label-head) (r i ) h (label-dep) i = FNN (label-dep) (r i ) s (edge) ij = Biaff (edge) (h (edge-dep) i , h (edge-head) j ) (1) s (label) ij = Biaff (label) (h (label-dep) i , h (label-head) j ) (2)</formula><p>In Eq. 2, the tensor U in the biaffine function is (d × c × d)-dimensional and is diagonal (for any p a 1 a 2 a p 1 p 2 g p a grandparent siblings co-parents <ref type="figure">Figure 2</ref>: Second-order parts used in our model. i = j, u i,c,j = 0), where d is hidden size and c is the number of labels. In Eq. 1, the tensor U in the biaffine function is d × 1 × d-dimensional.</p><p>In the edge-prediction module, we further score second-order parts. We consider three types of second-order parts: siblings (sib), co-parents (cop) and grandparents (gp) (Martins and Almeida, 2014), as shown in <ref type="figure">Figure 2</ref>. For a specific type of second-order part, we use single-layer FNNs to compute a head representation and a dependent representation for each word. For grandparent parts, we additionally compute a head_dep representation for each word.</p><formula xml:id="formula_2">type ∈ {sib, cop, gp} h (type-head) i = FNN (type-head) (r i ) h (type-dep) i = FNN (type-dep) (r i ) h (gp-head_dep) i = FNN (gp-head_dep) (r i )</formula><p>We then apply a trilinear function to compute scores of second-order parts. A trilinear function is defined as follows.</p><formula xml:id="formula_3">Trilin(v 1 , v 2 , v 3 ) := v T 3 v T 1 Uv 2 where U is a (d × d × d)-dimensional tensor.</formula><p>To reduce the computation cost, we assume that U has rank d and can be represented as the product of three (d × d)-dimensional matrices U 1 , U 2 and U 3 . We can then compute second-order part scores as follows.</p><formula xml:id="formula_4">g i :=U i v i i ∈ [1, 2, 3] Trilin(v 1 , v 2 , v 3 ) := d i=1 g 1i • g 2i • g 3i (3) s (sib) ij,ik ≡ s (sib) ik,ij = Trilin (sib) (h (head) i , h (dep) j , h (dep) k ) (4) s (cop) ij,kj ≡ s (cop) kj,ij = Trilin (cop) (h (head) i , h (dep) j , h (head) k ) (5) s (gp) ij,jk = Trilin (gp) (h (head) i , h (head_dep) j , h (dep) k )<label>(6)</label></formula><p>where • represents element-wise product. We require j &lt; k in Eq. 4 and i &lt; k in Eq. 5. Poles <ref type="formula">4</ref> Sibling factor Co-parent factor Grandparent factor Unary factor 1,3 3,1 2,4 4,2 0,2 0,3 0,4 0,1 1,2 2,1 1,4 4,1 2,3 3,2 3,4 4,3 <ref type="figure">Figure 3</ref>: An example of our factor graph for a sentence with four words. The node &lt;TOP&gt; is the top of dependency graph. The boolean variable (i, j) indicates whether the directed edge (i, j) exists. For simplicity, we only depict factors connected to node (1, 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>In the label-prediction module, s (label)</p><p>i,j is fed into a softmax layer that outputs the probability of each label for edge (i, j). In the edge-prediction module, computing the edge probabilities can be seen as doing posterior inference on a Conditional Random Field (CRF). The corresponding factor graph is shown in <ref type="figure">Figure 3</ref>. Each Boolean variable X ij in the CRF indicate whether the directed edge (i, j) exists. We use Eq. 1 to define our unary potential ψ u representing scores of an edge and Eqs. (4-6) to define our binary potential ψ p . We define a unary potential φ u (X ij ) for each variable X ij .</p><formula xml:id="formula_5">φ u (X ij ) = exp(s (edge) ij ) X ij = 1 1 X ij = 0</formula><p>For each pair of edges (i, j) and (k, l) that form a second-order part of a specific type, we define a binary potential φ p (X ij , X kl ).</p><formula xml:id="formula_6">φ p (X ij , X kl ) = exp(s (type) ij,kl ) X ij = X kl = 1 1 Otherwise</formula><p>Exact inference on this CRF is intractable. We resort to iterative approximate inference algorithms as described below, which produce the posterior distribution Q ij (X ij ) of for each edge (i, j). We can then predict the parse graph by including every edge (i, j) such that Q ij (1) &gt; 0.5. The edge labels are predicted by maximizing the label probabilities computed by the label-prediction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Field Variational Inference</head><p>Mean field variational inference approximates a true posterior distribution with a factorized variational distribution and tries to iteratively minimize their KL divergence. We can derive the following iterative update equations of distribution Q ij (X ij ).</p><formula xml:id="formula_7">F (t−1) ij = k =i,j Q (t−1) ik (1)s (sib) ij,ik + Q (t−1) kj (1)s (cop) ij,kj + Q (t−1) jk (1)s (gp) ij,jk + Q (t−1) ki (1)s (gp) ki,ij (7) Q (t) ij (0) ∝ 1 Q (t) ij (1) ∝ exp{s (edge) ij + F (t−1) ij } The initial distribution Q (0) ij (X ij )</formula><p>is set by normalizing the unary potential φ u (X ij ). We iteratively update the distributions for T steps and then output Q</p><formula xml:id="formula_8">(T ) ij (X ij ), where T is a hyperparameter.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loopy Belief Propagation</head><p>Loopy belief propagation iteratively passes messages between variables and potential functions (factors). Because our CRF contains only unary and binary potentials, we can merge each variableto-factor message and its subsequent factor-tovariable message into a single variable-to-variable message M kl→ij , representing message from edge (k, l) to edge (i, j). The update function of the messages in each iteration is:</p><formula xml:id="formula_9">Q (t−1) ij (X ij ) = φ u (X ij ) ab∈N ij M (t−1) ab→ij (X ij ) M (t) kl→ij (0) ∝ x kl Q (t−1) kl (x kl )/M (t−1) ij→kl (x kl ) M (t) kl→ij (1) ∝ Q (t−1) kl (0)/M (t−1) ij→kl (0) + exp(s (type) ij,kl )Q (t−1) kl (1)/M (t−1) ij→kl (1)</formula><p>We initialize the messages with M (0) kl→ij = 1. We iteratively update the messages and distributions for T steps and then output normalized Q</p><formula xml:id="formula_10">(T ) ij (X ij ).</formula><p>Inference as Recurrent Layers <ref type="bibr" target="#b22">Zheng et al. (2015)</ref> proposed that a fixed number of iterations in mean field variational inference can be seen as a recurrent neural network that is parameterized by the potential functions. We follow the idea and unfold both mean field variational inference and loopy belief propagation as recurrent neural network layers that are parameterized by part scores.</p><p>The time complexity of our inference procedure is O(n 3 ), which is lower than the O(n 4 ) complexity of the exact quasi-second-order inference of <ref type="bibr" target="#b1">Cao et al. (2017)</ref> and on par with the complexity of the approximate second-order inference of Martins and Almeida (2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Given a gold parse graph y of sentence w, the conditional distribution over possible edge y (edge) ij and corresponding possible label y (label) ij is given by:</p><formula xml:id="formula_11">P (y (edge) ij = X ij |w) = Q (T ) ij (X ij ) P (y (label) ij |w) = softmax(s (label) ij )</formula><p>We define the following cross entropy losses:</p><formula xml:id="formula_12">L (edge) (θ) = − i,j log(P θ (y (edge) ij |w)) L (label) (θ) = − i,j 1(y (edge) ij ) log(P θ (y (label) ij |w))</formula><p>where θ is the parameters of our model, 1(y (edge) ij ) denotes the indicator function and equals 1 when edge (i, j) exists in the gold parse and 0 otherwise, and i, j ranges over all the words in the sentence. We optimize the weighted average of the two losses.</p><formula xml:id="formula_13">L = λL (label) + (1 − λ)L (edge)</formula><p>where λ is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperparameters</head><p>We tuned the hyperparameters of our baseline model from <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> and our second-order model on the DM development set. We followed <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> using 100-dimensional pretrained GloVe embeddings <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref> and transformed them to be 125-dimensional. Words and lemmas appeared less than 7 times are replaced with a special unknown token. We use the same dataset split as in previous approaches <ref type="bibr" target="#b10">(Martins and Almeida, 2014;</ref> with 33,964 sentences in the training set, 1,692 sentences in the development set,  <ref type="formula">(2018)</ref>, we used Adam (Kingma and Ba, 2014) for optimizing our model, annealing the learning rate by 0.5 for every 10,000 steps, and switched the optimizer to AMSGrad (Reddi et al., 2018) after 5,000 steps without improvement. We trained the model for 100,000 iterations with batch sizes of 6,000 tokens and terminated training early after 10,000 iterations with no improvement on the development sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We compare our model with previous state-of-theart approaches in <ref type="table" target="#tab_3">Table 2</ref>.  is a hybrid model. A&amp;M is from <ref type="bibr" target="#b0">Almeida and Martins (2015)</ref>. PTS17 proposed by <ref type="bibr" target="#b16">(Peng et al., 2017)</ref> and</p><p>Basic is single task parsing while Freda3 is a multitask parser across three formalisms. WCGL18 <ref type="bibr" target="#b21">(Wang et al., 2018</ref>) is a neural transition-based model. D&amp;M <ref type="bibr" target="#b4">(Dozat and Manning, 2018</ref>) is a graph-based model and "Baseline" is the first-order model from <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> that was trained by ourselves. For our model, we used mean field variational inference and loopy belief propagation for 3 iterations.</p><p>In the basic setting, on average our model outperforms the best previous one by 1.3% on the indomain test set and 1.3% on the out-of-domain test set. With lemma and character-based embeddings, our model leads to an average improvement of 0.3% and 0.6% over previous models. Our model also outperforms the baseline by 0.2% − −0.5% on average with different settings and test sets. <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> found that on the PAS dataset their model cannot benefit from lemma and character-based embeddings and hence speculated that they may have approached the ceiling of the PAS F1 score. As shown in our experiments on the PAS dataset, our model cannot benefit from lemma and character-based embeddings either, but it obtains higher F1 scores, which suggests that the ceiling may not have been reached.</p><p>Note that while we do not force our parser to predict a directed acyclic graph, we found that only 0.7% of the test sentences have cycles in their parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small Training Data</head><p>To evaluate the performance of our model on smaller training data, we repeated our experiments with randomly sampled 70%, 40% and 10% of the training set. <ref type="table" target="#tab_4">Table 3</ref> shows the F1 scores averaged over 5 runs (each time with a new randomly sampled training subset). It can be seen that the advantage of our model over the baseline increases significantly when the training data becomes smaller. We make the following speculation to explain this observation. The BiLSTM layer in the baseline and our model is capable of capturing high-order information to some extent. However, without prior knowledge of high-order parts, it may require more training data to learn this capability than a high-order decoder. So with small training data, the baseline loses the capability of utilizing highorder information, while our model can still rely on the decoder for high-order parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on Different Sentence Lengths</head><p>We want to study the impact of sentence lengths on first-order parsing and our second-order parsing. We split the test sets of all the formalisms into five subsets with different sentence length ranges DM PAS PSD Avg ID OOD ID OOD ID OOD ID OOD  89.   and evaluate our model and the baseline on them. <ref type="figure">Figure 4</ref> shows that our model has more advantage over the baseline when sentences get longer, especially when sentences are longer than 40. One possible explanation is that BiLSTM has difficulty in capturing long-range dependencies in long sentences, which leads to lower performance on the first-order baseline; but such long-range dependencies can still be captured with second-order parsing. It can also be seen that on long sentences, our model has more advantage over the baseline for the out-of-domain test set than for the in-domain test set, which suggests that our model has better generalizability especially on long sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Field vs. Loopy Belief Propagation</head><p>We compare mean field variational inference and loopy belief propagation algorithms in <ref type="table" target="#tab_6">Table 4</ref>. We tuned the hyperparameters of our model for each algorithm and iteration number separately. We find that in general mean field variational inference has very similar performance to loopy belief propagation. In addition, with more iterations, the performance of mean field variational inference steadily increases while the at the second iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We study how different types of second-order parts defined in Section 3.1 affect the performance of our parser. We trained our model with each  <ref type="table" target="#tab_6">Difference   MF%10  MF%40  MF%70  MF%100  LBP%10</ref> LBP%40 LBP%70 LBP%100 <ref type="figure">Figure 4</ref>: Relative improvements over our baseline in different sentence length intervals with different training data sizes. We report the average F1 score improvements over all the formalisms with 5 runs for each.   type of second-order parts without the other two types on the DM dataset using mean field variational inference and the result is shown in <ref type="table" target="#tab_7">Table 5</ref>. While all the three types of second-order parts can be seen to improve the parsing performance over the baseline, the sibling parts lead to the largest performance gain on both the in-domain test set and the out-of-domain test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>We provide a parsing example in <ref type="figure">Figure 5</ref> to show how our second-order parser with 3 iterations of mean field variational inference works. Before the first iteration, the marginal distributions of edges Q ij is initialized with unary potentials and thus is exactly what a first-order parser would produce. In the subsequent iterations, the distributions are updated with binary potentials taken into account. For each version of the distributions, we can extract a parse graph by collecting edges with probabilities larger than 0.5. From <ref type="figure">Figure 5</ref>, we can see that erroneous edges are gradually fixed through iterations. Edge (were, P oles) sends a strong negative co-parents message to edge (&lt;T OP &gt;, P oles) in the first iteration, so the latter has a lower probability in subsequent iterations. Edge (were, P oles) also sends a strong positive grandparent message to edge (&lt;T OP &gt;, were) to enhance its probability, and the latter sends an increasingly positive message back to the former in subsequent iterations. In the second and third iterations, (were, P oles) sends positive sibling messages and (&lt;T OP &gt;, were) sends positive grandparent messages to enhance probabilities of edges (were, T hey) and (were, not), which finally leads to the correct parse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Running Speed</head><p>Our model have a time complexity of O(d 2 u + d 2 b + n 3 ) while the first order model of <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> has a time complexity of O(d 2 u + n 2 ) in scoring and decoding (where d u and d b are the hidden sizes of the biaffine layer and trilinear layer and n is the sentence length). We compare these models with respect to training speed and parsing  <ref type="figure">Figure 5</ref>: An example of message passing (left) and the corresponding graph parses (right) in our second-order parser with mean field variational inference. We regard terms in Eq. 7 as messages sent from other arcs. Blue arcs and red arcs on the left represent positive messages (which encourage the target edge to exist) and negative messages (which discourage the target edge to exist) respectively. Lightness of the arc color represents the message intensity. Blackness of each nodes represents the probability of edge existence. A Node with a double circle means the corresponding edge is predicted to exist. Messages with low intensities are omitted in the graph. Dotted arcs and red arcs on the right represent missed predictions and wrong predictions compared to the golden parse. The period in the sentence is omitted for simplicity.  speed on an Nvidia Tesla P40 server. The result is shown in <ref type="table" target="#tab_10">Table 6</ref>. Mean field variational inference slows down training and parsing by 35% and 20% respectively compared with the baseline. However, loopy belief propagation slows down training and parsing by 65% and 67% respectively compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Significance Test</head><p>We trained 25 basic models of our approach and the baseline with the same hyperparameters in <ref type="table">Table 1</ref> on each formalism. Student's t-test shows that our second-order model outperforms our baseline model on all the formalisms with a significance level of 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Dependency Parsing</head><p>Semantic dependency parsing can be classified as transition-based approaches and graph-based approaches. <ref type="bibr" target="#b21">Wang et al. (2018)</ref> proposed a transition-based parser for semantic parsing, while  proposed a hybrid parser that benefits from both transition-based approaches and graph-based approaches. <ref type="bibr" target="#b16">Peng et al. (2017)</ref> proposed a graph-based approach that trains on all the three formalisms simultaneously <ref type="bibr" target="#b17">and Peng et al. (2018)</ref> further proposed to learn from different corpora. <ref type="bibr" target="#b4">Dozat and Manning (2018)</ref> proposed a graph-based simple but powerful neural network for semantic dependency parsing using a bilinear or biaffine <ref type="bibr" target="#b3">(Dozat and Manning, 2016)</ref> layer to encode the interaction between words. Most of these approaches proposed first-order dependency parser while Martins and Almeida (2014) proposed a way to encode higher-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. They used discrete optimizing algorithm alternating directions dual decomposition (AD 3 ) as their decoder. <ref type="bibr" target="#b1">Cao et al. (2017)</ref> also proposed a quasi-second-order semantic dependency parser with dynamic programming. Our model contains second-order information comparing with the first-order approaches and benefits from endto-end training comparing with other second-order approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Higher-Order Dependency Parsing</head><p>Higher-order parsing has been extensively studied in the literature of syntactic dependency parsing. Much of these work is based on the first-order maximum spanning tree (MST) parser of <ref type="bibr" target="#b12">McDonald et al. (2005)</ref> which factorizes a dependency tree into individual edges and maximizes the summation of the scores of all the edges in a tree. <ref type="bibr">Mc-Donald and Pereira (2006)</ref> introduced a secondorder MST that factorizes a dependency tree into not only edges but also second-order sibling parts, which allows interactions between adjacent sibling words. <ref type="bibr" target="#b2">Carreras (2007)</ref> defined second-order grandparent parts representing grandparental relations. <ref type="bibr" target="#b8">Koo and Collins (2010)</ref> introduced thirdorder grand-sibling and tri-sibling parts. A grandsibling part represents a grandparent with two grandchildren and a tri-sibling part represents a parent with three children. <ref type="bibr" target="#b9">Ma and Zhao (2012)</ref> defined grand-tri-sibling parts for fourth-order dependency parsing. Many previous approaches to higher-order dependency parsing perform exact decoding based on dynamic programming, but there is also research in approximate higher-order parsing. <ref type="bibr" target="#b11">Martins et al. (2011)</ref> proposed an alternating directions dual decomposition (AD 3 ) algorithm which splits the original problem into several local subproblems and solves them iteratively. They employed AD 3 for second-order dependency parsing to speed up decoding. <ref type="bibr" target="#b20">Smith and Eisner (2008)</ref> and <ref type="bibr" target="#b6">Gormley et al. (2015)</ref> proposed to use belief propagation for approximate higher-order parsing, which is closely related to our work.</p><p>While higher-order parsing has been shown to improve syntactic dependency parsing accuracy, it receives less attention in semantic dependency parsing. Martins and Almeida <ref type="formula">(2014)</ref> proposed second-order semantic dependency parsing and employed AD 3 for approximate decoding. <ref type="bibr" target="#b1">Cao et al. (2017)</ref> proposed a quasi-second-order parser and used dynamic programming for decoding with time complexity of O(n 4 ). <ref type="bibr" target="#b22">Zheng et al. (2015)</ref> are probably the first to propose the idea of unfolding iterative inference algorithms on CRFs as a stack of recurrent neural network layers. They unfolded mean field variational inference in a neural network designed for semantic segmentation. There is a lot of subsequent work that employs this technique, especially in the computer vision area. For example, <ref type="bibr" target="#b23">Zhu et al. (2017)</ref> proposed a structured attention neural model for Visual Question Answering with a CRF over image regions and unfolded both mean field variational inference and loopy belief propagation algorithms as recurrent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CRF as Recurrent Neural Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel graph-based second-order parser for semantic dependency parsing. We constructed an end-to-end neural network that uses a trilinear function to score second-order parts and finds the highest-scoring parse graph by either mean field variational inference or loopy belief propagation algorithms unfolded as recurrent neural network layers. Our experimental results show that our model outperforms previous state-of-the-art model and has higher accuracies especially on out-of-domain data and long sentences. Our code is publicly available at https://github.com/ wangxinyu0922/Second_Order_SDP</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of labeled F1 scores achieved by our model and previous state-of-the-arts. The F1 scores of Baseline and our models are averaged over 5 runs. ID denotes the in-domain (WSJ) test set and OOD denotes the out-of-domain (Brown) test set. +Char and +Lemma means augmenting the token embeddings with character-level and lemma embeddings.</figDesc><table><row><cell></cell><cell>DM</cell><cell>PAS</cell><cell>PSD</cell><cell>Avg</cell></row><row><cell></cell><cell cols="4">ID OOD ID OOD ID OOD ID OOD</cell></row><row><cell cols="5">Baseline: 70% 92.0 87.0 93.8 90.6 79.8 77.7 88.5 85.1</cell></row><row><cell>MF: 70%</cell><cell cols="4">92.4 87.5 93.9 90.8 80.2 78.0 88.8 85.4</cell></row><row><cell>LBP: 70%</cell><cell cols="4">92.3 87.4 94.0 90.9 80.2 78.1 88.8 85.5</cell></row><row><cell cols="5">Baseline: 40% 90.8 85.5 93.2 89.6 78.4 76.4 87.4 83.8</cell></row><row><cell>MF: 40%</cell><cell cols="4">91.2 86.0 93.4 90.0 78.9 76.7 87.8 84.2</cell></row><row><cell>LBP: 40%</cell><cell cols="4">91.2 86.0 93.5 90.0 78.9 76.8 87.9 84.3</cell></row><row><cell cols="5">Baseline: 10% 86.1 80.0 90.8 86.4 73.5 71.2 83.4 79.2</cell></row><row><cell>MF: 10%</cell><cell cols="4">86.9 81.0 91.3 87.1 74.5 72.1 84.2 80.1</cell></row><row><cell>LBP: 10%</cell><cell cols="4">86.8 80.9 91.3 87.0 74.5 72.3 84.2 80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of labeled F1 scores achieved by our model and our baseline on 10%, 40%, 70% of the training data. We report the average F1 score over 5 runs with different randomly sampled training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Comparison of labeled F1 scores of mean field</cell></row><row><cell cols="3">and loopy belief propagation with different iteration</cell></row><row><cell>numbers on the DM dataset.</cell><cell></cell></row><row><cell></cell><cell>ID</cell><cell>OOD</cell></row><row><cell>Baseline</cell><cell cols="2">92.60 87.98</cell></row><row><cell>+Siblings</cell><cell cols="2">92.85 88.31</cell></row><row><cell>+Co-parents</cell><cell cols="2">92.80 88.23</cell></row><row><cell cols="3">+Grandparents 92.84 88.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The performance comparison between three types of second-order parts on the DM dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Training and parsing speed (sentences/second) comparison of the baseline and our model (3 iterations for our second-order parser). long means the parsing speed on sentences longer than 40 and short means the parsing speed on sentences no longer than 10.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Major Program of Science and Technology Commission Shanghai Municipal (17JC1404102).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lisbon: Evaluating turbosemanticparser on multiple languages and out-of-domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mariana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Ft</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quasi-second-order parsing for 1-endpoint-crossing, pagenumber-2 graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experiments with a higherorder projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simpler but more accurate semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01396</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Peking: Building semantic dependency graphs with a hybrid parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international workshop on semantic evaluation</title>
		<meeting>the 9th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="927" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation-aware dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: posters</title>
		<meeting>COLING 2012: posters</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Priberam: A turbo semantic parser with second order features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana Sc</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="471" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual decomposition with many overlapping components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval 2015 task 18: Broad-coverage semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="915" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Broad-coverage semantic dependency parsing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06855</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning joint semantic parsers from disjoint data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05990</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural transition-based approach for semantic dependency graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1291" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
