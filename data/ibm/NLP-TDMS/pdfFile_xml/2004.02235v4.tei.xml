<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Generalized zero-shot learning to long-tail with class descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvir</forename><surname>Samuel</surname></persName>
							<email>dvirsamuel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
							<email>yatzmon@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<email>gal.chechik@biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Generalized zero-shot learning to long-tail with class descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world data is predominantly unbalanced and longtailed, but deep models struggle to recognize rare classes in the presence of frequent classes. Often, classes can be accompanied by side information like textual descriptions, but it is not fully clear how to use them for learning with unbalanced long-tail data. Such descriptions have been mostly used in (Generalized) Zero-shot learning (ZSL), suggesting that ZSL with class descriptions may also be useful for longtail distributions.</p><p>We describe DRAGON, a late-fusion architecture for long-tail learning with class descriptors. It learns to (1) correct the bias towards head classes on a sampleby-sample basis; and (2) fuse information from classdescriptions to improve the tail-class accuracy. We also introduce new benchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with class-descriptions, building on existing learning-with-attributes datasets and a version of Imagenet-LT with class descriptors. DRAGON outperforms state-of-the-art models on the new benchmark. It is also a new SoTA on existing benchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only) long-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365-LT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world data is predominantly unbalanced, typically following a long-tail distribution. From text data (Zipf's law), through acoustic noise (the 1-over-f rule) to the longtail distribution of classes in object recognition <ref type="bibr" target="#b44">[45]</ref>, few classes are frequently observed, while the many remaining ones are rarely encountered.</p><p>Long-tail data poses two major challenges to learning: data paucity and data imbalance. First, at the tail of the distribution, classes are poorly sampled and one has to use few-shot and zero-shot learning techniques. Second, when training a single model for both richly-sampled classes and poorly-sampled classes, the common classes dominate (a) (b) (c) <ref type="figure">Figure 1</ref>: Training with unbalanced data leads to a "familiarity bias", where models are more confident and more over-confident about frequent classes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b5">6]</ref>. (a) Class distribution of a long-tailed ImageNet <ref type="bibr" target="#b11">[12]</ref>. Classes are ordered from left to right by decreasing number of samples.</p><p>(b) When training a ResNet-10 on ImageNet-LT, validation (and test) predictions tend to have low confidence for tail classes. We show the mean output of softmax for each class, conditioned on samples from that class. (c) A reliability graph for the model in b. Predictions are grouped based on confidence. The model has larger confidence gaps (pink boxes) for more confident predictions, which usually come from head classes. This result suggests that overconfidence is strongly affected by class frequency, and we can learn to correct it if the number of samples is known.</p><p>training, and as we show below, this skews prediction confidence towards rich-sampled classes.</p><p>To address data paucity of tail classes, note that visual examples can very often be augmented with class descriptors. Namely, semantic information about classes given as text or attributes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52]</ref>. This approach, learning with class-descriptors has been studied mostly for zeroshot and generalized zero-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Here, we propose to adapt it to generalized few-shot learning (GFSL) by fusing information from two modalities. A visual classifier, expected to classify correctly head classes, and a semantic classifier, trained with per-class descriptors and is expected to classify correctly tail classes. We explain the subtleties of GZSL and GFSL in Section 2 (Related work).</p><p>To address data imbalance, we first note that learning with unbalanced data leads to a familiarity effect, where models become biased to favor the more familiar, richsampled classes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b5">6]</ref>. Since deep models tend to be overly confident about high-confidence sample predictions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, they become over-confident about head classes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b5">6]</ref>. <ref type="figure">Figure 1</ref> illustrates this phenomenon. It shows that a model trained on unbalanced data <ref type="figure">(Figure 1a</ref>), has higher confidence for head classes <ref type="figure">(Figure 1b</ref>). It is also an over-estimate of the true accuracy <ref type="figure">(Figure 1c</ref>), especially for head classes. See further analysis in Appendix A.</p><p>As an interesting side note, studies of human decision making and preference learning show a similar bias towards familiar classes. This effect is widely observed and has been connected to the availability heuristic studied by Tversky and Kahneman <ref type="bibr" target="#b43">[44]</ref>.</p><p>A natural way to correct the familiarity bias would be to penalize high-frequency classes, either during training using a balanced loss, or post-training <ref type="bibr" target="#b22">[23]</ref>. However, it would be a grave mistake to penalize all samples from rich classes, because confidence is sometimes justified, as in the case of "easy" prototypical examples of a class. Indeed, we show below that addressing the familiarity bias benefits from per-sample debiasing, going beyond class-based debiasing. To summarize, model overconfidence is affected by class frequency. It can be estimated by observing the full vector of predictions to correct for overconfidence. Not all samples of a class should be penalized for belonging to a frequent class.</p><p>Importantly, the familiarity effect caused by data imbalance has a crippling effect on model accuracy and on aggregating predictions from multiple modalities. Several approaches attempted calibrating predictions of deep networks to remedy the above biases (see e.g. a survey in <ref type="bibr" target="#b16">[17]</ref>) and some became common practice. Unfortunately, the problem is still far from being solved.</p><p>We propose to address both the data-imbalance and the data paucity learning challenges, using a single late-fusion architecture. We describe an easy-to-implement debiasing module that offsets the familiarity effect by learning to predict the magnitude of the bias for any given sample. It fur-ther improves learning at the tail by learning to fuse and balance information from visual and semantic modalities. It can easily be reduced to address long-tail learning with a single modality (vision only), where it improves over current baselines.</p><p>The paper has four main novel contributions:</p><p>(1) A new late-fusion architecture (DRAGON) that learns to fuse predictions based on vision with predictions based class descriptors.</p><p>(2) A module that rebalances class predictions across classes on a sample-by-sample basis.</p><p>(3) New benchmarks CUB-LT, SUN-LT, and AWA-LT for evaluating long-tail learning with textual class descriptors (LT-d). DRAGON is SoTA on these datasets, and also on ImageNet-LT augmented with class descriptors and on existing two-level benchmarks. (4) A new SoTA on existing (vision-only) long-tail learning benchmarks: CIFAR-10, CIFAR-100, ImageNet-LT and Places365-LT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related methods</head><p>Long-tail learning: Learning with unbalanced data causes models to favor head classes <ref type="bibr" target="#b5">[6]</ref>. Previous efforts to address this effect can be viewed as either algorithmic or datamanipulations approaches.</p><p>Algorithmic approaches encourage learning of tail classes using a non-uniform cost per misclassification function. A natural approach is to rescale the loss based on class frequency <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b26">[27]</ref> proposed to down-weigh the loss of well-classified examples, preventing easy negatives from dominating the loss. <ref type="bibr" target="#b36">[37]</ref> dynamically rescaled the cross-entropy loss based on the difficulty to classify a sample. <ref type="bibr" target="#b6">[7]</ref> proposed a loss that encourages larger margins for rare classes. <ref type="bibr" target="#b22">[23]</ref> decoupled the learning procedure into representation learning and classification and studied four approaches. Among them, LWS L 2 -normalizes the last-layer, since the weight magnitude correlates with class cardinality. The effect of this approach is similar to that presented in this paper, but here we apply recalibration dynamically on a sample-by-sample basis.</p><p>Data-manipulation approaches aim to flatten long-tail datasets to correct the bias towards majority classes. Popular techniques employ over-sampling of minority classes (more likely to overfit) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, under-sampling the majority classes (wastes samples) <ref type="bibr" target="#b12">[13]</ref>, or generating samples from the minority classes (can be costly to develop) <ref type="bibr" target="#b4">[5]</ref>.</p><p>Another approach is to transfer meta-level-knowledge from data-rich classes to data-poor classes. <ref type="bibr" target="#b48">[49]</ref> gradually transfer hyperparameters from rich classes to poor classes by representing knowledge as trajectories in model space that capture the evolution of parameters with increasing training samples. <ref type="bibr" target="#b28">[29]</ref> first learns representations on the unbalanced data and then fine-tunes them using a classbalanced sampling and a memory module. Learning with class descriptors: Learning with class descriptors is usually applied to zero-shot learning (ZSL) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4]</ref>, where a classifier is trained to recognize (new) unseen classes based on their semantic description, which can include a natural-language textual description or predefined attributes. In several ZSL studies, attributes detected in a test image are matched with ground-truth attributes of each class, and several studies focused on this matching <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>A series of papers proposed to learn a shared representation of visual and text features (class-descriptors). As one example, <ref type="bibr" target="#b42">[43]</ref> learns such a shared latent manifold using autoencoders and then minimizes the MMD loss between the two domains. Another recent line of work synthesizes feature vectors of unseen classes using generative models like VAE of GAN, and then use them in training a conventional classifier <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref>. The major baseline we compare our approach with is CADA-VAE <ref type="bibr" target="#b37">[38]</ref>, the current SoTA for Generalized FSL with class descriptors. CADA-VAE uses a variational autoencoder that aligns the distributions of image features and semantic (attribute) class embedding in a shared latent space. A recent work, <ref type="bibr" target="#b52">[53]</ref>, uses a mixture of VAEs and GANs. We could not directly compare with <ref type="bibr" target="#b52">[53]</ref> because their FSL protocol deviates from the standard benchmark of <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52]</ref> by fine-tuning the CNN features. Without fine-tuning, their reported metrics for GZSL are similar to CADA-VAE.</p><p>Some studies fused information from vision and per sample descriptors (e.g., <ref type="bibr" target="#b57">[58]</ref>). This is outside the scope of this paper because it may require extensive labeling.</p><p>Generalized ZSL (GZSL) and Generalized FSL: GZSL extends ZSL to the scenario where the test data contains both seen and unseen classes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40]</ref>. Recently, GZSL extended to Generalized Few-Shot-Learning with class descriptors (GFSL-d), where the unseen classes are augmented with a fixed number of few training samples <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref>. Namely, the distribution of samples across classes is a 2-level distribution, with many "head" classes and a smaller set of "tail" classes all having the same (small) number of samples per class. Both GZSL and GFSL-d can be viewed as special cases of long-tail learning with classdescriptors, but with a short-tailed unnatural distribution.</p><p>Most related GZSL approaches are <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref>. They use a gating mechanism to weigh the decisions of seen-classes experts and a ZSL expert. The gating module is modeled as an out-of-distribution estimator. The current paper differs from their work by (1) The problem setup is different.</p><p>Here, all samples are in-distribution and the distribution of classes is smooth and long-tail with a much smaller number of head classes. (2) DRAGON architecture first quantifies and corrects the (smooth) familiarity effect. Then it learns how to fuse the debiased decision of the two experts.</p><p>Early vs late fusion: When learning from multiple modalities, one often distinguishes between early and late fusion models <ref type="bibr" target="#b27">[28]</ref>. Early fusion models combine features from multiple modalities to form a joint representation. Late fusion methods combine decisions of per-modality models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35]</ref>. Our approach addresses the long-tail setup, by leveraging the information in the familiarity bias to debias experts predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Long-tail learning with class descriptors</head><p>We start with a formal definition of the problem of learning over unbalanced distributions with class descriptors.</p><p>We are given a training set of n labeled (image) samples: {(x 1 , y 1 ), . . . , (x n , y n )}, where each x i is a feature vector and y i is a label in {1, 2, . . . k}. Samples are drawn from a distribution D = p(x, y) such that the marginal distribution over the classes p(y) is strongly non uniform. For example, p(y) may be exponential p(y) ∼ exp(−ky). As a second supervision signal, each class y is also accompanied with a class-description vector a j , j = 1, .., k, in the form of semantic attributes <ref type="bibr" target="#b25">[26]</ref> or natural-language embedding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b39">40]</ref>. For example, classes in CUB <ref type="bibr" target="#b45">[46]</ref> are annotated with attributes like Head-color:red.</p><p>At test time, a new set of m test samples {x n+1 , . . . , x n+m } is given from the same distribution D. We wish to predict their correct classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our approach</head><p>Our approach is based on two observations: (1) Semantic descriptions of classes are easy to collect and can be very useful for tail (low-shot) classes, because they allow models to recognize classes even with few or no training samples <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4]</ref> (and Appendix B). (2) The average prediction confidence over samples of a class is correlated with the number of training samples of that class <ref type="figure">(Figure 1</ref>).</p><p>Our architecture leverages these observations and learns to (1) Combine predictions of two expert classifiers: A conventional visual expert which is more accurate at head classes and a semantic expert which excels at tail classes;</p><p>(2) Reweigh the scores of each class prediction, taking into account the number of training samples for that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The architecture</head><p>The DRAGON architecture 1 follows two design considerations: modularity and low-complexity. First, modularity; DRAGON allows to plug-in existing multi-modal experts, each trained for its own modality. Below we show experiments with language-based experts and a visual expert, but other modalities can be considered (e.g., mesh, depth, motion, or multi-spectral information). Second, limiting the model to have a small number of parameters is important because tail classes only have few training samples and the model must perform well at the tail.</p><p>Our general architecture ( <ref type="figure" target="#fig_0">Figure 2</ref>) takes a late fusion approach. It consists of two experts modules: A visual expert and a semantic expert. Each expert outputs a prediction vector which is fed to a fusion module. The fusion module combines the expert predictions and learns to debias the familiarity effect, by weighing the experts and re-scaling their class predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A fusion-module</head><p>The fusion module takes as input the prediction vectors of two experts p V , p S for a given image, and a vector containing the number of training samples per class. It has three outputs: λ ∈ (0, 1) is a scalar to trade-off the visual expert against the semantic expert. w V ∈ (0, 1) k is a vector that weighs the predictions of the visual expert. Similarly, w S weighs the semantic expert predictions. Given these three outputs, a debiased score is computed for a class y: <ref type="figure" target="#fig_1">Figure 3</ref> describes the architecture of the fusion-module. It has two main parts. The first part maps the prediction scores to a meaningful joint space, by first aligning the prediction of both classifiers, and then sorting according to confidence.</p><formula xml:id="formula_0">S(y) = λw V (y)p V (y) + (1 − λ)w S (y)p S (y). (1)</formula><p>In more detail, the first part has four steps. (a) Stacking together the predictions of two experts to a Y × 2 vector. This makes the following convolution meaningful across the 2 experts axis. (b) To make convolution meaningful also along the classes axis, and since classes are categorical, we reorder classes by their prediction score according to one of the experts. Section 4.3 explains the rationale of this reordering. (c) Now that predictions are sorted, feed the sorted scores to a N f ilters × 2 × 2 convolutional network. (d) Follow with an average-pooling layer (per class), yielding a (Y − 1)-dimensional vector h.</p><p>The goal of the second part is simply to predict a debiasing coefficient for each prediction, namely, learn a function from n y to (0, 1) k . We know from <ref type="figure">Figure 1</ref> that the bias is inversely related to the number of samples. Aiming for a simple model, we train a polynomial regression that takes as input the number of samples and outputs a debiasing weight (w V (y)). The coefficients of this polynomial v 0 , ..., v d−1 are learned as a deep function over h. Similarly for f S (y).</p><p>More formally, let n y be the number of training samples of class y, and letn y = n y / max y n y be the normalized counts, then we have</p><formula xml:id="formula_1">w V (y) = σ d−1 j=0 v j (h)n j y , w S (y) = σ d−1 j=0 s j (h)n j y ,<label>(2)</label></formula><p>where d is the polynomial degree and σ denotes a sigmoid that ensures that the resulting scale is in [0, 1].</p><p>Finally, the fusion module also predicts the trade-off scalar λ to control the relative weight of the visual and semantic experts. This is achieved using a fully connected layer over h. Section 9 analyzes the contribution of each component of the approach with an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Architecture design decision</head><p>DRAGON is designed with a small number of model parameters so it can improve predictions at the tail, where very few samples are available.</p><p>Debiasing based on all expert predictions. To achieve the above goals, DRAGON implicitly learns how frequent is a class of a given sample. Namely, when the model receives a new test sample, it predicts if it is from a head class, a tail class, or somewhere between, and adjusts the confidence of the experts accordingly. To do this, it has been shown in the context of zero-shot learning that profile of confidence values are a good predictor if a sample comes from a seen or unseen class <ref type="bibr" target="#b2">[3]</ref>. DRAGON generalizes this idea to long-tail distributions. To do this it takes as input all class predictions from both experts ( <ref type="figure" target="#fig_1">Figure 3a</ref>).</p><p>Using order statistics over predictions. To process expert prediction, we point out that order statistics over the prediction vector -the maximum confidence, 2 nd max, etc. . . -provides a strong signal about confidence calibration. Using the maximum of a vector is a very common operator in deep learning, known as max pooling. Here however, there is additional important information the gap between subsequent order statistics, like max −2 nd max. As an intuitive example, a maximal prediction of 0.6 should be interpreted differently if the 2 nd max is 0.4 or 0.1.</p><p>Order statistics can be easily computed by sorting the vector of predictions <ref type="figure" target="#fig_1">(Figure 3b</ref>). Sorting also increases the sample efficiency for learning, because later layers have each order statistic located at a fixed position in their input regardless of class. The function learned over orderstatistics gaps is therefore shared across all classes.</p><p>The 2 × 2 convolution ( <ref type="figure" target="#fig_1">Figure 3c</ref>) works well with the sorted expert predictions. Its filters capture two signals: <ref type="bibr" target="#b0">(1)</ref> the confidence gaps between the two experts for each class; and (2) the confidence gaps between order-statistics for each expert alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate DRAGON in three unbalanced benchmark scenarios. (1) "Smooth-Tail", the long-tailed distribution of classes decays smoothly ( <ref type="figure" target="#fig_2">Figure 4</ref>) and each class is accompanied with textual class descriptors. (2) "Two-Level", the distribution has a step-shape as in <ref type="bibr" target="#b37">[38]</ref>; Most classes have many samples and the rest have few samples ( <ref type="figure">Figure 5</ref>).</p><p>(3) "Vision-only", a long-tail setup, as in Smooth-Tail, but without class descriptors.</p><p>We compare DRAGON with SoTA approaches on standard benchmarks for each of these three scenarios. See Appendix D for implementation details.</p><p>Code available at https://github.com/ dvirsamuel/DRAGON</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overview of main results</head><p>For Smooth-Tail distributed data, we evaluated DRAGON on four benchmarks that we created from existing datasets. We added textual descriptors to ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, and generated long-tail versions of CUB, SUN and AWA. Dragon outperforms all baselines on various metrics.</p><p>For Two-Level distributed data, DRAGON surpasses the current SoTA <ref type="bibr" target="#b37">[38]</ref>, tested using their experimental setup.</p><p>For Vision-only long-tail data, we tested the calibration component of DRAGON (without fusion). It achieves a new SoTA on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places365-LT <ref type="bibr" target="#b28">[29]</ref>, Unbalanced CIFAR-10/100 <ref type="bibr" target="#b6">[7]</ref> and comparable results on iNat-uralist2018 <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Smooth-Tail distribution 6.1. Datasets</head><p>To evaluate long-tail learning with class descriptors, we created benchmark datasets in two ways. First, we created long-tail versions of existing learning-with-classdescriptors benchmarks. Second, we augmented existing long-tail benchmark (ImageNet-LT) with class descriptors.</p><p>Specifically, we created new long-tail variants of the 3 main learning-with-class-descriptors benchmarks: CUB <ref type="bibr" target="#b45">[46]</ref>, SUN <ref type="bibr" target="#b32">[33]</ref> and AWA <ref type="bibr" target="#b25">[26]</ref>, illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. We ranked classes by the number of samples in each class   after assigning tail classes to be consistent with those in the Two-Level benchmark <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b37">38]</ref> (See Appendix D.2 for more details). We then computed a frequency level for each class following an exponentially decaying function of the form f (class) = ab −rank(class) . a and b were selected such that the first class has the maximum number of samples, and the last class has 2 or 3 samples depending on the dataset. We then drew a random subset of samples from each class based on their assigned frequency f (class). To create the validation set, we randomly drew a constant number of samples per class, while keeping an overall size of 20% of the training set. See dataset statistics in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-LT SUN-LT AWA-LT</head><p>As a second type of benchmark, we used the existing long-tailed ImageNet <ref type="bibr" target="#b28">[29]</ref> and augmented it with class descriptors. Specifically, we used the word2vec embeddings provided by <ref type="bibr" target="#b7">[8]</ref>, which are widely used in the literature. Their word embeddings were created by training a skipgram language model on a Wikipedia corpus to extract a 500-dimensional word vector for each class. See <ref type="bibr" target="#b7">[8]</ref>  <ref type="figure" target="#fig_2">Figure  4</ref> for visualization.</p><p>Together, this process yielded the following datasets: 1. CUB-LT, based on <ref type="bibr" target="#b45">[46]</ref>, consists of 2,945 training visual images of 200 bird species. Each species is described by 312 attributes (like tail-pattern:solid, wing-color:black). Classes have between 43 and 3 images per class. 2. SUN-LT, based on <ref type="bibr" target="#b32">[33]</ref>, consists of 4,084 training images, from 717 visual scene types and 102 attributes (like material:rock, function:eating, surface:glossy). Classes have between 12 and 2 images per class. 3. AWA-LT, based on <ref type="bibr" target="#b25">[26]</ref>, consists of 6,713 training images of 50 animal classes and 85 attributes (like texture:furry, or color:black). Classes have between 720 and 2 images per class. 4. ImageNet-LT-d, based on <ref type="bibr" target="#b28">[29]</ref>, consists of 115.8K images from 1000 categories with 1280 to 5 images per class. We use Word2Vec <ref type="bibr" target="#b29">[30]</ref> class embeddings features provided by <ref type="bibr" target="#b7">[8]</ref>, as textual descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Training scheme</head><p>The familiarity effect is substantial in the validation and test data, but not in the training data, where models may actually become more confident on rare classes. We observed this effect in CUB-LT, SUN-LT, and AWA-LT. Since we wish to train the fusion module using data that exhibits the familiarity bias, we hold-out a subset of the training data and use it to simulate the response of experts to test samples. Note that in large-scale datasets, like ImageNet-LT-d, no hold-out set is needed and DRAGON is trained on the training set. There, the familiarity bias is also present in the training data, as the models did not overfit the tail classes. Appendix C illustrates this effect in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Baselines and variants</head><p>We compared DRAGON with long-tail learning and unbalanced data approaches: Focal Loss <ref type="bibr" target="#b26">[27]</ref>, Anchor Loss <ref type="bibr" target="#b36">[37]</ref>, Range Loss <ref type="bibr" target="#b56">[57]</ref>, and LDAM Loss <ref type="bibr" target="#b6">[7]</ref> are loss manipulation approaches for long-tail distributions. FSLwF <ref type="bibr" target="#b15">[16]</ref>, OLTR <ref type="bibr" target="#b28">[29]</ref> and Classifier-Balancing (CB) <ref type="bibr" target="#b22">[23]</ref> are algorithmic approaches in the long-tail learning benchmarks. Mixture and Class Balanced Experts <ref type="bibr" target="#b38">[39]</ref> are late fusion approaches. Mixture resembles mixture-ofexperts (MoE) <ref type="bibr" target="#b20">[21]</ref> without EM optimization. It fuses the raw outputs of the two experts by a gating module. As with standard MoE models, the gating module is trained with visual-features as inputs.</p><p>For CUB-LT, SUN-LT, and AWA-LT, the visual expert was a linear layer over a pre-trained ResNet from <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b49">50]</ref>, which we trained with a balanced xent loss (CE Loss). The semantic expert was LAGO <ref type="bibr" target="#b3">[4]</ref>. For ImageNet-LT-d, we (a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-LT SUN-LT AWA-LT Method</head><p>Acc P C Acc LT Acc P C Acc LT Acc P C Acc LT  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation metrics and protocol</head><p>Evaluation Protocol: The experiments for CUB-LT, SUN-LT, and AWA-LT follow the standard protocols set by <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50]</ref>, including their ResNet-101 features. Their split ensures that none of the test classes appear in the training data used to train the ResNet-101 model. For ImageNet-LT-d we used the protocols in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23]</ref> with the pre-trained ResNet-10 and ResNeXt-50 provided by <ref type="bibr" target="#b22">[23]</ref>.  the distribution over test classes is long-tailed like the training distribution. This is expected to be the typical case in real-world scenarios. See Appendix D.1 for more details.</p><p>(c) Many-Shot, Medium-shot and Few-Shot accuracies: For ImageNet-LT-d, we follow <ref type="bibr" target="#b28">[29]</ref> and report accuracy for: Acc M S (&gt;100 training images), Acc M ED (20-100 images) and Acc F S (&lt; 20 images). <ref type="table" target="#tab_3">Table 2</ref> (a) provides the test accuracy for three longtail benchmark datasets and compares DRAGON to baselines and individual components of the DRAGON model. DRAGON achieves higher accuracy compared with all competing methods, both with respect to class-balanced accuracy (Acc P C ) and to test-distribution accuracy (Acc LT ). Improving Acc LT indicates that DRAGON effectively classifies head classes, which are heavily weighted in Acc LT . At the same time, improving Acc P C indicates that DRAGON also effectively classifies tail classes, which are up-weighted in Acc P C . <ref type="table" target="#tab_3">Table 2</ref> (b) provides the Acc P C accuracy for ImageNet-LT-d. We can directly see the benefit of fusion information between modalities -the visual expert excels on many-shot classes, Acc M S , while the semantic expert excels only on few-show classes, Acc F S . DRAGON recalibrate and fuse both experts to excel in all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Results with smooth-tail distribution</head><p>We further trained DRAGON with a balanced crossentropy loss (Bal.). This strategy has a synergistic effect with DRAGON (last row in <ref type="table" target="#tab_3">Table 2</ref>): It improves tail accuracy Acc P C for all benchmarks while only marginally hurting head accuracy AccLT . <ref type="table" target="#tab_4">Table 3</ref> compares DRAGON against common late fusion strategies, on the validation set of CUB-LT: AVG (averaging expert predictions), Max (taking the largest prediction), Product (multiplying expert predictions) and Mixture. We show that those approaches, which late fuse predictions of the two experts, are usually better at head classes (Acc LT ) while giving less accurate results for tail classes (Acc P C ). DRAGON achieves better results on both metrics because it also calibrates expert predictions. In the ablation study (Table 8) we compare our fusion module to more ablated fusion <ref type="figure">Figure 5</ref>: Two-level variants of CUB, SUN and AWA as in <ref type="bibr" target="#b37">[38]</ref>. Blue: training set, green: test set. components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Two-Level (GFSL-d) benchmark</head><p>We follow the protocol of <ref type="bibr" target="#b37">[38]</ref> on the original CUB, SUN, and AWA ( <ref type="figure">Figure 5</ref>), to compare DRAGON in a Two-Level setting.</p><p>For those datasets, many-shot classes are kept as in the original train-set, while few-shot classes have an increasing number of shots: 1,2,5,10 and 20 (in SUN up to 10 shots).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Baselines and variants</head><p>We compared DRAGON with LDAM Loss <ref type="bibr" target="#b6">[7]</ref> and with SoTA multi-modal GFSL-d approaches: ReViSE <ref type="bibr" target="#b42">[43]</ref>, CA-VAE <ref type="bibr" target="#b37">[38]</ref>, DA-VAE <ref type="bibr" target="#b37">[38]</ref> and CADA-VAE <ref type="bibr" target="#b37">[38]</ref>. Their results were obtained from the authors of <ref type="bibr" target="#b37">[38]</ref>, while LDAM results were reproduced by us.</p><p>The visual expert was a linear layer over a pre-trained ResNet from <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b49">50]</ref>, which we trained with a balanced cross-entropy loss. The semantic expert was LAGO <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluation metrics</head><p>Following <ref type="bibr" target="#b37">[38]</ref>, we evaluated the Two-Level benchmark with the Harmonic mean metric (Acc H ): It quantifies the overall performance of head and tail classes, by Acc H = 2(Acc ms Acc f s )/(Acc ms + Acc f s ). Where, Acc ms is the per-class accuracy over many-shot classes and Acc f s is the per-class accuracy over few-shot classes. <ref type="table" target="#tab_6">Table 4</ref> compares DRAGON with SoTA baselines on the Two-Level setup. Our model wins in CUB and SUN on all shots but loses on AWA for fewer than 10 samples. Furthermore, DRAGON gains better results when the number of shots increases in contrast to complex generative models like CADA-VAE <ref type="bibr" target="#b37">[38]</ref>. Appendix E.1 provides results for Acc f s and Acc ms with 1,2,5,10,20 shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Results with two-level distribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Vision-only long-tail learning</head><p>The approach presented in this paper focuses on learning from two modalities, vision and language. To understand the effect of re-calibrating we now study a simpler variant of DRAGON that can be applied to the more common vision-only long-tail learning. We name it smDRAGON for   In other words, smDRAGON is a simplified version of DRAGON that is trained on the predictions of the visualexpert only (no class descriptors being used). smDRAGON takes the predictions of a freezed visual-expert and rescale it by learning a single set of polynomial coefficients. During inference, smDRAGON balances the visual-expert predictions in a sample-by-sample basis. <ref type="table" target="#tab_9">Tables 5 and 6</ref> compare smDRAGON against approaches in the unbalanced CIFAR-10 and CIFAR-100 benchmarks, as presented in <ref type="bibr" target="#b6">[7]</ref>: CE Loss, Resample <ref type="bibr" target="#b10">[11]</ref>, Reweight <ref type="bibr" target="#b10">[11]</ref>, Focal <ref type="bibr" target="#b10">[11]</ref> and LDAM Loss <ref type="bibr" target="#b6">[7]</ref>. DRW <ref type="bibr" target="#b6">[7]</ref> denotes models that were trained with the training schedule proposed by <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_11">Table 7</ref> compares smDRAGON with popular baselines and recent long-tail learning approaches in the ImageNet-LT and Places-LT benchmarks. Those are the same baselines as in the Smooth-Tail setup (Section 6).</p><p>The results demonstrate that (1) smDRAGON outperforms all baselines and (2) combining smDRAGON with SoTA approaches (LDAM or DRW) has a synergistic effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Ablation study</head><p>To understand the contribution of individual components of DRAGON, we carried ablation experiments. We report results on the validation set, which were consistent with the test set (Appendix E.2).</p><p>Fusion-Module Architecture: <ref type="table" target="#tab_12">Table 8</ref>   <ref type="formula">6)</ref> DRAGON is our full approach described in Section 4. The comparison shows that rescaling expert predictions significantly improves Acc P C and that reducing the number of parameters using the convolutional layer is important.</p><p>To quantify the contribution of per-sample weighting, <ref type="table" target="#tab_13">Table 9</ref> compares it against per-class weighting on three long-tail benchmarks: ImageNet-LT, Places365-LT and CIFAR100-LT. To keep the comparison fair, this was done using vision-only (λ = 1), and sweeping over the same set of hyper parameters. To gain more intuition on how persample weighting helps, <ref type="figure" target="#fig_6">Fig. 7</ref>    <ref type="table">Table 6</ref>: Vision-only long-tail: smDRAGON was trained on top models trained with DRW (VE1) or LDAM-DRW (VE2) <ref type="bibr" target="#b6">[7]</ref>. Similar to <ref type="table" target="#tab_9">Table 5</ref> except that all models were trained with DRW schedule <ref type="bibr" target="#b6">[7]</ref>. Reported values are top-1 validation error. Asterisks * denote results that we reproduced using code published by the authors of <ref type="bibr" target="#b6">[7]</ref>.  <ref type="bibr" target="#b6">[7]</ref> 64.6 LDAM-DRW <ref type="bibr" target="#b6">[7]</ref> 68.0 CB τ −norm <ref type="bibr" target="#b22">[23]</ref> 69.3 CB LWS <ref type="bibr" target="#b22">[23]</ref> 69.5 smDRAGON (ours) 69.1      <ref type="table" target="#tab_1">Table 11</ref>: Ablation study, quantifying the contribution of sorting the fusion-module inputs (validation set, CUB-LT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Only</head><p>class. At the same time, per-sample weighting gives more chance to tail classes, reducing the familiarity bias.</p><p>Sharing order statistics (sorting): <ref type="table" target="#tab_1">Table 11</ref> quantifies the benefit of sorting expert predictions. As discussed in Section 4.3, sorting enables sharing of information across classes by fixing the input location of each order statistic (max, 2 nd max etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>This paper discussed two key challenges for learning with long-tail unbalanced data: A "familiarity bias", where models favor head classes, and low accuracy over tail classes due to lack of samples. We address these challenges, with DRAGON, a late-fusion architecture for visual recognition that learns with per-class semantic information. It outperforms existing methods on new long-tailed versions of ImageNet, CUB, SUN, and AWA. It further sets new SoTA on a Two-Level benchmark <ref type="bibr" target="#b37">[38]</ref>. Finally, a singlemodality variant of DRAGON improves accuracy over standard long-tail learning benchmarks, including ImageNet-LT, Places365-LT, and unbalanced CIFAR-10/100. These results show that information about the number of samples per-class can be effectively used to reduce prediction biases.</p><p>Strongly unbalanced data with a long-tail is ubiquitous in numerous domains and problems. The results in this paper show that a light-weight late-fusion model can be used to address many of the challenges posed by class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional analysis of the familiarity effect</head><p>Here we provide a deeper analysis showing that DRAGON effectively addresses the "familiarity bias".</p><p>The familiarity bias causes models to incorrectly favor head classes: <ref type="figure" target="#fig_7">Figure S8</ref>(a) shows the confusion matrix of a standard ResNet-101 trained on CUB-LT, as computed on the validation set. Classes, of the confusion matrix, are ordered by a decreasing number of training samples, with class #1 having many samples and class #200 have few samples. Black dots denote count larger than 15.</p><p>It illustrates two effects. First, the trained model correctly classifies head classes, based on the fact that the top rows have no incorrect (off-diagonal) predictions. Second, for mid and tail classes, predictions are clearly biased towards the head, since there are many more off-diagonal predictions to the left (head class predictions).</p><p>DRAGON corrects for the familiarity bias: <ref type="figure" target="#fig_7">Figures  S8(b)</ref> and S8(c) demonstrate that DRAGON learns to offset the familiarity bias. The left panel (b) shows the familiarity effect on CUB-LT before recalibration. The right panel <ref type="bibr">(c)</ref> shows that DRAGON corrects the familiarity bias and produces a more balanced average confidence across the head and tail classes.</p><p>DRAGON re-calibrate predictions: In the main paper <ref type="figure">(Figure 1(c)</ref>) we showed that a model that is trained on unbalanced data has higher confidence for head classes and it over-estimate them. By reversing the familiarity bias, smDRAGON, implicitly, also re-calibrate experts predictions. <ref type="figure" target="#fig_8">Figure 9</ref> compares the reliability diagrams for smDRAGON against raw ResNeXt-152, Temp Scaling <ref type="bibr" target="#b16">[17]</ref> and Dirichlet Calibration <ref type="bibr" target="#b24">[25]</ref> (common and SoTA calibration approaches). We report both per-class-accuracy (ACC) and expected-calibration-error (ECE) for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual experts are better at the head, semantic experts excel at the tail</head><p>Here we provide supporting evidence to our observation from Section 4 of the main paper that semantic experts are better at the tail: "Semantic descriptions of classes can be very useful for tail (low-shot) classes, because they allow models to recognize classes even with few or no training samples <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4]</ref>". Additionally, we demonstrate that the visual expert is better for the many-shot regime.</p><p>We focus on the Two-Level CUB distribution, and evaluate the accuracy for the many shot classes when restricting predictions to these classes (many-among-many), and separately the accuracy for the few-shot classes when predictions are restricted to these tail classes (few-among-few). <ref type="figure">Figure S10(a)</ref> shows the accuracy over few-shot classes of both experts in few-among-few setting. The semantic expert outperforms the visual one, and this effect stronger with fewer samples. For example, with 1-shot learning, the se- mantic expert is almost 100% better than the Visual Expert. Additionally, when we measure the accuracy of the manyshot classes in the many-among-many setup (accuracy at the head), the visual expert is better than the semantic expert <ref type="figure">Figure S10</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training the fusion-module in small scale datasets</head><p>Our goal is to have the fusion-module learn to capture the correlations between the number of training samples and the output confidence (the familiarity bias), so it can adjust for it. Unfortunately, while the familiarity effect is substantial in the validation data and the test data, it may not present in the training data in small scale datasets. The reason is: Models tend to overfit and become overconfident over rare classes in the training set. This effect is illustrated in <ref type="figure" target="#fig_9">Figure  S11</ref>(a) (compare Train versus Validation curves) for CUB-LT. We observed the effect in also in SUN-LT and AWA-LT.</p><p>To address this mismatch, we hold-out 50% of the samples of the tail classes and 20% of the samples of the head classes of the training data and use it to simulate the re- sponse of experts to test samples. This set is used for training the fusion-module.</p><p>Note, that after training the fusion module, we re-train the experts on all the training data (including the hold-out set), in order to use all data available. (See Section D for more details).</p><p>In large-scale datasets, like ImageNet-LT, no hold-out set is needed and DRAGON is trained on the training set. There, the familiarity bias is also present on the training data ( <ref type="figure" target="#fig_9">Figure S11(b)</ref>), as the models did not overfit the tail classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>Training: Considering the observation from Section C, regarding CUB-LT, SUN-LT and AWA-LT, we train the architecture in three steps: First, we train each expert on the training data excluding the hold-out set. Second, we freeze the expert weights and train the fusion-module on all the training set. Finally, we re-train the experts on all the training data in order to use all data available. For the hold-out set, we randomly draw half the samples of the tail classes and 20% of the samples of the head classes. For inference, we use the fusion-module trained at the second step with the experts trained at the third step. Platt-scaling: We used Platt-scaling <ref type="bibr" target="#b33">[34]</ref> to tune the combination coefficient λ by adding constant bias β and applying a sigmoid on top of its scores: λ = σ f 0 − β , where β is a hyperparameter selected with cross validation. Fusion-module: We trained the fusion-module using ADAM <ref type="bibr" target="#b23">[24]</ref>  For Focal Loss <ref type="bibr" target="#b26">[27]</ref>: We applied a grid-search for gamma ∈ [1, 2, ..., 15] and alpha ∈ [0.1, 0.2, 0.5, 0.75, 0.9, 1]. For Range Loss <ref type="bibr" target="#b26">[27]</ref>:</p><p>We applied a grid-search for alpha ∈ [0.1, 0.2, 0.5, 0.75, 0.9, 1] and beta ∈ [0.1, 0.2, 0.5, 0.75, 0.9, 1]. For Anchor Loss <ref type="bibr" target="#b26">[27]</ref>:</p><p>We applied a gridsearch for gamma ∈ [0.1, 0.5, 1, ..., 15] and slack ∈ [0.001, 0.005, 0.01, ..., 0.5]. For LDAM Loss <ref type="bibr" target="#b26">[27]</ref> we applied a grid-search for C ∈ [0.1, 0.2, ..., 0.9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Computing Acc LT :</head><p>Acc LT measures the accuracy over a test distribution that resembles the training distribution. However, the test and validation samples of CUB-LT, SUN-LT and AWA-LT have a different distribution because they were originated from an approximate uniform distribution. Thus, to compute Acc LT we measure the accuracy for each individual class, and then take a weighted sum according to the class frequencies in the training set. Specifically, for each class, we assign a weight P train (y) according to the train-set distribution such that 0 &lt; P train (y) &lt; 1 and y p train (y) = 1. Then we compute the accuracy per class and report the weighted average across all classes: Acc LT = k y=1 p train (y)acc(y). This is equivalent to transforming the test set to have the same distribution as the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. A clarification about the Smooth-Tail benchmark</head><p>In this section, we explain how the long-tail benchmark was aligned with the two-level benchmark, as was mentioned in the paragraph that describes the long-tailed datasets (Section 6 of the main paper).</p><p>To align the long-tail benchmark with the two-level benchmark, we first ordered the classes according to their number of samples in the two-level distribution. Then we calculated the number of samples for each class according to the required long-tail distribution, and accordingly drew samples to construct the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Acc P C Acc LT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Training CADA on Smooth-Tail benchmark</head><p>In this section, we explain how we trained CADA-VAE <ref type="bibr" target="#b37">[38]</ref> for the long-tail benchmark.</p><p>To evaluate CADA-VAE <ref type="bibr" target="#b37">[38]</ref> on long-tail benchmarks we used the code published by the authors and followed the training protocol exactly as they used for the two-level distribution. Since the protocol relies on a hard distinction between head classes and tail classes, we had to choose where to partition the smooth long-tail distribution to head and tail. Our solution is simple. It is based on the fact that we aligned the order of classes in the long-tail distribution to be the same order as in the two-level split (Section D.2). The alignment allowed us to use the same partition to head and tail as used for the two-level benchmark.</p><p>E. Additional metrics E.1. Acc f s and Acc ms on Two-Level benchmarks <ref type="table" target="#tab_1">Table S15</ref> provides the results of Acc f s and Acc ms (described in section 7) for the Two-Level benchmark. We show results for 1,2,5 and 10-shots. At the main paper we reported the results for the Acc H metrics, which is derived from Acc f s and Acc ms reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Ablation results on the test set</head><p>In the main paper (9) we described results for ablation study on the validation set. Here we report results for the <ref type="table">Table S</ref> 15: Comparing DRAGON with SoTA GFSL models and baselines with increasing number of few-shot training samples on the CUB, SUN and AWA datasets. We report per-class Acc ms , Acc f s and Acc H . Each cell represents 1-shot,2shot,5-shot and 10-shot accuracies same model variant on the test set.</p><p>Tables S12 and S13 show the results of the ablation study on the test set. It shows the same behavior as the ablation study on the validation set that was reported in the main paper. <ref type="table" target="#tab_1">Table S14 compares</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The DRAGON architecture for long-tail learning with class-descriptors. The visual-expert and attribute-expert each outputs a prediction vector fed to a fusion module. The fusion module combines expert predictions and debias them. Blue, network components. Yellow, input to the fusion module. Green, the outputs of the fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the fusion-module for long-tail learning with class-descriptors. In blue, network components. In yellow, inputs to the fusion-module and in green, activations or outputs of the fusion-module. The inputs P V denote the softmax prediction vector of the Visual Expert, and P S that of the Semantic Expert. The outputs W V , W S and λ are used in Eq. (1) for re-weighting the inputs. See Section 4.2 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Long-tailed versions of CUB, SUN, AWA and ImageNet. Number of samples for the training, validation and test sets are shown respectively by blue, yellow and green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Evaluation metrics: We evaluated DRAGON on the Smooth-Tail benchmark with the following metrics: (a) Per-Class Accuracy (Acc P C ): Balanced accuracy metric that uniformly averages the accuracy of each class 1 k k y=1 Acc(y), where Acc(y) is the accuracy of class y. (b) Long-Tailed Accuracy (Acc LT ): Test accuracy, where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of vision-only smDRAGON. single-modality-DRAGON, and show that it achieves new state-of-the-art results, compared to uni-modals baselines, on ImageNet-LT, Places365-LT, CIFAR-10 and CIFAR-100. On iNaturalist smDRAGON is comparable to SoTA. To adapt to single-modality, we train smDRAGON only on the predictions of the visual-expert. It outputs a single set of coefficients {w V (y)} y∈Y to rescale the predictions of the visual expert, instead of two sets of coefficients. Subsequently, Eq. (1) reduces to S(y) = w V (y)p V (y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>compares the performance of various components of the fusion-module on CUB-LT. (1) F.C.: predicts λ using a fully-connected layer over p V , p S , no re-balancing (w V (y) = w S (y) = 1, ∀y). (2) F.C. &amp; 1/n y rescale: learns λ as in F.C., rescales experts predictions by n y . (3) F.C. &amp; non-parametric rescale: learns λ as F.C. and rescales both experts predictions by a learned non-parametric weight for each class instead of a polynomial. (4) Conv. &amp; non-parametric rescale: like (3), then applies sorting and convolution (Section 4.2). (5) Conv. &amp; single parametric rescale replaces the nonparametric re-scaling weights by a single polynomial of parametrized weights. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Using per-sample weighting, images typical for the class Mousetrap (softmax has low-entropy) are weighed more strongly than non-typical images (softmax has highentropy). Per-class weighting reweighs all samples for that class the same (0.7), hurting recognition of typical images.Acc P C Acc LT VISUAL EXPERT + SMDRAGON 55.8 66.0 SEMANTIC EXPERT + SMDRAGON 57.7 63.4 DRAGON (OURS) 60.1 67.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S 8 :</head><label>8</label><figDesc>DRAGON learns to offset the familiarity bias. (a) A confusion matrix of a ResNet101 trained on CUB-LT as a function of the number of samples per class. The matrix shows markers for pairs of (gt, predicted) whose count is larger than 15. (b) Average-confidence per-class of the classifier. (c) Similar curve as (b) but for DRAGON. Black lines depict a linear regression line. DRAGON per-class confidence has smaller dependence on the number of samples in the train.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. S 9 :</head><label>9</label><figDesc>Reliability Diagrams on ImageNet-LT, for (a) raw ResNext-50, (b) smDRAGON, (c) Temperature-Scaling [17] and (d) Dirichlet-Calibration [25]. We report expectedcalibration-error (ECE) and per-class accuracy (ACC) (a) Few-among-few (b) Many-among-many Fig. S 10: Accuracy as a function of number of samples at the tail, of the Visual Expert and the Semantic Expert used in our study. (a) Accuracy among few-shot classes; The Semantic expert outperforms the visual expert. (b) Accuracy among many-shot classes; The Visual expert outperforms, regardless of the number of samples at the tail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. S 11 :</head><label>11</label><figDesc>The familiarity bias effect: (a) On CUB-LT the effect is strong on validation samples (blue) but not on training samples (orange). (b) On ImageNet-LT the effect is prominent on both train and validation samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>three different training protocols: (1) All-Train: Training the DRAGON fusion-module naively without a hold-out set. (2) End-To-End: Training all the architecture (both experts and fusion-module) end to end in an early fusion manner. (3) Three-Stage-Training: Training our models as explained in section D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Properties of CUB-LT, SUN-LT and AWA-LT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Smooth-tail distribution: Rows with * denote results reproduced by us. The rest were taken from [23, 29]. VE and SE refer to the visual-expert and semantic-expert that were used to train DRAGON. Bal. refers to training DRAGON with a balanced loss. (a) Comparing DRAGON with baselines on the long-tailed benchmark datasets. We report Per-Class Accuracy Acc P C and Long-Tailed Accuracy Acc LT . (b) Comparing DRAGON with baselines on the long-tailed ImageNet with word embeddings.followed [23] and set the visual expert to be ResNet-10 or ResNeXt-50. The semantic expert was DEM [56].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparing DRAGON against common late-fusion approaches on the validation set of CUB-LT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>10.9 36.0 52.2 61.5 4.3 11.5 26.6 37.0 12.4 24.8 41.1 57.0 68.6 REVISE [43] 36.3 41.1 44.6 50.9 -27.4 33.4 37.4 40.8 56.1 60.3 64.1 67.8 -CA-VAE [38] 50.6 54.4 59.6 62.2 -37.8 41.4 44.2 45.8 64.0 71.3 76.DRAGON (ours) 55.3 59.2 63.5 67.8 69.9 41.0 43.8 46.7 48.2 67.1 69.1 76.7 81.9 83.3</figDesc><table><row><cell>Two-Level</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell></cell><cell>SUN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AWA</cell><cell></cell><cell></cell></row><row><cell># shots</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>LDAM [7]*</cell><cell cols="13">2.4 6 79.0</cell><cell>-</cell></row><row><cell>DA-VAE [38]</cell><cell cols="4">49.2 54.6 58.8 60.8</cell><cell>-</cell><cell cols="8">37.8 40.8 43.6 45.1 68.0 73.0 75.6 76.8</cell><cell>-</cell></row><row><cell cols="15">CADA-VAE [38] 55.2 59.2 63.0 64.9 66.0 40.6 43.0 46.0 47.6 69.6 73.7 78.1 80.2 80.9</cell></row><row><cell>CE Loss* (VE)</cell><cell cols="14">1.2 6.9 30.2 50.2 60.9 1.8 8.9 25.1 38.3 11.0 20.0 47.8 69.9 73.9</cell></row><row><cell>LAGO* (SE)</cell><cell cols="14">23.0 33.2 49.0 58.6 64.8 19.5 23.2 25.6 27.8 20.2 33.0 59.0 68.7 75.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Two-Level distributions: Comparing DRAGON on Two-Level CUB, SUN and AWA with SoTA GFSL models and baselines and with increasing number of few-shot training samples. Values denote the Harmonic mean Acc H . VE and SE refer to the visual-expert and semantic-expert that were used to train DRAGON.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10</head><label>10</label><figDesc></figDesc><table /><note>compares DRAGON with smDRAGON on CUB- LT. It shows that fusing information between modalities (third row) gives better results than re-scaling expert predic- tions alone (first and second rows). Finally, on iNaturalist 2018, smDRAGON is comparable to SoTA, reaching 69.1% compared to 69.5% (CB-LWS [23]).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ReSample 29.45 13.21 38.14 15.41 66.56 44.94 66.23 46.92 [11] ReWeight 27.63 13.46 38.06 16.20 66.01 42.88 78.69 47.52 [11] Focal 25.43 12.90 39.73 16.54 63.98 42.01 80.24 49.98 CE [7] 29.64 13.61 36.70 17.50 61.68 44.30 61.45 45.37 CE* (VE) 29.81 13.12 36.61 17.78 61.72 43.77 61.59 45.75 Focal [27] 29.62 13.34 36.09 16.36 61.59 44.22 61.43 46.54 LDAM [7] 26.65 13.04 33.42 15.00 60.40 43.09 60.42 43.73 smDRAGON (ours) 22.08 12.17 27.10 12.38 58.01 42.22 54.43 40.97</figDesc><table><row><cell>Vision-Only</cell><cell></cell><cell cols="2">Unb. CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">Unb. CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tail</cell><cell cols="2">two-level</cell><cell cols="2">long-tail</cell><cell cols="2">two-level</cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>[11]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">plots the per-sample weights</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">of four images from a ImageNet-LT head class (mouse-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Vision-only long-tail. Error rate of ResNet32 on unbalanced CIFAR-10 and CIFAR-100 [7], comparing smDRAGON and SoTA techniques. smDRAGON was trained over predictions of the cross-entropy model (CE*). Reported values are the top-1 validation error. Asterisks * denote results reproduced using published code. 13.57 26.67 13.17 59.49 43.49 58.91 44.72 LDAM-DRW [7] 22.97 11.84 23.08 12.19 57.96 41.29 54.64 40.54 LDAM-DRW* [7] (VE2) 22.96 11.84 23.41 12.20 57.89 41.61 54.65 43.48 VE1 + smDRAGON 20.37 12.06 21.54 11.94 56.50 42.11 53.32 40.66 VE2 + smDRAGON 21.22 11.84 20.64 12.37 56.70 41.23 54.07 40.35</figDesc><table><row><cell>Vision-Only</cell><cell></cell><cell cols="2">Unb. CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">Unb. CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tail</cell><cell cols="2">two-level</cell><cell cols="2">long-tail</cell><cell cols="2">two-level</cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>CE-DRW* (VE1)</cell><cell cols="8">24.73 13.52 28.65 13.90 59.23 42.19 58.93 45.00</cell></row><row><cell>M-DRW [15]</cell><cell>24.94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Vision-only long-tail: Baseline results where copied directly from<ref type="bibr" target="#b6">[7]</ref> and<ref type="bibr" target="#b22">[23]</ref>. Left: smDRAGON achieves better Acc P C on Places365-LT and ImageNet-LT. Right: Comparing smDRAGON on long-tailed iNatrualist. smDRAGON achieve comparable results compared to SoTA baselines. Acc LT #params</figDesc><table><row><cell>F.C.</cell><cell>54.0 67.1</cell><cell>403</cell></row><row><cell>F.C. &amp; 1/n y RESCALE</cell><cell>56.7 60.3</cell><cell>403</cell></row><row><cell>F.C. &amp; NON-PARAMETRIC RESCALE</cell><cell cols="2">58.2 68.0 81,406</cell></row><row><cell>CONV. &amp; NON-PARAMETRIC RESCALE</cell><cell cols="2">58.7 68.2 40,612</cell></row><row><cell cols="2">CONV. &amp; SINGLE PARAMETRIC RESCALE 59.0 67.5</cell><cell>613</cell></row><row><cell>DRAGON (OURS)</cell><cell>60.0 70.9</cell><cell>1,015</cell></row></table><note>trap). Per-sample weighs more strongly "easy" samples (low entropy) than non-typical samples. This illustrates that per-sample weighting does not penalize "justified" high- confidence predictions if they happen to arrive from a head Acc P C</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ablation study, comparing different fusion and re-scaling approaches. The results show the contribution of the convolutional backbone and the re-scaling method for the two experts (validation set, CUB-LT).</figDesc><table><row><cell></cell><cell>Places365-LT</cell><cell cols="2">ImageNet-LT</cell><cell>CIFAR100-LT</cell></row><row><cell></cell><cell cols="4">ResNet-50 ResNet-10 ResNeXt-50 ResNet-32</cell></row><row><cell>CE Loss (VE)</cell><cell>30.2</cell><cell>34.8</cell><cell>44.4</cell><cell>38.3</cell></row><row><cell>Per-class</cell><cell>36.9</cell><cell>40.0</cell><cell>49.2</cell><cell>40.5</cell></row><row><cell>Per-sample</cell><cell>38.1</cell><cell>42.0</cell><cell>50.1</cell><cell>42.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Ablation of per-sample weighting on vision-only benchmarks. VE refers to visual-expert.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Ablation study, comparing smDRAGON to DRAGON on CUB-LT: Fusing information between modalities improves performance (test set, CUB-LT).</figDesc><table><row><cell>Sorting</cell><cell cols="2">Acc P C Acc LT</cell></row><row><cell>No Sorting</cell><cell>58.7</cell><cell>68.2</cell></row><row><cell>Sorting By Visual Expert</cell><cell>60.0</cell><cell>70.9</cell></row><row><cell>Sorting By Semantic Expert</cell><cell>60.0</cell><cell>70.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>optimizer. For large-scale datasets, like ImageNet-LT, Places-LT and iNaturalist, we used L 2 regularization, selected by hyperparameter optimization using grid search ∈ {10 −5 , 10 −4 , 10 −3 }], to avoid overfitting. Hyper-parameter tuning: We determined the number of training epochs (early-stopping), selected architecture alternatives, and tuned hyperparameters using the validation set, using Acc LT for Smooth-Tail and Vision-only, and Acc P C for Two-Level.For DRAGON: We optimized the following hyperparameters: (1) Number of filters in the convolution layer ∈ {1, .., 4}. (2) Degree of polynomial in Eq.2 ∈ {2, 3, 4}. (3) Learning rate ∈ {10 −5 , 10 −4 , 10 −3 }. [0.0001, 0.00015, ..., 0.015] and classifier learning rate ∈ [0.0001, 0.0005, ..., 0.1]. We used a batch size of 64. Sorting-By-Semantic-Expert 59.8 67.7Table S12: Ablation study, quantifying the contribution of sorting the fusion-module inputs (test set, CUB-LT).</figDesc><table><row><cell>(4) Bias term of Platts rescaling β ∈ [−2, 2]. For CADA-VAE [38]: We applied a grid search for the latent embedding space ∈ [12, 25, 50, 64, 100, 200, 250], variational-autoencoder learning rate ∈ Sorting Acc P C Acc LT No-Sorting 58.5 57.0 Sorting-By-Visual-Expert 60.1 67.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>F.C. 56.4 66.3 F.C. &amp; 1/n y re-scale 56.4 56.2 F.C. &amp; non-parametric re-scale 58.2 64.3 Conv. &amp; non-parametric re-scale 58.4 67.1 Conv. &amp; single parametric re-scale 59.3Table S13: Ablation study, comparing different fusion and re-scaling approaches. The results show the contribution of the convolutional backbone and the re-scaling method for the two experts (test set, CUB-LT).Table S 14: Ablation study, quantifying the contribution the effect of three-stage training as proposed in Section D. (testset, CUB)</figDesc><table><row><cell></cell><cell></cell><cell>64.3</cell></row><row><cell>DRAGON (ours)</cell><cell>60.1</cell><cell>67.7</cell></row><row><cell>Training Process</cell><cell>Acc P C</cell><cell></cell></row><row><cell>All-Train</cell><cell>56.6</cell><cell></cell></row><row><cell>End-To-End</cell><cell>46.4</cell><cell></cell></row><row><cell cols="2">Three-Stage-Training 60.1</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Dragons, like many distributions, have long-tails and are cool. For acronym lovers, DRAGON also stands for "a moDulaR Approach for lonG-tail classificatiON".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>DS was funded by a grant from the Israeli innovation authority, through the AVATAR consortium and by a grant from the Israel Science Foundation (ISF 737/2018). Study was also funded by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V-K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal vehicle detection: fusing 3d-lidar and color camera data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive confidence smoothing for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic and-or attribute grouping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthetic examples improve generalization for rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smote: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">C 4 . 5 , class imbalance , and cost sensitivity : Why under-sampling beats oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-modal cycle-consistent generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dynamic few-shot visual learning without forgetting. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komodakis</forename><forename type="middle">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Borderline-smote: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The inaturalist challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perelló-Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kängsepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learn to combine modalities in multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generative model for zero shot learning using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative model with semantic embedding and integrated classifier for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pambala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in large margin classifiers</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A multi-label multimodal deep learning framework for imbalanced data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anchor loss: Modulating loss scale based on prediction difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Long-tailed recognition using class-balanced experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03706</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning robust visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Availability: A heuristic for judging frequency and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving class probability estimates for imbalanced data. Knowledge and Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dahabreh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Class probability estimates are unreliable for imbalanced data (and how to fix them)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Dahabreh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot learning -A comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot learning -the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">F-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Model selection for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-shot kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Model Acc ms Acc f s Acc H Most Common Class</title>
		<imprint/>
	</monogr>
	<note>* 0.7, 0.7, 0.7, 0.7 0, 0, 0, 0 0, 0, 0, 0</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca-Vae</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>38] 58.2, 57.6, 60.0, 62.2 44.8, 51.6, 59.4, 62.3 50.6, 54.4, 59.6, 62.2 DA-VAE [38] 50.6, 56.0, 56.8, 56.8 47.9, 53.2, 61.0, 65.4 49.2, 54.6, 58.8, 60.8 CADA-VAE [38] 59.6, 60.9, 62.3, 63.1 51.4, 57.5, 63.6, 68.8 55.2, 59.2, 63.0, 64.9 CE Loss* (VE) 72.7, 72.9, 72.7, 72.0 0.6, 3.7, 19.1, 38.6 1.2, 6.9, 30.2, 50.2</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca-Vae</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>38] 35.8, 37.5, 37.5, 39.0 40.0, 46.5, 53.8, 55.7 37.8, 41.4, 44.2, 45.8</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Da-Vae</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>38] 34.8, 37.3, 38.6, 38.2 41.4, 45.1, 50.2, 54.8 37.8, 40.8, 43.6, 45.1 CADA-VAE [38] 37.6, 38.2, 39.4, 41.9 44.1, 49.0, 55.3, 55.1 40.6, 43.0, 46.0, 47.6 CE Loss* (VE) 46.3, 46.3, 46.2, 45.6 0.9, 4.9, 17.2, 33.0 1.8, 8.9, 25.1, 38.3</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>SE) 82.6, 81.9, 81.7, 81.5 11.5, 20.6, 46.2, 59.4 20.2, 33.0, 59.0, 68.7 DRAGON (ours) 74.5, 76.7, 79.2, 81.7 61.1, 62.9, 74.3, 82.1 67.1, 69.1, 76.7, 81.9 (c) Two-Level AWA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
