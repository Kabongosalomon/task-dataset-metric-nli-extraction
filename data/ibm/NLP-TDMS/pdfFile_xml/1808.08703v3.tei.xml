<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Text through Adversarial Training using Skip-Thought Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Ahamad</surname></persName>
							<email>afroz.sahamad@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">BITS Pilani Hyderabad Campus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Text through Adversarial Training using Skip-Thought Vectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GANs have been shown to perform exceedingly well on tasks pertaining to image generation and style transfer. In the field of language modelling, word embeddings such as GLoVe and word2vec are state-of-the-art methods for applying neural network models on textual data. Attempts have been made to utilize GANs with word embeddings for text generation. This study presents an approach to text generation using Skip-Thought sentence embeddings with GANs based on gradient penalty functions and f-measures. The proposed architecture aims to reproduce writing style in the generated text by modelling the way of expression at a sentence level across all the works of an author. Extensive experiments were run in different embedding settings on a variety of tasks including conditional text generation and language generation. The model outperforms baseline text generation networks across several automated evaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability and effectiveness in real life tasks are demonstrated through human judgement scores.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inducing a particular style in generated text is a promising development which can lead to producing acceptable responses in dialogue generation, image captioning and artificial chat bot systems. In unsupervised text generation, estimating the distribution of real text from a corpus is a challenging task. Recent approaches using adversarial training have addressed this issue by trying to overcome the exposure bias that models trained for maximum likelihood suffer from. This work proposes an approach for text generation using a Generative Adversarial Network (GAN) with Skip-Thought vectors <ref type="bibr">(STGAN)</ref>. GANs <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref> are a class of neural networks that explicitly train a generator to produce highquality samples by pitting the generator against an adversarial discriminative model. GANs output differentiable values and the task of discrete text generation is challenging because of the nondifferentiable nature of generating discrete symbols. Hence, in the present work, the GANs are trained with sentence embedding vectors as a differentiable input. The sentence embeddings are produced using Skip-Thought , a neural network model for learning fixed length representations of sentences.</p><p>People's way of expression and communication intention is more diverse across utterances than the vocabulary. To imitate this, the proposed STGAN architecture models the variability at the utterance level in a corpus rather than at word or character level. The effectiveness of this approach is evaluated on automated corpus-based metrics: BLEU-n <ref type="bibr" target="#b17">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b2">(Banerjee and Lavie, 2005)</ref> and ROUGE <ref type="bibr" target="#b11">(Lin, 2004)</ref> using different embeddings: Average GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref>, Vector Extrema GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref> and Skip-Thought . We perform an empirical study with human judgements to assess both the quality and the style reproduction in the generated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Deep neural network architectures have demonstrated strong results on natural language generation tasks such as dialogue response generation and machine translation. Early techniques for generating text conditioned on some input information were template or rule-based engines <ref type="bibr" target="#b13">(McRoy et al., 2000)</ref>, or probabilistic models such as ngram. In the recent past, state-of-the-art results on these tasks have been achieved by recurrent <ref type="bibr" target="#b19">(Press et al., 2017;</ref><ref type="bibr" target="#b15">Mikolov et al., 2010)</ref> and con-volutional neural network models trained for likelihood maximization. Very recently, attempts have been made to generate text using purely generative adversarial training .</p><p>Unsupervised learning with deep neural networks in the framework of encoder-decoder models has become the state-of-the-art methods for approaching NLP problems <ref type="bibr" target="#b26">(Young et al., 2017)</ref>. Recent text generation models have used a wide variety of GANs such as policy-gradient based sequence generation framework <ref type="bibr" target="#b27">(Yu et al., 2016)</ref>. <ref type="bibr" target="#b6">Fedus et al. (2018)</ref> have used an actor-critic conditional GAN to fill in missing text conditioned on the surrounding text for natural language generation tasks. GANs have also been used for text style transfer by <ref type="bibr" target="#b25">Yang et al. (2018)</ref> where language models act as the discriminator and by <ref type="bibr" target="#b3">Chen et al. (2018)</ref> with the introduction of a new f-measure termed as feature-mover's distance.</p><p>Using adversaries of word and character level embeddings for text generation has been explored by <ref type="bibr" target="#b21">Rajeswar et al. (2017)</ref>. Models trained using generative adversarial networks or variational autoencoders have been shown to learn representations of continuous structures by leveraging deep latent variables such as text embeddings <ref type="bibr" target="#b28">(Zhao et al., 2017)</ref>. This work explores injecting sentence embeddings produced using the Skip Thought architecture  into GANs in different setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Skip-Thought Generative Adversarial Network</head><p>In literature corpora such as fantasy and science fiction novels, the vocabulary does not vary significantly across the authors, but the manner of expression does, which is intuitively best captured at the level of sentences than words. The approach, hence, that this work takes in generating sentences with the writing style of one author is to make the adversarial model approximate the distribution of all sentences (rather than words or characters) in a latent space using skip-thought architecture. Previous attempts on text generation have used the character and word-level embeddings instead with GANs <ref type="bibr" target="#b21">(Rajeswar et al., 2017)</ref>. This section introduces Skip-Thought Generative Adversarial Network with a background on neural network models that it is based on. The Skip-Thought model  produces embedding vectors for sentences present in train-ing corpus. These vectors constitute the real distribution for the discriminator network. The generator network produces sentence vectors similar to those from the encoded real distribution. The generated vectors are sampled over the course of training and then decoded to produce sentences using a Skip-Thought decoder conditioned on the same text corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skip-Thought Vectors</head><p>Skip-Thought is an encoder-decoder framework with an unsupervised approach to train a generic, distributed sentence encoder. The encoder maps sentences sharing semantic and syntactic properties to similar vector representations and the decoder reconstructs the surrounding sentences of an encoded passage. The sentence encoding approach draws inspiration from the skip-gram model in producing vector representations using previous and next sentences.</p><p>The Skip-Thought model uses an RNN encoder with GRU activations <ref type="bibr" target="#b5">(Chung et al., 2014)</ref> and an RNN decoder with conditional GRU. This combination is identical to the RNN encoder-decoder of  used in neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip-Thought Architecture</head><p>For a given sentence tuple (s i−1 , s i , s i+1 ), let w t i denote the t-th word in sentence s i and x t i denote its word embedding. The model is described as:</p><p>Encoder. Encoded vectors for a sentence s i with N words w i , w i+1 ,...,w n are computed by iterating over the following sequence of equations:</p><formula xml:id="formula_0">r t = σ(W r x t + U r h t−1 ) z t = σ(W z x t + U z h t−1 ) t = tanh(Wx t + U(r t h t−1 )) h t = (1 − z t ) h t−1 + z t t</formula><p>where h t i is a hidden state at each time step and interpreted as a sequence of words w 1 i ,...,w n i , t is the proposed state update at time t, z t is the update gate and r t is the reset gate. Both update gates take values between zero and one.</p><p>Decoder. A neural language model conditioned on the encoder output h i serves as the decoder. Bias matrices C z , C r , C are introduced for the update gate, reset gate and hidden state computation by the encoder. Two decoders are used in parallel, </p><formula xml:id="formula_1">r t = σ(W d r x t−1 + U d r h t−1 + C r h i ) z t = σ(W d z x t−1 + U d z h t−1 + C z h i ) t = tanh(W d x t−1 + U d (r t h t−1 ) + Ch i ) h t i+1 = (1 − z t ) h t−1 + z t t</formula><p>Objective. For the same tuple of sentences, objective function is the sum of log-probabilities for the forward and backward sentences conditioned on the encoder representation:</p><formula xml:id="formula_2">t logP (w t i+1 |w &lt;t i+1 , h i )+ t logP (w t i−1 |w &lt;t i−1 , h i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Adversarial Networks</head><p>Generative Adversarial Networks <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref> are deep neural net architectures comprised of two networks, contesting with each other in a zero-sum game framework. For a given data, GANs can mimic learning the underlying distribution and generate artificial data samples similar to those from the real distribution. Generative Adversarial Networks consists of two players: a Generator and a Discriminator. The generator G tries to produce data close to the real distribution P (x) from some stochastic distribution P (z) termed as noise. The discriminator D's objective is to differentiate between real and generated data G(z).</p><p>The two networks -generator and discriminator compete against each other in a zero-sum game. The minimax strategy dictates that each network plays optimally with the assumption that the other network is optimal. This leads to Nash equilibrium which is the point of convergence for GAN model.</p><p>Objective. <ref type="bibr" target="#b7">Goodfellow et al. (2014)</ref> have formulated the minimax game for a generator G, discriminator D adversarial network with value function V (G, D) as:</p><formula xml:id="formula_3">min G max D V (D, G) = E x∼p data (x) [logD(x)]+ E z∼pz(z) [log(1 − D(G(z)))]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>The STGAN architecture <ref type="figure" target="#fig_0">(Figure 1)</ref> has two components: Skip Thought encoder-decoder and a generative adversarial network. The model uses a deep convolutional generative adversarial network, similar to the one used in DCGAN <ref type="bibr" target="#b20">(Radford et al., 2015)</ref>. During the training, the generator network is updated twice for each discriminator network update to prevent fast convergence of the discriminator network.</p><p>The Skip-Thought encoder for the model encodes sentences using 2400 GRU units <ref type="bibr" target="#b5">(Chung et al., 2014)</ref> with a word vector dimensionality of 620. The encoder combines the sentence embeddings to produce 4800-dimensional combineskip vectors with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model. This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments 1 . For training of the STGAN, the Skip-Thought encoder produces sentence embedding vectors which are labelled as real samples for GAN discriminator.</p><p>The decoder uses greedy decoding by taking the argmax over the softmax output distribution for a given time-step which also acts as input for next time-step. It reconstructs sentences conditioned on a sentence vector by randomly sampling from the predicted distributions with a preset beam width. A 620 dimensional RNN word embedding is used for the decoder with 1600 hidden GRU decoding units. All experiments are performed using the Adam optimizer (Kingma and Ba, 2014) with gradient clipping and a batch size of 16.</p><p>Each sentence is appended with a start token &lt;s&gt; and an end token &lt;/s&gt; before encoding. During the process of training generator network with these embeddings, some generated vectors are randomly sampled. The sampled vectors are decoded using pretrained Skip-Thought decoder to produce a probability distribution over the vocabulary in order to reconstruct sentences. The decoding is terminated when the stop token &lt;/s&gt; is encountered during reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Improving Training and Loss</head><p>The training process of a GAN is notably difficult <ref type="bibr" target="#b22">(Salimans et al., 2016)</ref> and several improvement techniques such as batch normalization, feature matching, historical averaging <ref type="bibr" target="#b22">(Salimans et al., 2016)</ref> and unrolling GAN <ref type="bibr" target="#b14">(Metz et al., 2016)</ref> have been suggested for making the training more stable. Training the Skip-Thought GAN often results in mode dropping <ref type="bibr" target="#b24">Srivastava et al., 2017)</ref> with a parameter setting where it outputs a very narrow distribution of points. To overcome this, it uses minibatch discrimination by looking at an entire batch of samples and modeling the distance between a given sample and all the other samples present in that batch.</p><p>The minimax formulation for an optimal discriminator in a vanilla GAN is Jensen-Shannon Distance between the generated distribution and the real distribution.  used Wasserstein distance or earth mover's distance to demonstrate how replacing distance measures can improve training loss for a GAN. <ref type="bibr" target="#b8">Gulrajani et al. (2017)</ref> have incorporated a gradient penalty regularizer term in WGAN objective for discriminator's loss function. The experiments in this work use the above f-measures to improve performance of Skip-Thought GAN on text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conditional Generation of Sentences.</head><p>GANs can be conditioned on certain attributes to generate real valued data <ref type="bibr" target="#b16">(Mirza and Osindero, 2014;</ref><ref type="bibr" target="#b20">Radford et al., 2015)</ref>. In this experiment, both the generator and discriminator are conditioned on the Skip-Thought encoded vectors.</p><p>The data used for this setup consists of 250,000 sentences chosen from the BookCorpus dataset  with a training/test/validation split of 5/1/1. All the sentences belong to one series of fantasy novels by a particular author of English language. This selection implies that the author's word choice, sentence structure, figurative language, and sentence arrangement are consistent and well-represented across the dataset. Conditioning on this high-level outline gives more robustness to the model in terms of generated samples.</p><p>The decoded sentences form the evaluation set for measuring performance of different models under corpus level BLEU-n <ref type="bibr" target="#b17">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b2">(Banerjee and Lavie, 2005)</ref> and ROUGE <ref type="bibr" target="#b11">(Lin, 2004)</ref> metrics. <ref type="table" target="#tab_1">Table 1</ref> compares these results for the proposed STGAN against standard LSTM and Attention Bidirectional LSTM models in two settings -using Skip Thought vectors and using tied GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref> embeddings. For this experiment, GloVe Average is obtained by averaging GloVe embeddings of all the words composing a given sentences while GloVe Extreme is computed by taking the most extreme value for each dimension across embeddings of all the words. All the three models used in the experiment: LSTM, Attention BiLSTM and Wasserstein GAN take the above three embeddings as input in separate runs and output vectors which are decoded to reconstruct sentences using the corresponding embedding's decoder. <ref type="table" target="#tab_2">Table 2</ref> shows improvements in metric scores when using Wasserstein distance and gradient penalty regularizer as discussed in section 3.4. WGAN-GP gives the strongest across-the-board performance in both the GloVe and Skip-Thought settings, so we use this as the basis for the rest of our experiments. The METEOR scores are reportedly better for other models because though it does not rely on embeddings but it includes notions of synonymy and paraphrasing to compute alignment between hypothesis and reference sentences <ref type="bibr" target="#b23">(Sharma et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Generation.</head><p>Language generation is performed on a dataset comprising simple English sentences referred to    <ref type="bibr" target="#b21">(Rajeswar et al., 2017)</ref>. The CMU-SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words. The vectors are extracted in batches of same-lengthed sentences for encoding. The samples represent how mode collapse is manifested when using least-squares distance <ref type="bibr" target="#b12">(Mao et al., 2016)</ref> f-measure without minibatch discrimination.  ) and 3(d) contains samples when using a gradient penalty regularizer term as WGAN-GP <ref type="bibr" target="#b8">(Gulrajani et al., 2017)</ref>. The two models generate longer human-like sentences and over a more diverse vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Scores and Correlations</head><p>The performance of this approach to generate new sentences has been evaluated in reproducing writ-2 https://github.com/clab/sp2016. 11-731/tree/master/hw4/data ing style of a particular author. The participant group consisted of 14 individuals, who were familiar with writing style of the said author by having read all but a few of the literary works of the author. The setup prevents them from being certain whether a sentence in question has or has not appeared in any work of the author that they have already read. To form the evaluation set of sen-   tences, the generated samples were mixed with real sentences from the author's writing. 10 sentences from this mixed pool were chosen at random to be presented to each person. The participants were asked to mark on a scale of 1 to 5 if they thought that a sentence seemed to belong to the author's works or was generated from a model, with 1 being certainly from the author and 5 being certainly from a model. <ref type="table" target="#tab_5">Table 4</ref> shows the weighted scores computed as |rating − 3| to account for the degree of uncertainty addressed by a participant when rating. The models performs well with 39.02% of the generated samples being marked as written by the author while a greater 62.96% of the actual sentences from author's writing being marked as fake generated ones. <ref type="figure" target="#fig_1">Figure 2</ref> compiles Pearson's correlation coefficients between the obtained human scores and Skip-Thought GAN scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work presents a simple and effective model for text generation based on adversarial training using sentence embeddings. It shows how the use of sentence-level embeddings allows modelling the way of expression of an author in generated text in a better way than when using word-level embeddings. A performance comparison across several metrics is made between different GAN architectures with improved training stability and attention augmented LSTM models. Finally, it discusses how the automated corpus-based evaluations correlate with human judgements. In future, this work aims to be applied for synthesizing images from text, exploring complementary architectures to projects like neural-storyteller 3 where skip-thought embeddings are already used to perform image captioning with story-style transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Skip-Thought Generative Adversarial Network model architecture one each for sentences s i+1 and s i−1 . The following equations are iterated over for decoding:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Pearson's correlation coefficient between automated computed metrics and human scores. Human scores correlate well with BLEU-3 and ROUGE scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of models on word-overlap based automated metrics when trained with different embeddings. Skip-Thought gives better results than GloVe for BLEU-n and ROUGE metrics, while the METEOR scores are comparable to that when using averaged GloVe embedding with Attention BiLSTM generator.</figDesc><table><row><cell>MODEL</cell><cell cols="2">BLEU-2 GLOVE ST</cell><cell cols="2">BLEU-3 GLOVE ST</cell><cell cols="2">METEOR GLOVE ST</cell><cell cols="2">ROUGE GLOVE ST</cell></row><row><cell>GAN</cell><cell>0.710</cell><cell>0.745</cell><cell>0.593</cell><cell>0.607</cell><cell>0.667</cell><cell>0.670</cell><cell>0.654</cell><cell>0.649</cell></row><row><cell>WGAN</cell><cell>0.786</cell><cell>0.833</cell><cell>0.645</cell><cell>0.669</cell><cell>0.681</cell><cell>0.681</cell><cell>0.681</cell><cell>0.675</cell></row><row><cell>WGAN-GP</cell><cell>0.807</cell><cell>0.836</cell><cell>0.668</cell><cell>0.682</cell><cell>0.694</cell><cell>0.692</cell><cell>0.702</cell><cell>0.731</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>BLEU-2, BLEU-3 METEOR and ROUGE metric scores across GAN models with different f-measures.</figDesc><table /><note>GloVe: GLoVe Average, ST: Skip-Thought, WGAN: Wasserstein GAN, GP: Gradient Penalty as CMU-SE 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(a) contains sen-</cell></row><row><cell>tences generated from a vanilla STGAN which</cell></row><row><cell>mode collapse is observed, while 3(b) contains</cell></row><row><cell>examples wherein it is not observed when using</cell></row><row><cell>minibatch discrimination. Table 3(c) shows gen-</cell></row><row><cell>erated samples from STGAN when using Wasser-</cell></row><row><cell>stein distance f-measure as WGAN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Sentences sampled from STGAN when training on CMU-SE Dataset; mode collapse is overcome by using minibatch discrimination. Sample quality in terms of length and diversity further improved by using Wasserstein distance f-measure with gradient penalty regularizer. WGAN: Wasserstein GAN, GP: Gradient Penalty</figDesc><table><row><cell></cell><cell cols="2">Real Fake</cell><cell>% real</cell><cell>% fake</cell></row><row><cell>Real</cell><cell>30</cell><cell>51</cell><cell cols="2">37.04% 62.96%</cell></row><row><cell>Fake</cell><cell>48</cell><cell>75</cell><cell cols="2">39.02% 60.98%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Weighted human scores for sentences. |rating − 3| is weight given to each sentence's rating. 39.02% of the generated samples were marked as real.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ryankiros/ skip-thoughts/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ryankiros/ neural-storyteller</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The author would like to thank Aruna Malapati for providing insights and access to an Nvidia Titan X GPU for the experiments; and Pranesh Bhargava, Greg Durrett and Yash Raj Jain for providing helpful feedback. The author also acknowledges the support of Microsoft Research India Travel Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<title level="m">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<title level="m">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial text generation via feature-mover&apos;s distance. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno>abs/1809.06297</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MaskGAN: Better Text Generation via Filling in the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-class generative adversarial networks with the L2 loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Yag: A template-based generator for realtime systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsak</forename><surname>Susan W Mcroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Channarukul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Natural language generation</title>
		<meeting>the first international conference on Natural language generation</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="264" to="267" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>abs/1611.02163</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<editor>IN-TERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language generation with recurrent generative adversarial networks without pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1706.01399</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adversarial generation of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">Joseph</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1705.10929</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<idno>abs/1706.09799</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised text style transfer using language models as discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno>abs/1805.11749</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno>abs/1708.02709</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1609.05473</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarially Regularized Autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
