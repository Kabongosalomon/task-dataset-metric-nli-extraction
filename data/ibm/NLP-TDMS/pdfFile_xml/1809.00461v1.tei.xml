<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YouTube-VOS: Sequence-to-Sequence Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<email>linjie.yang@snap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Snapchat Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
							<email>yuchenf4@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
							<email>jianchao.yang@snap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Snapchat Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
							<email>yliang35@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
							<email>bprice@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
							<email>scohen@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">YouTube-VOS: Sequence-to-Sequence Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Object Segmentation</term>
					<term>Large-scale Dataset</term>
					<term>Spatial- Temporal Information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities 4 . This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning effective spatial-temporal features has been demonstrated to be very important for many video analysis tasks. For example, Donahue et al . <ref type="bibr" target="#b9">[10]</ref> propose long-term recurrent convolution network for activity recognition and video captioning. Srivastava et al . <ref type="bibr" target="#b37">[38]</ref> propose unsupervised learning of video representation with a LSTM autoencoder. Tran et al . <ref type="bibr" target="#b41">[42]</ref> develop a 3D convolutional network to extract spatial and temporal information jointly from a video. Other works include learning spatial-temporal information for precipitation prediction <ref type="bibr" target="#b45">[46]</ref>, physical interaction <ref type="bibr" target="#b13">[14]</ref>, and autonomous driving <ref type="bibr" target="#b46">[47]</ref>.</p><p>Video segmentation plays an important role in video understanding, which fosters many applications, such as accurate object segmentation and tracking, interactive video editing and augmented reality. Video object segmentation, which targets at segmenting a particular object instance throughout the entire video sequence given only the object mask on the first frame, has attracted much attention from the vision community recently <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>. However, existing state-of-the-art video object segmentation approaches primarily rely on single image segmentation frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b43">44]</ref>. For example, Caelles et al . <ref type="bibr" target="#b5">[6]</ref> propose to train an object segmentation network on static images and then fine-tune the model on the first frame of a test video over hundreds of iterations, so that it remembers the object appearance. The fine-tuned model is then applied to all following individual frames to segment the object without using any temporal information. Even though simple, such an online learning or one-shot learning scheme achieves top performance on video object segmentation benchmarks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref>. Although some recent approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref> have been proposed to leverage temporal consistency, they depend on models pretrained on other tasks such as optical flow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> or motion segmentation <ref type="bibr" target="#b39">[40]</ref>, to extract temporal information. These pretrained models are learned from separate tasks, and therefore are suboptimal for the video segmentation problem.</p><p>Learning long-term spatial-temporal features directly for video object segmentation task is, however, largely limited by the scale of existing video object segmentation datasets. For example, the popular benchmark dataset DAVIS <ref type="bibr" target="#b33">[34]</ref> has only 90 short video clips, which is barely sufficient to learn an end-to-end model from scratch like other video analysis tasks. Even if we combine all the videos from available datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref>, its scale is still far smaller than other video analysis datasets such as YouTube-8M <ref type="bibr" target="#b0">[1]</ref> and ActivityNet <ref type="bibr" target="#b16">[17]</ref>. To solve this problem, we present the first large-scale video object segmentation dataset called YouTube-VOS (YouTube Video Object Segmentation dataset) in this work. Our dataset contains 3,252 YouTube video clips featuring 78 categories covering common animals, vehicles, accessories and human activities. Each video clip is about 3∼6 seconds long and often contains multiple objects, which are manually segmented by professional annotators. Compared to existing datasets, our dataset contains a lot more videos, object categories, object instances and annotations, and a much longer duration of total annotated videos. <ref type="table">Table 1</ref> provides quantitative scale comparisons of our new dataset against existing datasets. We retrain existing algorithms on YouTube-VOS and benchmark their performance on our test set which contains 322 videos. In addition, our test set contains 10 categories unseen in the training set and are used to evaluate the generalization ability of existing approaches. <ref type="table">Table 1</ref>: Scale comparison between YouTube-VOS and existing datasets. "Annotations" denotes the total number of object annotations. "Duration" denotes the total duration (in minutes) of the annotated videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale</head><p>JC <ref type="bibr" target="#b12">[13]</ref> ST <ref type="bibr" target="#b25">[26]</ref> YTO <ref type="bibr" target="#b20">[21]</ref> FBMS <ref type="bibr" target="#b29">[30]</ref> DAVIS <ref type="bibr" target="#b32">[33]</ref>  Based on Youtube-VOS, we propose a new sequence-to-sequence learning algorithm to explore spatial-temporal modeling for video object segmentation. We utilize a convolutional LSTM <ref type="bibr" target="#b45">[46]</ref> to learn long-term spatial-temporal information for segmentation. At each time step, the convolutional LSTM accepts last hidden states and an encoded image frame, it then outputs encoded spatialtemporal features which are decoded into a segmentation mask. Our algorithm is different from existing approaches in that it fully exploits the long-term spatialtemporal information in an end-to-end manner and does not depend on existing optical flow or motion segmentation models. We evaluate our algorithm on both YouTube-VOS and DAVIS 2016 and it achieves better or comparable results compared to the current state of the arts.</p><p>The rest of our paper is organized as follows. In Section 2 we briefly introduce the related works. In Section 3 and 4 we describe our YouTube-VOS dataset and the proposed algorithm in detail. Experimental results are presented in Section 5. Finally we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the past decades, several datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref> have been created for video object segmentation. All of them are in small scales which usually contain only dozens of videos. In addition, their video content is relatively simple (e.g. no heavy occlusion, camera motion or illumination change) and sometimes the video resolution is low. Recently, a new dataset called DAVIS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> was published and has become the benchmark dataset in this area. Its 2016 version contains 50 videos with a single foreground object per video while the 2017 version has 90 videos with multiple objects per video. In comparison to previous datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref>, DAVIS has both higher-quality of video resolutions and annotations. In addition, their video content is more complicated with multiobject interactions, camera motion, and occlusions.</p><p>Early methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref> for video object segmentation often solve some spatial-temporal graph structures with hand-crafted energy terms, which are usually associated with features including appearance, boundary, motion and optical flows. Recently, deep-learning based methods were proposed due to its great success in image segmentation tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>. Most of these methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b43">44]</ref> build their model based on an image segmentation network and do not involve sequential modeling. Online learning <ref type="bibr" target="#b5">[6]</ref> is commonly used to improve their performance. To make the model temporally consistent, the predicted mask of the previous frame is used as a guidance in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b18">19]</ref>. Other methods have been proposed to leverage spatial-temporal information. Jampani et al . <ref type="bibr" target="#b21">[22]</ref> use spatial-temporal consistency to propagate object masks over time. Tokmakov et al . <ref type="bibr" target="#b40">[41]</ref> use a two-stream network to model objects' appearance and motion and use a recurrent layer to capture the evolution. However, due to the lack of training videos, they use a pretrained motion segmentation model <ref type="bibr" target="#b39">[40]</ref> and optical-flow model <ref type="bibr" target="#b19">[20]</ref>, which leads to suboptimal results since the model is not trained end-to-end to best capture spatial-temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">YouTube-VOS</head><p>To create our dataset, we first carefully select a set of object categories including animals (e.g. ant, eagle, goldfish, person), vehicles (e.g. airplane, bicycle, boat, sedan), accessories (e.g. eyeglass, hat, bag), common objects (e.g. potted plant, knife, sign, umbrella), and humans in various activities (e.g. tennis, skateboarding, motorcycling, surfing). The videos containing human activities have diversified appearance and motion, so instead of treating human videos as one class, we divide different activities into different categories. Most of these videos contain interactions between a person and a corresponding object, such as tennis racket, skateboard, motorcycle, etc. The entire category set includes 78 categories that covers diverse objects and motions, and should be representative for everyday scenarios.</p><p>We then collect many high-resolution videos with the selected category labels from the large-scale video classification dataset YouTube-8M <ref type="bibr" target="#b0">[1]</ref>. This dataset consists of millions of YouTube videos associated with more than 4,700 visual entities. We utilize its category annotations to retrieve candidate videos that we are interested in. Specifically, up to 100 videos are retrieved for each category in our segmentation category set. There are several advantages to using YouTube videos to create our segmentation dataset. First, YouTube videos have very diverse object appearances and motions. Challenging cases for video object segmentation, such as occlusions, fast object motions and change of appearances, commonly exist in YouTube videos. Second, YouTube videos are taken by both professionals and amateurs and thus different levels of camera motions are shown in the crawled videos. Algorithms trained on such data could potentially handle camera motion better and thus are more practical. Last but not the least, many YouTube videos are taken by today's smart phone devices and there are demanding needs to segment objects in those videos for applications such as video editing and augmented reality.</p><p>Since the retrieved videos are usually long (several minutes) and have shot transitions, we use an off-the-shelf video shot detection algorithm 5 to automat- ically partition each video into multiple video clips. We first remove the clips from the first and last 10% of the video, since these clips have a high chance of containing introductory subtitles and credits lists. We then sample up to five clips with appropriate lengths (3∼6 seconds) per video and manually verify that these clips contain the correct object categories and are useful for our task (e.g. no scene transition, not too dark, shaky, or blurry). After the video clips are collected, we ask human annotators to select up to five objects of proper sizes and categories per video clip and carefully annotate them (by tracing their boundaries instead of rough polygons) every five frames in a 30fps frame rate, which results in a 6fps sampling rate. Given a video and its category, annotators are first required to annotate objects belonging to that category. If the video contains other objects that belong to our 78 categories, we ask the annotators to label them as well, so that each video has multiple objects annotated. In human activity videos, both the human subject and the object he/she interacts with are labeled, e.g., both the person and the skateboard are required to be labeled in a "skateboarding" video. Some annotation examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Unlike dense per-frame annotation in previous datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, we believe that the temporal correlation between five consecutive frames is sufficiently strong that annotations can be omitted for intermediate frames to reduce the annotation efforts. Such a skip-frame annotation strategy allows us to scale up the number of videos and objects under the same annotation budget, which are important factors for better performance. We find empirically that our dataset is effective in training different segmentation algorithm.</p><p>As a result, our dataset YouTube-VOS consists of 3,252 YouTube video clips and 133,886 object annotations, 33 and 10 times more than the best of the existing video object segmentation datasets, respectively (See <ref type="table">Table 1</ref>). YouTube-VOS is the largest dataset for video object segmentation to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sequence-to-Sequence Video Object Segmentation</head><p>Based on our new dataset, we propose a new sequence-to-sequence video object segmentation algorithm. Different from existing approaches, our algorithm learns long-term spatial-temporal features directly from training data in an end-to-end manner, and the offline trained model is capable of propagating an initial object segmentation mask accurately by memorizing and updating the object charactersitics, including appearance, location and scale, and temporal movements, automatically over the entire video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem formulation</head><p>Let us denote a video sequence with T frames as {x t |t ∈ [0, T − 1]} where x t ∈ R H×W ×3 is the RGB frame at time step t, and denote an initial binary object mask at time step 0 as y 0 ∈ R H×W . The target of video object segmentation is to predict the object mask automatically for the remaining frames from time</p><formula xml:id="formula_0">step 1 to T − 1, i.e. {ŷ t |t ∈ [1, T − 1]}.</formula><p>To obtain a predicted maskŷ t for x t , many existing deep learning methods only leverage information at time step 0 (e.g. online learning or one-shot learning <ref type="bibr" target="#b5">[6]</ref>) or time step t − 1 (e.g. optical flow <ref type="bibr" target="#b31">[32]</ref>) while the long-term history information is totally dismissed. Their frameworks can be formulated aŝ y t = arg max ∀ȳt P(ȳ t |x 0 , y 0 , x t ) orŷ t = arg max ∀ȳt P(ȳ t |x 0 , y 0 , x t , x t−1 ). They are effective when the object appearance is similar between time 0 and time t or when the object motion from time t − 1 to t can be accurately measured. However, these assumptions will be violated when the object has drastic appearance variation and rapid motion, which is often case in many real-world videos. In such cases, the history information of the object in all previous frames becomes critical and should be leveraged in an effective way. Therefore, we propose to solve a different objective function, i.e.ŷ t = arg max ∀ȳt P(ȳ t |x 0 , x 1 , ..., x t , y 0 ), which can be transformed into a sequence-to-sequence learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Algorithm</head><p>Recurrent Neural Networks (RNN) has been adopted by many sequence-tosequence learning problems because it is capable to learn long-term dependency from sequential data. LSTM <ref type="bibr" target="#b17">[18]</ref> as a special RNN structure solves vanishing or exploding gradients issue <ref type="bibr" target="#b2">[3]</ref>. A convolutional variant of LSTM (convolutional LSTM) <ref type="bibr" target="#b45">[46]</ref> is later proposed to preserve the spatial information of the data in the hidden states of the model. Our algorithm is inspired by the convolutional encoder-decoder LSTM structure <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref> which has achieved much success in machine translation, where an input sentence in language A is first encoded by a encoder LSTM and its outputs are fed into a decoder LSTM which can generate the desired output sentence in language B. In video object segmentation, it is essential to capture the object characteristics over time. To generate the initial states for our convolutional LSTM (ConvLSTM ), we use a feed-forward neural network to encode both the first image frame and the segmentation mask. Specifically, we concatenate the initial frame x 0 and segmentation mask y 0 and feed it into a trainable network, denoted as Initializer, which outputs the initial memory state c 0 and hidden state h 0 . These initial states capture object appearance, object location and scale. And they are are feed into ConvLSTM for sequence learning. At time step t, frame x t is first processed by a convolutional encoder, denoted as Encoder, to extract feature mapsx t . Thenx t is sent as the inputs of ConvLSTM. The internal states c t and h t are automatically updated given the new observationx t , which capture the new characteristics of the object. The output h t is passed into a convolutional decoder, denoted as Decoder, to get the full-resolution segmentation resultsŷ t . Binary cross-entropy loss is computed betweenŷ t and y t during training process. The entire model is trained end-to-end using back-propagation to learn parameters for the Initializer network, the Encoder and Decoder networks, and ConvLSTM network. <ref type="figure">Figure 2</ref> illustrates our sequence learning algorithm for video object segmentation. The learning process can be formulated as follows:</p><formula xml:id="formula_1">c 0 , h 0 = Initializer(x 0 , y 0 ) (1) x t = Encoder(x t ) (2) c t , h t = ConvLST M (x t , c t−1 , h t−1 ),<label>(3)</label></formula><formula xml:id="formula_2">y t = Decoder(h t ) (4) L = −(y t log(ŷ t )) + ((1 − y t ) log(1 −ŷ t ))<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Model structures Both our Initializer and Encoder use VGG-16 <ref type="bibr" target="#b36">[37]</ref> network structures. In particular, all the convolution layers and the first fully connected layer of VGG-16 are used as backbone for the two networks. The fully connected layer is transformed to a 1 × 1 convolution layer to make our model fully convolutional. On top of it, Initializer has two additional convolution layers with ReLU <ref type="bibr" target="#b28">[29]</ref> activation to produce c 0 and h 0 respectively. Each convolution layer has 512 1 × 1 filters. The Encoder has one additional convolution layer with ReLU activation which has 512 1 × 1 filters. The VGG-16 layers of the Initializer and Encoder are initialized with pre-trained VGG-16 parameters while the other layers are randomly initialized by Xavier <ref type="bibr" target="#b15">[16]</ref>.</p><p>All the convolution operations of the ConvLSTM layer use 512 3 × 3 filters, initialized by Xavier. Sigmoid activation is used for gate outputs and ReLU is used for state outputs (empirically we find ReLU activation produces better results than tanh activation for our model). Following <ref type="bibr" target="#b22">[23]</ref>, we set the bias of the forget gate to be 1s at initialization.</p><p>The Decoder has five upsampling layers with 5 × 5 kernel size and 512, 256, 128, 64 and 64 filters respectively. The last layer of the Decoder produces segmentation results, which has one 5 × 5 filter with sigmoid activation. All the parameters are initialized by Xavier.</p><p>Training Our algorithm is trained on the YouTube-VOS training set. At each training iteration, our algorithm first randomly samples an object and T (5 ∼ 11) frames from a random training video sequence. Then the original RGB frames and annotations are resized to 256×448 for memory and speed concern. At the early stage of training, we only select frames with ground truth annotation as our training samples so that the training loss can be computed and back-propagated at each time step. When the training losses become stable, we added frames without annotations to training data. For those frames without ground truth annotations, loss is set to be 0. Adam <ref type="bibr" target="#b23">[24]</ref> is used to train our network and the initial learning rate is set to 10 −5 , and our model converges in 80 epochs.</p><p>Inference Our offline-trained model is able to learn features for general object characteristics effectively. It is able to produce good segmentation results by directly applying it to a new test video with unseen categories. This is in contrast to recent state-of-the-art approaches, which have to fine-tune their models on each new test video over hundreds of iterations. In our experiments, we show that our algorithm without online learning can achieve comparable or better results compared to previous state of the arts with online learning, which implies much faster inference speed for practical applications. Neverthless, we find that the performance of our model can be further improved with online learning.</p><p>Online Learning Given a test video, we generate random pairs of online training examples {(x 0 , y 0 ), (x 1 , y 1 )} through affine transformations from (x 0 , y 0 ). We treat (x 0 , y 0 ) as the initial frame and mask and (x 1 , y 1 ) as the first frame and ground truth mask. We then fine tune our Initializer, Encoder and Decoder networks on such randomly generated pairs. The parameters of ConvLSTM are fixed as it models long-term spatial-temporal dependency that should be independent of object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first evaluate our algorithm and recent state-of-the-art algorithms on our YouTube-VOS dataset. Then we compare our results on the DAVIS 2016 validation dataset <ref type="bibr" target="#b32">[33]</ref>, which is an existing benchmark dataset for video object segmentation. Finally, we do an ablation study to explore the effect of data scale and model variants to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>We split the YouTube-VOS dataset of 3,252 videos into training <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">796)</ref>, validation (134) and test (322) sets. To evaluate the generalization ability of existing approaches on unseen categories, the test set is further split into test-seen and test-unseen subsets. We first select 10 categories (i.e. ant, bull riding, butterfly, chameleon, flag, jellyfish, kangaroo, penguin, slopestyle, snail ) as unseen categories during training and treat their videos as test-unseen set. The validation and test-seen subsets are created by sampling two and four videos per category, respectively. The rest of videos are the training set. We use the region similarity J and the contour accuracy F as the evaluation metrics as in <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">YouTube-VOS</head><p>For fair comparison, we re-train previous methods (i.e. SegFlow <ref type="bibr" target="#b7">[8]</ref>, OSMN <ref type="bibr" target="#b49">[50]</ref>, MaskTrack <ref type="bibr" target="#b31">[32]</ref>, OSVOS <ref type="bibr" target="#b5">[6]</ref> and OnAVOS <ref type="bibr" target="#b43">[44]</ref>) on our training set with the same settings as our algorithm. One difference is that other methods leverage postprocessing steps to achieve additional gains while our models do not.</p><p>The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. All the comparison methods use static image segmentation models and four of them (i.e. SegFlow, MaskTrack, OS-VOS and OnAVOS) require online learning. Our algorithm leverages long-term spatial-temporal characteristics and achieves better performance even without online learinng (the second last row in <ref type="table" target="#tab_1">Table 2</ref>), which effectively demonstrates the importance of long-term spatial-temporal information for video object segmentation. With online learning, our model is further improved and achieves around 8% absolute improvement over the best previous method OSVOS on J mean. Our method also outperforms previous methods on contour accuracy and decay rate with a large margin. Surprisingly, OnAVOS which is the best performing method on DAVIS does not achieve good results on our dataset. We believe the drastic appearance changes and complex motion patterns in our dataset makes the online adaptation fail in many cases. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the changes of J mean over the duration of video sequences. Without online learning, our method is worse than online learning methods such as OSVOS at the first few frames since the object appearance usually has not changed too much from the initial frame and online learning is effective under such scenario. However, our method degrades slower than the other methods and starts to outperform OS-VOS at around 25% of the videos, which demonstrates that our method indeed propagates object segmentations more accurately over time than previous methods. With the help of online learning, our method outperforms previous methods in most parts of the video sequences, while maintaining a small decay rate.</p><p>Next we compare the generalization ability of existing methods on unseen categories in <ref type="table" target="#tab_1">Table 2</ref>. Most methods have better performance on seen categories  than unseen categories, which is expected. But the differences are not obvious, e.g. usually within 2% absolute differences on each metric. On one hand, it suggests that existing methods are able to alleviate the mismatch issue between training and test categories by approaches such as online learning. On the other hand, it also demonstrates the diverse training categories in YouTube-VOS helps different methods to generalize to new categories. Experiments on dataset scale in Section 5.4 further suggests the power of data scale on our model. Compared to other single-frame based methods, OSMN has a more obvious degradation on unseen categories since it does not use online learning. Our method without online learning does not have the issue since it leverages spatial-temporal information which is more robust to unseen categories. MaskTrack and OnAVOS have better performance on unseen than seen categories. We believe that they benefit from the guidance of previous segmentation or online adaption, which have advantages to deal with videos with slow motion. There are indeed several objects with slow motion in the unseen categories such as snail and chameleon. Some test results produced by our model without online learning are visualized in <ref type="figure">Figure 4</ref>. The first two rows are from seen categories while the last <ref type="figure">Fig. 4</ref>: Some visual results produced by our model without online learning on the YouTube-VOS test set. The first column shows the initial ground truth object segmentation (green color) while the second to the last column are predictions. two rows are from unseen categories. In addition, each example represents some challenging cases in video object segmentation. For example, the person in the first example has large changes in appearance and illumination. The second and third examples both have multiple similar objects and heavy occlusions. The last example has strong camera motion and the penguin changes its pose frequently. Our model obtains accurate results on all the examples, which demonstrates the effectiveness of spatial-temporal features learned from large-scale training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DAVIS 2016</head><p>DAVIS 2016 is a popular prior benchmark dataset for video object segmentation. To evaluate our algorithm, we first fine-tune our pretrained model in 200 epochs on the DAVIS training set which contains 30 videos. The comparison results between our fine-tuned models and previous methods are shown in <ref type="table">Table 3</ref>.</p><p>BVS and OFL are based on hand-crafted features and graphical models, while the rest are all deep learning based methods. Among the methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref> using image segmentation frameworks, OnAVOS achieves the best performance. However, its online adaption process makes the inference pretty slow (∼13s per frame). Our model without online learning (the second last row) achieves comparable results to other online learning methods without post-processing (e.g. MaskTrack 69.8% and OSVOS 77.4%), but with a significant speed-up (60 times faster). Previous methods using spatial-temporal information including SegFlow, VPN and ConvGRU get inferior results compared to ours. Among them, Con-vGRU is most related to ours since it also incorporates RNN memory cells in its model. However, it is an unsupervised methods to only segment moving foreground, while our method can segment arbitrary objects given the mask super- <ref type="table">Table 3</ref>: Comparisons of our approach and previous methods on the DAVIS 2016 dataset. Different components used in each algorithm are marked. "OL" denotes online learning. "PP" denotes post processing by CRF <ref type="bibr" target="#b24">[25]</ref> or Boundary Snapping <ref type="bibr" target="#b5">[6]</ref>. "OF" denotes optical flows. "RNN" denotes RNN and its variants.  vision. Finally, online learning helps our model segment object boundary more accurately. <ref type="figure" target="#fig_3">Figure 5</ref> shows such an example. To demonstrate the scale limitation of existing datasets, we train our models on three different settings and evaluate on DAVIS 2016. Our models trained on setting 1 and 2 only get 51.3% and 51.9% mean IoU, which suggests that existing video object segmentation datasets do not have sufficient data to train our models. Therefore our YouTube-VOS dataset is one of the key elements for the success of our algorithm. In addition, there is only little improvement by adding videos from the SegTrackv2, JumpCut and YoutubeObjects datasets, which suggests that the small scale is not the only problem for previous datasets. For example, videos in the three datasets usually only have one main foreground. SegTrackv2 has low-resolution videos. The annotation of YoutubeObjects videos is not accurate along object boundaries, etc. However, our YouTube-VOS dataset is carefully created to avoid all these problems. Setting 3 is a common detour for existing methods to bypass the data-insufficiency issue, i.e. using pre-trained models on other large-scale datasets to reduce the parameters to be learned for their models. However, our model using this strategy gets even worse results (45.6%) than training from scratch, which suggests that spatial-temporal features cannot be trivially transfered from representations learned from static images. Thus large scale training data such as our dataset is essential to learn spatial-temporal representation for video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study</head><p>In this subsection, we perform an ablation study on the YouTube-VOS dataset to evaluate different variants of our algorithm.</p><p>Dataset scale. Since the dataset scale is very important to our models, we train several models on different portions of the training set of YouTube-VOS to explore the effect of data. Specifically, we randomly select 25%, 50% and 75% of the training set and retrain our models from scratch. The results are listed in <ref type="table" target="#tab_3">Table 4</ref>. It can be seen that using only 25% of the training videos (∼700 videos) drops the performance almost 30% from the original model. In addition, the performance of the model on unseen categories are much worse than its performance on seen categories, which suggests that the model fails to capture general features for objectness. Since the scale of adding all the videos from all existing datasets is still much less than 700 videos, there is no doubt that existing datasets cannot satisfy the needs of our algorithm. With more and more training videos, our algorithm is improved rapidly, which well demonstrates the importance of large-scale data on our algorithm. We can see the trend of accuracies for 100% data still have not reached a plateau. We are working on collecting more data to explore the impact of data on the algorithm further.</p><p>Initializer variants. The Initializer in our original model is a VGG-16 network which encodes a RGB frame and an object mask and outputs initial hidden states of ConvLSTM. We would like to explore using the object mask directly as the initial hidden states of ConvLSTM. We train an alternative model by removing the Initializer and directly using the object mask as the hidden states, i.e. the object mask is reshaped to match the size of the hidden states. The J mean of the adapted model are 45.1% on the seen categories and 38.6% on the unseen categories. This suggests that the object mask alone does not have enough information for localizing the object.</p><p>Encoder variants. The Encoder in our original model receives a RGB frame as input at each time step. Alternatively, we can use the segmentation mask of the previous step as additional inputs to explicitly provide extra information to the model, similar to MaskTrack <ref type="bibr" target="#b31">[32]</ref>. In this way, our Initializer and Encoder can be replaced with a single VGG-16 network since the inputs at every time step have same dimensions and similar meaning. However, such a framework potentially has the error-drifting issue since segmentation mistakes made at previous steps will be propagated to the current step.</p><p>In the early stage of training, the model is unable to predict good segmentation results. Therefore we use the ground truth annotation of the previous step as the input. Such strategy is known as teacher forcing <ref type="bibr" target="#b44">[45]</ref> which can make the training faster. After the training losses become stable, we replace the ground truth annotation with the model's prediction of the previous step so that the model is forced to correct its own mistakes. Such a strategy is known as curriculum learning <ref type="bibr" target="#b1">[2]</ref>. Empirically we find that both the two strategies are important to make the model to work well. The J mean results of the model are 59.4% on the seen categories and 60.7% on the unseen categories, which is similar to our original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce the largest video object segmentation dataset (YouTube-VOS) to date. The new dataset, much larger than existing datasets in terms of number of videos and annotations, allows us to design a new deep learning algorithm to explicitly model long-term spatial-temporal dependency from videos for segmentation in an end-to-end learning framework. Thanks to the large scale dataset, our new algorithm achieves better or comparable results compared to existing state-of-the-art approaches. We believe the new dataset will foster research on video-based computer vision in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This research was partially supported by a gift funding from Snap Inc. and UIUC Andrew T. Yang Research and Entrepreneurship Award to Beckman Institute for Advanced Science &amp; Technology, UIUC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The ground truth annotations of sample video clips in our dataset. Different objects are highlighted with different colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Fig. 2 :</head><label>12</label><figDesc>The framework of our algorithm. The initial information at time 0 is encoded by Initializer to initialize ConvLSTM. The new frame at each time step is processed by Encoder and the segmentation result is decoded by Decoder. ConvLSTM is automatically updated over the entire video sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The changes of J mean values over the length of video sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The comparison results between our model without online learning (upper row) and with online learning (bottom row). Each column shows predictions of the two models at the same frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>-Setting 1 :</head><label>1</label><figDesc>We train our model from scratch on the 30 training videos. -Setting 2: We train our model from scratch on the 30 training videos, plus all the videos from the SegTrackv2, JumpCut and YoutubeObjects datasets, which results in a total of 192 training videos. -Setting 3: Following the idea of ConvGRU, we use a pretrained object segmentation model DeepLab [7] as our Encoder and train the other parts of our model on the 30 training videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of our approach and other methods on YouTube-VOS test set. The results in each cell show the test results for seen/unseen categories. "OL" denotes online learning. The best results are highlighted in bold.</figDesc><table><row><cell>Method</cell><cell cols="2">J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓</cell></row><row><cell>SegFlow [8]</cell><cell cols="2">40.4/38.5 45.4/41.7 7.2/8.4 35.0/32.7 35.3/32.1 6.9/9.1</cell></row><row><cell>OSVOS [6]</cell><cell cols="2">59.1/58.8 66.2/64.5 17.9/19.5 63.7/63.9 69.0/67.9 20.6/23.0</cell></row><row><cell cols="3">MaskTrack [32] 56.9/60.7 64.4/69.6 13.4/16.4 59.3/63.7 66.4/73.4 16.8/19.8</cell></row><row><cell>OSMN [50]</cell><cell cols="2">54.9/52.9 59.7/57.6 10.2/14.6 57.3/55.2 60.8/58.0 10.4/13.8</cell></row><row><cell>OnAVOS [44]</cell><cell cols="2">55.7/56.8 61.6/61.5 10.3/9.4 61.3/62.3 66.0/67.3 13.1/12.8</cell></row><row><cell cols="3">Ours (w/o OL) 60.9/60.1 70.3/71.2 7.9/12.9 64.2/62.3 73.0/71.4 9.3/14.5</cell></row><row><cell cols="3">Ours (with OL) 66.9/66.8 78.7/76.5 10.2/9.5 74.1/72.3 82.8/80.5 12.6/13.4</cell></row><row><cell cols="2">(a) Seen categories</cell><cell>(b) Unseen categories</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effect of data scale on our algorithm. We use different portions of training data to train our models and evaluate on the YouTube-VOS test set.</figDesc><table><row><cell>Scale J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓</cell></row><row><cell>25% 46.7/40.1 53.5/45.6 8.3/13.6 46.7/40.0 52.2/41.6 8.5/13.2</cell></row><row><cell>50% 51.5/50.3 59.2/58.8 10.3/13.1 51.8/50.2 59.5/55.8 11.1/13.3</cell></row><row><cell>75% 56.8/56.0 65.7/67.1 7.6/10.0 59.6/56.3 68.8/64.1 8.5/11.1</cell></row><row><cell>100% 60.9/60.1 70.3/71.2 7.9/12.9 64.2/62.3 73.0/71.4 9.3/14.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is the statistics when we submit this paper, see updated statistics on our website.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://johmathe.name/shotdetect.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jumpcut:non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cárdenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sorkine-Hornung: Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00243</idno>
		<title level="m">Deep grabcut for object selection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
