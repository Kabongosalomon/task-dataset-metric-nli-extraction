<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Ranran</surname></persName>
							<email>haoranz6@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61820</postCode>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<postCode>606-8501</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Changsha University of Science &amp; Technology</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint extraction of entities and relations has received significant attention due to its potential of providing higher performance for both tasks. Among existing methods, CopyRE is effective and novel, which uses a sequence-to-sequence framework and copy mechanism to directly generate the relation triplets. However, it suffers from two fatal problems. The model is extremely weak at differing the head and tail entity, resulting in inaccurate entity extraction. It also cannot predict multi-token entities (e.g. Steven Jobs). To address these problems, we give a detailed analysis of the reasons behind the inaccurate entity extraction problem, and then propose a simple but extremely effective model structure to solve this problem. In addition, we propose a multi-task learning framework equipped with copy mechanism, called CopyMTL, to allow the model to predict multi-token entities. Experiments reveal the problems of CopyRE and show that our model achieves significant improvement over the current state-of-the-art method by 9% in NYT and 16% in WebNLG (F1 score). Our code is available at https://github. com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As a key technology for automatic Knowledge Graph (KG) construction, relation extraction has received widespread attention in recent years. Relation extraction aims to automatically learn triplets (relation, head, tail) from the unstructured text without human intervention.</p><p>Early studies use pipeline models <ref type="bibr" target="#b11">(Nadeau and Sekine 2007;</ref><ref type="bibr" target="#b2">Chan and Roth 2011)</ref>, where they cast the relation extraction problem into two separate tasks, i.e. Named Entity Recognition (NER) to extract the entities and Relation Classification. They first recognize the entities and then predict the relations between entities. However, pipeline models suffer from obvious drawbacks <ref type="bibr" target="#b14">(Roth and Yih 2007)</ref>. Each component limits the performance because of the error cascading effect and there is no chance for the model to correct mistakes. In addition, such pipeline models cannot capture the explicit relation between the two subtasks (Li and Ji 2014), where joint models can benefit from such interdependencies.</p><p>Recent studies on joint models of entity and relation extraction have three major research lines: <ref type="table">Table Filling</ref>, Tagging, and Sequence-to-Sequence (Seq2Seq). Among these approaches, the table filling method <ref type="bibr" target="#b6">(Gupta, Schütze, and Andrassy 2016;</ref><ref type="bibr" target="#b0">Adel and Schütze 2017)</ref> requires the model to enumerate over all possible entity pairs, which leads to a heavy computational burden. The tagging method <ref type="bibr" target="#b22">(Zheng et al. 2017</ref>) suffers from the overlapping relation problem that the model cannot assign different relation tags to one token. To solve this problem, the followers <ref type="bibr" target="#b17">(Takanobu et al. 2018;</ref><ref type="bibr" target="#b3">Dai et al. 2019</ref>) run tagging on a sentence for multiple turns, which is akin to the table filling method together with the heavy computational burden. Relatively speaking, the Seq2Seq method is neither plagued with overlapping relations nor with excessive computations. Seq2Seq model receives the unstructured text as input and directly decodes the entity-relation triples as a sequential output. This concise approach also matches with the human annotation process, that the annotators first read the sentences, understand the meaning and then point out the entity-relation pairs sequentially.</p><p>Currently, CopyRE <ref type="bibr" target="#b20">(Zeng et al. 2018</ref>) is the most powerful Seq2Seq based joint extraction method which expands a Seq2Seq framework with copy mechanism in the decoder. The copying mechanism allows the model to avoid the outof-vocabulary (OOV) problem. Despite their promising result, the model still suffers from two major drawbacks.</p><p>First, the entity copying in CopyRE is unstable and it depends on an unnatural mask to differ the head (h) and tail (t) entities. Experimental results show that CopyRE nearly ran-domly predicts the head-tail order of the two entities. The model also needs an unnatural mask that masks the probability of h while predicting t. Without this mask, when predicting t, the model would choose the same token as h, and the accuracy drops to zero. After analysis, we prove that CopyRE actually uses the same distribution to model h and t, chooses the highest probability as h, and the secondhighest would be chosen as t after masking the highest probability, so without this mask, it cannot differ h and t. Modeling the h and t distribution in such manner can cause various problems, the model not only is extremely weak at differing h and t, but also cannot get information about h while predicting t.</p><p>Second, CopyRE cannot extract entities that have multiple tokens. The copy-based decoder always points to the last token of any entities, which limits the applicability of the model. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref> we show that CopyRE only predicts "Jobs" rather than the whole entity "Steven Jobs" when the entity has two tokens. In real-word scene multi-token entities are common, so this can greatly drag the model performance.</p><p>To address these two problems mentioned above, we propose CopyMTL, which is a multi-task learning based model with a new architecture for entity copying. We first provide a detailed analysis of why CopyRE is unstable during copying and propose a new model architecture to improve the shortcomings. Our new model architecture merely adds one more non-linear fully connection layer so that the model predicts separate distributions for the head and tail entity, and the tail prediction receives information from the head prediction. This architecture no longer needs the unnatural mask and increases the accuracy of entity copying, resulting in the overall improvements over the state-of-the-art model.</p><p>Then we propose a multi-task learning based Seq2Seq model to predict multi-token entities. A sequence labeling layer is added at the encoding stage to assist the entity recognition process. We use multi-task learning of NER to predict the start token of each entity while the decoder points at the last token while decoding. During training, we optimize the multi-task loss function jointly.</p><p>In conclusion, the contribution of this work is as follow: 1. We analyze the reasons for the unstable performance of entity copying in CopyRE and propose a simple but effective architecture to address this problem.</p><p>2. We propose a multi-task framework to enhance the capability of handling with multi-token entities.</p><p>3. Experimental results show that our model achieves state-of-the-art results and outperforms previous approaches by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>In this section, we first introduce the CopyRE model that is based on the Seq2Seq framework. Then, we give a detailed description of the two existing problems. As shown in <ref type="figure" target="#fig_1">Fig.  2</ref>, CopyRE consists two parts: an encoder and a decoder. Given a sentence s = {x 1 , x 2 ...x n }, the encoder transforms the input s into a vector representation. The decoder predicts the relation-entity triplets (r, h, t) each three time steps. Inspired by <ref type="bibr">CopyNet (Gu et al. 2016)</ref>, the first step uses Generate-Mode to predict a relation. Then, the model switches to Copy-Mode and selects the head and tail entities one by one in two different time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>To model the semantics of the input sentence better, CopyRE adopts Bidirectional LSTM (BiLSTM) <ref type="bibr" target="#b15">(Schuster and Paliwal 1997)</ref> as the encoder, which has shown great strength in many areas of NLP. Given a sentence of word embeddings {e E 1 , ..., e E n } as input, the hidden states from two directions are computed:</p><formula xml:id="formula_0">− → h i = − −−−−− → LST M E (e E i , h i−1 ) ← − h i = ← −−−−− − LST M E (e E i , h i+1 ) h E i = ( − → h i + ← − h i )/2<label>(1)</label></formula><p>where hidden states − → h i and ← − h i from two directions are averaged 1 into one vector h E i as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>The decoder uses a one direction LSTM to predict the outputs from left to right. The last hidden state of the encoder is used to initialize the decoder hidden state. The attention score is assigned to each hidden state of the encoder and then summed up to obtain an attentive sum. Then the sum is combined with the decoded hidden states at the last time step to be fed into the decoder LSTM:</p><formula xml:id="formula_1">c t = Attention(h D t−1 , h E 1:n ) u t = [e D t ; c t ] · W u h D t = LST M D (u t , h D t−1 )<label>(2)</label></formula><p>where Attention calculates the attentive sum of all encoder hidden states h E 1:n = {h E 1 , ...h E n } according to the last decoder hidden state h D t−1 . [·; ·] is the concatenation operator, e t is the embedding of the decoder output in the last time step, W u ∈ R (de+dc)×de is the parameter of linear transformation. All biases are omitted for convenience.</p><p>Every three time steps form a loop in which the decoder predicts relation, last token of head and then last token of tail to form a triplet, respectively. The confidence q i t for each token at position i to be copied as an entity is calculated by:</p><formula xml:id="formula_2">q i t = [h D t ; h E i ] · W e (3) where W e ∈ R 2do×1 .</formula><p>Then, the decoder computes the logits according to the time step t (we count the time step from 1): where W r ∈ R do×rel , rel is the cardinality of relations, q t is the concatenation of all q i t , M is the mask which records the predicted head entity and prevents the decoder predicting it at the t%3 = 0 time step. This is based on the fact that an entity cannot be both the head and the tail in the same triplet at the same time. But the mask makes no contributions for minimizing the cross entropy loss we will describe below.</p><formula xml:id="formula_3">logit t =    [h D t · W r ; q N A ], if t%3 = 1; [q t ; q N A ], if t%3 = 2; [M ⊗ q t ; q N A ], if t%3 = 0.</formula><p>Through the unnormalized logit, we can obtain the probability of output entity or relation by softmax:</p><formula xml:id="formula_4">p(y t |y &lt;t , s) = e logit j t i e logit i t<label>(5)</label></formula><p>At time step t%3 = 1 when the model should predict relation, the softmax score is calculated over all relations types; when the model should predict the entity, the softmax score is calculated over all positions in the source sentence. Then, the model can be trained via minimizing the cross entropy loss, which measures the difference between the output y t and the label y * t .</p><formula xml:id="formula_5">L D = − t log(p(y * t |y &lt;t , s))<label>(6)</label></formula><p>CopyRE also use padding triplets (NA, NA, NA) during training, which do not have any valid relations and entities. The confidence q N A of NA-relation and NA-position of the corresponding entity is calculated through a shared parameter:</p><formula xml:id="formula_6">q N A = h D t · W N A (7) where W N A ∈ R do×1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problems of CopyRE</head><p>As mentioned in the introduction, we found that CopyRE has two problems. First, the prediction of the entity is unstable. In detailed experiments, we observed that CopyRE cannot even fit the training set well, in which the F1 scores are approximately 0.75 and 0.40 on two datasets (see <ref type="figure">Fig.  4</ref>). In addition, if we remove the mask M in Eq. (4), the F1 score will turn to zero immediately. To find out the reason behind it, we evaluate CopyRE for the predicted relations and entities in the triplets separately. The experiments show that CopyRE can gain 0.84 F1 score for relations, while the F1 score for entities dramatically drops to 0.64 (see <ref type="table" target="#tab_5">Table 4</ref>).</p><p>In addition, when we inspected the prediction errors, we find that CopyRE is prone to mix up the order of head and tail. Thus, we can conclude that entity copying is the bottleneck of the model, which causes the performance decline. Second, since CopyRE only predicts the last token of the entity, when the target entity contains multiple tokens, the outputs are incomplete. There are straightforward ways to solve this problem. For example, we can extend the predicted triplets to quintuple by adding the length of entities. However, such methods indirectly use or simply ignore the interactions between relation extraction and entity recognition. We propose a multi-task manner method to solve this problem and give detailed comparisons in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Method</head><p>As described in the last section, CopyRE is suffering the entity copying and the multi-token entity problems. We propose a model named CopyMTL <ref type="figure" target="#fig_1">(Fig. 2</ref>) to address these two problems. CopyMTL is based on a new model structure and uses a multi-task framework which adds a sequence labeling task to CopyRE encoder. In this section, we first reveal the reasons behind the entity copying problem, then propose a simple but reasonable solution. After that, we introduce an additional tagging layer of the encoder and the multi-task training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New Structure for Entity Copying</head><p>Strangely, in CopyRE entity copying highly depends on the entity mask M , and the predicted distributions of head and tail entities are identical. Through our analysis, the main culprit is found in Eq. (3), who calculates the concatenation of h D t and h E i , then passes it to a linear transformation. Eq. (3) can be expanded and simplified to get the following form:</p><formula xml:id="formula_7">q t i = [h D t ; h E i ] · W e = [h D t ; h E i ] · [W e 1 ; W e 2 ] = h D t · W e 1 + h E i · W e 2<label>(8)</label></formula><p>where W e 1 , W e 2 ∈ R do×1 . Note that this is a summation of two scalars and the first term is independent of i. If we omit the q N A , the probability of entity copying is calculated by softmax:  <ref type="figure">Figure 3</ref>: The problematic entity copying of CopyRE. After predicting relation BirthPlace, the model will copy the head entity Jobs, then mask the predicted head and copy the tail Francisco.</p><formula xml:id="formula_8">p(y t |y &lt;t , s) = e q t i j e q t j = e h D t ·W e 1 · e h E i ·W e 2 e h D t ·W e 1 · j e h E j ·W e 2 = e h E i ·W e 2 j e h E j ·W e 2<label>(9)</label></formula><p>Abnormal dependency to the mask: In Eq. (9), we can see that prob t i does not rely on the time step t. In other words, the output distribution of entity copying at t%3 = 1 and t%3 = 2 are identical, which causes the dependency on the mask. We visualize the output distribution of the entity copying in <ref type="figure">Fig. 3</ref>. In the figure, the model first copies the token with the highest probability, Jobs. Then, in the next time step, as the pointed token Jobs is masked, the model copies the token with the second highest probability, Francisco.</p><p>Unstable entity copying: Because the distributions of two time steps are the same and the mask is only used in evaluation rather than optimization, the entity copying, especially for the head entity, becomes unstable. In the training stage, CopyRE maximizes the likelihood for the head at t%3 = 2 and for the tail at t%3 = 0, while the likelihood at each time step is identical. However, as the mask is not used for optimization, there is no explicit constraint to ensure that the head has the highest probability and tail has the second highest probability. In fact, CopyRE tries to maximize both the head and the tail. Thus, which one would be the highest and be predicted at t%3 = 2 is random.</p><p>To fix the problem in Eq. (9), we simply map h D t and h E i to a fused feature space via one additional non-linear layer:</p><formula xml:id="formula_9">q t i = σ([h D t ; h E i ] · W f ) · W o<label>(10)</label></formula><p>where σ is the selu(·) activation function <ref type="bibr" target="#b7">(Klambauer et al. 2017</ref>)</p><formula xml:id="formula_10">, W f ∈ R 2do×d W f and W o ∈ R d W f ×1 .</formula><p>Due to the non-linearity of the activation function, the reduction of Eq. (9) does not hold true. Now, the entity copying depends on both i and t and there is only one target output to maximize instead of that in <ref type="figure">Fig. 3</ref>. Thus, by replacing Eq. (3) with Eq. (10), the decoder no longer needs to struggle with ranking head and tail at t%3 = 2, and the mask is no longer urgently needed 2 . Therefore, the entity copying becomes stable with our new structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Labeling Layer</head><p>CopyRE only copies the last token of the entity. To predict entities with multiple tokens, we cast the problem into a sequence labeling problem and use the NER results to calibrate the entities with multiple tokens. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we first derive the emission potential from the encoder output. Then, an additional Conditional Random Field (CRF) layer <ref type="bibr" target="#b8">(Lafferty, McCallum, and Pereira 2001)</ref> is employed to calculate the most probable tag for each token. We use the BIO scheme (Begin, Inside, Outside) to recognize all of the entities in the sentence. The predicted tags are used to post-process the extracted entities.</p><p>The conditional probability of target tags * given sentence s are computed by path probability: p(tags * |s) = e score(s,tags * ) tags e score(s,tags )</p><p>where the denominator is computed via dynamic programming. The unnormalized path score is defined as:</p><formula xml:id="formula_12">score(s, tags) = i φ i,tagi + b tagi−1→tagi<label>(12)</label></formula><p>where b tagi−1→tagi is the transition score from tag i−1 to tag i . φ i,tagi is the score of the tag i for the i-th input token, which is comes from the hidden state of the Bi-LSTM at timestep i. The loss function of the sequence labeling is:</p><p>L E = −log(p(tag * |s)) (13) In the inference stage, we use the NER results to postprocess the decoded entities. Since we use BIO tagging scheme, there are three circumstances for the decoded last token of entities:</p><p>• 'B'. a single token entity.</p><p>• 'I', an entity with multiple tokens, it will look for the token before the current token until it finds 'B'. • 'O', a single token entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Overall, the input sentence is fed into the encoder part. All of the hidden states of the encoder are used to label the input sequence and calculate the attention of the decoder. Initialized by the last hidden state of the encoder, the decoder generates triplets each three time steps. Thus, the loss function  contains two parts: the encoder part introduces an additional CRF loss, and in the decoder part the cross entropy loss is used to measure the difference between the output triplets and the gold triplets.</p><p>We define the loss function as the weighted summation of encoder loss and decoder loss:</p><formula xml:id="formula_13">L = λ · L E + L D<label>(14)</label></formula><p>where λ is the weight of the tagging loss. The loss is calculated as the average over shuffled minibatch, and the derivatives of each parameter can be computed via back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Setting</head><p>We evaluated models on two datasets: New York Times (NYT) <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type="bibr" target="#b5">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised relation extraction task (DSRE), which aims to leverage the strength of the knowledge base to generate a largescale dataset <ref type="bibr" target="#b10">(Mintz et al. 2009</ref>). To make joint extraction more challenging than DSRE experiment setting, <ref type="bibr" target="#b20">Zeng et al. (2018)</ref> additionally modified the data to include more overlapping relations. WebNLG is originally used for natural language generation, in which all of the sentences are written by annotators. To avoid that the model only remembers the entity linking instead of the relation pattern, we only use the first sentence for each instance, which is the same as other baselines. The data statistics of both datasets are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Our experiments settings also followed most of the settings of CopyRE. The hidden number of LSTM was set to 1000. The max number of decoded triplets was 5. This was because the average triplet number in both dataset is about 2. We did not use "end-of-sentence" token to stop decoding, but to decode all padding triples (NA, NA, NA). The embedding dimension was 100, and we used the same pretrained embeddings 3 . Adam (Kingma and Ba 2014) was used to optimize the neural networks and the learning rate was 0.001. The weight of L E , λ, was set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Evaluation Metrics</head><p>We compare CopyMTL with CopyRE <ref type="bibr" target="#b20">(Zeng et al. 2018)</ref>, NovelTagging <ref type="bibr" target="#b22">(Zheng et al. 2017)</ref> and GraphRel <ref type="bibr" target="#b4">(Fu, Li, and Ma 2019)</ref>. NovelTagging uses sequence labeling to assign one label to each word, which contains both entity and relation information. GraphRel is the state-of-the-art model, which uses a post-editing method to revise the triplets phase by phase. For Seq2Seq model, CopyRE and our CopyMTL, we give a more detailed comparison to show the advantages of our new structure. We also evaluate the OneDecoder and MultiDecoder trick for the Seq2Seq models (denoted as -One and -Mul). The main difference between the two decoders is the parameter sharing strategy. OneDecoder uses shared parameters for predicting all triplets and is exactly what we described in the background section. MultiDecoder uses unshared decoders, each decoder predicts one triplet.</p><p>We use precision, recall, and micro-F1 score to evaluate the models. The evaluation metrics we use are stricter than that of the original CopyRE. That is to say, instead of leaving out the incomplete entity problem, the outputs of our experiments are regarded as correct only if both the relation types and all entity tokens are correct. This stricter metric meets real-world usage and the comparison is fairer to Nov-elTagging and GraphRel because they are not haunted by the multi-token problem. For an intuitive comparison, we also list the result of CopyRE in the table, although their evaluation is not so strict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Baselines</head><p>To evaluate the performance of the proposed method, we compare CopyMTL with the baseline methods. The results 4 are shown in <ref type="table">Table 1</ref>.</p><p>As shown, CopyMTL is the best model in WebNLG and NYT. Both the precision and the recall are significantly improved. In the NYT dataset, compared with the state-of-theart model, GraphRel-2p, CopyMTL-One outperforms it by   WebNLG. These models do not consider entities with multiple tokens and use less strict evaluation that ignores entity with multiple tokens. 8.8% for precision and 9.2% for recall. In the WebNLG dataset, the effect is more significant. The improvements are 13.1% in precision and 19% in the recall. These observations verify the effectiveness of our proposed method. Nov-elTagging is characterized by a low recall, which is caused by its deficiency in overlapping relations. CopyRE has already solved this problem well, with 8% and 19% absolute F1 improvement in WebNLG and NYT. Our method further brings 33% and 19% F1 enhancement compared to CopyRE, which shows great potentials of Seq2Seq methods.</p><p>CopyRE argued that MultiDecoder is better than OneDecoder, which is validated by our reproduction experiment. However, with our novel CopyMTL, MultiDecoder is better than OneDecoder in NYT but worse in WebNLG. This is probably because NYT is a bigger dataset, in which Mul-tiDecoder with more parameters works better. In practice, which decoder to use should be determined by the size of the dataset and we cannot conclude that one is better than another in every situation. For simplicity, we only discuss OneDecoder in the following sections.   Note that the new model architecture only considers the entity copying while the F1 score computed considers the whole triplet. In order to uncover the performance of CopyRE' in relation classification and entity recognition, we calculate the F1 scores for the two subtasks in <ref type="table" target="#tab_5">Table 4</ref>. For the entity recognition subtask, the F1 score of CopyRE' is 10% higher in NYT and 19% higher in WebNLG. This is the main contribution of the new model architecture. For the relation classification subtask, the F1 score of CopyRE' is marginally higher (less than 3%) than that of CopyRE. This implies the better entity recognition helps relation classification learning, which confirms the argument that the interactions between two task are beneficial to each other. In the decoding stage, a more precise prediction of the entity is fed into the decoder, which aids the relation classification in the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of the Revised Entity Copying Method</head><p>Except for the final result, the learning processes of the two models are also different. We plot the overall F1 score varying with the training epochs in <ref type="figure">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Multi-Task Learning</head><p>CopyMTL aims to solve the multi-token problem. In addition to the multi-task learning used by CopyMTL, there can be other straightforward methods. For example, the decoder of CopyRE' can predict the length of the entity when it copies the entities, which forms quintuples, called CopyRE'5. This is similar to predicting both the begin and the end of the entities and should work the same. We compare the models in <ref type="table" target="#tab_7">Table 5</ref>, from which we can see that CopyRE'5 is worse than CopyMTL in all evaluations, but both outperform GraphRel. We conjecture that the three tasks in CopyRE'5, include relation classification, entity recognition, and entity length prediction, varying in their degree of difficulty. The entity length prediction task may interfere with the learning of other tasks, as this easier task prolongs the dependence distance of harder tasks.</p><p>In addition, we also evaluate how precisely does the encoder of CopyMTL completes the whole entities. It gains 99% F1 score in NYT and 96% F1 score in WebNLG. This evaluation is less strict than conventional NER tasks as we consider neither the types of the entities nor the entities out of relations. We can conclude that NER in joint extraction is powerful enough for triplet extraction and the main difficulty in joint extraction is to make a better prediction for both the relations and the positions of the corresponding entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Extraction of entities and relations is of significance to many NLP tasks. In recent years, there have been four mainstream methods.</p><p>Pipeline methods: Previous works mainly use pipeline methods (Nadeau and Sekine 2007), a.k.a extract entities first then classify the relations. Most of the recent neural models also focus on pipeline methods, include (1) Fully-Supervised Relation Classification (Hendrickx et al. 2009) (2) Distant Supervised Relation Extraction <ref type="bibr" target="#b10">(Mintz et al. 2009</ref>). In spite of the recent progress of neural models <ref type="bibr" target="#b1">(Cai, Zhang, and Wang 2016;</ref><ref type="bibr" target="#b18">Zeng et al. 2014;</ref><ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou 2018;</ref><ref type="bibr" target="#b12">Qin, Xu, and Wang 2018)</ref>, the pipeline methods introduce error propagation problem <ref type="bibr" target="#b9">(Li and Ji 2014)</ref>, which does harm to the overall performance.</p><p>Table filling: The joint extraction task is formalized as a table constituted by the Cartesian product of the input sentence to itself. The table blanks, except for that on the diagonal, are to be predicted as relations. The models include history-based searching (Miwa and Sasaki 2014), neuralbased prediction <ref type="bibr" target="#b6">(Gupta, Schütze, and Andrassy 2016)</ref> and global normalization <ref type="bibr" target="#b0">(Adel and Schütze 2017)</ref>. The stateof-the-art model, GraphRel, also belongs to this genre. This model innovative takes the interaction between entities and relations into account via 2-phase GCN. The main problem of table filling is the over redundant computation for the permutation of all word pairs in a sentence. As a result, most of the blanks in the table are empty and it is the sparsity that hinders the learning of the models.</p><p>Tagging: The tagging models originally solved the tasks separately through a shared parameter: the model tags the entities first, then predicts the relations. SPTree (Miwa and Bansal 2016) used a structural neural model with the help of linguistic features. This model was promoted by an attention-based model (Katiyar and Cardie 2017). Besides, NovelTagging <ref type="bibr" target="#b22">(Zheng et al. 2017</ref>) proposed a new tagging scheme, by which the model can predict a single tag for each word, containing both the entities and relations. However, this tagging scheme cannot handle overlapping relations because it cannot assign one token with multiple labels. To solve it, multi-pass tagging training, HRL 5 , has been purposed <ref type="bibr" target="#b17">(Takanobu et al. 2018)</ref>, based on the reinforcement learning framework and <ref type="bibr" target="#b3">(Dai et al. 2019</ref>) leverage attention mechanism. Although these methods solve the overlapping relation problem, their nature and complexities are akin to table filling.</p><p>Seq2Seq: CopyRE <ref type="bibr" target="#b20">(Zeng et al. 2018</ref>) is another method for solving the overlapping relation problem, which extracts triplets by a Seq2Seq framework (Sutskever, Vinyals, and Le 2014) with copy mechanism (Gu et al. 2016). But it cannot predict the entire entities. In addition, the weak performance hinders it from real-world usage. Our work resolves the problems and boosts the performance to a new level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>In this paper, we revisit the CopyRE model, which jointly extracts entities and relations by a Seq2Seq model. We find that there are two problems in the model: the performance of the model is limited by the inaccurate entity copying and the generated entities are not complete. We give a theoretical analysis to reveal the reason behind the first problem, then propose a new model architecture to solve it. For the second problem, we propose a multi-task learning framework to complete the entities. Detailed experiments show the effectiveness of our method, which also outperforms the current state-of-the-art model by a huge margin.</p><p>For future work, CopyMTL still has much potential, for example, the current model can only extract a fixed number of triplets. We would also like to extend CopyMTL to extract any number of triplets. CopyMTL can build a strong baseline for future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CopyRE predicts the entity pointer refers to the word position in the source sentence. The colored tokens show the limitation of CopyRE which cannot predict multiple tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overview of CopyMTL model for joint extraction of relation and entity. The CopyRE model does not contain the CopyMTL-Tagging part, i.e., the sequence-labeling part in thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Although</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>One (ours) .612 .530 .571 .312 .272 .291 CopyRE-Mul (ours) .610 .566 .587 .319 .273 .294 GraphRel-1p .629 .573 .600 .423 .392 .407 GraphRel-2p .639 .600 .619 .447 .411 .429 CopyMTL-One .727 .692 .709 .578 .601 .589 CopyMTL-Mul .757 .687 .720 .580 .549 .564 Table 1: Results of the compared models on NYT and WebNLG, in which CopyRE uses less strict evaluation.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell>NYT Prec Rec</cell><cell>F1</cell><cell>WebNLG Prec Rec F1</cell></row><row><cell></cell><cell cols="2">NovelTagging</cell><cell cols="2">.642 .317 .420 .525 .193 .283</cell></row><row><cell cols="3">CopyRE-Dataset NYT WebNLG</cell><cell></cell></row><row><cell>Relation types</cell><cell>24</cell><cell>246</cell><cell></cell></row><row><cell cols="2">Dictionary size 90760</cell><cell>5928</cell><cell></cell></row><row><cell cols="2">Train sentence 56195</cell><cell>5019</cell><cell></cell></row><row><cell>Test sentence</cell><cell>5000</cell><cell>703</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of CopyRE and CopyRE' on NYT and</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CopyMTL outperforms baselines by a huge margin, it is still unclear which component in CopyMTL plays the pivot role. To reveal the strength of the new model architecture, we compare CopyRE with the modified model, called CopyRE', which only substitutes Eq. (3) for Eq. (10). The comparison is inTable 3, from which we can observe that Eq. (10) is extremely effective. CopyRE' model gains</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">Relation Entity</cell></row><row><cell>NYT</cell><cell>CopyRE CopyRE'</cell><cell>.846 .869</cell><cell>.647 .756</cell></row><row><cell>WebNLG</cell><cell>CopyRE CopyRE'</cell><cell>.767 .797</cell><cell>.595 .782</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: F1 scores on subtasks.</cell></row><row><cell>13% F1 boost in NYT dataset and 31% F1 boost in WebNLG</cell></row><row><cell>dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. The curve shows that CopyRE does not fit the training set well and the model saturates at epoch 20, where the F1 score of NYT is 75% and the F1 score of WebNLG is 40%. By contrast, CopyRE' gains 97% F1 score in the NYT training set and 97% F1 score in the WebNLG training set. In addition, the performance of CopyRE' continues increasing until epoch 40 on both datasets. The fact that the model gains lower training error which also generalizes to the test set may explain the effectiveness of CopyRE'.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Prec Rec</cell><cell>F1</cell></row><row><cell></cell><cell cols="3">GraphRel-2p .639 .600 .619</cell></row><row><cell>NYT</cell><cell>CopyRE'5</cell><cell cols="2">.680 .663 .671</cell></row><row><cell></cell><cell>CopyMTL</cell><cell cols="2">.727 .692 .709</cell></row><row><cell></cell><cell cols="3">GraphRel-2p .447 .411 .429</cell></row><row><cell>WebNLG</cell><cell>CopyRE'5</cell><cell cols="2">.572 .536 .553</cell></row><row><cell></cell><cell>CopyMTL</cell><cell cols="2">.578 .601 .589</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of different multi-token models on NYT and WebNLG</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The original paper uses concatenation, but actually they use average in the released code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In experiments, we found that adding the mask to our method brings no enhancement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/xiangrongzeng/copy re 4 As NovelTagging is significantly better than previous works, we do not add more comparisons.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the National Natural Science Foundation of China(No.61602059, 61972057), "Double First-class" International Cooperation and Development 5  We did not use it as a compared baseline because HRL requires complicated preprocessing procedure and is not transferable to different datasets. Scientific Research Project of Changsha University of Science and Technology: 2018IC25.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07719</idno>
		<title level="m">Global normalization of convolutional neural networks for joint entity and relation classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Portland, Oregon, USA; Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using position-attentive sequence labeling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma ;</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<idno>Gu et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Florence, Italy; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schütze</forename><surname>Andrassy ; Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Andrassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions<address><addrLine>Osaka, Japan; Boulder, Colorado; Vancouver, Canada; Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccallum</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Pereira ; Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML, ICML &apos;01</title>
		<meeting>ICML, ICML &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Endto-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-AFNLP</title>
		<meeting>ACL-AFNLP<address><addrLine>Singapore; Berlin, Germany; Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification. Lingvisticae Investigationes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
	<note>Nadeau and Sekine</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang ; Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09927</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccallum ; Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>Balcázar, J. L.</editor>
		<editor>Bonchi, F.</editor>
		<editor>Gionis, A.</editor>
		<editor>and Sebag, M.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Global inference for entity and relation identification via a linear programming formulation. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="553" to="580" />
		</imprint>
	</monogr>
	<note>Roth and Yih</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A hierarchical framework for relation extraction with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Takanobu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03925</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ireland</forename><surname>Dublin</surname></persName>
		</author>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Australia</forename><surname>Melbourne</surname></persName>
		</author>
		<title level="m">Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
