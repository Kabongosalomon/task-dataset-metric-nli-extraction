<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Contextualized Representation for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengshun</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<email>zhaohai@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Contextualized Representation for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) models are typically based on the architecture of Bi-directional LSTM (BiLSTM). The constraints of sequential nature and the modeling of single input prevent the full utilization of global information from larger scope, not only in the entire sentence, but also in the entire document (dataset). In this paper, we address these two deficiencies and propose a model augmented with hierarchical contextualized representation: sentence-level representation and document-level representation. In sentencelevel, we take different contributions of words in a single sentence into consideration to enhance the sentence representation learned from an independent BiLSTM via label embedding attention mechanism. In document-level, the key-value memory network is adopted to record the document-aware information for each unique word which is sensitive to similarity of context information. Our two-level hierarchical contextualized representations are fused with each input token embedding and corresponding hidden state of BiLSTM, respectively. The experimental results on three benchmark NER datasets (CoNLL-2003 and Ontonotes 5.0 English datasets, CoNLL-2002 Spanish dataset) show that we establish new state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Named Entity Recognition (NER) is one of the fundamental tasks in natural language processing (NLP) that intends to identify words or phrases as the proper names of PER (Person), ORG (Organization), LOC (Location), etc. Currently, most state-of-the-art NER systems <ref type="bibr" target="#b6">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b8">Lample et al. 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy 2016;</ref><ref type="bibr" target="#b3">Chiu and Nichols 2016)</ref> employ BiRNNs, specially BiL-STM <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> as the encoder to extract the sequential information.</p><p>BiLSTM architectures exist limitations in making full use of global information. First, at each time step, BiLSTM takes current word embedding and past summary states as <ref type="figure">Figure 1</ref>: Incorporating hierarchical contextualized representation for NER. The sentence-level representation is assigned to each token and fed to the sequence labeling BiL-STM encoder. The document-level representation is fused with the hidden state of the BiLSTM and fed to the decoder.</p><p>inputs, making it difficult to capture sentence-level information. <ref type="bibr" target="#b20">(Zhang, Liu, and Song 2018)</ref> simultaneously model the sub-states for individual words and an overall sentence-level state. <ref type="bibr" target="#b10">(Liu et al. 2019b</ref>) use a global contextual encoder and mean pooling strategy to capture sentence-level features, though they ignore the different importance of words in the same sentence. Second, though BiLSTM updates the parameters with the iteration of all training instances, it only consumes one instance during both training and predicting. This nature prevents the model from effectively capturing document (dataset)-level information, e.g. for a unique token, its representations in training instances are indicative for recognizing the concerned token. <ref type="bibr" target="#b0">(Akbik, Bergmann, and Vollgraf 2019</ref>) use a pooling operation on different contextualized embeddings to generate global word representations. While they only consider the changes of embeddings for each unique word.</p><p>In this paper, we propose a hierarchical contextualized representation architecture to enhance NER modeling. For sentence-level representation, inspired by <ref type="bibr" target="#b13">(Wang et al. 2018)</ref>, we embed labels in the same space with word embeddings, and label embeddings are learned from attention mechanism computed with word embeddings to ensure that, each word embedding is much closer to their corresponding label embedding, and farther to other label embeddings.</p><p>Then, the similarity between a word embedding and its nearest label embedding is regarded as a confidence score for this word. Hence, words with higher confidence scores contribute more to sentence-level representation. The sentencelevel representations are then assigned to each token and fed to the encoder as shown in <ref type="figure">Figure 1</ref>. For document-level representation, we adopt a key-value memory component <ref type="bibr" target="#b10">(Miller et al. 2016</ref>) which memorizes all the word embeddings of training instances and their corresponding representations. The attention mechanism is adopted to compute the output of the memory component. The retrieved documentlevel representation is fused with the original hidden state and fed to the decoder as shown in <ref type="figure">Figure 1</ref>. In this case, the training instances are not only used to train the model parameters, but also involved in inference.</p><p>To verify the effectiveness our model, we conduct extensive experiments on three benchmark NER datasets. Experimental results on these benchmarks suggest that our model can achieve state-of-the-art performance on CoNLL-2003 (91.96 F 1 without external knowledge and 93.37 F 1 with BERT), OntoNotes 5.0 (87.98 F 1 without external knowledge and 90.30 F 1 with BERT), 87.08 F 1 on CoNLL-2002, meaning that our model truly learns and benefits from useful contextualized representations.</p><p>Our contributions in this paper are summarized as follows.</p><p>• We are the first to introduce hierarchical contextualized representations, namely sentence-level and documentlevel representation, for NER to take full advantage of non-local information.</p><p>• We introduce the label embedding attention mechanism for sentence-level representation and propose an effective approach to distill document-level information using keyvalue memory network.</p><p>• The evaluation results on three benchmark NER datasets show that our model outperforms all previously reported results without external knowledge. Furthermore, with pre-trained language model BERT, we establish new state-of-the-art results on CoNLL-2003 and ontonotes 5.0 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Neural Named Entity Recognition Recently, with the development of deep neural network in a wide range of NLP tasks <ref type="bibr" target="#b5">(He et al. 2018;</ref><ref type="bibr" target="#b5">He, Li, and Zhao 2019;</ref><ref type="bibr" target="#b21">Zhou and Zhao 2019;</ref><ref type="bibr" target="#b15">Xiao et al. 2019;</ref><ref type="bibr" target="#b19">Zhang et al. 2020a;</ref><ref type="bibr" target="#b19">Zhang et al. 2020b</ref>), neural network based models build reliable NER systems without hand-crafted features or taskspecific knowledge. <ref type="bibr" target="#b6">(Huang, Xu, and Yu 2015)</ref> firstly proposed the BiLSTM-CRF architecture, which is used by most state-of-the-art models. Later, character-level embeddings are concatenated to enhance the representation of rare and out-of-vocabulary words, these embeddings are generated with LSTM <ref type="bibr" target="#b8">(Lample et al. 2016)</ref>, CNN (Ma and Hovy 2016), and recently IntNet <ref type="bibr" target="#b16">(Xin et al. 2018)</ref>. (Tran, MacKinlay, and Jimeno Yepes 2017) stack BiLSTMs with residual connections between different layers of BiLSTM to add more representational power. More recently, pretrained language models from huge corpus are adopted to enhance the representation of words <ref type="bibr" target="#b10">(Peters et al. 2018;</ref><ref type="bibr" target="#b0">Akbik, Bergmann, and Vollgraf 2019;</ref><ref type="bibr" target="#b4">Devlin et al. 2019)</ref>. Sentence-level Representation has been adopted to eliminate the limitations of RNNs due to their sequential nature. <ref type="bibr" target="#b18">(Yang, Zhang, and Dong 2017)</ref> leverage RNN models to learn sentence-level patterns for NER reranking. <ref type="bibr" target="#b20">(Zhang, Liu, and Song 2018)</ref> model the sub-states for individual words and an overall sentence-level state simultaneously to capture local and non-local contexts. ) use contextual layer and relation layer to model the relations between words in sentences, and then use gates to fuse local context features into global ones. <ref type="bibr" target="#b10">(Liu et al. 2019b</ref>) simplify sentence-level state to average of the hidden states of each individual word from an independent global contextual encoder. Inspired by <ref type="bibr" target="#b13">(Wang et al. 2018</ref>) which use label information to construct text-sequence representations, we adopt label embedding attention to enhance the sentencelevel representation learned from an independent BiLSTM. The use of sentence-level information in <ref type="bibr" target="#b10">(Liu et al. 2019b)</ref> can be seen as a special case of our model where the attention weight vector is a uniform distribution that assigns equal probabilities to all the words in the sentence. Document-level Representation (Qian et al. 2019) considers the dependency structure of word sequence as the global information. <ref type="bibr" target="#b0">(Akbik, Bergmann, and Vollgraf 2019)</ref> dynamically aggregates contextualized embeddings for each unique string and then use a pooling operation to generate a global word representation from these contextualized instances for NER. Different from their work which only uses the contextual word embeddings, our memory component uses the key-value memory networks to memorize the word representations (key) and the hidden states (value) from the sequence labeling encoder. The attention mechanism is then called to calculate the document-level representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>This section presents our NER model in detail. The overall model architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, which consists of four components: a decoder (top part), a sequence labeling encoder (upper right part), a sentence-level encoder (bottom right part), and a document-level encoder (left part).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Model</head><p>We adopt the IntNet-LSTM-CRF model proposed by <ref type="bibr" target="#b16">(Xin et al. 2018</ref>) as our baseline, which consists of three parts: representation module, sequence labeling encoder and decoder module. Token Representation Given a sequence of N tokens X = {x 1 , x 2 , ..., x N }, for each word x i , we concatenate the word-level and character-level embedding as the joint word representation x i = [w i ; c i ]. w i is the pre-trained word embedding. The character-level embedding c i is learned from IntNet, which is a funnel-shaped wide convolutional neural architecture for learning representations of the internal structure of words. The network comprises of L convolutions, which implies (L−1)/2 convolutional blocks. In each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Labeling Encoder</head><p>Sentence-level Encoder convolutional block, the first layer is the N × 1 convolution which transforms the input, then the concatenation of convolutions with different kernel sizes in the second layer is fed to the next convolutional block. Direct connections from every other layer to all subsequent layers are used like dense connections. Sequence Labeling Encoder The concatenation of wordlevel and character-level embeddings x i = [w i ; c i ] is then fed into the sequence labeling BiLSTM, which represents the sequential information at each step.</p><formula xml:id="formula_0">Document-level Encoder w l BiLSTM memory w 1-1 w 1-2 w 1-3 w 2-1 w 2-2 w 3-1 w 3-2 w 3-3 w 3-4 h 1-1 h 1-2 h 1-3 h 2-1 h 2-2 h 3-1 h 3-2 h 3-3 h 3-4 w 1 w 2 w 3 w c</formula><formula xml:id="formula_1">h i = [ − → h i ; ← − h i ] − → h i = LST M (x i , − → h i−1 ; − → θ ) ← − h i = LST M (x i , ← − h i−1 ; ← − θ )<label>(1)</label></formula><p>where − → θ and ← − θ are trainable parameters, respectively. Decoder Conditional random field (CRF) <ref type="bibr" target="#b7">(Lafferty, Mccallum, and Pereira 2001)</ref> has been widely used in state-of-theart NER models <ref type="bibr" target="#b8">(Lample et al. 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy 2016)</ref> to help make decisions when considering strong connections between output tags. During decoding, the Viterbi algorithm is applied to search the label sequence with the highest probability. For y = {y 1 , ..., y N } being a predicted sequence of labels with same length as x. We define its score as:</p><formula xml:id="formula_2">sc(x, y) = N −1 i=0 T r yi,yi+1 + N i=1 P i,yi<label>(2)</label></formula><p>where T r yi,yi+1 represents the transmission score from the y i to y i+1 , P i,yi is the score of the j th tag of the i th word from the sequence labeling encoder. The CRF model defines a family of conditional probability p(y|x) over all possible tag sequences y:</p><formula xml:id="formula_3">p(y|x) = exp sc(x,y) ỹ∈y exp sc(x,ỹ)<label>(3)</label></formula><p>during training, we consider the maximum log probability of the correct sequence of tags. While decoding, we search the label sequence with maximum score:</p><formula xml:id="formula_4">y * = arg max y∈y sc(x,ỹ)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-level Representation</head><p>Sentence-level information has been shown highly useful for model sequence <ref type="bibr" target="#b20">(Zhang, Liu, and Song 2018;</ref><ref type="bibr" target="#b10">Liu et al. 2019b)</ref>. We adopt an independent BiLSTM to generate contextualized features, which takes word representation</p><formula xml:id="formula_5">x i = [w i ; c i ]</formula><p>as the input, we denote the hidden states of this BiLSTM as v ∈ R N ×ds , where N is the length of the sequence and d s is the hidden size. Considering that words may contribute differently to the sentence-level representation, we adopt label embedding attention <ref type="bibr" target="#b13">(Wang et al. 2018)</ref> to get an attention score for the entire sentence and then transform the hidden states v ∈ R N ×ds into a fixed-sized sentence-level representation s ∈ R ds . As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we embed all the label types (e.g. LOC, PER, etc.) in the same space as the word embeddings. We denote the label embeddings as l = [l 1 , l 2 , ...l P ], l ∈ R P ×dw , where P is the number of labels, d w is the dimension of the word embeddings. We train on the training instances to ensure that, each word embedding is more closer to their corresponding label embedding, and farther to other label embeddings. For example, the token Italy is labeled as LOC type in the example of <ref type="figure">Figure 1</ref>, we attempt to make it closer to the label embedding of LOC, and farther to other label embeddings (e.g. the label embedding of PER). The cosine similarity 1 e(x i , l j ) between the word embeddings x i and the label embedding l j can be taken to measure the confidence score of this word-label pair:</p><formula xml:id="formula_6">e(x i , l j ) = x T i l j x i l j<label>(5)</label></formula><p>We use convolutional neural network (CNN) to capture the relative spatial information among consecutive words in the sentence. Further, the largest confidence score m i ∈ R P between the i-th word and all labels is obtained by a maxpooling operation:</p><formula xml:id="formula_7">m i = max(W T   e(i − k−1 2 , :) · · · e(i + k−1 2 , :)   + b)<label>(6)</label></formula><p>where W ∈ R k and b ∈ R P are trainable parameters, k is the kernel size, max denotes max pooling. The attention (confidence) score β ∈ R N for the entire sentence is:</p><formula xml:id="formula_8">β = sof tmax(m)<label>(7)</label></formula><p>The sentence-level representation s ∈ R ds can be simply obtained via averaging the hidden states v ∈ R N ×ds , weighted by the attention score calculated above:</p><formula xml:id="formula_9">s = N i=1 β i v i<label>(8)</label></formula><p>The sentence-level representation s ∈ R ds is then concatenated with the word representation x i = [x i ; s] and fed to the sequence labeling encoder. Note that the training and test process are the same. We use the label embeddings l ∈ R P ×dw of all labels, not the ground-truth label embedding. The intuition for using the label embedding attention is that each word in the sentence contributes differently to sentence-aware representation. The similarity between each word embedding and its nearest label embedding can be regarded as the confidence score of this word-label pair. Words with higher confidence score should contribute more to the sentence-level contextualized representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level Representation</head><p>In terms of memory network, we introduce document-aware representations of the unique word in training instances as an extra knowledge source to help the prediction. Memory network was originally proposed by <ref type="bibr" target="#b14">(Weston, Chopra, and Bordes 2014)</ref> in the domain of question answering (QA) for prediction, where the long-term memory acts as a dynamic knowledge base. <ref type="bibr" target="#b10">(Miller et al. 2016</ref>) further introduces a key-value memory networks, which utilize different encodings in the addressing and output stages. The keys are designed to help match the question, while the values are to generate the response.</p><p>We adopt the key-value memory component M to memorize document-level contextualized representation. Memory slots are defined as pairs of vectors (k 1 , v 1 ), ..., (k m , v m ). In each single slot, the key represents the word embedding w i , and the value is the corresponding hidden states h i from sequence labeling encoder for each token in training instances. So the same word may occupy in many different slots because of changing embeddings and representations under different contexts. <ref type="table" target="#tab_0">Table 1</ref> shows an example of using training instances to help indicate the NE type of queried token. Memory Update The word embeddings are fine-tuned during training and used to update the key part of the memory. The sequence labeling encoder generates the hidden states to update the value part. Supposing the states of the i-th token is changed after computation, the i-th slot in the memory M will be rewritten. Each memory slot will be updated once in one epoch. Memory Query For the i-th word in the sentence, we distill all the contextualized representations for this word in the memory M through an inverted index that finds a subset (k sub1 , v sub1 ), ..., (k sub T , v sub T ) of size T , where the inverted index records the positions of the unique word in the memory M as shown in <ref type="table" target="#tab_0">Table 1</ref>. T represents the number of occurrences of this word among the training instances.</p><p>The attention operation is called to compute the weight of document-level representation. For the unique word, the memory key k j ∈ [k sub1 ; ...; k sub T ] is used as the attention key, the memory value v j ∈ [v sub1 ; ...; v sub T ] is used as the attention value. Then the embedding w qi of the queried word serves as the attention query q i . Here, we consider three compatibility functions u ij = o(q i , k j ):</p><p>(1) dot-product attention</p><formula xml:id="formula_10">o 1 (q i , k j ) = q i k T j (9) (2) scaled dot-product attention (Vaswani et al. 2017) o 2 (q i , k j ) = q i k T j √ d w<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test instance</head><p>Italy recalled Marcello Cuttitt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training instances</head><p>1. ORVIETO (0), Italy (1) 1996-08-24 (2). 2. Rohrabacher (3) had (4) recently (5) visited (6) Italy <ref type="formula" target="#formula_8">(7)</ref> 3. Andrea (8) Ferrigato (9) of (10) Italy (11) sprinted (12)...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverted index</head><p>Italy: [1, 7, 11, ...] </p><formula xml:id="formula_11">o 3 (q i , k j ) = q i k T j q i k j<label>(11)</label></formula><p>where d w represents the dimension of word embeddings. Memory Response The document-level representation is computed as:</p><formula xml:id="formula_12">α ij = exp(u ij ) T z=1 exp(u iz ) r i = T j=1 α ij v j<label>(12)</label></formula><p>Then the fusion representation g i ∈ R d h of the original hidden representation and this document-level representation are fed to the CRF layer, where d h is the hidden size of the sequence labeling encoder.</p><formula xml:id="formula_13">g i = λh i + (1 − λ)r i<label>(13)</label></formula><p>where λ is a hyperparameter, indicating how much document-aware information is adopted, 0 for documentlevel representation only and 1 for discarding all documentlevel information at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>Our proposed representations are evaluated on three benchmark NER datasets: <ref type="bibr">CoNLL-2003 (Sang and</ref><ref type="bibr" target="#b12">De Meulder 2003)</ref>   <ref type="bibr" target="#b18">(Yang, Zhang, and Dong 2017)</ref> 91.62  91.24 ± 0.12 <ref type="bibr" target="#b17">(Yang and Zhang 2018)</ref> 91.35 <ref type="bibr" target="#b20">(Zhang, Liu, and Song 2018)</ref> 91.57 <ref type="bibr" target="#b16">(Xin et al. 2018)</ref> 91.64 ± 0.17 <ref type="bibr" target="#b9">(Liu et al. 2019a)</ref> 91.10  91.44 ± 0.10 <ref type="bibr" target="#b11">(Qian et al. 2019)</ref> 91.74 <ref type="bibr" target="#b10">(Liu et al. 2019b</ref>  is also tagged with four linguistic entity types (PER, LOC, ORG, MISC).</p><p>• OntoNotes 5.0 consists of 76,714 sentences from a wide variety of sources (magazine, telephone conversation, newswire, etc.). Following <ref type="bibr" target="#b3">(Chiu and Nichols 2016;</ref>, we use the portion of the dataset with gold-standard named entity annotations, and thus exclude the New Testaments portion. It is tagged with eighteen entity types (PERSON, CARDINAL, LOC, PRODUCT, etc.).</p><p>Metric We use the BIOES sequence labeling scheme instead of BIO for these three datasets during training. As for test, we convert the prediction results back to the BIO scheme and use the standard conlleval script to compute the F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Pre-trained Word Embeddings. For the CoNLL-2003 and OntoNotes 5.0 English datasets, we use the publicly available pre-trained 100D GloVe (Pennington, Socher, and Manning 2014) embeddings. For CoNLL-2002 Spanish dataset, we train 64D GloVe embeddings with the minimum frequency of occurrence as 3, and the window size of 5. The <ref type="bibr">F 1 (Gillick et al. 2015)</ref> 82.95 <ref type="bibr" target="#b8">(Lample et al. 2016)</ref> 85.75 <ref type="bibr" target="#b17">(Yang, Salakhutdinov, and Cohen 2017)</ref> 85.77 <ref type="bibr" target="#b16">(Xin et al. 2018)</ref> 86.68 ± 0.35 Ours 87.08 ± 0.16  word embeddings are fine-tuned during training. Character Embeddings. We train the IntNet character embeddings <ref type="bibr" target="#b16">(Xin et al. 2018)</ref>. The dimension of character embeddings is 32, which is randomly initialized, the filter size of the initial convolution is 32 and that of other convolutions is 16. Different from <ref type="bibr" target="#b16">(Xin et al. 2018</ref>), we set filters as size [3; 5] for all the kernels, and the number of convolutional layers is 7. Parameters. We follow the work of <ref type="bibr" target="#b17">(Yang and Zhang 2018)</ref>, and conduct optimization with the stochastic gradient descent 2 . The batch size is set as 10, the initial learning rate is set to 0.015 and will shrunk by 5% after each epoch. The hidden size of sequence labeling encoder and the sentencelevel encoder are set as 256 and 128, respectively. We apply dropout to embeddings and hidden states with a rate of 0.5. The λ used to fuse original hidden state and document-level representation is set as 0.3 empirically. For each type of NEs, we randomly select hundreds of NEs, and calculate the average of the word embeddings as its label embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Comparisons</head><p>Tables 2, 3, 4 compare our model to existing state-of-theart approaches on the three benchmark datasets. Our model surpasses previous state-of-the-art approaches on all the three datasets. On CoNLL-2003 dataset, we compare our model with the state-of-the-art models, including the models that use global information to enhance the representation <ref type="bibr" target="#b18">(Yang, Zhang, and Dong 2017;</ref><ref type="bibr" target="#b20">Zhang, Liu, and Song 2018;</ref><ref type="bibr">CoNLL03</ref>     <ref type="bibr" target="#b11">Qian et al. 2019;</ref><ref type="bibr" target="#b10">Liu et al. 2019b)</ref>. We also incorporate pretrained language model BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref> for fair comparisons with the models which also use pre-trained language models or other external knowledge. Some of the results <ref type="bibr" target="#b1">(Akbik, Blythe, and Vollgraf 2018;</ref><ref type="bibr" target="#b1">Akbik, Blythe, and Vollgraf 2018)</ref> are not comparable to our results directly, because their final models are trained on both training and development datasets. On <ref type="bibr">CoNLL-2002</ref> Spanish dataset, our model achieves 87.08 F 1 score without external knowledge, which surpasses previous best score by 0.4. Considering that the above two datasets are relatively small, we further conduct experiment on a much more large OntoNotes 5.0 dataset, which also has more entity types. We compare our model with the previous model that also reported results on it <ref type="bibr" target="#b3">(Chiu and Nichols 2016;</ref><ref type="bibr" target="#b13">Shen et al. 2018;</ref><ref type="bibr" target="#b4">Ghaddar and Langlais 2018)</ref>. As shown in <ref type="table" target="#tab_5">Table 4</ref>, our model shows a significant advantage on this dataset, which outperforms previous state-of-the-art results substantially at 87.08 (+0.31) without BERT, and 90.30 (+0.59) with BERT.</p><p>More notably, our model without external knowledge surpasses the previous model <ref type="bibr" target="#b4">(Ghaddar and Langlais 2018)</ref>, which use extra lexicon information of 120 entity types from Wikipedia. Overall, the comparisons on these three benchmark datasets well demonstrate that our model truly learns and benefits from useful sentence-level and document-level representation without the support from external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this experiment, we individually adopt two hierarchical contextualized representations to enhance the representation of tokens: sentence-level representation for assigning the sentence state to each token and document-level representation for inference. <ref type="table" target="#tab_7">Table 5</ref> shows the F 1 score raise and relative error reduction brought by each of the two hierarchical representation on the three benchmark datasets. We discover that both sentence-level and document-level repre-  sentations enhance the baseline. By combing these two representations together, we get a larger gain of 0.36 / 0.43 / 0.40, respectively. We further analyze the two hierarchical representations by adopting different strategies. <ref type="bibr" target="#b10">(Liu et al. 2019b)</ref> perform mean pooling over all the tokens to generate sentence-level representation. We further conduct experiments to investigate the three compatibility functions used to employ memorized information. As shown in <ref type="table" target="#tab_8">Table 6</ref>, compared with the mean pooling strategy, our label-embedding attention mechanism raises the F 1 score by 0.25. Among the three compatibility functions to compute the weight of query word and memorized slots, cosine similarity performs best, while dot-product performs worst. <ref type="bibr" target="#b13">(Vaswani et al. 2017</ref>) use scaled dot-product to counteract the dot products growth in magnitude, showing better than dot product. Cosine similarity calculates the inner product of word vectors with unit length, and can further solve the inconsistency between the embeddings and the similarity measurement. Thus, we eventually adopt cosine similarity as the compatibility function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Size and Time Consuming</head><p>Figure 3 illustrates our model performance and time proportion compared to the baseline with respect to the max queried subset size T for each unique word in the memory query step. For words occurring more than T times in the corpus (these words are more likely to be stop words when T is large), we only randomly select T slots in the subset to compute the document-level representation. For fair comparisons, we keep the IntNet layer, sequence labeling layer and CRF layer the same for all the experiments. The consumed time of our model is only 19% more than the baseline on CoNLL-2003 dataset even with the max memory size as 500. Therefore, our model brings slight increase on time consumption. When T is less than 500, the larger T may incorporate more useful contextualized representation for practice words and improve the results accordingly, when T is larger than 500, which may involve more stop words, our model drops slightly. <ref type="table" target="#tab_10">Table 7</ref> presents the F 1 score of in-both-vocabulary words (IV), out-of-training-vocabulary words (OOTV), out-ofembedding-vocabulary words (OOEV), and out-of-bothvocabulary words (OOBV) on CoNLL-2003 datset. According to our statistic, 63.40% / 52.43% / 84.68% of the NEs in the test set of <ref type="bibr">CoNLL-2003, CoNLL-2002, and</ref><ref type="bibr">OntoNotes</ref>  datasets are located in the IV part, respectively. Therefore, it is of great importance to focus on this part. We adopt memory network to memorize and retrieve the global representations and use the memorized training instances directly to participate in inference, which greatly improves both the precision and recall of the NEs in IV part, in which our model outperforms baseline by 0.39 in terms of F 1 score. For OOV NEs, sentence-level representation can help these concerned tokens aware of the entire sentence, thus enhance the performance. The improvement is 0.44 / 0.43 F 1 score for OOTV NEs and OOBV NEs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we adopt hierarchical contextualized representations to enhance the performance of named entity recognition (NER). Our model makes full use of the training instances and the spatial information of the embedding space by incorporating sentence-level representation and document-level representation. We consider the importance of words in the sentences and weight their contributions with the label embedding attention for the sentencelevel representation. For words shown in training instances, we memorize the representations of these instances, and involve these representations for inference during test. Empirical results on three benchmark datasets <ref type="bibr">(CoNLL-2003 and</ref><ref type="bibr">Ontonotes 5.0 English datasets, CoNLL-2002 Spanish dataset)</ref> show that our model outperforms previous state-ofthe-art systems with or without pre-trained language models respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The main architecture of our NER model. The sequence labeling encoder (upper right) generates representations for the decoder and updates the memory component. The sentence-level encoder (bottom right part) generates the sentence-level representation, and the memory network (left part) computes the document-level contextualized information, in which the same color represents the memory slots for the unique word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>F 1 score and time proportion with respect to the max memory size. Time proportion represents the ratio of our training time compared to baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Query operation for the word Italy. The numbers in parentheses indicate slot index of tokens in memory M . The memory slots of these bold tokens in training instances are retrieved according to the inverted index for Italy.</figDesc><table><row><cell>and (3) cosine similarity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>F 1 scores on CoNLL-2003. † refers to models trained on both training and development datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>F 1 scores on CoNLL-2002.</figDesc><table><row><cell>Models</cell><cell>F 1</cell></row><row><cell>(Durrett and Klein 2014)</cell><cell>84.04</cell></row><row><cell>(Chiu and Nichols 2016)</cell><cell>86.28 ± 0.26</cell></row><row><cell>(Shen et al. 2018)</cell><cell>86.63 ± 0.49</cell></row><row><cell>(Strubell et al. 2017)</cell><cell>86.84 ± 0.19</cell></row><row><cell>(Ghaddar and Langlais 2018)</cell><cell>87.44</cell></row><row><cell>(Chen et al. 2019)</cell><cell>87.67 ± 0.17</cell></row><row><cell>Ours</cell><cell>87.98 ± 0.05</cell></row><row><cell>+ Language Models / External knowledge</cell><cell></cell></row><row><cell>(Ghaddar and Langlais 2018)</cell><cell>87.95</cell></row><row><cell>(Clark et al. 2018)</cell><cell>88.88</cell></row><row><cell>(Akbik, Bergmann, and Vollgraf 2019) 3</cell><cell>89.71</cell></row><row><cell>Ours + BERT</cell><cell>90.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>F 1 scores on OntoNotes 5.0.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the three benchmark datasets.</figDesc><table><row><cell></cell><cell>Strategy</cell><cell>F 1</cell><cell>ERR</cell></row><row><cell>base model</cell><cell>-</cell><cell>91.60</cell><cell>-</cell></row><row><cell>sentence-level</cell><cell>mean-pooling label-embedding</cell><cell>91.65 91.80</cell><cell>0.60 2.23</cell></row><row><cell></cell><cell>dot-product</cell><cell>91.63</cell><cell>1.55</cell></row><row><cell>document-level</cell><cell cols="2">scaled dot-product 91.75</cell><cell>1.79</cell></row><row><cell></cell><cell>cosine similarity</cell><cell>91.79</cell><cell>2.38</cell></row><row><cell>ALL</cell><cell>-</cell><cell>91.96</cell><cell>3.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different strategies on CoNLL-2003 dataset. ERR is the relative error rate reduction of our model compared to the baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>IV 94.58 93.16 93.87 94.96 93.58 94.26 OOTV 93.46 91.57 92.51 94.07 91.85 92.95 OOEV 94.12 94.12 94.12 94.12 94.12 94.12 OOBV 88.42 84.81 86.58 88.51 85.56 87.01</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Detailed results on the CoNLL-2003 dataset for IV, OOTV, OOEV, OOBV.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The reason for using cosine similarity is the same as the next subsection, and will be analyzed later.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Through personal communication, the authors confirmed that they directly tested the BIOES tagged results with the official conlleval script (which can only works for BIO tagged entities), giving the results reported in their paper 91.96 / 93.47, while our reevaluation results are 91.54 / 93.23 with strict BIO tag converting from the released file by the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code will be available at https://github.com/cslydia/Hire-NER.3  The authors of (Akbik, Bergmann, and Vollgraf 2019) released the result of 89.3 in their github https://github.com/ zalandoresearch/flair, 89.71 is our re-implement result.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bergmann</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vollgraf ; Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blythe</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GRN: Gated relation network to enhance convolutional neural network for named entity recognition</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Semi-supervised sequence modeling with cross-view training</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NAACL. Gillick et al. 2015. Multilingual language processing from bytes. Computer Science</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntax for semantic role labeling, to be, or not to be</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu ;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccallum</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Pereira ; Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>AAAI. [Liu et al. 2019a</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Contextualized non-local neural networks for sequence learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GCDT: A global context enhanced deep transition architecture for sequence labeling</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GraphIE: A graph-based framework for information extraction</title>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De Meulder ;</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2002 shared task: Languageindependent named entity recognition</title>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chopra</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bordes ; Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lattice-based transformer encoder for neural machine translation</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning better internal structure of words for sequence labeling</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural reranking for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong ;</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DCMN+: Dual co-matching network for multi-choice reading comprehension</title>
		<idno>Zhang et al. 2020b</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentence-state LSTM for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Headdriven phrase structure grammar parsing on Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
