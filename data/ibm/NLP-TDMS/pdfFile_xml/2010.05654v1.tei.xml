<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ragusa</surname></persName>
							<email>francesco.ragusa@unict.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IPLAB</orgName>
								<orgName type="institution" key="instit2">University of Catania XGD-XENIA s.r.l</orgName>
								<address>
									<settlement>Acicastello</settlement>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Livatino</surname></persName>
							<email>s.livatino@herts.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Hertfordshire</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
							<email>furnari@dmi.unict.it</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">IPLAB</orgName>
								<orgName type="institution" key="instit2">University of Catania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
							<email>gfarinella@dmi.unict.it</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">IPLAB</orgName>
								<orgName type="institution" key="instit2">University of Catania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wearable cameras allow to collect images and videos of humans interacting with the world. While human-object interactions have been thoroughly investigated in third person vision, the problem has been understudied in egocentric settings and in industrial scenarios. To fill this gap, we introduce MECCANO, the first dataset of egocentric videos to study human-object interactions in industrial-like settings. MECCANO has been acquired by 20 participants who were asked to build a motorbike model, for which they had to interact with tiny objects and tools. The dataset has been explicitly labeled for the task of recognizing human-object interactions from an egocentric perspective. Specifically, each interaction has been labeled both temporally (with action segments) and spatially (with active object bounding boxes). With the proposed dataset, we investigate four different tasks including 1) action recognition, 2) active object detection, 3) active object recognition and 4) egocentric human-object interaction detection, which is a revisited version of the standard human-object interaction detection task. Baseline results show that the MECCANO dataset is a challenging benchmark to study egocentric humanobject interactions in industrial-like scenarios. We publicy release the dataset at https://iplab.dmi.unict. it/MECCANO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being able to analyze human behavior from egocentric observations has many potential applications related to the recent development of wearable devices [1, 2, 3] which range from improving the personal safety of workers in a factory <ref type="bibr" target="#b6">[10]</ref> to providing assistance to the visitors of a mu- seum <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b7">11]</ref>. In particular, with the rapid growth of interest in wearable devices in industrial scenarios, recognizing human-object interactions can be useful to prevent safety hazards, implement energy saving policies and issue notifications about actions that may be missed in a production pipeline <ref type="bibr" target="#b53">[56]</ref>.</p><p>In recent years, progress has been made in many research areas related to human behavior understanding, such as action recognition <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b52">55,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b65">68,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b37">40]</ref>, object detection <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b48">51]</ref> and human-object interaction detection <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b41">44</ref>]. These advances have been possible thanks to the availability of large-scale datasets <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b4">8]</ref> which have been curated and often associated with dedicated challenges. In the egocentric vision domain, in particular, previous investigations have considered the contexts of kitchens <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b14">17]</ref>, as well as daily living activity at home and in offices <ref type="bibr" target="#b44">[47,</ref><ref type="bibr" target="#b55">58,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr">46]</ref>. While these contexts provide interesting test-beds to study user behavior in general, egocentric human-object interactions have not been previously studied in industrial environments such as factories, building sites, mechanical workshops, etc. This is mainly due to the fact that data acquisition in industrial domains is difficult because of privacy issues and the need to protect industrial secrets.</p><p>In this paper, we present MECCANO, the first dataset of egocentric videos to study human-object interactions in industrial-like settings. To overcome the limitations related to data collection in industry, we resort to an industrial-like scenario in which subjects are asked to build a toy model of a motorbike using different components and tools (see <ref type="bibr">Figure 1)</ref>. Similarly to an industrial scenario, the subjects interact with tools such as a screwdriver and a wrench, as well as with tiny objects such as screws and bolts while executing a task involving sequential actions (e.g., take wrench, tighten bolt, put wrench). Despite the fact that this scenario is a simplification of what can be found in real industrial settings, it is still fairly complex, as our experiments show. MECCANO was collected by 20 different participants in two countries (Italy and United Kingdom). We densely annotated the acquired videos with temporal labels to indicate the start and end times of each human-object interaction, and with bounding boxes around the active objects involved in the interactions. We hope that the proposed dataset will encourage research in this challenging domain. The dataset is publicly released at the following link: https://iplab.dmi.unict.it/MECCANO.</p><p>We show that the proposed dataset can be used to study four fundamental tasks related to the understanding of human-object interactions: 1) Action Recognition, 2) Active Object Detection, 3) Active Object Recognition and 4) Egocentric Human-Object Interaction Detection. While past works have already investigated the tasks of action recognition <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b54">57]</ref>, active object detection <ref type="bibr" target="#b44">[47]</ref>, and active object recognition <ref type="bibr" target="#b9">[13]</ref> in the context of egocen-tric vision, Human-Object Interaction (HOI) detection has been generally studied in the context of third person vision <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b66">69,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b61">64]</ref>. Since we believe that modelling actions both semantically and spatially is fundamental for egocentric vision applications, we instantiate the Human-Object Interaction detection task in the context of the proposed dataset.</p><p>HOI detection consists in detecting the occurrence of human-object interactions, localizing both the humans taking part in the action and the interacted objects. HOI detection also aims to understand the relationships between humans and objects, which is usually described with a verb. Possible examples of HOIs are "talk on the cell phone" or "hold a fresbee". HOI detection models mostly consider one single object involved in the interaction <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b64">67,</ref><ref type="bibr" target="#b5">9]</ref>. Hence, an interaction is generally defined as a triplet in the form &lt;human, verb, object&gt;, where the human is the subject of the action specified by a verb and an object. Sample images related to human-object interactions in a third-person scenario are shown in <ref type="figure" target="#fig_1">Figure 2</ref>-top. We define Egocentric Human-Object Interaction (EHOI) detection as the task of producing &lt;verb, objects&gt; pairs describing the interaction observed from the egocentric point of view. Note that in EHOI, the human interacting with the objects is always the camera wearer, while one or more objects can be involved simultaneously in the interaction. The goal of EHOI detection is to infer the verb and noun classes, and to localize each active object involved in the interaction. We perform experiments with baseline approaches to tackle the four considered tasks. Results suggest that the proposed dataset is a challenging benchmark for understanding egocentric human-object interactions in industriallike settings. In sum, the contributions of this work are as follows: 1) we present MECCANO, a new challenging egocentric dataset to study human-object interactions in an industrial-like domain; 2) we instantiate the HOI definition in the context of egocentric vision (EHOI); 3) we show that the current state-of-the-art approaches achieve limited performance, which suggests that the proposed dataset is an interesting benchmark for studying egocentric human-object interactions in industrial-like domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Datasets for Human Behavior Understanding Previous works have proposed datasets to tackle the task of Human-Object Interaction (HOI) detection. Among the most notable datasets, we can mention V-COCO <ref type="bibr" target="#b29">[32]</ref>, which adds 26 verb labels to the 80 objects of COCO <ref type="bibr" target="#b38">[41]</ref>, HICO-Det <ref type="bibr" target="#b4">[8]</ref>, labeled with 117 verbs and 80 objects, HOI-A <ref type="bibr" target="#b36">[39]</ref>, which focuses on 10 verbs and 11 objects indicating actions dangerous while driving. Other works have proposed datasets for action recognition from video. Among these, ActivityNet <ref type="bibr" target="#b17">[20]</ref> is a large-scale dataset composed of videos depicting 203 activities that are relevant to how humans spend their time in their daily lives, Kinetics <ref type="bibr" target="#b31">[34,</ref><ref type="bibr" target="#b2">6]</ref> is a dataset containing 700 human action classes, Something-Something <ref type="bibr" target="#b27">[30]</ref> includes low-level concepts to represent simple everyday aspects of the world. Previous works also proposed datasets of egocentric videos. Among these datasets, EPIC-Kitchens <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b11">14]</ref> focuses on unscripted activities in kitchens, EGTEA Gaze+ <ref type="bibr" target="#b35">[38]</ref> contains videos paired with gaze information collected from participants cooking different recipes in a kitchen, CMU <ref type="bibr" target="#b14">[17]</ref> is a multimodal dataset of egocentric videos including RGB, audio and motion capture information, ADL <ref type="bibr" target="#b44">[47]</ref> contains egocentric videos of subjects performing daily activities, THU-READ <ref type="bibr" target="#b55">[58]</ref> contains RGB-D videos of subjects performing daily-life actions in different scenarios. <ref type="table">Table 1</ref> compares the aforementioned datasets with respect to the proposed dataset. MECCANO is the first dataset of egocentric videos collected in an industrial-like domain and annotated to perform EHOI Detection. It is worth noting that previous egocentric datasets have considered scenarios related to kitchens, offices, and daily-life activities and that they have generally tackled the action recognition task rather than EHOI detection.</p><p>Action Recognition Action recognition from video has been thoroughly investigated especially in the third person vision domain. Classic works <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b8">12]</ref> relied on motionbased features such as optical flow and space-time features. Early deep learning works fused processing of RGB and optical flow features with two-stream networks <ref type="bibr" target="#b52">[55,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b60">63]</ref>, 3D ConvNets are commonly used to encode both spatial and temporal dimensions <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b57">60,</ref><ref type="bibr" target="#b3">7]</ref>, long-term filtering and pooling has focused on representing actions considering their full temporal extent <ref type="bibr" target="#b59">[62,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b60">63,</ref><ref type="bibr" target="#b65">68,</ref><ref type="bibr" target="#b65">68,</ref><ref type="bibr" target="#b37">40]</ref>. Other works separately factor convolutions into separate 2D spatial and 1D temporal filters <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b63">66,</ref><ref type="bibr" target="#b46">49]</ref>. Among recent works, Slow-Fast networks <ref type="bibr" target="#b21">[24]</ref> avoid using pre-computed optical flow and encodes motion into a "fast" pathway (which operates at a high frame rate) and simultaneously a "slow" pathway which captures semantics (operating at a low frame rate). We asses the performance of state-of-the-art action recognition methods on the proposed dataset considering SlowFast networks <ref type="bibr" target="#b21">[24]</ref>, I3D <ref type="bibr" target="#b3">[7]</ref> and 2D CNNs as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOI Detection Previous works have investigated HOI detection mainly from a third person vision point of view.</head><p>The authors of <ref type="bibr" target="#b29">[32]</ref> proposed a method to detect people performing actions able to localize the objects involved in the interactions on still images. The authors of <ref type="bibr" target="#b26">[29]</ref> proposed a human-centric approach based on a three-branch architecture (InteractNet) instantiated according to the definition of HOI in terms of a &lt;human, verb, object&gt; triplet. Some works <ref type="bibr" target="#b45">[48,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b66">69]</ref> explored HOI detection using graph convolutional neural networks after detecting humans and objects in the scene. Recent works <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b61">64]</ref> represented the relationship between both humans and objects as the intermediate point which connects the center of the human and object bounding boxes. The aforementioned works addressed the problem of HOI detection in the third person vision domain. In this work, we look at the task of HOI detection from an egocentric perspective considering the proposed MECCANO dataset.</p><p>EHOI Detection EHOI detection is understudied due to the limited availability of egocentric datasets labelled for this task. While some previous datasets such as EPIC-KITCHENS <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b11">14]</ref> and ADL <ref type="bibr" target="#b44">[47]</ref> have been labeled with bounding box annotations, these datasets have not been explicitly labeled for the EHOI detection task indicating relationships between labeled objects and actions, hence preventing the development of EHOI detection approaches. Some related studies have modeled the relations between entities for interaction recognition as object affordances <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b19">22]</ref>. Other works tackled tasks related to EHOI detection proposing hand-centric methods <ref type="bibr" target="#b1">[5,</ref><ref type="bibr" target="#b0">4,</ref><ref type="bibr" target="#b50">53]</ref>. Despite these related works have considered human-object interaction from an egocentric point of view, the EHOI detection task has not yet been defined or studied systematically in past works. With this paper we aim at providing a  definition of the task, a suitable benchmark dataset, as well as an initial evaluation of baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MECCANO Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>The MECCANO dataset has been acquired in an industrial-like scenario in which subjects built a toy model of a motorbike (see <ref type="figure" target="#fig_0">Figure 1</ref>). The motorbike is composed of 49 components with different shapes and sizes belonging to 19 classes. In our settings, the components A054 and A051 of <ref type="figure" target="#fig_0">Figure 1</ref> have been grouped under the "screw" class, whereas A053, A057 and A077 have been grouped under the "washers" class. As a result, we have 16 component classes 3 . Note that multiple instances of each class are necessary to build the model. In addition, 2 tools, a screwdriver and a wrench, are available to facilitate the assembly of the toy model. The subjects can use the instruction booklet to understand how to build the toy model following the sequential instructions.</p><p>For the data collection, the 49 components related to the considered 16 classes, the 2 tools and the instruction booklet have been placed on a table to simulate an industrial-like en-3 See supplementary material for more details. vironment. Objects of the same component class have been grouped and placed in a heap, and heaps have been placed randomly (see <ref type="figure" target="#fig_3">Figure 3</ref>). Other objects not related to the toy model were present in the scene (i.e., clutter background). We have considered two types of table: a light-colored table and a dark one. The dataset has been acquired by 20 different subjects in 2 countries (Italy and United Kingdom) between May 2019 and January 2020. Participants were from 8 different nationalities with ages between 18 and 55. <ref type="figure" target="#fig_4">Figure 4</ref> reports some statistics about participants. We asked participants to sit and build the model of the motorbike. No other particular instruction was given to the participants, who were free to use all the objects placed in the table as well as the instruction booklet. Some examples of the captured data are reported in <ref type="figure" target="#fig_3">Figure 3</ref>. Data was captured using an Intel RealSense SR300 device which has been mounted on the head of the participant with an headset. The headset was adjusted to control the point of view of the camera with respect to the different heights and postures of the participants, in order to have the hands located approximately in the middle of the scene to be acquired. Videos were recorded at a resolution of 1280x720 pixels and with a framerate of 12fps. Each video corresponds to a complete assembly of the toy model starting from the 49 pieces placed on the table. The average duration of the captured videos is 21.14min, with the longest one being 35.45min and the shortest one </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Annotation</head><p>We annotated the MECCANO dataset in two stages. In the first stage, we temporally annotated the occurrences of all human-object interactions indicating their start and end times, as well as a verb describing the interaction. In the second stage, we annotated the active objects with bounding boxes for each temporal segment.</p><p>Stage 1: Temporal Annotations We considered 12 different verbs which describe the actions performed by the participants: take, put, check, browse, plug, pull, align, screw, unscrew, tighten, loosen and fit. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, the distribution of verb classes of the labeled samples in our dataset follows a long-tail distribution, which suggests that the taxonomy captures the complexity of the considered scenario. Since a participant can perform multiple actions simultaneously, we allowed the annotated segments to overlap (see <ref type="figure">Figure 6</ref>). In particular, in the MECCANO dataset there are 1401 segments (15.82 %) which overlap with at least another segment. We consider the start time of a segment as the timestamp in which the hand touches an object, changing its state from passive to active. The only exception is for the verb check, in which case the user doesn't need to touch an object to perform an interaction.</p><p>In this case, we annotated the start time when it is obvious from the video sequence that the user is looking at the object (see <ref type="figure">Figure 6</ref>). With this procedure, we annotated 8857 video segments.</p><p>Stage 2: Active Object Bounding Box Annotations We considered 20 object classes which include the 16 classes categorizing the 49 components, the two tools (screwdriver and wrench), the instructions booklet and a partial model class. The latter object class represents assembled components of the toy model which are not yet complete (e.g., a screw and a bolt fixed on a bar which have not yet been assembled with the rest of the model 4 ). For each temporal segment, we annotated the active objects in frames sampled every 0.2 seconds. Each active object annotation consists in a (class, x, y, w, h) tuple, where class represents the class of the object and (x, y, w, h) defines a bounding box around the object. We annotated multiple objects when they were active simultaneously (see <ref type="figure">Figure 7</ref> first row). Moreover, if an active object is occluded, even just in a few frames, we annotated it with a (class, x, y) tuple, specifying the class of the object and its estimated 2D position. An example of occluded active object annotation is reported in the second row of <ref type="figure">Figure 7</ref>. With this procedure, we labeled a total of 64349 frames.</p><p>Action Annotations Starting from the temporal anno-  tations, we defined 61 action classes <ref type="bibr" target="#b1">5</ref> . Each action is composed by a verb and one or more objects, for example "align screwdriver to screw" in which the verb is align and the objects are screwdriver and screw. Depending on the verb and objects involved in the interaction, each temporal segment has been associated to one of the 61 considered action classes. <ref type="figure" target="#fig_7">Figure 8</ref> shows the list of the 61 action classes, which follow a long-tail distribution. Each EHOI annotation is hence composed of a verb annotation and the active object bounding boxes. The MECCANO dataset is the first dataset of egocentric videos explicitly annotated for the EHOI detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Benchmarks and Baseline Results</head><p>The MECCANO dataset is suitable to study a variety of tasks, considering the challenging industrial-like scenario in which it was acquired. In this paper, we consider four tasks for which we provide baseline results: 1) Action Recognition, 2) Active Object Detection, 3) Active Object Recognition and 4) Egocentric Human-Object Interaction (EHOI) <ref type="bibr" target="#b1">5</ref> See the supplementary material for details on action class selection.</p><p>Detection. While some of these tasks have been considered in previous works, none of them has been studied in industrial scenarios from the egocentric perspective. Moreover, it is worth noting that the EHOI Detection task has never been treated in previous works. We split the dataset into three subsets (Training, Validation and Test) designed to balance the different types of desks (light, dark) and countries in which the videos have been acquired (IT, U.K.). <ref type="table" target="#tab_1">Table 2</ref> reports some statistics about the three splits, such as the number of videos, the total duration (in seconds), the number of temporally annotated EHOIs and the number of bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Action Recognition</head><p>Task: Action Recognition consists in determining the action performed by the camera wearer from an egocentric video segment. Specifically, let C a = {c 1 , c 2 , ..., c n } be the set of action classes and let A i = [t si , t ei ] be a video segment, where t si and t ei are the start and the end times of the action respectively. The aim is to assign the correct action class c i ∈ C a to the segment A i . Evaluation Measures: We evaluate action recognition using Top-1 and Top-5 accuracy computed on the whole test set. As class-aware measures, we report class-mean precision, recall and F 1 -score. Baselines: We considered 2D CNNs as implemented in the PySlowFast library <ref type="bibr" target="#b18">[21]</ref> (C2D), I3D <ref type="bibr" target="#b3">[7]</ref> and SlowFast <ref type="bibr" target="#b21">[24]</ref> networks, which are state-of-the-art methods for action recognition. In particular, for all baselines we used the PySlowFast implementation based on a ResNet-50 <ref type="bibr" target="#b30">[33]</ref> backbone pre-trained on Kinetics <ref type="bibr" target="#b31">[34]</ref>. See supplementary material for implementation details. Results: <ref type="table" target="#tab_3">Table 3</ref> reports the results obtained by the baselines for the action recognition task. All baselines obtained   <ref type="table">Table 4</ref>. Baseline results for the active object detection task. similar performance in terms of Top-1 and Top-5 accuracy with SlowFast networks achieving slightly better performance. Interestingly, performance gaps are more consistent when we consider precision, recall and F 1 scores, which is particularly relevant given the long-tailed distribution of actions in the proposed dataset (see <ref type="figure" target="#fig_7">Figure 8</ref>). Note that, in our benchmark, SlowFast obtained the best results with a Top-1 accuracy of 47.82 and an F 1 -score of 41.05. See supplementary material for qualitative results. In general, the results suggest that action recognition with the MECCANO dataset is challenging and offers a new scenario to compare action recognition algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Active Object Detection</head><p>Task: The aim of the Active Object Detection task is to detect all the active objects involved in EHOIs. Let O act = {o 1 , o 2 , ..., o n } be the set of active objects in the image. The goal is to detect with a bounding box each active object o i ∈ O act . Evaluation Measures: As evaluation measure, we use Average Precision (AP), which is used in standard object detection benchmarks. We set the IoU threshold equal to 0.5 in our experiments. Baseline: We considered the Hand-Object Detector proposed in <ref type="bibr" target="#b50">[53]</ref>. The model has been designed to detect hands and objects when they are in contact. This architecture is based on Faster-RCNN <ref type="bibr" target="#b49">[52]</ref> and predicts a box around the visible human hands, as well as boxes around the objects the hands are in contact with and a link between them. We used the Hand-Object Detector <ref type="bibr" target="#b50">[53]</ref> pretrained on EPIC-Kitchens <ref type="bibr" target="#b9">[13]</ref>, EGTEA <ref type="bibr" target="#b35">[38]</ref> and CharadesEGO <ref type="bibr" target="#b51">[54]</ref> as provided by authors <ref type="bibr" target="#b50">[53]</ref>. The model has been trained to recognize hands and to detect the active objects regardless their class. Hence, it should generalize to others domains. With default parameters, the Hand-Object Detector can find at most two active objects in contact with hands. Since our dataset tends to contain more active objects in a single EHOI (up to 7), we consider two variants of this model by changing the threshold on the distance between hands and detected objects. In the first variant, the threshold is set to the average distance between hands and active objects on the MECCANO dataset. We named this variant "Avg distance". In the second variant, we removed the thresholding operation and considered all detected objects as active objects. We named this variant "All objects". We further adapted the Hand-Object Detector <ref type="bibr" target="#b50">[53]</ref> re-training the Faster-RCNN component to detect all active objects of the MECCANO dataset. See supplementary material for implementation details. Results: <ref type="table">Table 4</ref> shows the results obtained by the active object detection task baselines. The results highlight that the Hand-Object Detector <ref type="bibr" target="#b50">[53]</ref> is not able to generalize to a domain different than the one on which it was trained. All the three variants of the Hand-Object Detector using the original object detector obtained an AP approximately equal to 11% (first three rows of <ref type="table">Table 4</ref>). Re-training the object detector on the MECCANO dataset allowed to improve performance by significant margins. In particular, using the standard distance threshold value, we obtained an AP of 20.18%. If we consider the average distance as the threshold to discriminate active and passive objects, we obtain an AP of 33.33%. Removing the distance threshold (last row of <ref type="table">Table 4</ref>), allows to outperform all the previous results obtaining an AP equal to 38.14%. This suggests that adapting the general object detector to the challenging domain of the proposed dataset is key to performance. Indeed, training the object detector to detect only active objects in the scene already allows to obtain reasonable results, while there still space for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Active Object Recognition</head><p>Task: The task consists in detecting and recognizing the active objects involved in EHOIs considering the 20 object classes of the MECCANO dataset. Formally, let O act = {o 1 , o 2 , ..., o n } be the set of active objects in the image and let C o = {c 1 , c 2 , ..., c m } be the set of object classes. The task consists in detecting objects o i ∈ O act and assigning them the correct class label c ∈ C o . Evaluation Measures: We use mAP <ref type="bibr" target="#b16">[19]</ref> with threshold on IoU equal to 0.5 for the evaluations. Baseline: As a baseline, we used a standard Faster-RCNN <ref type="bibr" target="#b49">[52]</ref> object detector. For each image the object detector predicts (x, y, w, h, class) tuples which represent the object bounding boxes and the associated classes. See supplementary material for implementation details. Results: <ref type="table">Table 7</ref> reports the results obtained with the base- ). Performance suggests that the proposed dataset is challenging due to the presence of small objects. We leave the investigation of more specific approaches to active object detection to future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">EHOI Detection</head><p>Task: The goal is to determine egocentric human-object interactions (EHOI) in each image. Given the definition of EHOIs as &lt;verb, objects&gt; pairs (see <ref type="figure" target="#fig_0">Equation 1</ref>), methods should detect and recognize all the active objects in the scene, as well as the verb describing the action performed by the human. Evaluation Measures: Following <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b26">29]</ref>, we use the "role AP" as an evaluation measure. Formally, a detected EHOI is considered as a true positive if 1) the predicted object bounding box has a IoU of 0.5 or higher with respect to a ground truth annotation and 2) the predicted verb matches with the ground truth. Note that only the active object bounding box location (not the correct class) is considered in this measure. Moreover, we used different values of IoU (e.g., 0.5, 0.3 and 0.1) to compute the "role AP". Baseline: We adopted three baselines for the EHOI detec-  <ref type="table">Table 6</ref>. Baseline results for the EHOI detection task. <ref type="figure">Figure 9</ref>. Qualitative results for the EHOI detection task.</p><p>tion task. The first one is based on InteractNet <ref type="bibr" target="#b26">[29]</ref>, which is composed by three branches: 1) the "human-branch" to detect the humans in the scene, 2) the "object-branch" to detect the objects and 3) the "interaction-branch' which predicts the verb of the interaction focusing on the humans and objects appearance. The second one is an extension of In-teractNet which also uses context features derived from the whole input frame to help the "interaction-branch" in verb prediction. The last baseline is based on the combination of a SlowFast network <ref type="bibr" target="#b18">[21]</ref> trained to predict the verb of the EHOI considering the spatial and temporal dimensions, and Faster-RCNN <ref type="bibr" target="#b49">[52]</ref> which detects and recognizes all active objects in the frame. See supplementary material for implementation details. Results: <ref type="table">Table 6</ref> reports the results obtained by the baselines on the test set for the EHOI detection task. The InteractNet method obtains low performance on this task with a mAP role of 4.92%. Its extension with context features, slightly improves the performance with a mAP role of 8.45%, whereas SlowFast network with Faster-RCNN achieved best results with a mAP equal to 25.93%. The results highlight that current state-of-the-art approaches developed for the analysis of still images in third person scenarios are unable to detect EHOIs in the proposed dataset, which is likely due to the presence of multiple tiny objects involved simultaneously in the EHOI and to the actions performed. On the contrary, adding the ability to process video clips with SlowFast allows for significant performance boosts. <ref type="figure">Figure 9</ref> shows qualitative results obtained with the SlowFast+Faster-RCNN baseline. Note that in the second example the method correctly predicted all the objects involved simultaneously in the EHOI. Despite promising performance of the suggested baseline, the proposed EHOI detection task needs more investigation due to the challenging nature of the considered industrial-like domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed MECCANO, the first dataset to study egocentric human-object interactions (EHOIs) in an industriallike scenario. We publicly release the dataset with both temporal (action segments) and spatial (active object bounding boxes) annotations considering a taxonomy of 12 verbs, 20 nouns and 61 unique actions. In addition, we defined the Egocentric Human-Object Interaction (EHOI) detection task and performed baseline experiments to show the potential of the proposed dataset on four challenging tasks: action recognition, active object detection, active object recognition and EHOI detection. Future works will explore approaches for improved performance on this challenging data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>This document is intended for the convenience of the reader and reports additional information about the proposed dataset, the annotation stage, as well as implementation details related to the performed experiments. This supplementary material is related to the following submission:</p><p>• F. <ref type="bibr">Ragusa</ref> The remainder of this document is organized as follows. Section 6 reports additional details about data collection and annotation. Section 7 provides implementation details of the compared methods. Section 8 reports additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional details on the MECCANO Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Component classes and grouping</head><p>The toy motorbike used for our data collection is composed of 49 components belonging to 19 classes <ref type="figure" target="#fig_0">(Figure 1)</ref>, plus two tools. In our settings, we have grouped two types of components which are similar in their appearance and have similar roles in the assembly process. <ref type="figure" target="#fig_0">Figure 10</ref> illustrates the two groups. Specifically, we grouped A054 and A051 under the "screw" class. These two types of components only differ in their lengths. We also grouped A053, A057 and A077 under the "washers" class. Note that these components only differ in the radius of their holes and in their thickness.</p><p>As a results, we have 20 object classes in total: 16 classes are related to the 49 motorbike components, whereas the others are associated to the two tools, to the instruction booklet and to a partial model class, which indicates a set of components assembled together to form a part of the model (see <ref type="figure" target="#fig_0">Figure 11</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Data Annotation</head><p>Verb Classes and Temporal Annotations We considered 12 verb classes which describe all the observed actions performed by the participants during the acquisitions. <ref type="figure" target="#fig_0">Figure 12</ref> reports the percentage of the temporally annotated instances belonging to the 12 verb classes. The considered verb classes are: take, put, check, browse, plug, pull, align, screw, unscrew, tighten, loosen and fit. We used the ELAN Annotation tool <ref type="bibr" target="#b42">[45]</ref> to annotate a temporal segment around each instance of an action. Each segment has been associated to the verb which best described the contained action.</p><p>Active Object Bounding Box Annotations For each annotated video segment, we sampled frames every 0.2 seconds. Each of these frames has been annotated to mark the presence of all active objects with bounding boxes and related component class label. To this aim, we used VGG Image Annotator (VIA) <ref type="bibr" target="#b15">[18]</ref> with a customized project which allowed annotators to select component classes from a dedicated panel showing the thumbnails of each of the 20 object classes to facilitate and speed up the selection of the correct object class. <ref type="figure" target="#fig_0">Figure 13</ref> reports an example of the  customized VIA interface. Moreover, to support annotators and reduce ambiguities, we prepared a document containing a set of fundamental rules for the annotations of active objects, where we reported the main definitions (e.g., active object, occluded active object, partial model) along with visual examples. <ref type="figure" target="#fig_0">Figure 14</ref> reports an example of such instructions.</p><p>Action Annotation In the MECCANO dataset, an action can be seen as a combination of a verb and a set of nouns (e.g., "take wrench"). We analyzed the combinations of our 12 verb classes and 20 object classes to find a compact, yet descriptive set of actions classes. The action class selection process has been performed in two stages. In the first stage, we obtained the distributions of the number of active objects generally occurring with each of the 12 verbs. The distributions are shown in <ref type="figure" target="#fig_0">Figure 15</ref>. For example, the dataset contains 120 instances of "browse" (second rowfirst column), which systematically involves one single object. Similarly, most of the instance of "take" appear with 1 object, while few instances have 2 − 3 objects.</p><p>In the second stage, we selected a subset of actions from all combinations of verbs and nouns. <ref type="figure" target="#fig_0">Figure 16</ref> reports all the action classes obtained from the 12 verbs classes of the MECCANO dataset as discussed in the following. Let O = {o 1 , o 2 , ..., o n } and V = {v 1 , v 2 , ..., v m } be the set of the objects and verb classes respectively. For each verb v ∈ V , we considered all the object classes o ∈ O involved in one or more temporal segments labeled with verb v. We considered the following rules:</p><p>• Take and put: We observed that all the objects o ∈ O occurring with v = take are taken by participants while they build the motorbike. Hence, we first defined 20 action classes as (v, o) pairs (one for each of the available objects). Since subjects can take more than one object at a time, we added an additional "take objects" action class when two or more objects are taken simultaneously. The same behavior has been observed for the verb v = put. Hence, we similarly defined 21 action classes related to this verb.</p><p>• Check and browse: We observed that verbs v = check and v = browse always involve only the object <ref type="figure" target="#fig_0">Figure 13</ref>. Customized VIA project to support the labeling of active objects. Annotators were presented with a panel which allowed them to identify object classes through their thumbnails. <ref type="figure" target="#fig_0">Figure 14</ref>. Active object definition given to the labelers for the active object bounding box annotation stage. o = instruction booklet. Hence, we defined the two action classes check instruction booklet and browse instruction booklet.</p><p>• Fit: When the verb is v = f it, there are systematically two objects involved simultaneously (i.e., o = rim and o = tire). Hence, we defined the action class fit rim and tire.</p><p>• Loosen: We observed that participants tend to loosen bolts always with the hands. We hence defined the action class loosen bolt with hands.</p><p>• Align: We observed that participants tend to align the screwdriver tool with the screw before starting to screw, as well as the wrench tool with the bolt before tightening it. Participants also tended to align objects to be assembled to each other. From these observations, we defined three action classes related to the verb v = align: align screwdriver to screw, align wrench to bolt and align objects.</p><p>• Plug: We found three main uses of verb v = plug related to the objects o = screw, o = rod and o = handlebar. Hence, we defined three action classes: plug screw, plug rod and plug handlebar.</p><p>• Pull: Similar observations apply to verb v = pull. Hence we defined three action classes involving "pull": pull screw, pull rod and pull partial model.</p><p>• Screw and unscrew: The main object involved in actions characterized by the verbs v = screw and v = unscrew is o = screw. Additionally, the screw or unscrew action can be performed with a screwdriver or with hands. Hence, we defined four action classes screw screw with screwdriver, screw screw with hands, unscrew screw with screwdriver and unscrew screw with hands.</p><p>• Tighten: Similar observation holds for the verb v = tighten, the object o = bolt and the tool o = wrench. We hence defined the following two action classes: tighten bolt with wrench and tighten bolt with hands.</p><p>In total, we obtained 61 action classes composing the MECCANO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Baseline Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Action Recognition</head><p>The goal of action recognition is to classify each action segment into one of the 61 action classes of the MECCANO dataset. The SlowFast, C2D and I3D baselines considered in this paper all require fixed-length clips at training time. Hence, we temporally downsample or upsample uniformly each video shot before passing it to the input layer of the network. The average number of frames in a video clip in the MECCANO dataset is 26.19. For SlowFast network, we set α = 4 and β = 1 8 . We set the batch-size to 12 for C2D and I3D, we used a batch-size of 20 for SlowFast. We trained C2D, I3D and SlowFast networks on 2 NVIDIA V100 GPUs for 80, 70 and 40 epochs with learning rates of 0.01, 0.1 and 0.0001 respectively. These settings allowed all baselines to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Active Object Detection</head><p>We trained Faster-RCNN on the training and validation sets using the provided active object labels. We set the learning rate to 0.005 and trained Faster-RCNN with a ResNet-101 backbone and Feature Pyramid Network for 100K iterations on 2 NVIDIA V100 GPUs. We used the Detectron2 implementation <ref type="bibr" target="#b62">[65]</ref>. The model is trained to recognize objects along with their classes. However, for the active object detection task, we ignore output class names and only consider a single "active object" class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Active Object Recognition</head><p>We used the same model adopted for the Active Object Detection task, retaining also object classes at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">EHOI Detection</head><p>For the "SlowFast + Faster-RCNN" baseline, we trained SlowFast network to recognize the 12 verb classes of the MECCANO dataset using the same settings as the ones considered for the action recognition task. We trained the network for 40 epochs and obtained a verb recognition Top-1 accuracy of 58.04% on the Test set. For the object detector component, we used the same model trained for the active object recognition task.</p><p>For the "human-branch" of the "InteractNet" model, we used the Hand-Object Detector <ref type="bibr" target="#b50">[53]</ref> to detect hands in the scene. The object detector trained for active object recognition has been used for the "object-branch". The MLPs used to predict the verb class form the appearance of hands and active objects are composed by an input linear layer (e.g., 1024-d for the hands MLP and 784-d for the objects one), a ReLU activation function and an output linear layer (e.g., 12-d for both MLPs). We fused by late fusion the output probability distributions of verbs obtained from the two MLPs (hands and objects) to predict the final verb of the EHOI. We jointly trained the MLPs for 50K iterations on an Nvidia V100 GPU, using a batch size of 28 and a learning rate of 0.0001.</p><p>In "InteractNet + Context", we added a third MLP which predicts the verb class based on context features. The context MLP has the same architecture of the others MLPs (hands and objects) except the input linear layer which is 640-d. In this case, we jointly trained the three MLPs (hands, objects and context) for 50K iterations on a TitanX GPU with a batch size equal to 18 and the learning rate equal to 0.0001. The outputs of the three MLPs are hence fused by late fusion. <ref type="figure" target="#fig_0">Figure 17</ref> shows some qualitative results of the SlowFast baseline. Note that, in the second and third example, the method predicts correctly only the verb or the object. <ref type="table">Table 7</ref> reports the results obtained with the baseline in the Active Object Recognition task. We report the AP values for each class considering all the videos belonging to the test set of the MECCANO dataset. The last column shows the average of the AP values for each class and the last row reports the mAP values for each test video. <ref type="figure" target="#fig_0">Figure 18</ref> reports some qualitative results for this task. In particular, in the first row, we report the correct active object predictions, while in the second row we report two examples of wrong predictions. In the wrong predictions, the right active object is recognized but other passive objects are wrongly detected and recognized as active (e.g., instruction booklet in the example bottom-left or the red bars in the example bottomright of <ref type="figure" target="#fig_0">Figure 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Additional Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Toy model built by subjects interacting with 2 tools, 49 components and the instructions booklet. Better seen on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Examples of Human-Object Interactions in third person vision (first row) and first person vision (second row) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2-bottom reports some examples of Egocentric Human-Object Interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples of data acquired by the 20 different participants in two countries (Italy, United Kingdom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Statistics of the 20 participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Long-tail distribution of verbs classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Example of two overlapping temporal annotations along with the associated verbs. Example of bounding box annotations for active objects (first row) and occluded active objects (second row). being 9.23min.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Distribution of action instances in the MECCANO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>EHOI Annotations Let O = {o 1 , o 2 , ..., o n } and V = {v 1 , v 2 , ..., v m } be the sets of objects and verbs respectively. We define an Egocentric Human-Object Interaction e as:e = (v h , {o 1 , o 2 , ..., o i }) (1) where v h ∈ V isthe verb characterizing the interaction and (o 1 , o 2 , ..., o i ) ⊆ O represent the active objects involved in the interaction. Given the previous definition, we considered all the observed combinations of verbs and objects to represent EHOIs performed by the participants during the acquisition (see examples in Figure 2-bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Grouped pieces belonging to screw and washer classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Examples of objects belonging to the partial model class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Fractions of instances of each verb in the MECCANO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the three splits: Train, Validation and Test.</figDesc><table><row><cell cols="3">Split #Videos Duration (min)</cell><cell>%</cell><cell cols="4">#EHOIs Segments Bounding Boxes Country (U.K/Italy) Table (Light/Dark)</cell></row><row><cell>Train</cell><cell>11</cell><cell>236.47</cell><cell>55%</cell><cell>5057</cell><cell>37386</cell><cell>6/5</cell><cell>6/5</cell></row><row><cell>Val</cell><cell>2</cell><cell>46.57</cell><cell>10%</cell><cell>977</cell><cell>6983</cell><cell>1/1</cell><cell>1/1</cell></row><row><cell>Test</cell><cell>7</cell><cell>134.93</cell><cell>35%</cell><cell>2824</cell><cell>19980</cell><cell>4/3</cell><cell>4/3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Top-1 Accuracy Top-5 Accuracy Avg Class Precision Avg Class Recall Avg Class F 1-score</figDesc><table><row><cell>C2D [21]</cell><cell>41.92</cell><cell>71.95</cell><cell>37.6</cell><cell>38.76</cell><cell>36.49</cell></row><row><cell>I3D [7]</cell><cell>42.51</cell><cell>72.35</cell><cell>40.04</cell><cell>40.42</cell><cell>38.88</cell></row><row><cell>SlowFast [24]</cell><cell>42.85</cell><cell>72.47</cell><cell>42.11</cell><cell>41.48</cell><cell>41.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Baseline results for the action recognition task.</figDesc><table><row><cell>Method</cell><cell>AP (IoU &gt;0.5)</cell></row><row><cell>Hand Object Detector [53]</cell><cell>11.17%</cell></row><row><cell>Hand Object Detector [53] (Avg dist.)</cell><cell>11.10%</cell></row><row><cell>Hand Object Detector [53] (All dist)</cell><cell>11.34%</cell></row><row><cell>Hand Object Detector [53] + Objs re-training</cell><cell>20.18%</cell></row><row><cell>Hand Object Detector [53] + Objs re-training (Avg dist.)</cell><cell>33.33%</cell></row><row><cell>Hand Object Detector [53] + Objs re-training (All dist.)</cell><cell>38.14%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Baseline results for the active object recognition task. line in the Active Object Recognition task. We report the AP values for each class considering all the videos belonging to the test set of the MECCANO dataset. The last column shows the average of the AP values for each class and the last row reports the mAP value for the test set. The mAP was computed as the average of the mAP values obtained in each test video. AP values in the last column show that large objects are easier to recognize (e.g. instruction booklet: 46.48%; screwdriver: 60.50%; tire: 58.91%; rim: 50.35%</figDesc><table><row><cell>ID</cell><cell>Class</cell><cell>AP (per class)</cell></row><row><cell>0</cell><cell>instruction booklet</cell><cell>46.18%</cell></row><row><cell>1</cell><cell>gray angled perforated bar</cell><cell>09.79%</cell></row><row><cell>2</cell><cell>partial model</cell><cell>36.40%</cell></row><row><cell>3</cell><cell>white angled perforated bar</cell><cell>30.48%</cell></row><row><cell>4</cell><cell>wrench</cell><cell>10.77%</cell></row><row><cell>5</cell><cell>screwdriver</cell><cell>60.50%</cell></row><row><cell>6</cell><cell>gray perforated bar</cell><cell>30.83%</cell></row><row><cell>7</cell><cell>wheels axle</cell><cell>10.86%</cell></row><row><cell>8</cell><cell>red angled perforated bar</cell><cell>07.57%</cell></row><row><cell>9</cell><cell>red perforated bar</cell><cell>22.74%</cell></row><row><cell cols="2">10 rod</cell><cell>15.98%</cell></row><row><cell cols="2">11 handlebar</cell><cell>32.67%</cell></row><row><cell cols="2">12 screw</cell><cell>38.96%</cell></row><row><cell cols="2">13 tire</cell><cell>58.91%</cell></row><row><cell cols="2">14 rim</cell><cell>50.35%</cell></row><row><cell cols="2">15 washer</cell><cell>30.92%</cell></row><row><cell cols="2">16 red perforated junction bar</cell><cell>19.80%</cell></row><row><cell cols="2">17 red 4 perforated junction bar</cell><cell>40.82%</cell></row><row><cell cols="2">18 bolt</cell><cell>23.44%</cell></row><row><cell cols="2">19 roller</cell><cell>16.02%</cell></row><row><cell></cell><cell>mAP</cell><cell>30.39%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Images in the first row were taken from the COCO dataset<ref type="bibr" target="#b38">[41]</ref> while those in the second row belong to the MECCANO dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See the supplementary material for examples of partial model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported by MIUR PON PON R&amp;I 2014-2020 -Dottorati innovativi con caratterizzazione industriale, by MIUR AIM -Attrazione e Mobilita Internazionale Linea 1 -AIM1893589 -CUP: E64118002540007, and by MISE -PON I&amp;C 2014-2020 -Progetto ENIGMA -Prog n. F/190050/02/X44 -CUP: B61B19000520008.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1949" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding hand-object manipulation with grasp types and object attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep vision shield: Assessing the use of hmd and wearable sensors in a smart safety device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Casalegno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM PETRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visions for augmented cultural heritage experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="82" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Baseline results for the active object recognition task. We report the AP values for each class which are the averages of the AP values for each class of the Test videos</title>
	</analytic>
	<monogr>
		<title level="m">Table 7</title>
		<imprint/>
	</monogr>
	<note>In the last column, we report the mAP per class, which is the average mAP of the Test videos</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The epic-kitchens dataset: Collection, challenges and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teesid</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osian</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detailed human data acquisition of kitchen activities: the cmu-multimodal activity database (cmu-mmac)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Montano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Valcarcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI 2009 Workshop. Developing Shared Home Behavior Datasets to Advance HCI and Ubiquitous Computing Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The VIA annotation software for images, audio and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Demo2vec: Reasoning object affordances from online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2139" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vedi: Vision exploitation for data interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Signorello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scuderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Santo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samarotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, NIPS&apos;16</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3476" to="3484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Piotr Dollár, and Kaiming He. Detecting and recognizing human-object interactions. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memisevic. The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5843" to="5851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Visual semantic role labeling. ArXiv, abs/1505.04474</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sudheendra Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PPDM: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno>arxiv:1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper into first-person activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grounded human-object interaction hotspots from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8687" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ego-topo: Environment affordances from egocentric video. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">The Language Archive Nijmegen: Max Planck Institute for Psycholinguistics. Elan (version</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>computer software</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Organizing egocentric videos of daily living activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>D&amp;apos;amico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Addesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Torrisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1808.07962</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">EGO-CH: Dataset and fundamental tasks for visitors behavioral understanding using egocentric vision. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Signorello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Understanding human hands in contact at internet scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Actor and observer: Joint modeling of first and third-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karteek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="7396" to="7404" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Generating notifications for missing actions: Don&apos;t forget to turn the lights off! pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4669" to="4677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lsta: Long short-term attention for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action recognition in rgb-d egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3410" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1711.08496</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Figure 16. 61 action classes definition from the 12 verb classes and the analysis performed observing the participant behavior</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Qualitative results for the action recognition task</title>
	</analytic>
	<monogr>
		<title level="m">Figure 17</title>
		<imprint/>
	</monogr>
	<note>Correct predictions are in green while wrong predictions are in red. Figure 18. Qualitative results for the active object recognition task</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
