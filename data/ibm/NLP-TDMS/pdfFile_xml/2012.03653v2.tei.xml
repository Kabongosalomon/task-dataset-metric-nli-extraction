<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffPrune: Neural Network Pruning with Deterministic Approximate Binary Gates and L 0 Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-06">6 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Shulman</surname></persName>
						</author>
						<title level="a" type="main">DiffPrune: Neural Network Pruning with Deterministic Approximate Binary Gates and L 0 Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-06">6 Mar 2021</date>
						</imprint>
					</monogr>
					<note>This is a draft version, content may be revisited in subsequent versions.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern neural network architectures typically have many millions of parameters and can be pruned significantly without substantial loss in effectiveness which demonstrates they are overparameterized. The contribution of this work is two-fold. The first is a method for approximating a multivariate Bernoulli random variable by means of a deterministic and differentiable transformation of any real-valued multivariate random variable. The second is a method for model selection by element-wise multiplication of parameters with approximate binary gates that may be computed deterministically or stochastically and take on exact zero values. Sparsity is encouraged by the inclusion of a surrogate regularization to the L 0 loss. Since the method is differentiable it enables straightforward and efficient learning of model architectures by an empirical risk minimization procedure with stochastic gradient descent and theoretically enables conditional computation during training. The method also supports any arbitrary group sparsity over parameters or activations and therefore offers a framework for unstructured or flexible structured model pruning. To conclude experiments are performed to demonstrate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern neural network architectures have achieved state-of-the-art results in various learning tasks and have the capacity to approximate highly complex functions. Typically such models have many millions of parameters which are densely connected. These dense structures enable efficient parallel computing using specialized software and hardware however they also require substantial resources to store the models and execute which precludes loading such models on hardware having modest resources such as low power embedded devices. Furthermore these large architectures can be pruned significantly without substantial loss in effectiveness which demonstrates that they are highly over-parameterized <ref type="bibr" target="#b13">[14]</ref>. Due to being over-parameterized they are prone to overfitting which can lead to poor generalization <ref type="bibr" target="#b31">[32]</ref>. Model compression is an approach that aims to address these aforementioned drawbacks by reducing the number of parameters while minimizing performance loss to an acceptable degree. There are many approaches suggested for model pruning or compression including weight pruning <ref type="bibr" target="#b6">[7]</ref>, network quantization <ref type="bibr" target="#b5">[6]</ref>, architecture learning <ref type="bibr" target="#b26">[27]</ref>, distilling knowledge <ref type="bibr" target="#b7">[8]</ref>, structured pruning <ref type="bibr" target="#b29">[30]</ref> and L 0 regularization <ref type="bibr" target="#b15">[16]</ref>.</p><p>Sparse representation is related to the notion that many of the explanatory factors are irrelevant and therefore are represented by many zeros <ref type="bibr" target="#b2">[3]</ref>. Consequntly a particularly useful approach is the structured or unstructured binary gating of parameters. This approach enables parameters to take on exact zero values thus enabling conditional computational savings during training without imposing additional constraints on the parameters <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>. However many learning algorithms such as neural networks utilize gradient-based optimizers such as the back-propagation algorithm <ref type="bibr" target="#b24">[25]</ref>. Models that are designed to have a continuous relationship between parameters and the training objective enable the computation of exact gradients which in turn enable efficient optimization <ref type="bibr" target="#b2">[3]</ref>. The challenge is then combining discrete binary gates acting on the parameters by means of gate-parameter-wise multiplication with efficient gradient based optimization. Deterministic discreteness can be smoothed and approximated reasonably well with the softmax and sigmoid functions for categorical and binary states respectively. However if a distribution over discrete states is needed, there is no clear solution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, the addition of useful continuous approximations to discrete random variables can be important for the development of new models. A practical way to overcome the limitations related to including discrete states in the computation graph is by the inclusion and the optimization of stochastic nodes and taking samples of their states. A well known method is the reparameterization of stochastic nodes by deterministic functions of their parameters and a fixed noise distribution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>The main contribution of this work is two-fold. The first is a method for approximating a multivariate Bernoulli random variable by means of a deterministic and differentiable transformation of any real-valued multivariate random variable. The second is a deterministic method for model selection by element-wise multiplication of parameters with approximate binary gates that can take on exact zero values. Since the method is differentiable the gates are learned jointly with the model parameters during training by means of empirical risk minimization and theoretically offers the benefit of conditional computation during training. Note the term differential is used in this paper in the context of training neural networks, i.e. allowing a small number of points where the first order derivatives do not exist. This is typically due to the very common use of rectifiers in the calculation graph such as the ubiquitous Relu activation <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DiffPrune</head><p>Let θ be the parameters of a hypothesis h(·, θ) : X → Y such as a neural network and let D be a training set consisting of N i.i.d. instances {(x 1 , y 1 ), . . . , (x N , y N )}. The empirical risk R associated with the regularized hypothesis h(·, θ) is defined as:</p><formula xml:id="formula_0">R h (θ) = 1 N          N i=1 L h(x i ; θ), y i          + λL reg (θ) (1) θ * = arg min θ R h (θ)<label>(2)</label></formula><p>Where L : Y × Y → R ≥0 is a loss function that measures the discrepancy between the true value y i and the predicted outcomeŷ i = h(x i ; θ); L reg : R |θ| → R ≥0 is a regularization loss that measures the hypothesis complexity; λ ∈ R ≥0 is a scaling factor for the regularization loss enabling trade-off between model complexity and primary task loss; and |θ| is the cardinality of θ. The goal of the optimization learning problem is to find θ * given the hypothesis h and data D for which the empirical risk R h (θ) is minimal. The term complexity usually corresponds to a differentiable function of the model parameters such as the L 1 and L 2 regularizers that impose soft constraints on the parameters.</p><p>Consider the hypothesis h and associated empirical risk R h following re-parametrization of θ given a partition of θ to M subsets θ 1 , . . . , θ M :</p><formula xml:id="formula_1">θ =θ π ,θ = M j=1 θ j ,π = M j=1 π j , π j ∈ {0, 1} |θj| , θ j ≥ 2 ,<label>(3)</label></formula><formula xml:id="formula_2">R h (θ,π) = 1 N          N i=1 L h(x i ; ∪ M j=1 θ j π j ), y i          + M j=1 λ j L reg (θ j , π j ) (4) θ * ,π * = arg min θ,π R h (θ,π)<label>(5)</label></formula><p>Where is the element-wise Hadamard product. Let θ jk and π jk denote the k-th element of θ j and π j respectively, then π jk enables a binary "gate" like function over the corresponding weight θ jk . Unfortunately R h (θ,π) is not differentiable w.r.t.π due to its binary nature. As an alternative, the next section describes a deterministic differentiable relaxation of the binary constraints over the gates that allows for the gates to take on exact zeros and enables solving a surrogate minimization problem efficiently and deterministically using common gradient based optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Approximating a multivariate Bernoulli random variable</head><p>To benefit from conditional computation in training it is desirable for parameters to take on exact zeros. One such way to achieve this is by utilizing multiplicative gates that can take on exact zero values. Whereas gates taking on exact zero value is crucial for efficient pruning, it is not as important for gates to be exactly one when enabled since in inference the original parameters may be substituted by the multiplication result of the gate with its corresponding parameter(s). Therefore the hard constraint of gates being binary may be relaxed and replaced 3 with a soft constraint of being approximately one when enabled. Let S ∼ q(S |φ) be a real-valued multivariate random variable with parameters φ. And let u(·) be a differential function from the support of S to the range (0, 1) |s| . Equations (6) -(10) define a deterministic and differentiable transformation g(·) that transforms samples taken from S to follow an approximate Bernoulli distribution such that g(S ) = Z ∼b(Z|φ).</p><formula xml:id="formula_3">z = max u(s) − β , 0 , β ∈ (0, 1) |s| (6) z + = {z k |z k &gt; 0 , k = 1, . . . ,|z|}<label>(7)</label></formula><p>z 0 =z − z + (8)</p><formula xml:id="formula_4">z 1 = z + −z + * e −ζ + 1 , ζ ∈ R ≥0 (9) z = z 0 ∪ z 1<label>(10)</label></formula><p>Where the operator − in equation <ref type="formula">(8)</ref> is the set difference andz + is the sample mean of z + . The transformation defined by g(·) conceptually comprises z of two partitions: z 0 and z 1 , such that by definition all z 0 = 0 and assuming that z 1 &gt; 0 then E(z 1 ) = 1. The variance of z 1 is controlled by ζ and since z + k − z + l &lt; 1 it may be set as small as practically useful and can therefore approximate a binary random variable to any finite practical precision. In addition note that Z becomes exactly binary in the limit as ζ → ∞. It is clear that the distributionb(Z|φ) depends on the choice of q(S |φ), u(·) and β. It is also notable thatb(Z|φ) is strongly bimodal in the nondegenerate case even if the distribution q(S |φ) is unimodal. Finally the gradient of z w.r.t. s is non-degenerate provided that z 1 ≥ 2 i.e. there are at least two members in z + .</p><p>There are many possible choices for u(·) however in this paper the focus is on two variants. The first is the element-wise sigmoid(·) function and the second is the so f tmax(·).</p><p>Sigmoid: Let u sigmoid(·), the marginal probability of z k being approximately one is:</p><formula xml:id="formula_5">p (z k &gt; 0) = p 1 1 + e −s k &gt; β k = p s k &gt; − ln 1/β k − 1 (11) = 1 − CDF s k − ln 1/β k − 1<label>(12)</label></formula><p>Where CDF s k is the marginal cumulative density function of s k . Therefore in the case that S is independent u sigmoid(·) results in the events z k ≈ 1 being independent, i.e.</p><formula xml:id="formula_6">p (z k ≈ 1, z l ≈ 1) = p (z k ≈ 1) p (z l ≈ 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax:</head><p>Let u so f tmax(·), the conditional probability of z k being approximately one given {S l = s l | l k} is:</p><formula xml:id="formula_7">p z k &gt; 0 | {S l = s l | l k} = p          e s k / |S | l=1 e s l &gt; β k | {S l = s l | l k}         <label>(13)</label></formula><p>Rearranging the inequality in equation <ref type="formula" target="#formula_1">(13)</ref>:</p><formula xml:id="formula_8">e s k / |S | l=1 e s l &gt; β k (14) e s k &gt; β k |S | l=1 e s l<label>(15)</label></formula><formula xml:id="formula_9">1 − β k e s k &gt; β k |S | l=1,l k e s l<label>(16)</label></formula><formula xml:id="formula_10">s k &gt; ln          β k 1 − β k |S | l=1,l k e s l         <label>(17)</label></formula><p>Therefore by combining equations <ref type="formula" target="#formula_1">(13)</ref> and <ref type="formula" target="#formula_3">(17)</ref> the conditional probability of z k being approximately one given {S l = s l | l k} is:</p><formula xml:id="formula_11">p z k &gt; 0 | {S l = s l | l k} = p           s k &gt; ln          β k 1 − β k |S | l=1,l k e s l                    (18) = 1 − CDF s k           ln          β k 1 − β k |S | l=1,l k e s l                    (19)</formula><p>Note that the choice of u so f tmax(·) leads to the events z k ≈ 1 having inherent dependencies regardless of the independence of S .</p><p>The formulation in equations (6) -(10) offers a number of benefits in the context of pruning which are discussed in subsequent sections. Most importantly the empirical risk in equation <ref type="formula">(4)</ref> can now be smoothed by replacing the binary Bernoulli gatesπ with the approximately binary gatesẑ and thus enable the use of gradient based optimizers:</p><formula xml:id="formula_12">θ =θ ẑ ,θ = M j=1 θ j ,ẑ = M j=1 z j , z j ∈ {0,1} |θj| , θ j ≥ 2 , Z j ∼b(g(S j ) | φ j ) (20) R h (θ,φ) = 1 N          N i=1 L h(x i ; ∪ M j=1 θ j z j ), y i          + M j=1 λ j L reg (θ j , z j ) (21) θ * ,φ * = arg min θ,φ R h (θ,φ)<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gates</head><p>By transforming the samples of continuous distributions that allow for the reparameterization trick <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> it is possible to express the stochastic objective in <ref type="bibr" target="#b20">(21)</ref> as an expectation over a deterministic differentiable transformation f (·) of the parametersφ and a parameter free noise distribution p( ) and perform Monte Carlo approximation to the intractable expectation over the 5 noise distribution. Whereas injecting noise to the gradients estimation can be useful such as in the case of dropout <ref type="bibr" target="#b28">[29]</ref>, it may also have the undesirable effect of increasing the variance of the estimator <ref type="bibr" target="#b8">[9]</ref>. Furthermore estimation by means of sampling to produce expectations has the drawback of increasing the computational requirements. Consequently it is proposed to choose q(S |φ) such that no sampling is required and the objective (21) becomes deterministic. Let S j follow an isotropic normal distribution N(µ j , σ 2 I), it is thus possible to perform a deterministic and differentiable maximum likelihood estimation of the gates values simply by computing z j = g(φ j ) = g(µ j ). Note that µ j are nuisance parameters that are only required during training and may be pruned with the corresponding model parameter(s). It is perhaps useful to provide an intuitive interpretation of g(·) in the context of computing gates over the model parameters. One way of many to select which gates are enabled is to rely on the notion of relative "utility" of the parameters in θ j and to rank them based on the probability that a parameter θ jk is the most "useful" or "important" in the set. In the case where u(·) so f tmax(·) the random variable S j is then interpreted as the logits of a categorical random variable denoting which of the elements of θ j has the "most utility". In the case where u(·) sigmoid(·) the random variable S j is interpreted as the logits of a multivariate Bernoulli random variable denoting the importance probability for each of the weights independently. In this context, for simplicity, β j can be reduced to a scaler and act as a hyperparameter specifying the probability selection threshold.</p><p>Given the choice of S ∼ N(µ, σ 2 I) and due to using deterministic maximum likelihood sampling it follows that:</p><formula xml:id="formula_13">p (z k &gt; 0) = 1 − CDF N(µ k ,σ 2 ) − ln 1/β − 1<label>(23)</label></formula><p>and:</p><formula xml:id="formula_14">p (z k &gt; 0) = 1 − CDF N(µ k ,σ 2 )             ln            β 1 − β |µ| l=1,l k e µ l                       <label>(24)</label></formula><p>for when u(·) sigmoid(·) and u(·) so f tmax(·) respectively. Note the edge case resulting in degenerate gradients when there is at most one active gate in a partition i.e. z 1 ≤ 1. However in practice the need to prune all parameters but one is unlikely and therefore this limitation is not expected to cause issues in application of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Regularization</head><p>If no regularization is performed, i.e. only the the first term in equation <ref type="formula" target="#formula_0">(21)</ref> is present, the gates will converge to some minima in respect to the primary objective with no additional restrictions. This is useful for configuration free model selection that is likely to retain the cumbersome model's accuracy. However in many cases it is desirable to encourage the model to achieve a higher sparsity with the trade-off of lower accuracy by the introduction of regularization loss. Note that for simplifying the notation in the remainder of this section it is assumed that the parameters are not partitioned and therefore the partition subscript is omitted. L 0 regularizer: The L 0 loss is simply the number of parameters of the model that are different from zero. This regularisation loss has desirable attributes in the context of pruning since it "punishes" the model for having non-zero parameters with no further restrictions and does not induce shrinkage. Assuming that p(w k = 0) = 0 and due to E(z 1 ) = 1 it is possible to compute the L 0 loss simply as L 0 (z) = z. Unfortunately this will yield zero gradients w.r.t. µ, or w.r.t. each µ j in the case where the parameters are partitioned. Instead the approach suggested in <ref type="bibr" target="#b15">[16]</ref> is adopted and rather than directly computing the number of enabled gates the expected number of enabled gates, whether sampling or not, is used as a surrogate loss. The expected L 0 loss over the parameters is obtained by summing the marginal probability or marginal conditional probability in the case of u(·) so f tmax(·) of every gate being enabled:</p><formula xml:id="formula_15">L reg (z) = |Z| k=1 p (z k &gt; 0)<label>(25)</label></formula><p>Or in the case where the parameters of the model are partitioned:</p><formula xml:id="formula_16">L reg (z) = M j=1 |Zj| k=1 p z jk &gt; 0<label>(26)</label></formula><p>Gaussian dropout: Experimental results discussed in subsequent sections demonstrate that applying Gaussian dropout <ref type="bibr" target="#b28">[29]</ref> to µ has a substantial effect on the results, both the compression rate and the overall accuracy of the model. From a practical perspective injecting noise has the side effect of mild stochastic binary dropout due to gates being randomly enabled and disabled. From a theoretical point of view the product of the two random variables, N(µ, σ 2 I) and N(1, p/(1 − p)) does not follow a multivariate normal distribution however since in practice the ML sampling yields a constant µ the resulting noise follows an uncorrelated multivariate normal distribution where the standard deviation of each of the components is proportional to the absolute value of its mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Group sparsity</head><p>In many applications it is desired to enforce group pruning of parameters since this can enable more substantial computational savings in practice with existing hardware and software libraries <ref type="bibr" target="#b29">[30]</ref>. A common scenario is the pruning of the activations computed by a layer, e.g. neuron pruning. Other applications require jointly pruning groups of activations such as when performing filter pruning of a CNN <ref type="bibr" target="#b12">[13]</ref>. DiffPrune can be used to this extent by virtue of associating a gate in z with a group of parameters or activations. Note the cardinality of nuisance parameters µ is equal to the number of gates which is typically substantially less than required for unstructured parameter pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Initialization and configuration</head><p>Partitioning and sparsity: The proposed method enables arbitrary pruning schemes in respect to any combination of parameters, groups of parameters, activations and groups of activations. Furthermore by including the regularization loss in the objective different target compression rates for each partition can be specified by configuration of the parameters λ. Larger values of λ j will accelerate pruning by balancing the magnitude of the L 0 regularization terms against the model's primary loss. Both the partition scheme and the values of the hyperparameters are 7 dependent on the objectives of the practitioner, the data and the architecture.</p><p>Initializing µ j : Let µ (0) j denote the initial value of µ j . µ (0) j can be considered as defining a priori probability distribution on the "utility" of weights in θ j and for certain values can result in pruning of parameters pre-training. Typically through application of the principle of indifference a uniform distribution is the default choice in cases such as this unless there is a particular reason to do otherwise. However for effective backpropagation it is required that the parameters µ j are initialized randomly and have both positive and negative values. Therefore it is recommended for µ (0) j to have a mean of zero and small variance so that it is close to being uniform and works well in the context of the SGD optimization framework. In experiments done as part of this work a truncated normal distribution with a standard deviation of 0.05 is used.</p><p>Selecting β j : Effectively acting as a pruning selection threshold where parameters that have a utility probability not greater than β j are pruned. Let p (0) j = u(µ (0) j ). Given no a priori preference as to which parameters are to be pruned it is desirable to select β j such that p jk − β j &gt; 0 , ∀p jk ∈ p (0) j as this leads to no parameters being pruned pre-training. Therefore β j can be selected heuristically during the initialization stage without the need to configure it by the practitioner. In theory it can be said that lower values of β j provide the model the opportunity to train for longer before pruning commences. In experiments done in this work values of β j = 0.99 min(p (0) j ) are used.</p><p>Learning ζ j : Since z + jk − z + jl &lt; 1 it is possible in theory to reduce the variance of the enabled gates z 1 j to an arbitrary value by increasing ζ j . In this work however ζ j is learned together with the other parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout rate:</head><p>The Gaussian dropout rate is reparameterized as σ(η j )/(1 − σ(η j )) where η j ∈ R can be learned jointly with the other parameters of the model and σ(·) is the sigmoid function. To enable backpropagation for training η j through the sampling stage gradient estimates are obtained by a single sample Monte Carlo and the reparameterization trick <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>Methods for approximating binary discrete random variables that can represent exact zero by rectifications of continuous random variables have been suggested before in various contexts. These include rectifying samples taken from the logistic distribution by a Relu for conditional computation <ref type="bibr" target="#b2">[3]</ref> however the resulting distribution is not strongly bi-modal and therefore cannot represent effectively binary random variables. The hard concrete distribution proposed in <ref type="bibr" target="#b15">[16]</ref> can emulate binary variables more closely by hard clipping a stretched concrete random variable <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref>. This approach can represent both exact zero and one however has some drawbacks, the gradients suffer from high-variance estimates; it is limited only to its own Concrete distribution and does not generalize to other multivariate distributions <ref type="bibr" target="#b8">[9]</ref>. Furthermore it is continuous in [0, 1] and the hard rectifiers on both extremes substantially increase the sparsity of gradients during backpropagation. Expanding on this framework is the work of <ref type="bibr" target="#b8">[9]</ref> where any mixture pair of distributions converging to δ(0) and δ(1) can be used to construct smoothed binary gates. By contrast the method proposed here can transform any underlying distribution including unimodal distributions into practically discrete binary random variables and maintain the tractability of continuous optimization. Furthermore due to the use of the so f tmax(·) function that creates 8 dependency between the components of the emulated binary random variable even components that are zero are affected by gradient updates and do not suffer to the same extent from gradient "dead zone". A different approach is proposed in <ref type="bibr" target="#b23">[24]</ref> for gradient based optimization of discrete random variables by smoothing transformations of binary latent variables with continuous noise in a way that allows for gradients reparameterization. Many methods have been proposed for compression and sparsification of neural networks during the last three decades. Due to the large volume of work it is not possible to mention the absolute majority of methods and instead the reader is referred to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> for a comprehensive review. Most methods proposed in the literature follow a three stage procedure where (i) an overparameterized large network is trained to partial or full convergence; (ii) following the training redundant weights or neurons are pruned based on heuristics; (iii) the network is trained again possibly from scratch to "fine-tune" the model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. This procedure may repeat a number of times to iteratively increase the model compression. One major shortcoming of many of the methods that follow this three stage procedure is they require training the over specified network to convergence which precludes conditional computational savings during training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Similar to the method here in the context of network pruning is the L 0 regularization for neural networks proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref> that prunes the network during training by the introduction of smooth Bernoulli stochastic gates. These frameworks however resort to Monte Carlo sampling to estimate the posterior probability distribution over the parameters. Other related methods are proposed in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26]</ref>, however the main difference and drawback of these methods is that they are non-differential and use the high variance straight-through estimator <ref type="bibr" target="#b2">[3]</ref> to compute the gradients of the gate distribution parameters. A rather differnet method that performs online pruning while training is proposed in <ref type="bibr" target="#b32">[33]</ref> where magnitude based heuristics determine which weights to prune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>It is generaly not feasible to perform exact comparisons between published methods due to the discrepancies related to differing frameworks, partial information, pre-processing pipelines and stochastic aspects <ref type="bibr" target="#b3">[4]</ref>. However to demonstrate the effectiveness of the proposed method an attempt is made to compare it with the results published in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref> using similar architectures, datasets and preprocessing. The models are implemented in TensorFlow <ref type="bibr" target="#b1">[2]</ref> using standard Dense and Conv2D layers with additional custom DiffPrune layers added between the standard layers to perform neuron and group pruning on the activations of the standard layers. Conditional computation is not performed in practice however an indicative number of FLOPs per training step is recorded and used to demonstrate potential theoretical benefits. All image datasets were taken from TensorFlow Datasets <ref type="bibr" target="#b0">[1]</ref> with the default train/test split. The source code is publicly available at https://bitbucket.org/YanivShu/diffprune public. Note there was no attempt to perform an exhaustive search of hyperparameters for the best possible result therefore these results should be taken as indicative only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">LeNet5 MNIST classification</head><p>The first experiment is the toy classification task of MNIST using the basic CNN LeNet5 <ref type="bibr" target="#b11">[12]</ref> with an original architecture of 20-50-800-500. The results for two variants of DiffPrune are reported in table 1. The first has no regularization loss and η is trained with η (0) = 0 for all partitions with u so f tmax(·) and thus demonstrates the capabilities of the method to perform  <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref> and DiffPrune on the MNIST classification task using LeNet5. The DiffPrune reg variant has a regularization loss term incorporated to the objective.</p><p>configuration free "no fuss" model selection during training. The second has a regularization loss term added to the objective as per equation <ref type="formula" target="#formula_0">(26)</ref> with u sigmoid(·), λ = {10 −5 , 10 −5 , 2 * 10 −5 , 2 * 10 −5 } and η was trained with η (0) ≈ −1.734 for all partitions. Training was performed for 200 epochs with testing following the completion of each epoch, of which a representative result is reported. The batch size was 100 and optimization is performed using Adam with default parameters and a learning rate of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Wide Residual Network CIFAR10 classification</head><p>The second experiment is the classification task of the CIFAR10 dataset using a pruned Wide Residual Network <ref type="bibr" target="#b30">[31]</ref>. The baseline architecture is the WRN-28-10 with no dropout and an attempt is made to maintain as closely as possible the training procedue and settings described in <ref type="bibr" target="#b30">[31]</ref> with some minor differences to the data augmentation pipeline and a smaller batch size of 120. Filter pruning is applied in all groups excluding group conv1, and in all blocks on the hidden Conv2D layer as well as jointly on the outputs of the second Conv2D layer and the skip connection convolution layer. Applying pruning jointly to both convolution layers at the same activation plane results in the same activation dimensions and enables conditional computation benefits. In practice this is implemented by adding a DiffPrune layer to prune the addition results of the convolution layers activations. The DiffPrune layers are all configured with u sigmoid(·) and to enable a fair comparison no Gaussian or other dropout was applied.</p><p>Two results are reported varying in their regularization settings. The first result demonstrates pruning with no accuracy loss and the second demonstrates more aggressive regularization with little loss of accuracy. These two variants are referred to as "low compression" and "high compression" respectively.</p><p>For the low compression variant the error rate achieved on the test set was the same as reported in <ref type="bibr" target="#b30">[31]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a novel distribution that approximates efficiently and effectively the multivariate Bernoulli distribution by employing a deterministic transformation to random variables with no restrictions on the underlying distribution. By choosing a continuous underlying distribution that allows for efficient ML estimation or the reparameterization trick it better emulates the binary nature of Bernoulli distributions while enabling the use of efficient gradient based optimization in deterministic or stochastic computation graphs respectively. Furthermore a deterministic framework for model selection with approximate binary gates and exact gradients was proposed which can prune models with or without regularization whilst maintaining the error rate of the original cumbersome model. Furthermore it is demonstrated that this method allows for deterministic optimization of the L 0 norm of parametric models by smoothing the combinatorial problem with the proposed approximate Bernoulli distribution. Experiments demonstrate that the proposed method can competitively sparsify neural networks in comparison to current approaches while theoretically allowing for conditional computation in training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>of ≈ 0.42%. The DiffPrune layers in all blocks are configured with λ = {10 −5 , 10 −5 , 5 * 10 −5 } for each of the groups conv2, conv3 and conv4 respectively. The pruned architecture learned is: 16-[(133,122)-(148,105)-(137,101)-(106,58)]-[(301,255)-(316,248)-(310,240)-(318,214)]-[(618,543)-(624,578)-(620,600)-(615,635)]. The high compression variant achieves an error rate of ≈ 4.5%. The DiffPrune layers in all blocks are configured with λ = {2 * 10 −5 , 7 * 10 −5 , 3 * 10 −4 } for each of the groups conv2, conv3 and conv4 respectively. The pruned architecture learned during training has substantially less enabled filters in deeper layers of the network: 16-[(127,120)-(137,112)-(139,102)-(113,61)]-[(286,230)-(296,235)-(289,238)-(304,231)]-[(148,180)-(162,165)-(167,143)-(153,169)]. (a) Accuracy measured on the test set for the CIFAR10 classification task with the original and pruned WRN-28-10 wide residual network. (b) The number of estimated FLOPs for the forward pass of a mini-batch .</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow Datasets, a collection of ready-to-use datasets</title>
		<ptr target="https://www.tensorflow.org/datasets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What is the state of neural network pruning? In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems 2020</title>
		<editor>I. S. Dhillon, D. S. Papailiopoulos, and V. Sze</editor>
		<meeting>Machine Learning and Systems 2020<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through mixture-distributed regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian compression for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3288" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through L 0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5068" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno>arxiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2017-08" />
			<publisher>International Convention Centre</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>J. Fürnkranz and T. Joachims</editor>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured bayesian pruning via log-normal multiplicative noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6775" to="6784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Rolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Representations by Back-propagating Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Generalized dropout</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning neural network architectures using backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<editor>R. C. Wilson, E. R. Hancock, and W. A. P. Smith</editor>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>E. R. H. Richard C. Wilson and W. A. P. Smith, editors</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
